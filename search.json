[{"title":"2018年终","url":"/2019/01/01/2018%E5%B9%B4%E7%BB%88/","content":"现在是2019年1月1日凌晨1点27分，对自己的2018年做一些总结。\n刚刚翻看了2017年的年终总结，目前来看完成了一部分目标。先简单罗列一下2017年末制定的2018年的计划：\n\n薪资增加30%（完成）\n给父母换一部手机（完成）\n一次旅行（未完成）\n开发第二技能（进行中）\n学习英语（半弃疗）\n啃完《Hadoop权威指南》（完成）\n了解营销知识（未完成）\n\n2018复盘这样看起来2018年还是做了一些事情的。不过有一件非常重要的事情没有做是非常不应该的，我竟然没有2018年的年中总结。wtf，以后一定要坚持每半年做一次复盘。\n啃完《Hadoop权威指南》从完成的事情来看，2018年完成的第一个计划就是啃完了《Hadoop权威指南》。这个当时也是在前公司一边准备面试一边去啃的，当时的目标是对对整个体系有个大概的了解，并没有深入学习的计划。所以这个目标算是勉强完成吧。后面有时间还是要补充一下这部分知识的。\n薪资增加30%薪资增加30%应该是18年少数能让我比较振奋的事情了。这个提薪幅度是因为今年完成了跳槽，现在看来还好跳的比较早，不然遇到年底的这些事情我可能还真没有信心去跳。当然这也要感谢我的女朋友每天对我换工作进度的监督，以及在我准备面试时候提供的帮助（猝不及防的狗粮）。\n其实每次换工作都有着类似的经历，从最开始克服自己的懒惰，逼迫自己走出舒适区（这个真的很不容易）。接着开始准备简历，在准备简历的时候，如果你有定期的复盘，那么就很简单了，把复盘的内容拿出来，就能直接用来丰富你的简历。接下来就是投简历了，由于我是在职，不可能像校招时那样海投，刚开始我先选择了一些小公司，毕竟有将近两年的时间没有经历过面试了，对于性格内向的我来说，面试首先要克服的就是紧张心理。所以就先拿一些小公司来练练手，不过刚开始也不会面很多，毕竟原公司的工作还是要做的。而且是能电话面试尽量电话面试，电话里谈好了，再约时间去公司面试，投递简历的前两周，我抽了一天的时间，去面了2家公司。也算是找到一些状态，然后决定开始认真的投递简历了。电话聊了一些，发现市面上大部分公司对Spring的掌握还是蛮高的，这就非常被动了，毕竟我在原公司根本就不接触Spring，所以我也只能一边面试一边总结。除了Spring之外，就是一些JVM原理啊，HashMap的底层实现啊之类的，总之大多是一些偏原理的东西，可能是我简历中的项目经历和大部分互联网公司所要求的不太相符吧，所以他们问的项目相关的东西挺少的。最终也是成功跳槽，新公司的氛围不错，大家做事都是有工匠精神的，感觉是真的在做技术，而不仅仅是“干活”。特别是当时面试我的leader，面试的时候简直就被他给折服了，那就是大牛啊，每天看着大牛比你还努力，你有什么资格不努力？不过后来leader的离职真的让我挺郁闷的。好在现在自己也养成了一些习惯，对于技术的学习不再浮于表面，而是希望有更深入的钻研。\n给父母换一部手机父母的手机是在我大四实习的时候给他们买的，在那之前，我妈妈还用着非智能机呢，能给她换一部智能机也算是让我有点小骄傲的事情了，虽然当时每月只有3k的实习工资，但是还是省下了一些钱给父母都换了手机，17年年末也盘算着该给父母再换一个手机了，现在薪资比当时高了好多倍，手机自然也要换好的。他们还是那样，嘴上嫌贵，用起来“真香”。这件事算是18年完成的最得意的一件事了。\n一次旅行今年竟然没有一次旅行，现在想想有点接受不了，不过也是由于跳槽导致请假的节奏有些乱，应该算是有得有失吧，没什么好说的，19年一定要有。\n开发第二技能关于第二技能，还是决定从写作开始，今年开始做了公号，不过有点三天打鱼两天晒网的意思，一方面是写了没多久就感觉自己被掏空了，于是开始强迫自己输入，另一方面也是刚到新公司，工作比较忙，所以基本就只能保证每周一篇的技术文章，除了写作，在理财方面也更进一步的学习，并且已经开始实践了。这个当时并没有做定量的目标，所以只能算是进行中，不过万事开头难，我已经迈出第一步了。\n学习英语这个……真的是半弃疗状态了，为什么是半弃疗呢，因为那些背单词，练听力的学习方法已经放弃了，实在是没时间了。现在提高英语能力的途径就只剩下看外国人的blog了。\n了解营销知识说实话，我忘了有这个计划了。不过18年虽然没有学到营销知识，但是学到了不少产品知识，这让我在怼产品的路上越走越远了，19年也会继续学习产品相关的知识。希望我们的产品经理不想砍我。\n2018年算是有得有失吧，不过现在看来还是做得不够好，中间也有一段时间放松了对自己的要求，希望2019年的自己比2018年的自己更好。所以，计划还是要有的，也会尽量都去实现。\n2019计划学习英语随着工作的深入，越来越感受到英语的重要性，作为一名程序员，想走在别人前面，必然要学好英语。不过我还是按照目前这个思路来吧，通过英文的文章来提高阅读能力。再就是跟着女朋友蹭一下网课。仅此而已吧，不能放弃自己。\n旅行香港迪士尼\nLeetCode 100题平均3天一道题，简单的可能一晚上能做3道，所以这个目标应该不难实现，贵在坚持。\n写作技术文章想要日更还是比较困难的，所以最少保证每周一篇干货吧，其他时间就写一点别的东西，目标是做到日更。\n阅读坚持阅读，2018年坚持的不错，利用通勤时间，看了很多有意义的文章，后面要在阅读的同时更加深入的思考。\n健身很有必要的事情，就是时间成本实在是太特么高了，2018年又一次放弃了，希望2019年可以坚持。\n收入今年不出意外是不会再次跳槽了，所以薪资的涨幅应该也不会有那么大了，不过今年的目标仍然是将收入提高30%。\n给父母每年的计划里都有给父母买东西这一项，今年计划买一些提升幸福感的东西，比如扫地机器人、洗碗机等。这项预计春节就能完成。\n给女朋友包治百病吧\n我一直向往自由却从未自由，一直在心里提醒自己“延迟满足”。2019，干就完了！\n","tags":["瞎扯"]},{"title":"2019年终","url":"/2020/01/01/2019%E5%B9%B4%E7%BB%88/","content":"2019年已经彻底过去了，按照惯例，我还是要对自己做一个总结，以及立下2020年的flag。\n&lt;/2019>先来回顾一下2019年吧，我昨天特意去翻了一下自己立下的flag。发现实现大概只实现了1.5个，这一年的遗憾有不少，不过也有一些很不错的收获。\n关于工作今年的环境大家应该都感受到了，本来我是想稳定一些的，不过在上一家公司遇到了一些问题，所以还是决定折腾一番。面试过程并不算顺利，认识到了自己的不足，特别是在算法方面，本来计划今年要刷LeetCode的，但是没有坚持住，面试过程中临时抱佛脚的作用也不是很大。不过好在今年把Redis的技能点加了不少，这也成为了我在面试过程中的加分项。\n最后在前leader的推荐下，来到了现在的公司，虽然来的时间不长，发现公司的问题也是不少，这让我又认清了一点现实，没有完全符合自己心意的公司，除非是自己开的。每个公司或多或少都会有些问题，能忍受就继续干，受不了就离开。如果你想在公司有好的发展，就尝试去解决这些问题。\n新工作就有新的同事，怎么说呢，总体还是不错的，也有些是比较难接触的。\n关于学习虽然没有坚持刷LeetCode，但学习却也没有落下，今年上半年以Redis为主，学习的还算比较深入。也算是对自己比较满意的一点，下半年开始学习Rust，也在看Elasticsearch，不过精力有限，同时学的压力还是比较大的。除了这两个，今年在极客时间上也花了比较多的时间，主要是架构方面的知识。目前在进行的还有设计模式和算法。\n今年的面试也是对之前学习成果的一个检验，也更加坚定了自己的信念：学习不会马上见成效，当你用到学过的知识时，你会感谢曾经的自己。\n本来今年还计划学习英语的，后来基本融入到平时了，现在每周翻译的文章也算是对英语的学习吧。\n关于阅读和写作阅读和写作一起说吧，今年阅读还是比较多的，周末不忙的话基本都会抽出时间来读一些书。并不限于技术书籍，其他方面的一些书读起来也是很有趣的。今年印象比较深刻的有《1984》、《乌合之众》和《启示录》，推荐大家阅读。今年读书计划算是圆满完成，因为把去年买的书都看完了。\n另外，在旅行的路上读书真的是个很好的选择，因为在路上的时间是很少有的一大块时间可以拿来阅读的。这里说的读书特指纸质书籍，碎片时间的话，更多的利用知识星球和微信读书来满足阅读需求把。\n至于写作，算上原创和翻译，2019年一共输出了59篇文章，平均一周一篇多一点，之前天真的我还希望做到日更，现实狠狠的给了我一耳光，告诉我技术文真的做不到日更啊。不过坚持也是有些收获的，在这里感谢所有给我赞赏的朋友，感谢你们的认可。\n关于生活本来计划今年去香港旅行，后来因为一些大家都懂的原因就耽搁了。不过今年也走了一些地方，离职那段时间去了苏杭两个城市，体会了在北京从没体会过的热。国庆时又去了重庆和四川，少不入川不是随便说说的，真心喜欢成都的生活，实在是太好吃了。还抽时间回了一趟母校，回顾了一下海鲜+啤酒的痛风套餐。\n今年认识了一些新朋友，都是很努力的，希望能跟着这些朋友一起成长。\n&lt;2020>过去的一年基本就是这样已经过去了，遗憾和收获都有，新年的目标还是要有的，万一实现了呢。\n关于工作就希望工作能顺利一些吧，自己也会尽力做好自己应该做的。\n关于学习今年的目标首先是搞定Rust，然后是Elasticsearch，希望能在社区有些贡献。另外在微服务治理方面也会花些经历，毕竟是工作中要用到的。\n关于阅读和写作阅读就没什么好说的，书都买好了，今年还是要抽时间读完。自己花钱买的书，自己也要花时间读啊。\n写作就继续坚持现在的节奏，每周一篇原创和一篇翻译，后面也会考虑把自己平时的笔记放出来。另外还欠一篇IDEA插件推荐，争取下周整理出来。\n关于生活今年有一件大事。\n感谢2019年给过我帮助的所有人。新的一年祝你们一切顺利。\n感谢各位读者大大，谢谢你们的支持和鼓励。我会努力做得更好。\n愿历尽千帆，归来仍少年。\n最后，湖人总冠军。\n","tags":["瞎扯"]},{"title":"Elasticsearch从入门到放弃：人生若只如初见","url":"/2019/09/19/Elasticsearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83%EF%BC%9A%E4%BA%BA%E7%94%9F%E8%8B%A5%E5%8F%AA%E5%A6%82%E5%88%9D%E8%A7%81/","content":"Elasticsearch系列开坑啦，入门总是愉快的，学一学再放弃啊。\nApache Lucene简介Lucene基本概念Apache Lucene是ElasticSearch使用的全文检索库。了解Lucene之前，需要先了解一些概念：\n\n文档：索引和搜索到主要数据载体，它包含一个或多个字段，存放将要写入索引或从索引搜索出来的数据\n字段：文档的一个片段，是一个K-V结构\n词项：搜索时的一个单位，代表文本中的某个词\n词条：词项在字段中的一次出现，包括词项的文本、开始和结束的位移以及类型\n倒排索引：倒排索引可以快速获取包含某个单词的文档。倒排索引由两部分组成：单词词典和倒排文件\n单词词典：单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向「倒排列表」的指针\n倒排列表：倒排列表记载了出现过某个单词的所有文档的列表以及该单词在文档中的位置，每条记录称为一个倒排项（Posting）\n倒排文件：所有单词的倒排列表往往顺序存在磁盘的某个文件，这个文件称为倒排文件\n\n\n其中最重要的是倒排索引，为了方便理解，我们看一个简单的例子。\n假设这里有三句话：\nT[0] = “it is what it is”\nT[1] = “what is it”\nT[2] = “it is a banana”\n倒排索引通常有两种表现形式：\n\ninverted file index{词项，词项所在文档ID}\n\n“a” : {2}\n“banana” : {2}\n“is” : {0, 1, 2}\n“it” : {0, 1, 2}\n“what” : {0,1}\n\nfull inverted index{词项，(词项所在文档ID，在具体文档中的位置)}\n\n“a” : {(2, 2)}\n“banana” : {(2, 3)}\n“is” : {(0, 1), (0, 4), (1, 1), (2, 1)}\n“it” : {(0, 0), (0, 3), (1, 2), (2, 0)}\n“what” : {(0, 2), (1, 0)}\nLucene查询语言在了解了Lucene的一些基本概念之后，还需要了解Lucene的查询语言。一个查询通常被分割为词项和操作符，词项可以是单个词或短语。操作符包括：\n\nAND：文档同时包含AND两边的词项时才返回\nOR：文档包含OR两边的词项中任意一个时就返回\nNOT：不包含NOT操作符后面的词项\n+：只有包含+操作符后面词项的文档才会返回。例如，查询+lucene apache表示必须包含lucene，apache可包含可不包含\n-：匹配的文档不能出现-操作符后的词项\n冒号：查询title:elasticsearch表示要查询所有在title字段中包含词项elastisearch的文档\n通配符（?/*）：?匹配任意一个字符，*匹配任意多个字符（出于性能考虑，通配符不能作为词项的第一个字符）\n~：用于Lucene中的模糊查询，~后面跟的整数值确定了近似词项与原始词项的最大编辑距离。例如查询boy~2，那么boy和boys这两个词项都能匹配，用于短语时，则表示词项之间可以接受的最大距离\n^：用于对词项进行加权\n花括号：表示范围查询\n\n对于一些特殊字符的查询，我们通常使用反斜杠进行转义。\nElasticsearch基本概念了解了Lucene的基本概念以后，我们回到正题，再来看一下Elasticsearch的一些基本概念，可能和Lucene有一些重复，不过还是有一些Elasticsearch特有的属性。\n\n索引（index）：数据存储在索引中，可以向索引写入文档或者从索引读取文档，Elasticsearch的索引可能由一个或多个Lucene索引构成。\n文档（document）：文档由字段构成，每个字段有它的字段名以及一个或多个字段值\n映射（mapping）：用于存储元信息，这些元信息决定了如何将输入文本分割为词条，哪些词条应该被过滤掉等\n类型（type）：每个文档都有与之对应的类型，同一类型下的文档数据结构通常保持一致，不同文档可以有不同的映射。但是在Elasticsearch7以后已经删除了这个概念\n节点（node）：集群中每个ES实例都称作一个节点\n集群（cluster）：在生产环境中，我们的数据量和查询压力可能超过了单机负载，因此需要多个节点协同处理\n分片（shard）：ES会将数据散落到多个Lucene索引上。这些Lucene索引称为分片。ES会自动进行分片处理\n副本（replica）：ES会为每个分片创建冗余的副本，一方面分摊请求压力，另一方面是为了保证数据不会丢失。ES支持在任意时间点添加或移除副本\n\nElasticsearch的启动过程当Elasticsearch启动时，它使用广播技术来发现同一集群内的其他节点，集群中会有一个节点被选为master节点。master节点负责管理集群状态，并在集群中节点数量变化时做出反应。但从用户角度来看，master节点与其他节点没有什么区别，命令可以发送的任意节点执行。\nmaster节点会检查所有分片，决定哪些分片为主分片。主分片确定以后，集群状态为黄色，此时可以接收查询。然后master节点会决定是否要对各个分片创建副本，副本也没有问题以后，集群状态变为绿色。\nElasticsearch的集群状态分为3种：\n\n绿色：一切完好\n黄色：所有数据都可用，但有些分片没有分配副本\n红色：有些数据不可用\n\n关于Elasticsearch的启动过程，后面还会有更加深入的讨论。\n敬请期待。\n","tags":["Elasticsearch笔记"]},{"title":"Elasticsearch从入门到放弃：文档CRUD要牢记","url":"/2019/11/24/Elasticsearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83%EF%BC%9A%E6%96%87%E6%A1%A3CRUD%E8%A6%81%E7%89%A2%E8%AE%B0/","content":"在Elasticsearch中，文档（document）是所有可搜索数据的最小单位。它被序列化成JSON存储在Elasticsearch中。每个文档都会有一个唯一ID，这个ID你可以自己指定或者交给Elasticsearch自动生成。\n如果延续我们之前不恰当的对比RDMS的话，我认为文档可以类比成关系型数据库中的表。\n元数据前面我们提到，每个文档都有一个唯一ID来标识，获取文档时，“_id”字段记录的就是文档的唯一ID，它是元数据之一。当然，文档还有一些其他的元数据，下面我们来一一介绍\n\n_index：文档所属的索引名\n_type：文档所属的type\n_id：文档的唯一ID\n\n有了这三个，我们就可以唯一确定一个document了，当然，7.0版本以后我们已经不需要_type了。接下来我们再来看看其他的一些元数据\n\n_source：文档的原始JSON数据\n_field_names：该字段用于索引文档中值不为null的字段名，主要用于exists请求查找指定字段是否为空\n_ignore：这个字段用于索引和存储文档中每个由于异常（开启了ignore_malformed）而被忽略的字段的名称\n_meta：该字段用于存储一些自定义的元数据信息\n_routing：用来指定数据落在哪个分片上，默认值是Id\n_version：文档的版本信息\n_score：相关性打分\n\n创建文档创建文档有以下4种方法：\n\nPUT /\\&lt;index>/_doc/&lt;_id&gt;\nPOST /\\&lt;index>/_doc/\nPUT /\\/_create/&lt;_id&gt;\nPOST /\\/_create/&lt;_id&gt;\n\n这四种方法的区别是，如果不指定id，则Elasticsearch会自动生成一个id。如果使用_create的方法，则必须保证文档不存在，而使用_doc方法的话，既可以创建新的文档，也可以更新已存在的文档。\n在创建文档时，还可以选择一些参数。\n请求参数\nif_seq_no：当文档的序列号是指定值时才更新\nif_primary_term：当文档的primary term是指定值时才更新\nop_type：如果设置为create则指定id的文档必须不存在，否则操作失败。有效值为index或create，默认为index\nop_type：指定预处理的管道id\nrefresh：如果设置为true，则立即刷新受影响的分片。如果是wait_for，则会等到刷新分片后，此次操作才对搜索可见。如果是false，则不会刷新分片。默认值为false\nrouting：指定路由到的主分片\ntimeout：指定响应时间，默认是30秒\nmaster_timeout：连接主节点的响应时长，默认是30秒\nversion：显式的指定版本号\nversion_type：指定版本号类型：internal、 external、external_gte、force\nwait_for_active_shards：处理操作之前，必须保持活跃的分片副本数量，可以设置为all或者任意正整数。默认是1，即只需要主分片活跃。\n\n响应包体\n_shards：提供分片的信息\n_shards.total：创建了文档的总分片数量\n_shards.successful：成功创建文档分片的数量\n_shards.failed：创建文档失败的分片数量\n_index：文档所属索引\n_type：文档所属type，目前只支持_doc\n_id：文档的id\n_version：文档的版本号\n_seq_no：文档的序列号\n_primary_term：文档的主要术语\nresult：索引的结果，created或者updated\n\n我们在创建文档时，如果指定的索引不存在，则ES会自动为我们创建索引。这一操作是可以通过设置中的action.auto_create_index字段来控制的，默认是true。你可以修改这个字段，实现指定某些索引可以自动创建或者所有索引都不能自动创建的目的。\n更新文档了解了如何创建文档之后，我们再来看看应该如何更新一个已经存在的文档。其实在创建文档时我们就提到过，使用PUT /\\/_doc/\\的方法就可以更新一个已存在的文档。除此之外，我们还有另一种更新文档的方法：\nPOST /\\/_update/\\&lt;_id&gt;\n这两种更新有所不同。_doc方法是先删除原有的文档，再创建新的。而_update方法则是增量更新，它的更新过程是先检索到文档，然后运行指定脚本，最后重新索引。\n还有一个区别就是_update方法支持使用脚本更新，默认的语言是painless，你可以通过参数lang来进行设置。在请求参数方面，_update相较于_doc多了以下几个参数：\n\nlang：指定脚本语言\nretry_on_conflict：发生冲突时重试次数，默认是0\n_source：设置为false，则不返回任何检索字段\n_source_excludes：指定要从检索结果排除的source字段\n_source_includes：指定要返回的检索source字段\n\n下面的一个例子是用脚本来更新文档\ncurl -X POST &quot;localhost:9200/test/_update/1?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;script&quot; : &#123;        &quot;source&quot;: &quot;ctx._source.counter += params.count&quot;,        &quot;lang&quot;: &quot;painless&quot;,        &quot;params&quot; : &#123;            &quot;count&quot; : 4        &#125;    &#125;&#125;&#x27;\nUpsertcurl -X POST &quot;localhost:9200/test/_update/1?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;script&quot; : &#123;        &quot;source&quot;: &quot;ctx._source.counter += params.count&quot;,        &quot;lang&quot;: &quot;painless&quot;,        &quot;params&quot; : &#123;            &quot;count&quot; : 4        &#125;    &#125;,    &quot;upsert&quot; : &#123;        &quot;counter&quot; : 1    &#125;&#125;&#x27;\n当指定的文档不存在时，可以使用upsert参数，创建一个新的文档，而当指定的文档存在时，该请求会执行script中的脚本。如果不想使用脚本，而只想新增/更新文档的话，可以使用doc_as_upsert。\ncurl -X POST &quot;localhost:9200/test/_update/1?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;doc&quot; : &#123;        &quot;name&quot; : &quot;new_name&quot;    &#125;,    &quot;doc_as_upsert&quot; : true&#125;&#x27;\nupdate by query这个API是用于批量更新检索出的文档的，具体可以通过一个例子来了解。\ncurl -X POST &quot;localhost:9200/twitter/_update_by_query?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;  &quot;script&quot;: &#123;    &quot;source&quot;: &quot;ctx._source.likes++&quot;,    &quot;lang&quot;: &quot;painless&quot;  &#125;,  &quot;query&quot;: &#123;    &quot;term&quot;: &#123;      &quot;user&quot;: &quot;kimchy&quot;    &#125;  &#125;&#125;&#x27;\n获取文档ES获取文档用的是GET API，请求的格式是：\nGET /\\/_doc/&lt;_id&gt;\n它会返回文档的数据和一些元数据，如果你只想要文档的内容而不需要元数据时，可以使用\nGET /\\/_source/&lt;_id&gt;\n请求参数获取文档的有几个请求参数之前已经提到过，这里不再赘述，它们分别是：\n\nrefresh\nrouting\n_source\n_source_excludes\n_source_includes\nversion\nversion_type\n\n而还有一些之前没提到过的参数，我们来具体看一下\n\npreference：用来 指定执行请求的node或shard，如果设置为_local，则会优先在本地的分片执行\nrealtime：如果设置为true，则请求是实时的而不是近实时。默认是true\nstored_fields：返回指定的字段中，store为true的字段\n\nmgetmget是批量获取的方法之一，请求的格式有两种：\n\nGET /_mget\nGET /\\/_mget\n\n第一种是在请求体中写index。第二种是把index放到url中，不过这种方式可能会触发ES的安全检查。\nmget的请求参数和get相同，只是需要在请求体中指定doc的相关检索条件\nrequest\nGET /_mget&#123;    &quot;docs&quot; : [        &#123;            &quot;_index&quot; : &quot;jackey&quot;,            &quot;_id&quot; : &quot;1&quot;        &#125;,        &#123;            &quot;_index&quot; : &quot;jackey&quot;,            &quot;_id&quot; : &quot;2&quot;        &#125;    ]&#125;\nresponse\n&#123;  &quot;docs&quot; : [    &#123;      &quot;_index&quot; : &quot;jackey&quot;,      &quot;_type&quot; : &quot;_doc&quot;,      &quot;_id&quot; : &quot;1&quot;,      &quot;_version&quot; : 5,      &quot;_seq_no&quot; : 6,      &quot;_primary_term&quot; : 1,      &quot;found&quot; : true,      &quot;_source&quot; : &#123;        &quot;user&quot; : &quot;ja&quot;,        &quot;tool&quot; : &quot;ES&quot;,        &quot;message&quot; : &quot;qwer&quot;      &#125;    &#125;,    &#123;      &quot;_index&quot; : &quot;jackey&quot;,      &quot;_type&quot; : &quot;_doc&quot;,      &quot;_id&quot; : &quot;2&quot;,      &quot;_version&quot; : 1,      &quot;_seq_no&quot; : 2,      &quot;_primary_term&quot; : 1,      &quot;found&quot; : true,      &quot;_source&quot; : &#123;        &quot;user&quot; : &quot;zhe&quot;,        &quot;post_date&quot; : &quot;2019-11-15T14:12:12&quot;,        &quot;message&quot; : &quot;learning Elasticsearch&quot;      &#125;    &#125;  ]&#125;\n删除文档CURD操作只剩下最后一个D了，下面我们就一起来看看ES中如何删除一个文档。\n删除指定id使用的请求是\nDELETE /\\/_doc/&lt;_id&gt;\n在并发量比较大的情况下，我们在删除时通常会指定版本，以确定删除的文档是我们真正想要删除的文档。删除请求的参数我们在之前也都介绍过，想要具体了解的同学可以直接查看官方文档。\ndelete by query类似于update，delete也有一个delete by query的API。\nPOST /\\/_delete_by_query\n它也是要先按照条件来查询匹配的文档，然后删除这些文档。在执行查询之前，Elasticsearch会先为指定索引做一个快照，如果在执行删除过程中，要索引发生改变，则会导致操作冲突，同时返回删除失败。\n如果删除的文档比较多，也可以使这个请求异步执行，只需要设置wait_for_completion=false即可。\n这个API的refresh与delete API的refresh参数有所不同，delete中的refresh参数是设置操作是否立即可见，即只刷新一个分片，而这个API中的refresh参数则是需要刷新受影响的所有分片。\nBulk API最后，我们再来介绍一种特殊的API，批量操作的API。它支持两种写法，可以将索引名写到url中，也可以写到请求体中。\n\nPOST /_bulk\n\nPOST /\\/_bulk\n\n\n在这个请求中，你可以任意使用之前的CRUD请求的组合。\ncurl -X POST &quot;localhost:9200/_bulk?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value1&quot; &#125;&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value3&quot; &#125;&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_index&quot; : &quot;test&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125;&#x27;\n请求体中使用的语法是newline delimited JSON（NDJSON）。具体怎么用呢？其实我们在上面的例子中已经有所展现了，对于index或create这样的请求，如果请求本身是有包体的，那么用换行符来表示下面的内容与子请求分隔，即为包体的开始。\n例如上面例子中的index请求，它的包体就是{ “field1” : “value1” }，所以它会在index请求的下一行出现。\n对于批量执行操作来说，单条操作失败并不会影响其他操作，而最终每条操作的结果也都会返回。\n上面的例子执行完之后，我们得到的结果应该是\n&#123;   &quot;took&quot;: 30,   &quot;errors&quot;: false,   &quot;items&quot;: [      &#123;         &quot;index&quot;: &#123;            &quot;_index&quot;: &quot;test&quot;,            &quot;_type&quot;: &quot;_doc&quot;,            &quot;_id&quot;: &quot;1&quot;,            &quot;_version&quot;: 1,            &quot;result&quot;: &quot;created&quot;,            &quot;_shards&quot;: &#123;               &quot;total&quot;: 2,               &quot;successful&quot;: 1,               &quot;failed&quot;: 0            &#125;,            &quot;status&quot;: 201,            &quot;_seq_no&quot; : 0,            &quot;_primary_term&quot;: 1         &#125;      &#125;,      &#123;         &quot;delete&quot;: &#123;            &quot;_index&quot;: &quot;test&quot;,            &quot;_type&quot;: &quot;_doc&quot;,            &quot;_id&quot;: &quot;2&quot;,            &quot;_version&quot;: 1,            &quot;result&quot;: &quot;not_found&quot;,            &quot;_shards&quot;: &#123;               &quot;total&quot;: 2,               &quot;successful&quot;: 1,               &quot;failed&quot;: 0            &#125;,            &quot;status&quot;: 404,            &quot;_seq_no&quot; : 1,            &quot;_primary_term&quot; : 2         &#125;      &#125;,      &#123;         &quot;create&quot;: &#123;            &quot;_index&quot;: &quot;test&quot;,            &quot;_type&quot;: &quot;_doc&quot;,            &quot;_id&quot;: &quot;3&quot;,            &quot;_version&quot;: 1,            &quot;result&quot;: &quot;created&quot;,            &quot;_shards&quot;: &#123;               &quot;total&quot;: 2,               &quot;successful&quot;: 1,               &quot;failed&quot;: 0            &#125;,            &quot;status&quot;: 201,            &quot;_seq_no&quot; : 2,            &quot;_primary_term&quot; : 3         &#125;      &#125;,      &#123;         &quot;update&quot;: &#123;            &quot;_index&quot;: &quot;test&quot;,            &quot;_type&quot;: &quot;_doc&quot;,            &quot;_id&quot;: &quot;1&quot;,            &quot;_version&quot;: 2,            &quot;result&quot;: &quot;updated&quot;,            &quot;_shards&quot;: &#123;                &quot;total&quot;: 2,                &quot;successful&quot;: 1,                &quot;failed&quot;: 0            &#125;,            &quot;status&quot;: 200,            &quot;_seq_no&quot; : 3,            &quot;_primary_term&quot; : 4         &#125;      &#125;   ]&#125;\n批量操作的执行过程相比多次单个操作而言，在性能上会有一定的提升。但同时也会有一定的风险，所以我们在使用的时候要非常的谨慎。\n总结本文我们先介绍了文档的基本概念和文档的元数据。接着又介绍了文档的CRUD操作和Bulk API。相信看完文章你对Elasticsearch的文档也会有一定的了解。那最后就请你启动你的Elasticsearch，然后亲自动手试一试这些操作，看看各种请求的参数究竟有什么作用。相信亲手实验过一遍之后你会对这些API有更深的印象。\n","tags":["Elasticsearch笔记"]},{"title":"Elasticsearch从入门到放弃：索引基本使用方法","url":"/2019/10/13/Elasticsearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83%EF%BC%9A%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","content":"前文我们提到，Elasticsearch的数据都存储在索引中，也就是说，索引相当于是MySQL中的数据库。是最基础的概念。今天分享的也是关于索引的一些常用的操作。\n创建索引curl -X PUT &quot;localhost:9200/jackey?pretty&quot;\nES创建索引使用PUT请求即可，上面是最简单的新建一个索引的方法，除此之外，你还可以指定：\n\nSettings\nMappings\naliases\n\n索引名称有以下限制：\n\n必须是小写\n不能包含：\\,/,*, ?, &quot;, &lt;, &gt;, |, (空格),,, #\n在ES7.0以前索引名可以包含冒号，但是7.0之后不支持了\n不能以-,_和+开头\n不能是.或..\n长度不能超过255字节\n\n请求支持的一些参数有：\n\nwait_for_active_shards：继续操作前，必须处于active状态的分片数，默认是1，也可以设置为all或者不大于总分片数的任意正整数\ntimeout：设置等待响应的超时时间，默认是30秒\nmaster_timeout：连接master节点响应的超时时间，默认是30秒\n\n前面我们提到创建索引时可以指定三种属性，这三种属性都需要放在body中。\naliases索引的别名，一个别名可以赋给多个索引。\n给一个index起别名的方式有两种，一种是创建index时候在body中增加aliases，另一种是通过更新已有索引的方式增加。\n方式一：\ncurl -X PUT &quot;localhost:9200/jackey?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;aliases&quot; : &#123;        &quot;alias_1&quot; : &#123;&#125;,        &quot;alias_2&quot; : &#123;            &quot;filter&quot; : &#123;                &quot;term&quot; : &#123;&quot;user&quot; : &quot;kimchy&quot; &#125;            &#125;,            &quot;routing&quot; : &quot;kimchy&quot;        &#125;    &#125;&#125;&#x27;\n方式二：\ncurl -X POST &quot;localhost:9200/_aliases?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;actions&quot; : [        &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;jackey&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;    ]&#125;&#x27;\n方式一中，我们还在body中增加了filter和routing。这主要是用于指定使用别名的条件。指定了filter后，通过alias_2，只能访问user为kimchy的document。而routing的值被用来路由，即alias_2只能路由到指定的分片。此外还有index_routing和search_routing，它们和routing类似，这里不做过多解释了。还有一个比较重要的属性是is_write_index，这个属性默认是false，如果设置成true，表示可以通过这个别名来写索引，默认情况下，别名像一个软链接，是不可以修改原索引的。\n此外，还可以使用通配符为多个索引增加相同的别名\ncurl -X POST &quot;localhost:9200/_aliases?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;actions&quot; : [        &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test*&quot;, &quot;alias&quot; : &quot;all_test_indices&quot; &#125; &#125;    ]&#125;&#x27;\n除了add，还可以使用remove来删除别名\ncurl -X POST &quot;localhost:9200/_aliases?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;actions&quot; : [        &#123; &quot;remove&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;    ]&#125;&#x27;\nSettings先看一个例子：\ncurl -X PUT &quot;localhost:9200/twitter?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;settings&quot; : &#123;        &quot;index&quot; : &#123;            &quot;number_of_shards&quot; : 3,             &quot;number_of_replicas&quot; : 2         &#125;    &#125;&#125;&#x27;\n索引的setting分为静态和动态两种。静态的只能在索引创建或关闭时设置；动态的则可以使用update-index-settings API来实时设置。上面的例子中，number_of_shards属于静态设置，number_of_replicas属于动态设置。\n索引可以设置的setting可以在官方文档的Index modules查看，下面我会挑几个我认为比较重要的介绍一下。\n先从静态开始：\n\nindex.number_of_shards：指定索引的分片数，只能在创建索引时设置。默认是1，最大可以设置为1024。这是出于安全考虑的一种保护措施。最大值可以通过设置系统变量来控制export ES_JAVA_OPTS=”-Des.index.max_number_of_shards=128”\nindex.routing_partition_size：可以路由的分片数量，同样只能在创建索引时指定，默认值为1.这个值必须小于number_of_shards（除非number_of_shards的值也是1）\n\n动态setting：\n\nindex.number_of_replicas：每个分片的副本数，默认是1\nindex.auto_expand_replicas：基于数据节点可以自动扩展的副本数，默认为为false。可以设置为一个区间，以短线分隔，例如「0-5」，也可以设置成all。需要注意的是，副本的自动扩展并不会考虑其他的分配规则。这有可能导致集群状态变成黄色\nindex.search.idle.after：分片被认为搜索空闲之前没有收到请求或搜索的时间。默认30秒。\nindex.refresh_interval：刷新操作的执行频率，默认是1s。如果设置成-1，表示不会刷新。如果没有显式设置，分片在收到搜索请求前至少index.search.idle.after秒内不会后台刷新\nindex.max_result_window：返回结果的最大数量，默认是10000（一万）。搜索返回结果占用的内存和时间受到这个值的限制\nindex.routing.rebalance.enable：是否允许分片的自平衡。默认是all，允许所有分片重新平衡。还可以设置为primaries，只允许主分片重新平衡。replicas只允许从分片重新平衡。none不允许分片重新平衡。\n\n除了以上静态setting和动态setting之外，setting中还可以设置一些其他的值，例如分词器等，这些我们以后再做更详细的介绍。\nMappingscurl -X PUT &quot;localhost:9200/test?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;    &quot;settings&quot; : &#123;        &quot;number_of_shards&quot; : 1    &#125;,    &quot;mappings&quot; : &#123;        &quot;properties&quot; : &#123;            &quot;field1&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125;        &#125;    &#125;&#125;&#x27;\nMapping主要用于帮助Elasticsearch理解每个域中数据的类型。7.0.0之前mapping的定义通常包括type名称。Elasticsearch支持的数据类型比较多，其中比较核心的简单数据类型包括：\n\n字符串: text和keyword\n整数 : byte, short, integer, long\n浮点数: float, double\n布尔型: boolean\n日期: date\n\n其他的类型，我们以后会做更加详细的介绍。\n删除索引删除索引使用的是DELETE请求。\ncurl -X DELETE &quot;localhost:9200/jackey?pretty&quot;\n你可以在路径中指定具体索引，也可以使用通配符，需要删除多个索引时，可以使用逗号分隔。如果要删除全部索引，可以指定索引为_all或*（不要这么做）。在生产环境，我们通过在elasticsearch.yml文件中将action.destructive_requires_name配置为true来禁止这些危险的操作。\n删除操作支持的参数有以下几种：\n\nallow_no_indices：如果设置为true，则通配符或_all匹配不到索引时不会报错\nexpand_wildcards：控制通配符可以扩展到的索引类型。all：可以扩展到所有的索引。open：只能扩展到打开的索引。closed：只能扩展到关闭的索引。none：不接受通配符表达式。默认是open\nignore_unavailable：如果设置为true，不存在或关闭的索引不会在返回中。默认是false\ntimeout：指定等待返回响应的最长时间。默认是30秒\nmaster_timeout：连接master节点响应的超时时间，默认是30秒\n\n打开/关闭索引前面我们已经提到过了打开/关闭索引。被关闭的索引几乎不能对它进行任何操作，它只是用来保留数据的。而打开或关闭索引通常需要重启分片来使操作生效。具体的操作如下：\ncurl -X POST &quot;localhost:9200/jackey/_open?pretty&quot;\ncurl -X POST &quot;localhost:9200/jackey/_close?pretty&quot;\n支持的参数有：\n\nallow_no_indices\nexpand_wildcards\nignore_unavailable\nwait_for_active_shards\ntimeout\nmaster_timeout\n\n这些参数在前面都有介绍。这里就不再赘述了。\n拆分索引随着数据的越来越多，我们可能会有拆分索引的需求，感谢ES为我们提供了便利。\ncurl -X POST &quot;localhost:9200/twitter/_split/split-twitter-index?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;  &quot;settings&quot;: &#123;    &quot;index.number_of_shards&quot;: 2  &#125;&#125;&#x27;\n在拆分索引之前，要保证索引是只读状态，并且集群健康状态为green。设置只读的方法是：\ncurl -X PUT &quot;localhost:9200/my_source_index/_settings?pretty&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123;  &quot;settings&quot;: &#123;    &quot;index.blocks.write&quot;: true   &#125;&#125;&#x27;\n拆分索引的具体操作是：\n\n创建一个和源索引相同的目标索引，主分片要大于源索引\n建立从源索引到目标索引的硬连接\n创建低级索引后，再对document做Hash操作。这是为了删除属于不同分片的document\n恢复目标索引，就像重新打开关闭的索引一样\n\n总结关于索引的使用就先介绍到这里。还有很多不完善的地方，以后会继续补充。想要了解更多详细信息的同学可以查看官方文档。\n","tags":["Elasticsearch笔记"]},{"title":"Flink学习笔记：多流 Join","url":"/2025/07/19/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%A4%9A%E6%B5%81join/","content":"前面我们已经了解了 Flink 几个核心概念，分别是时间、Watermark 已经窗口。今天我们来一起了解下 Flink 是怎么进行多个流的 Join 的。\n我们今天从两个流的 Join 来入手，扩展到多个流也是一样的道理。Flink 中的 Join 可以分为两种：Window Join 和 Interval Join。\nWindow JoinWindow Join 是将两个流中在相同窗口中且有相同 key 的元素进行关联。关联后，可以使用 JoinFunction 和 FlatJoinFunction 进行处理。Window Join 可以根据窗口类型分为三种：Tumbling Window Join、Sliding Window Join 和 Session Window Join。\nTumbling Window Join首先来看Tumbling Window Join，其实就是对应的使用滚动窗口进行 Join。\n \n具体使用方法如下：\nDataStream&lt;Tuple2&lt;String, Double&gt;&gt; result = source1.join(source2)        .where(record -&gt; record.f0)        .equalTo(record -&gt; record.f0)        .window(TumblingEventTimeWindows.of(Time.seconds(2L)))        .apply(new JoinFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;() &#123;            @Override            public Tuple2&lt;String, Double&gt; join(Tuple2&lt;String, Double&gt; record1, Tuple2&lt;String, Double&gt; record2) throws Exception &#123;                return Tuple2.of(record1.f0, record1.f1);            &#125;        &#125;);\n其中 source1 和 source2 分别代表两个流，where 为 source1 的 join key 提取方法，equalTo 为 source2 的 join key 提取方法，最后，join 好之后的数据通过 JoinFunction 来处理。\nSliding Window JoinSliding Window Join 和 Tumbling Window Join 的用法基本一致，只是将窗口指定为滑动窗口。\n\nSession Window JoinSession Window Join 也类似，只是指定的窗口不同，具体的处理流程都是一样的，这里也不过多解释。\nInterval JoinInterval Join 是将两个流中 key 相同，且一个流的 timestamp 处于另一个流的 timestamp 上下波动范围内。\n假设我们有两个流 a 和 b，Interval Join可以表达为b.timestamp ∈ [a.timestamp + lowerBound; a.timestamp + upperBound] 或 a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound。\n需要注意的是，目前 Interval Join 仅支持 event time。\n\n它的使用方法也很简单，只需要定义上下偏移量以及处理函数即可。\nDataStream&lt;Tuple2&lt;String, Double&gt;&gt; intervalJoinResult = source1.keyBy(record -&gt; record.f0)        .intervalJoin(source2.keyBy(record -&gt; record.f0))        .between(Time.seconds(-2), Time.seconds(2))        .process(new ProcessJoinFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;() &#123;            @Override            public void processElement(Tuple2&lt;String, Double&gt; record1, Tuple2&lt;String, Double&gt; record2, ProcessJoinFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;.Context context, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out) throws Exception &#123;                out.collect(Tuple2.of(record1.f0, record1.f1 + record2.f1));            &#125;        &#125;);\nCoGroup前面介绍的两种 Join 都是 inner join，那么 Flink 有没有办法支持 left join 呢？答案是肯定的，我们可以使用 coGroup 来实现。\ncoGroup 的通用用法如下：\nstream.coGroup(otherStream)\t\t.where(&lt;KeySelector&gt;)\t\t.equalTo(&lt;KeySelector&gt;)\t\t.window(&lt;WindowAssigner&gt;)\t\t.apply(&lt;CoGroupFunction&gt;);\n我们通过自定义 CoGroupFunction 来实现 left join。\nprivate static class LeftJoinFunction implements CoGroupFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt; &#123;    @Override    public void coGroup(Iterable&lt;Tuple2&lt;String, Double&gt;&gt; iterable1, Iterable&lt;Tuple2&lt;String, Double&gt;&gt; iterable2, Collector&lt;Tuple2&lt;String, Double&gt;&gt; collector) throws Exception &#123;        for (Tuple2&lt;String, Double&gt; record1 : iterable1) &#123;            boolean match = false;            for (Tuple2&lt;String, Double&gt; record2 : iterable2) &#123;                match = true;                collector.collect(Tuple2.of(record1.f0, record1.f1 + record2.f1));            &#125;            if (!match) &#123;                System.out.println(&quot;没有join的元素 key:&quot; + record1.f0);                collector.collect(Tuple2.of(record1.f0, record1.f1));            &#125;        &#125;    &#125;&#125;\n在 coGroupFunction 中，需要实现 coGroup 方法，方法的参数包括两个输入流的 Iterable 和输出的 collector。如果第二个流中没有匹配的元素，那么就直接输出第一个流的元素。\n总结最后来总结一下，Flink 中有两种 Join 方法，分别为 Window Join 和 Interval Join，Window Join 是依赖窗口来执行，对窗口内的元素进行 join，Interval Join 不依赖窗口，是根据 event time 的范围来进行 join。最后还介绍了 CoGroup，我们可以使用 CoGroup 来实现 left join 和 right join。\n","tags":["Flink"]},{"title":"Flink学习笔记：整体架构","url":"/2025/06/09/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84/","content":"开一个新坑，系统性的学习下 Flink，计划从整体架构到核心概念再到调优方法，最后是相关源码的阅读。\n\n今天就来学习 Flink 整体架构，我们先看官网的架构图\n\n图中包含三部分，分别是 Client、JobManager 和 TaskManager。其中 Client 并不属于 Flink 集群，它主要用来把用户编写的程序翻译成 StreamGraph 然后优化成 JobGraph，再将 JobGraph 提交到 Flink 集群执行。\nJobManagerFlink 集群的 JobManager 是用来接收 Client 提交的任务，并且分发给 TaskManager 去执行。此外，JobManager 还有一些其他的职责，例如任务调度，协调 checkpoint 和协调从失败中恢复。\n每个 Flink 集群至少要有一个 JobManager，但在生产环境中通常是高可用模式部署，即部署多台 JobManager，其中一台作为 Leader，其他的作为 Standby 节点。当 Leader 挂掉时，其他的 Standby 节点会有一台被选举为新的 Leader 提供服务。这样就能避免 JobManager 单机故障影响到整个 Flink 集群的可用性。\nJobManager 主要由以下几部分组成，下面我们分别来看每部分的作用。\n\nDataFlow GraphJobManager 收到 JobGraph 之后，根据并行度的设置，将各个算子任务拆分成并行的子任务，最终生成 ExecutionGraph。\nCheckpoint coordinatorCheckpoint 是 Flink 最核心的概念之一，Flink 的容错机制主要靠 checkpoint 来保障。而 checkpoint 的生成会恢复则由 checkpoint coordinator 来负责。\nJob DispatchJob Dispatch 提供了 REST 接口用于提交 Flink 任务，并为每个任务启动一个 JobMaster。JobMaster 负责管理单个 JobGraph 的执行。\nTask SchedulingTask Scheduling 负责 Task 部署和调度，值得一提的是，JobManager 和 TaskManager 以及 Client 之间的通信都是通过一个叫 Actor System 的 RPC 系统实现的。\nResource ManagerResource Manager 负责集群中的资源的分配回收，它管理的资源单元叫做 task slot，对于不同的部署环境，Resource Manager 有不同的实现，\nActor SystemActor System 是 Flink 集群中的一种 RPC 通信的组件，JobManager 和 TaskManager 以及 Client 之间的通信都是基于 Actor System 的。而 TaskManager 之间的数据传递是基于 Netty 的。\nTaskManager聊完了 JobManager，我们再来看下 TaskManager 的结构。TaskManager 主要负责执行作业的 task，并缓存和交换数据流。TaskManager 中最小的资源调度单位是 task slot，这点在前面介绍 Resource Manager 时也提到过。它表示并发处理 task 的数量。\n\nTask ExecutionTaskManager 在接到 JobManager 部署的任务后，就会申请相应的 task slot 去执行任务。\nData ExchangeData Execution 主要负责 TaskManager 之间的数据交互的一些操作，这里主要关注逻辑层面，例如一些 shuffle 操作。而网络传输则主要是由 Network Manager 来实现。\nMemory ManagementMemory Management 负责 TaskManager 的内存管理，在执行任务过程中，接收到的一些数据是需要放在内存中进行处理的。相应的内存管理操作依赖于 Memory Management 模块。\nActor SystemActor System 我们在前面提到过，TaskManager 和 JobManager 之间的通信全靠它。\nNetwork ManagerNetwork Manager 主要负责 TaskManager 之间的数据交互，它是基于 Netty 实现的。\n最后多提一个 Graph 的概念，前面我们已经了解到了 JobManager 会将 JobGraph 根据并行度的配置转换成 ExecutionGraph。在这之后，JobManager 会对作业进行调度，将 task 部署到各个 TaskManager 上，最终就形成了物理执行图，也就是 PhysicalGraph。\n这里小结一下，Flink 中四种图的生成顺序是：用户编写的代码生成 StreamGraph，Client 将其进行优化，主要是将多个符合条件的节点 chain 在一起，生成了 JobGraph，然后将 JobGraph 提交到 JobManager，再由 JobManager 生成并行版本的 ExecutionGraph，待JobManager 将 task 调度后，生成的图被称为 PhysicalGraph。\nFlink 的几种部署模式根据集群的生命周期、资源隔离以及 main() 方法的执行，通常将 Flink 的部署模式分为三种：Session Mode、Per-Job Mode 和 Application Mode。下面我们分别介绍这三种部署模式。\nSession ModeSession Mode 下，所有的任务共享 JobManager 和 TaskManager，JobManager 的生命周期不受提交的 Job 影响，会长期运行。\n\nPer-Job ModePer-Job Mode 下，每个任务独享 JobManager 和 TaskManager，资源充分隔离。JobManager 的生命周期和 Job 的生命周期绑定。\n\nApplication ModeApplication Mode 下，每个 Application 对应一个 JobManager，且可以运行多个作业。客户端无需将依赖包上传到 JobManager，只负责提交作业，减轻了客户端的压力。提交作业后，JobManager 主动从 HDFS 拉取依赖包。\n\n三种模式的对比\n\n\n\nSession\nPer-Job\nApplication\n\n\n\n\n优点\n1、资源充分共享，提升资源利用率2、作业集中管理，运维简单\n1、资源充分隔离2、每个作业的 TM Slots 可以不同\n1、有效降低带宽和客户端负载2、Application 之间实现资源隔离，Application 中的资源共享\n\n\n缺点\n1、资源隔离差2、TM 不易扩展，伸缩性差\n1、资源浪费\n1、仅支持 Yarn 和 Kubunetes （个人感觉够用了）\n\n\n\n总结最后来总结一下，今天主要学习了 Flink 的整体架构和三种部署模式。\n1、Flink 的集群架构上主要包含 JobManager 和 TaskManager，其中 JobManager 主要负责一些作业调度和资源协调的工作，TaskManager 则主要负责执行任务。\n2、Flink 的部署模式分为 Session、Per-Job 和 Application 三种，Session 模式是所有 Job 共享 JobManager 和 TaskManager，Per-Job 则是作业独享的，而 Application 模式则是在 Application 中共享 JobManager。\n","tags":["Flink"]},{"title":"Flink学习笔记：时间与Watermark","url":"/2025/06/30/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%97%B6%E9%97%B4%E4%B8%8EWatermark/","content":"在前文中，我学习 Flink 的整体架构，接下来的几篇文章，我将重点学习一下 Flink 的几个核心概念。包括时间属性、Watermark、窗口、状态以及容错机制。今天就来学习时间属性和 Watermark。\n时间属性首先来学习 Flink 的时间属性，作为流处理引擎，时间是实时数据处理的重要依赖，特别是在做时序分析或者特定时间段数据处理时，时间的概念更显得尤为重要。\nFlink 中支持三种时间属性，分别是：\n\nEventTime：事件时间，即为事件产生的时间。\n\nProcessTime：处理时间，Flink 算子处理事件的时间。\n\nIngestionTime：摄入时间，Flink 读取事件的时间。\n\n\n这样描述可能比较抽象，我们通过一张图来看一下。\n\n从上图中可以看出，在时间产生/存储时，记录一个设备时间，就是 Event Time。当 Flink 的 DataSource 读取到事件时，这时再记录一个时间，这就是 Ingestion Time。在 Flink 程序中，每个算子处理事件时，又会记录一个时间，这个时间就是 Process Time。\nWatermark介绍完了时间概念，再来看下 Watermark 的概念。它是 Flink 处理迟到事件的妙招。\nWatermark 本身也属于一种特殊的事件，它由 Source 生成，同时携带由 Timestamp，并且会跟随正常的事件一起在 Flink 算子之间流转。Watermark 的作用是定义何时停止等待较早的事件。这么介绍可能比较抽象，下面我们通过一些具体的例子来进行更进一步的说明。\n\n上图代表的是一段乱序的事件数据流。假设我们定义 maxOutOfOrderness 为4，也就是容忍最大迟到时间为4（这里不带具体时间单位，可能是4秒也可能是4分钟）。当我们收到时间戳为7的事件时，就会生成一个时间为3的 Watermark。这代表着3之前的数据都已就绪。如果此时再有小于3的数据，我们认为它是迟到数据。\n而对于迟到的数据，通常有三种处理方法：\n\n重新开启已经关闭的窗口，重新计算并修正结果\n\n将迟到事件使用旁路输出收集起来单独处理\n\n将迟到事件视为错误消息丢弃\n\n\n在 Flink 中 Watermark 本身是没有意义的，它的主要作用是作为窗口的触发条件。窗口可以认为是一个时间段，它有开始时间和结束时间。在窗口内可以计算一批事件的统计结果。关于窗口，我们后面再做详细介绍。\n那么 Watermark 是如何触发窗口的呢？答案是必须要满足以下两个条件：\n\nWatermark 的时间戳 &gt;= 窗口的 end_time\n\n窗口中有数据\n\n\n从概念上看还是比较抽象，我们还用上面的数据流作为例子，Watermark 设置为最大时间减 4，假设我们设置10秒一个窗口。关键代码如下：\n***SingleOutputStreamOperator&lt;Event&gt; withTimestampsAndWatermarks = source                .assignTimestampsAndWatermarks(                        WatermarkStrategy.forGenerator(ctx -&gt; new CustomWatermarkGenerator())                                .withTimestampAssigner(((event, l) -&gt; event.timestamp))                );OutputTag&lt;Event&gt; lateTag = new OutputTag&lt;Event&gt;(&quot;late-tag&quot;) &#123;&#125;;SingleOutputStreamOperator&lt;String&gt; windowResult = withTimestampsAndWatermarks        .keyBy(event -&gt; event.num)        .window(TumblingEventTimeWindows.of(Time.seconds(10)))        .sideOutputLateData(lateTag)        .process(new ProcessWindowFunction&lt;Event, String, Long, TimeWindow&gt;() &#123;         @Override         public void process(Long key, Context context, Iterable&lt;Event&gt; elements, Collector&lt;String&gt; out) &#123;             // 一些逻辑处理             out.collect(result);         &#125;&#125;);// 处理迟到数据DataStream&lt;Event&gt; lateStream = windowResult.getSideOutput(lateTag);lateStream.process(new ProcessFunction&lt;Event, String&gt;() &#123;    @Override    public void processElement(Event event, Context ctx, Collector&lt;String&gt; out) &#123;        out.collect(&quot;迟到事件: &quot; + event);    &#125;&#125;).print();******@Overridepublic void onEvent(Event event, long l, WatermarkOutput watermarkOutput) &#123;    long eventTime = event.timestamp;    // 使用CAS确保线程安全    while (true) &#123;        long current = currentMaxTime.get();        if (eventTime &lt;= current) break;        if (currentMaxTime.compareAndSet(current, eventTime)) break;    &#125;&#125;@Overridepublic void onPeriodicEmit(WatermarkOutput watermarkOutput) &#123;    watermarkOutput.emitWatermark(new Watermark(currentMaxTime.get() - timeDiff));&#125;***\n完整代码我放在 github 上了\n当我们输入测试数据时\n4,17508672040002,17508672020007,175086720700010,17508672100009,175086720900015,175086721500012,175086721200013,175086721300025,175086722500014,175086721400035,1750867235000\n可以看到如下输出：\n\n通过输出的日志，我们可以看出，当watermark推进到大于等于时间窗口的结束时间时，窗口就会完成计算并关闭。而对于迟到的数据，我们可以通过侧输出流单独处理，也可以通过设置allowedLateness，使窗口重新打开。\n生成 Watermark了解了 Watermark 的原理之后，我们再来看一下如何生成 Watermark。在 Flink 中，需要使用 WatermarkStrategy 来定义如何生成时间戳和 watermark。WatermarkStrategy 继承了 TimestampAssignerSupplier 和 WatermarkGeneratorSupplier 两个接口，其中 TimestampAssignerSupplier 定义了抽取 EventTime 的方法，而 WatermarkGeneratorSupplier 则是定义了如何生成 Watermark 的方法。\nFlink 内置的 Watermark 生成器Flink 中内置了两个 watermark 生成器。分别是 AscendingTimestampsWatermarks 和 BoundedOutOfOrdernessWatermarks。\n我们先来看 BoundedOutOfOrdernessWatermarks，它定义了一个 watermark 滞后于最大事件时间一个固定值的 watermark 生成器。在使用时，可以给定一个时间，这样 Flink 就会 根据最大的 eventTime 来周期性的生成 watermark，例如，我们前面定义的 watermark 滞后4秒，就可以写成：\nWatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(4));\nAscendingTimestampsWatermarks 是单调递增时间分配器，也就是只处理有序的数据，它继承了 BoundedOutOfOrdernessWatermarks，并且最大容忍时间为0。在使用时，可以直接通过以下方法生成：\nWatermarkStrategy.forMonotonousTimestamps();\n自定义 WatermarkGenerator除了上面两个内置的 WatermarkGenerator 外，我们还可以自定义，实现起来也比较简单。只需要实现 WatermarkGenerator 接口并重写 onEvent 和 onPeriodicEmit 两个方法即可。onEvent 是每个事件到来时调用一次，可以用来记录最大事件时间。onPeriodicEmit 则是周期性调用，可以生成 watermark。在前面的例子中，我使用的 CustomWatermarkGenerator 就是自定义的 watermark，对应的实现也在前文中贴了。\n如何处理空闲数据源最后，再补充一个与 watermark 相关的比较重要的特性。在 Flink 中，会有一些算子有多个输入源。这时，这个算子的 watermark 是以它收到的数据源中最小的 eventTime 来计算的。直接看官网的例子：\n\n那么这里就存在一个问题：如果一个输入源数据量很少，很久才发一条消息，而另一个数据源发了很多消息，那么就会在下游算子中积累很多消息等待处理，这对于整个系统的稳定性造成了很大的风险。\n那这种情况有办法处理吗？答案是肯定的，Flink 提供了 withIdleness 方法，它可以用来检测空闲数据源，如果超过一定时间没有数据到来，Flink 认为这个数据源属于空闲数据源，这时就不会再阻塞下游算子触发窗口。达到定期处理数据的目的。\n总结今天我们先了解了 Flink 中时间的概念，EventTime 是事件产生的时间，通常由上游数据源生成，ProcessTime 是处理时间，通常由处理算子本身生成，IngestionTime是摄入时间，通常由 Flink 的 Source 生成。\n接着我们由了解了 Flink 的 watermark，它是窗口触发的条件，在处理迟到数据时发挥着重要的作用。我们可以定义可以容忍的最大迟到时间，这样当遇到乱序数据时也可以得到正确的结果。\n","tags":["Flink"]},{"title":"Flink学习笔记：反压","url":"/2025/08/26/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%8E%8B/","content":"今天来聊在 Flink 运维过程中比较常见的一个问题：反压。\n什么是反压反压是流式系统中关于数据处理能力的动态反馈机制，并且是从下游到上游的反馈，一般发生在上游节点的生产速度大于下游节点的消费速度的情况。\n数据如何传输在了解反压的细节之前，首先要知道 Flink 中数据是如何传输的。在 Flink 中，两个算子之间的关系分为三种：\n\n部署在同一个 TaskManager 上，且属于同一算子链。\n\n部署在同一个 TaskManager 上，但不是同一个算子链。\n\n部署在不同的 TaskManager 上。\n\n\n三种不同的关系，对应的算子间的数据传输方式也不同。先说第一种。\n同一线程数据传输同一线程中的两个算子共享内存，因此数据传输非常简单，上游产出好数据后，直接调用下游的 processElement 方法即可。\n本地线程数据传输对于第二种关系，两个算子不在同一线程，但是部署在同一个 TaskManager 上，也就是算子之间的数据传输是跨线程的。我们通过一个图来解释。\n\n图中，Flat Map Task 是上游算子，sum 是下游的算子。它们共享一块 Buffer 内存。当 Buffer 中没有数据可以消费时，sum 所在的线程会阻塞（步骤1）。随着数据的流入，Flat Map Task 会将处理好的数据写入到 ResultSubpartition（步骤2），然后 flush 到 Buffer 中（步骤3）。此时会唤醒 sum 所在的线程（步骤4），它就可以从 Buffer 中读取数据了（步骤5）。\n远程数据传输第三种跨 TaskManager 的数据传输，与第二种类似，不过也有些区别。我们还是通过一张图来解释。\n\n从图中可以看到，当 sum 所在线程没有 Buffer 可以消费时，会通过 PartitionRequestClient 向 Flat Map Task 所在的进程发送请求。Flat Map Task 所在进程接收到请求后，会读取 Buffer 中的数据并返回。\nFlink 的反压了解了 Flink 的数据传输方式之后，我们再来看下 Flink 是如何感知反压的。\n\n上图是一个数据传输的简图。当 Task1 有 Buffer 空间时，记录 A 被序列化并写入 LocalBufferPool 中，接着发送到 Task2 的 LocalBufferPool 中，Task2 读取并反序列化后交由程序处理。\n这里我们也分两个场景讨论。\n本地传输Task1 和 Task2 在同一个 TaskManager 节点，Task1 和 Task2 共用 Buffer，一旦 Task2 消费了 Buffer，该 Buffer 就会被回收。如果 Task2 的处理速度比 Task1 慢，那么 Buffer 的回收速度就赶不上 Task1 取 Buffer 的速度，这样会导致无 Buffer 可用，最终 Task1 就会降速。\n远程传输Task1 和 Task2 运行在不同的 TaskManager 上，那 Buffer 会发送到网络后，等接收端消费完再回收。在发送端，会通过 Netty 水位机制来保证不往网络中写太多数据，如果网络中的数据超过了高水位值，就会等其下降到低水位值以下才会继续写数据。如果网络有堆积，发送端就会暂停发送，Buffer 也不会被回收，这就会阻塞 writer 往 ResultSubPartition 中写数据。\n反压监控在 Flink Web UI 中，可以找到反压的监控\n\n它有三种状态：\n\nOK: 0% &lt;= 反压比例 &lt;= 10%，此时一般不用处理。\nLOW: 10% &lt; 反压比例 &lt;= 50%，这种状态需要关注。\nHIGH: 50% &lt; 反压比例 &lt;= 100%，已经反压，需要赶快处理。\n\n总结今天我们聊了什么是反压，以及 Flink 中数据传输方法和 Flink 任务是如何感知反压的。Flink 的传输方式分为三种，分别是同线程传输、本地跨线程传输以及远程传输。Flink 任务在感知反压时也分别针对本地传输和远程传输做了讨论。\n","tags":["Flink"]},{"title":"Flink学习笔记：如何做容错","url":"/2025/08/17/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%A6%82%E4%BD%95%E5%81%9A%E5%AE%B9%E9%94%99/","content":"现在我们已经了解了 Flink 的状态如何定义和使用，那 Flink 是如何做容错的呢？今天我们一起来了解一下。\n先来回答问题， Flink 是通过状态快照来做容错的，在 Flink 中状态快照分为 Checkpoint 和 Savepoint 两种。\nCheckpointCheckpoint 是一种自动执行的快照，其目的是让 Flink 任务可以从故障中恢复。它可以是增量的，并且为快速恢复进行了优化。\n如何开启 CheckpointCheckpoint 默认是关闭的，开启的方法很简单，只需要调用 enableCheckpointing() 方法即可。除了这个方法之外，Checkpoint 还有一些高级特性。我们来看几个比较常用的，更多的选项可以查看官方文档。\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 每 1000ms 开始一次 checkpointenv.enableCheckpointing(1000);// 高级选项：env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);env.getCheckpointConfig().setCheckpointTimeout(60000);env.getCheckpointConfig().setTolerableCheckpointFailureNumber(2);env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);env.getCheckpointConfig().setExternalizedCheckpointRetention(        ExternalizedCheckpointRetention.RETAIN_ON_CANCELLATION);env.getCheckpointConfig().enableUnalignedCheckpoints();\n\nCheckpointingMode：支持 EXACTLY_ONCE 和 AT_LEAST_ONCE 两种，精确一次有更好的数据一致性，而至少一次可以提供更低的延迟。\n\nMinPauseBetweenCheckpoints：Checkpoint 之间最小间隔时间，单位是毫秒，即前一次 Checkpoint 执行完成之后必须间隔 n 毫秒之后才会开启下一次 Checkpoint。\n\nCheckpointTimeout：Checkpoint 超时时间，单位为毫秒，表示 Checkpoint 必须在 n 毫秒内完成，否则就会因超时失败。\n\nTolerableCheckpointFailureNumber：可容忍连续失败次数，默认是0。超过这个阈值之后，整个 Flink 作业会触发 fail over。\n\nMaxConcurrentCheckpoints：Checkpoint 并发数，默认情况下是1，在同一时间只允许一个 Checkpoint 执行。这个参数不能和最小间隔时间一起使用。\n\nExternalizedCheckpointRetention：周期存储 Checkpoint 到外部存储，这样在任务失败时 Checkpoint 也不会被删除。\n\nenableUnalignedCheckpoints：使用非对齐的 Checkpoint，可以减少在产生背压时 Checkpoint 的创建时间。\n\n\nCheckpoint 存储Flink 提供了两种存储类型：JobManagerCheckpointStorage 和 FileSystemCheckpointStorage。默认是 JobManagerCheckpointStorage，即将 Checkpoint 快照存储在 JobManager 的堆内存中，也可以设置 Checkpoint 目录，将快照存储在外部存储系统中。\nCheckpoint 目录通过 execution.checkpointing.dir 设置项设置。其目录结构如下：\n/user-defined-checkpoint-dir    /&#123;job-id&#125;        |        + --shared/        + --taskowned/        + --chk-1/        + --chk-2/        + --chk-3/        ...   \nCheckpoint 工作原理在前文中，我们曾经提到过 Checkpoint Coordinator，它是 JobManager 的其中一个模块。它在 Checkpoint 过程中担任着重要的角色。\n现在来看下 Checkpoint 的完整流程\n\n1、Checkpoint Coordinator 触发所有 Source 节点开始 Checkpoint，Source 收到触发命令后，会将自己的 State 进行持久化（图中三角形），并且向下游发送 barrier 事件（图中的小矩形）。当 Source 节点的 State 持久化完成之后，会数据存储的地址发送给 Checkpoint Coordinator。\n\n2、barrier 事件随着事件流传输到下游节点，当下游节点收到所有的上游 barrier 事件后，也会将自己的 State 持久化，并继续向下传播 barrier 事件。持久化完成后，也同样将数据存储地址发送给 Checkpoint Coordinator。\n3、当所有的算子都完成持久化过程后，Checkpoint Coordinator 会将一些元数据进行持久化。\n\n至此，一次完整的 Checkpoint 流程就结束了。\nSavepoint学习完 Checkpoint 之后，我们再来了解下另一种快照——Savepoint。\nSavepoint 是依据 checkpoint 机制创建的一致性镜像。通常用来做 Flink 作业的重启或更新等运维操作。Savepoint 包含稳定存储上的二进制文件（作业状态的镜像）和元数据文件两部分。\n使用 Savepoint根据官方文档的提示，在我们的程序中，最好显式调用 uid() 方法来为算子指定一个 ID，这些 ID 被用来恢复每个算子的状态。如果不指定的话，Flink 任务会自动生成算子 ID，但是生成的 ID 与程序结构有关，也就是说，如果程序的结构改变了的话，就没有办法从 Savepoint 恢复对应算子的状态了。\n有了这个前提条件之后，我们就可以使用命令来操作 Savepoint 了。\n// 触发 savepoint$ bin/flink savepoint :jobId [:targetDirectory]// 触发 savepoint, 指定 type，默认是 canonical$ bin/flink savepoint --type [native/canonical] :jobId [:targetDirectory]// 触发 savepoint，客户端拿到 trigger id 后立即返回$ bin/flink savepoint :jobId [:targetDirectory] -detached// 使用 savepoint 停止作业$ bin/flink stop --type [native/canonical] --savepointPath [:targetDirectory] :jobId// 从 savepoint 恢复$ bin/flink run -s :savepointPath [:runArgs]// 删除 savepoint$ bin/flink savepoint -d :savepointPath\n在 触发 savepoint 时，我们可以指定格式，两种格式的区别是：\n\ncanonical（标准格式）：在任何存储都保持统一格式，重在保证兼容性。\n\nnative（原生格式）：标准格式创建和恢复都很慢，原生格式是以特定的状态后端的格式生成，可以更快的创建和恢复。\n\n\nCheckpoint 与 Savepoint 区别这是面试最常见的问题之一，有了 checkpoint，为什么还需要 savepoint？或者说两者之间有什么区别？\n从概念上来讲，Checkpoint 类似数据库的恢复日志，而 Savepoint 类似数据库的备份。Checkpoint 主要用于作业故障的恢复，它的管理和删除也都是 Flink 内部处理，用户不需要过多关注。Savepoint 主要用于有计划的手动运维，例如升级 Flink 版本。它的创建、删除操作都需要用户手动执行。\n下面是官方文档给出的 Checkpoint 和 Savepoint 支持的操作。✓表示完全支持，x表示不支持，!表示目前有效，但没有正式保证支持，使用时存在一定风险。\n\n\n\n操作\n标准 Savepoint\n原生 Savepoint\n对齐 Checkpoint\n非对齐 Checkpoint\n\n\n\n\n更换状态后端\n✓\nx\nx\nx\n\n\nState Processor API (写)\n✓\nx\nx\nx\n\n\nState Processor API (读)\n✓\n!\n!\nx\n\n\n自包含和可移动\n✓\n✓\nx\nx\n\n\nSchema 变更\n✓\n!\n!\n!\n\n\n任意 job 升级\n✓\n✓\n✓\nx\n\n\n非任意 job 升级\n✓\n✓\n✓\n✓\n\n\nFlink 小版本升级\n✓\n✓\n✓\nx\n\n\nFlink bug/patch 版本升级\n✓\n✓\n✓\n✓\n\n\n扩缩容\n✓\n✓\n✓\n✓\n\n\n\n总结本文我们介绍了 Flink 是如何做容错的，分别介绍了 Checkpoint 和 Savepoint，以及它们之间的区别。本文多次提到了 Checkpoint 和 Savepoint 依赖的稳定存储，我会在下一篇文章进行详细的介绍。\n","tags":["Flink"]},{"title":"Flink学习笔记：状态后端","url":"/2025/08/24/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF/","content":"我们继续来聊 Flink 容错相关的内容。前面在介绍 Checkpoint 和 Savepoint 时提到了 State 的稳定存储，那究竟如何存储以及存储在什么地方呢？相信通过读完本文之后，你会有答案。\nState Backend 分类在 Flink 中状态后端（State Backend）是用来管理状态如何存储的。当前内置了两种 State Backend，分别是 HashMapStateBackend 和 EmbeddedRocksDBStateBackend。Flink 默认使用的是 HashMapStateBackend。\nHashMapStateBackend在 HashMapBackend 中，数据是以 Java 对象的形式存储的，它适用于有较大 State，较长 window 和 较大 key/value 状态的场景。同时适用于高可用场景。在使用 HashMapStateBackend 时，建议把 managed memory 设置为 0，以此来增加用户代码可使用的内存。\nEmbeddedRocksDBStateBackend对于 EmbeddedRocksDBStateBackend 而言，数据是存储在 RocksDB 中的，在存储之前，要对数据进行序列化。EmbeddedRocksDBStateBackend 也存在一定局限性，那就是最大只能支持每个 key/value 存储 2^31 字节大小的数据，这是当前 RocksDB JNI 的限制。\nEmbeddedRocksDBStateBackend 也有一定的优势，其一是它是目前唯一支持增量 Checkpoint 的 State Backend。其二是因为它是外部存储，因此它可以支持非常大的 State，非常长的窗口。\n增量快照只包含自上一次快照完成后被修改的记录，所以增量快照的一大优点就是可以显著减少快照的耗时。在恢复时间上，要分情况讨论，如果瓶颈在网络带宽，那么增量快照的恢复时间要比全量快照更长，因为增量快照包含的多个 sst 文件之间可能存在重复数据。如果瓶颈在 CPU 或 IO，那增量快照恢复时间更短，因为增量快照不需要恢复不需要解析 Flink 统一的存储格式来重建本地的 RocksDB 表，而是直接基于 sst 文件加载。\nCheckpoint 存储类型了解了 State Backend 分类之后，我们再来看 Checkpoint 的存储类型。它也分为两类：JobManagerCheckpointStorage 和 FileSystemCheckpointStorage。\nJobManagerCheckpointStorageJobManagerCheckpointStorage 是将快照存储在 JobManager 的堆内存中。JobManagerCheckpointStorage 在使用时有一定限制：\n\n默认每个 State 大小最大为 5MB\n\n总的状态大小不能超过 JobManager 内存\n\n\n基于这些限制，JobManagerCheckpointStorage 只适用于本地的开发和调试。\nFileSystemCheckpointStorageFileSystemCheckpointStorage 是将状态数据保存在外部存储中，要适用 FileSystemCheckpointStorage，需要配置文件系统的 URL。例如：“hdfs://namenode:40010/flink/checkpoints”。而元数据则存储在 JobManager 的内存中。\nCheckpoint 存储设置有了前面 State Backend 和 存储类型的分类之后，我们就可以将其进行组合，得到最终 Checkpoint 的存储了。\n目前共有三种组合，也对应了旧版本的三种 State Backend。\nMemoryStateBackendMemoryStateBackend 对应了 HashMapStateBackend 和 JobManagerCheckpointStorage 的组合。\n设置方法为\nstate.backend: hashmap# Optional, Flink will automatically default to JobManagerCheckpointStorage# when no checkpoint directory is specified.execution.checkpointing.storage: jobmanager\n或\nConfiguration config = new Configuration();config.set(StateBackendOptions.STATE_BACKEND, &quot;hashmap&quot;);config.set(CheckpointingOptions.CHECKPOINT_STORAGE, &quot;jobmanager&quot;);env.configure(config);\nFsStateBackendFsStateBackend 对应了 HashMapStateBackend 和 FileSystemCheckpointStorage 的组合。\n它的设置方法为：\nstate.backend: hashmapexecution.checkpointing.dir: file:///checkpoint-dir/# Optional, Flink will automatically default to FileSystemCheckpointStorage# when a checkpoint directory is specified.execution.checkpointing.storage: filesystem\n或\nConfiguration config = new Configuration();config.set(StateBackendOptions.STATE_BACKEND, &quot;hashmap&quot;);config.set(CheckpointingOptions.CHECKPOINT_STORAGE, &quot;filesystem&quot;);config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, &quot;file:///checkpoint-dir&quot;);env.configure(config);// Advanced FsStateBackend configurations, such as write buffer size// can be set manually by using CheckpointingOptions.config.set(CheckpointingOptions.FS_WRITE_BUFFER_SIZE, 4 * 1024);env.configure(config);\nRocksDBStateBackendRocksDBStateBackend 对应了 EmbeddedRocksDBStateBackend 和 FileSystemCheckpointStorage 的组合。\n它的设置方法为\nstate.backend: rocksdbexecution.checkpointing.dir: file:///checkpoint-dir/# Optional, Flink will automatically default to FileSystemCheckpointStorage# when a checkpoint directory is specified.execution.checkpointing.storage: filesystem\n或\nConfiguration config = new Configuration();config.set(StateBackendOptions.STATE_BACKEND, &quot;rocksdb&quot;);config.set(CheckpointingOptions.CHECKPOINT_STORAGE, &quot;filesystem&quot;);config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, &quot;file:///checkpoint-dir&quot;);env.configure(config);// If you manually passed FsStateBackend into the RocksDBStateBackend constructor// to specify advanced checkpointing configurations such as write buffer size,// you can achieve the same results by using CheckpointingOptions.config.set(CheckpointingOptions.FS_WRITE_BUFFER_SIZE, 4 * 1024);env.configure(config);\nState 序列化与反序列化我们前面在创建 State 的描述符时，指定了 State 的类型，这其实就是告诉 Flink 应该如何去序列化我们的 State。当然，也可以自定义 State 序列化器，自定义序列化器需要 TypeSerializer，然后在创建描述符时指定。\npublic class CustomTypeSerializer extends TypeSerializer&lt;Tuple2&lt;String, Integer&gt;&gt; &#123;...&#125;;ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =    new ListStateDescriptor&lt;&gt;(        &quot;state-name&quot;,        new CustomTypeSerializer());checkpointedState = getRuntimeContext().getListState(descriptor);\nFlink 中状态分为两种类型，一种是基于 Heap，一种是不基于 Heap。\nHeap state backends首先看基于 Heap 的，HashMapStateBackend 是基于 Heap 的。\n\nHeap state backend 存在本地的状态后端中的是非序列化的数据，当触发 Checkpoint / Savepoint 时，会用指定的序列化器将数据序列化，然后存储到指定的稳定存储中。\n如果我们对程序进行了升级，这时要从 State 恢复的话，需要先将稳定存储中的数据进行反序列化，然后将结果加载到 TM 的内存中，供 user code 使用。\nOff-heap state backendsEmbeddedRocksDBStateBackend 就是一种不基于 Heap 的状态。\n\n不基于 Heap 的状态在写入本地 State 时就会进行序列化，序列化后的数据会写入到堆外内存。在触发 Checkpoint 时，就只是把数据文件转存到稳定存储中。\n当我们的任务完成升级后，会先将二进制文件恢复到 TM 的内存中，这里是一个文件加载的过程。当我们要使用 State 时，才会进行反序列化，注意这里只会对使用到的 State 进行反序列化读取以及后续的更新，没有使用到的还是保持旧版本的数据。\n总结本文我们重点介绍了状态后端的存储。State Backend 分为 HashMapStateBackend 和 EmbeddedRocksDBStateBackend，其存储类型又分为 JobManagerCheckpointStorage 和 FileSystemCheckpointStorage。最终我们会有三种不同的状态后端：MemoryStateBackend、FsStateBackend 和 RocksDBStateBackend。最后我们还介绍了 State 的两种不同的序列化。\n相信通过本文的介绍，你已经可以回答开篇的问题了。\n","tags":["Flink"]},{"title":"Flink学习笔记：状态类型和应用","url":"/2025/08/04/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%BA%94%E7%94%A8/","content":"Flink 被广泛应用的原因，除了我们前面提到的对时间以及窗口的应用之外，另一点就是它强大的容错机制，以及对 Exactly Once 的支持。\n今天就来了解一下 Flink 的状态以及应用，首先第一个问题是：什么是有状态计算？\n基本概念在数据流处理中，大部分操作都是每次只处理一个事件，比如对输入的数据进行结构化解析，这类操作我们称为无状态计算。而有些操作则需要记住多个事件并进行处理，比如前面我们在窗口中对数据做的求和操作，这类操作我们称之为有状态计算。\n在 Flink 中，状态的另一个重要作用是用来做故障恢复，故障恢复主要依赖于 checkpoint 和 savepoint。当我们使用状态时，通常需要从 State Backend 读取。\n通过介绍有状态计算的基本概念，我们又引出了 checkpoint、State Backend 等概念，下面我们再来一一解释。\n状态分类Flink 状态分类可以参考下图\n\n首先是分为 Raw State 和 Managed State 两大类，我们分别从管理方式、数据类型、适用场景这三个方面来看它们的区别\n\n\n\n\nRaw State\nManaged State\n\n\n\n\n管理方式\n开发者自行管理，需要手动序列化和反序列化\n由 Flink Runtime 管理，自动存储和恢复数据\n\n\n数据类型\n仅支持 byte 数组\n支持 value, list, map\n\n\n适用场景\n需要自定义 Operator\n支持大部分计算场景\n\n\n\nManaged State 又分为 Keyed State 和 Operator State 两类，下面我们详细介绍这两类状态。\nKeyed StateKeyed State 只能用在 KeyedStream 上，也就是在使用前，要先对数据流进行 keyBy 操作。Keyed State 支持以下几种状态类型：\n\nValueState：保存一个值，可以通过 update() 方法更新，通过 value() 方法获取保存的值。\n\nListState：保存一个 list，可以通过 add() 或 addAll() 方法向 list 中添加元素，也可以通过 update() 直接覆盖。使用 get() 方法获取整个列表。\n\nReducingState：保存一个值，表示添加到状态所有值的聚合，使用 add() 方法添加元素，使用 get() 方法获取保存的值。\n\nAggregatingState&lt;IN, OUT&gt;：保存一个值，与 ReducingState 不同的是，输入和输出的元素类型可以不同。\n\nMapState&lt;UK, UV&gt;：保存一个 map，可以使用 put() 或 putAll() 添加键值对，使用 get() 获取值。\n\n\n在知道了各个类型的 Keyed State 怎么用之后，我们再来看如何创建一个 Keyed State。以 ValueState 为例。\nValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =        new ValueStateDescriptor&lt;&gt;(                &quot;average&quot;,                TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum = getRuntimeContext().getState(descriptor);\n要想创建一个 State，必须先创建一个 StateDescriptor，然后通过 RuntimeContext 来获取 State。每个 State 都对应一种 StateDescriptor。\nValueState&lt;T&gt; getState(ValueStateDescriptor&lt;T&gt;)ReducingState&lt;T&gt; getReducingState(ReducingStateDescriptor&lt;T&gt;)ListState&lt;T&gt; getListState(ListStateDescriptor&lt;T&gt;)AggregatingState&lt;IN, OUT&gt; getAggregatingState(AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;)MapState&lt;UK, UV&gt; getMapState(MapStateDescriptor&lt;UK, UV&gt;)\nOperator State算子状态也称为非 keyed 状态，是绑定到一个并行算子实例的状态。State 需要支持重新分布。 最典型的是 Kafka Connector 中，维护了一个 topic partitions 和 offset 的 map 作为一个算子状态。\n和 Keyed State 类似，想要创建一个 Operator State，同样也需要一个 StateDescriptor，同时，需要实现 CheckpointedFunction，它提供了两个方法，分别是在 checkpoint 时 调用的 snapshotState() 和 自定义函数初始化时调用的 initializeState()。\nTalk is cheap, show me your code!\n我们来看 Flink 官方文档提供的 Demo\npublic class BufferingSink        implements SinkFunction&lt;Tuple2&lt;String, Integer&gt;&gt;,                   CheckpointedFunction &#123;    private final int threshold;    private transient ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;    private List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;    public BufferingSink(int threshold) &#123;        this.threshold = threshold;        this.bufferedElements = new ArrayList&lt;&gt;();    &#125;    @Override    public void invoke(Tuple2&lt;String, Integer&gt; value, Context context) throws Exception &#123;        bufferedElements.add(value);        if (bufferedElements.size() &gt;= threshold) &#123;            for (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;                // send it to the sink            &#125;            bufferedElements.clear();        &#125;    &#125;    @Override    public void snapshotState(FunctionSnapshotContext context) throws Exception &#123;        checkpointedState.update(bufferedElements);    &#125;    @Override    public void initializeState(FunctionInitializationContext context) throws Exception &#123;        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =            new ListStateDescriptor&lt;&gt;(                &quot;buffered-elements&quot;,                TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));        checkpointedState = context.getOperatorStateStore().getListState(descriptor);        if (context.isRestored()) &#123;            for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;                bufferedElements.add(element);            &#125;        &#125;    &#125;&#125;\n在这个例子中，我们在 initializeState 方法中使用 getOperatorStateStore().getListState() 创建了一个 ListState，然后将数据缓存到这个 list 中，当缓存数据大小超过一个阈值时，再统一发送到下游。\n这里还有一个方法值得注意，就是 isRestored()，它是用来判断当前任务是否是从故障中恢复的，如果是，我们需要执行故障恢复相关的逻辑。在这个例子中就是把 state 的数据恢复到本地的变量中。\nBroadcast State了解了如何创建和使用 Operator State 之后，我们再来看一种特殊的 Operator State —— Broadcast State。\nBroadcast State 本身是类似于 Map 类型的格式，使用时需要指定 key 和 value 的类型。它的作用是将一条数据流的数据广播到下游算子的各个节点。\nBroadcast State 的一个比较常见的作用就是大流关联小流。例如，我们有一个订单流，需要关联商品详情，这时可以把商品详情的流作为 broadcast 流进行广播，这样在每个 TaskManager 中会有一份商品详情数据，订单流就可以直接查询 broadcast 的数据，不需要再访问 MySQL 数据库来做查询操作。\n那么具体要怎么实现呢？其实也很简单，可以看下面这段代码\nMapStateDescriptor&lt;String, Product&gt; productStateDescriptor =        new MapStateDescriptor&lt;&gt;(&quot;productBroadcastState&quot;, String.class, Product.class);BroadcastStream&lt;Product&gt; broadcastProductStream = productStream.broadcast(productStateDescriptor);BroadcastConnectedStream&lt;Order, Product&gt; connectedStreams = orderStream.connect(broadcastProductStream);\n拿到 BroadcastConnectedStream 之后，我们就可以调用 process 方法进行处理了。完整的代码我放到 GitHub 上了。感兴趣的可以查看。\n在使用 Broadcast State 的时需要注意，目前 RocksDB 不支持保存 Broadcast State，因此，广播流吞吐量必须要小，并且 Flink 任务要预留足够的内存。\n聊完了 Broadcast State，我们再来看看 Operator State 是如何进行重新分布的。正常 Operator State 支持两种重新分布的方式，按照不同的方式，我们可以划分为 ListState 和 UnionListState。\n\nListState：所有的 element 均匀分布到 task 上\n\nUnionListState：每个 element 都要在所有的 task 上\n\n\n\nBroadcast State 由于本身就是广播状态，因此重新分布后仍然是需要进行广播的。\n状态有效期最后再来扩展一个知识点，就是状态的有效期。在 Flink 中，只有 Keyed State 支持有效期。具体使用方法如下。\nStateTtlConfig ttlConfig = StateTtlConfig    .newBuilder(Duration.ofSeconds(1))    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)    .build();    ValueStateDescriptor&lt;String&gt; stateDescriptor = new ValueStateDescriptor&lt;&gt;(&quot;text state&quot;, String.class);stateDescriptor.enableTimeToLive(ttlConfig);\n这里有三个属性，我们分别来解释一下，首先第一个是过期时间，在调用 newBuilder 时就要传入。\n第二个是 UpdateType，也就是更新策略，默认是 OnCreateAndWrite，表示在创建和写入时更新，也可以设置为 OnReadAndWrite，表示在读取和写入时更新。\n第三个是可见性，默认是 NeverReturnExpired，即不返回过期数据，也可以设置为 ReturnExpiredIfNotCleanedUp，表示会返回过期但未被清理的数据。\n状态数据清理策略也分为两种：一种是做全量快照时进行清理，创建 ttl 时调用 cleanupFullSnapshot() 方法即可。\n另一种是增量数据清理，在访问或处理状态时，状态后端保留一个所有状态的惰性迭代器，每次清理时选择已经过期的数据进行清理。设置方法时在创建 ttl 时调用 cleanupIncrementally(10, true) ，可以看到它提供两个参数，第一个参数是设置每次检查的条数，默认是5。第二个参数是是否在处理每条记录时都触发清理，默认是 false。\n总结最后我们来总结一下，本文我们主要介绍了 Flink 的状态及应用，首先介绍有状态计算的概念。接着重点学习了 Keyed State 和 Operator State。我们通过一个表格来进行总结。\n\n\n\n\nKeyed State\nOperator State\n\n\n\n\n使用算子类型\n只能被用于 KeyedStream 中的Operator 上\n可以被用于所有 Operator\n\n\n状态分配\n每个 Key 对应一个状态，单个 Operator 中可以包含多个 Key\n单个 Operator 对应一个状态\n\n\n创建和访问方式\n重写 RichFunction，通过访问 RuntimeContext 对象获取\n实现 CheckpointedFunction 或 ListCheckpointed 接口\n\n\n横向拓展\n状态随着 Key 自动在多个算子 Task 上迁移\n有多种重新分配的方式：均匀分布。将所有状态合并再分发到每个实例上\n\n\n支持数据类型\nValueState, ListState, ReducingState, AggregatingState, MapState\nListState, UnionListState, Broadcast State\n\n\n\n最后，我们又介绍了状态有效期的定义和使用方法。有了状态之后，Flink 就可以为我们提供非常强大的容错能力了，具体怎么做的我们后面再聊。\n","tags":["Flink"]},{"title":"Flink源码阅读：Checkpoint机制（上）","url":"/2025/12/09/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ACheckpoint%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"前文我们梳理了 Flink 状态管理相关的源码，我们知道，状态是要与 Checkpoint 配合使用的。因此，本文我们就一起来看一下 Checkpoint 相关的源码。\n写在前面在Flink学习笔记：如何做容错一文中，我们介绍了 Flink 的 Checkpoint 机制。Checkpoint 分为 EXACTLY_ONCE 和 AT_LEAST_ONCE 两种模式。\n我们一起回顾一下一次完整的 Checkpoint 具体流程：Checkpoint 是由 CheckpointCoordinator 触发，Source 节点收到触发请求后，会将 State 进行持久化，同时向下游发送 Barrier 消息，下游节点收到 Barrier 消息后，也同样对 State 进行持久化和发送 Barrier 消息。当所有节点都完成持久化过程后 CheckpointCoordinator 会将一些元数据进行持久化。\n带着这些背景知识，我们再来梳理一下 Checkpoint 相关的代码。\nJobManager 端触发流程JobManager 在调用 DefaultExecutionGraphBuilder.buildGraph 生成 ExecutionGraph 之后，会调用 executionGraph.enableCheckpointing 方法来设置 Checkpoint 相关的配置，这个方法中创建了 CheckpointCoordinator 并注册了 CheckpointCoordinatorDeActivator 这个监听，它负责启动和停止 Checkpoint 的调度。\n当作业变成 RUNNING 状态时，CheckpointCoordinator 会部署一个定时任务 ScheduledTrigger，这个定时任务就是用来周期性的触发 Checkpoint。\n触发 Checkpoint 的核心逻辑在  CheckpointCoordinator.startTriggeringCheckpoint 这个方法中。这个方法中使用了多个 CompletableFuture 来完成整个流程的编排。具体流程见下图（图中不同颜色代表着使用不同线程池执行）。\n\n\ncheckpointPlanFuture：这是生成 Checkpoint 执行计划的 Future，Checkpoint Plan 中维护了三个关键的集合：tasksToTrigger、tasksToWaitFor 和 tasksToCommitTo。tasksToTrigger 是所有的 Source 节点，表示触发 Checkpoint 的节点，另外两个集合都包含了全部节点，分别表示等待进行 Checkpoint 的节点和等待提交的节点。\n\npendingCheckpointCompletableFuture：生成完 Checkpoint Plan 之后，会创建 pendingCheckpointCompletableFuture，这个 Future 中有两个执行任务，分别是生成自增的 CheckpointID 和 创建 PendingCheckpoint。PendingCheckpoint 中维护了等待完成的 task 列表，当所有 task 都确认完成之后，PendingCheckpoint 会变成 CompletedCheckpoint。\n\ncoordinatorCheckpointsComplete：这个 Future 也有两个任务，第一个是初始化存储路径，第二个是触发所有 OperatorCoordinator Checkpoint，并确认它们的状态。\n\nmasterStatesComplete：触发快照所有的 Master Hook，这一步主要是 CheckpointCoordinator 用来收集 JobManager 级别状态。\n\nmasterTriggerCompletionPromise：在 masterStatesComplete 和 coordinatorCheckpointsComplete 都执行完成后，会开始执行 masterTriggerCompletionPromise。masterTriggerCompletionPromise 的任务是调用 triggerCheckpointRequest 来产生 Barrier 消息。具体的触发流程见下图。\n\n\n\n至此，JobManager 端的触发流程就完成了，接下来就到了 TaskManager 端了。\nTaskManager 端执行流程进入 TaskExecutor 后，具体调用过程如下图。\n\nTaskManager 的核心逻辑在 SubtaskCheckpointCoordinatorImpl.checkpointState 方法中。这个方法中的注释也很详细，整体上分为6个步骤：\n\n判断是否是需要终止的 Checkpoint，如果是，则向下游发送取消 Checkpoint 的广播消息。\n\n做一些前置的准备工作，这一步通常情况下是一个空实现。\n\n向下游发送 Barrier 消息。\n\n注册 Alignment timer，当 aligned 超时时，转换为 unaligned。\n\n通知 StateWriter，当前 Subtask 对输出通道的写入已经完成，并提交状态句柄。\n\n异步执行状态写入并完成上报。\n\n\n下面我们来关注几个重点的步骤。\nBarrier 消息在步骤2中，首先是创建 Barrier，Barrier 消息包括三个部分\n// checkpointIdprivate final long id;// 时间戳private final long timestamp;// checkpoint 相关参数，包括对齐类型、checkpoint 类型、目前地址private final CheckpointOptions checkpointOptions;\n生成 Barrier 之后，会调用 operatorChain.broadcastEvent 进行广播消息。这里广播消息就是向下游所有的节点的所有 ResultSubpartition 发送。\n状态写入SubtaskCheckpointCoordinatorImpl.takeSnapshotSync 方法用来构建 OperatorSnapshotFutures 中的四个 Future，每个 Future 的任务是为不同类型的 State 提供写入逻辑。\n@Nonnull private RunnableFuture&lt;SnapshotResult&lt;KeyedStateHandle&gt;&gt; keyedStateManagedFuture;@Nonnull private RunnableFuture&lt;SnapshotResult&lt;KeyedStateHandle&gt;&gt; keyedStateRawFuture;@Nonnull private RunnableFuture&lt;SnapshotResult&lt;OperatorStateHandle&gt;&gt; operatorStateManagedFuture;@Nonnull private RunnableFuture&lt;SnapshotResult&lt;OperatorStateHandle&gt;&gt; operatorStateRawFuture;\n在底层逻辑中，会为每个 Operator 设置对应的 State 的 Future。具体调用流程如下\n\n设置好这些 Future 之后，会在 finishAndReportAsync 方法中创建 AsyncCheckpointRunnable 线程调用 get 来获取执行结果，拿到执行结果后会将 Checkpoint 信息上报给 CheckpointCoordinator。\n\nJobManager 端确认流程TaskManager 通过调用 checkpointCoordinatorGateway.acknowledgeCheckpoint 上报 Checkpoint 信息后，流程就又回到 JobManager 了。\nJobManager 的确认流程主要做了两件事：\n\n将 pendingCheckpoint 转换成 completedCheckpoint，在这个转换过程中，还做了清理过期 Checkpoint 和持久化元数据等操作。\n\n向所有 commit 的 Task 发送 Checkpoint 完成的通知。收到这个通知后，大部分 Task 没有什么特殊逻辑，也有一部分 Source 或者 Sink 会做提交事务等操作。\n\n\n至此，JobManager 和 Source 端算子的一次 Checkpoint 就完成了。接下来我们再看一下非 Source 节点是如何做 Checkpoint 的。\n非 Source 节点处理流程非 Source 节点处理 Barrier 的入口和处理业务数据的入口相同，都是 StreamTask.processInput 方法。我们还是先来看具体的调用流程。\n\n跟着调用链路，我们一路找到了 processBarrier 方法，这里区分了两个 barrierHandler。SingleCheckpointBarrierHandler 负责处理 EXACTLY_ONCE 语义，CheckpointBarrierTracker 负责处理 AT_LEAST_ONCE 语义。\nEXACTLY_ONCEEXACTLY_ONCE 在处理 Barrier 的逻辑如下：\n\n如果只有一个 channel，就立即触发 Checkpoint。\n\n如果有多个 channel，分为三种情况\na) 如果收到的是第一个 channel，标记开始进行 barrier 对齐，并阻塞 channel。\nb) 如果不是第一个 channel，也不是最后一个 channel，只对 channel 进行阻塞。\nc) 如果收到最后一个 channel，就会触发 Checkpoint，并取消所有 channel 阻塞状态。\n\n\n这里触发的逻辑与 Source 节点相同，通过调用链路可以一直找到 performCheckpoint。\nAT_LEAST_ONCEAT_LEAST_ONCE 处理 Barrier 的逻辑如下：\n\n如果只有一个 channel，就立即触发 Checkpoint。\n\n如果有多个 channel，同样分为三种情况\na) 如果收到的是第一个 channel，则更新当前 checkpointID，标记开始 barrier 对齐。\nb) 如果收到的不是第一个 channel，也不是最后一个 channel，就只做计数。\nc) 如果收到的是最后一个 channel，就会开始触发 Checkpoint。\n\n\n这里触发逻辑也是调用 performCheckpoint，与 Source 节点逻辑相同。\n总结本文我们梳理了 Checkpoint 的源码逻辑。最开始由 JobManager 中的 CheckpointCoordinator 进行调度，并向 TaskManager 发送触发请求。Source 节点收到请求后会向下游发送 Barrier 消息然后写入状态数据和上报 Checkpoint 信息。CheckpointCoordinator 收集完确认消息后，会持久化元数据并通知所有 Task 完成 commit。最后还分别介绍了 EXACTLY_ONCE 和 AT_LEAST_ONCE 模式下非 Source 节点的处理逻辑。\n这里埋一个 Hook，状态数据写入逻辑的细节我们没有深入了解，会在下篇进行深入分析。\n","tags":["Flink"]},{"title":"Flink学习笔记：窗口","url":"/2025/07/19/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%AA%97%E5%8F%A3/","content":"在前文中，我们已经了解了 Flink 时间和 Watermark 两个概念，今天就来聊一下 Flink 实时数据处理的另一个核心概念：窗口。\n所谓窗口，可以理解为是对数据流中的一段数据进行处理的方法。那我们为什么需要窗口呢？在生产环境中，数据流中的数据往往是源源不断的，如果我们想要获取一段时间内数据的一些统计指标（最大值/最小值/平均值），这时就需要利用窗口来将数据收集起来，然后再进行计算。\n如果按照处理的数据流的类型来划分，Flink 中的窗口可以分为 Keyed Window 和 Non-Keyed Window，其中 Keyed Window 是用来处理按照 key 分片之后的数据流，也就是需要先调用 keyBy 方法，再调用 window 方法。而 Non-Keyed Window 处理的则是未按照 key 分片的数据流，在使用的时候直接调用 windowAll 方法。\n在官方文档中，列举了两种窗口的使用方法。\nKeyed Window\nstream       .keyBy(...)               &lt;-  仅 keyed 窗口需要       .window(...)              &lt;-  必填项：&quot;assigner&quot;      [.trigger(...)]            &lt;-  可选项：&quot;trigger&quot; (省略则使用默认 trigger)      [.evictor(...)]            &lt;-  可选项：&quot;evictor&quot; (省略则不使用 evictor)      [.allowedLateness(...)]    &lt;-  可选项：&quot;lateness&quot; (省略则为 0)      [.sideOutputLateData(...)] &lt;-  可选项：&quot;output tag&quot; (省略则不对迟到数据使用 side output)       .reduce/aggregate/apply()      &lt;-  必填项：&quot;function&quot;      [.getSideOutput(...)]      &lt;-  可选项：&quot;output tag&quot;\nNon-Keyed Window\nstream       .windowAll(...)           &lt;-  必填项：&quot;assigner&quot;      [.trigger(...)]            &lt;-  可选项：&quot;trigger&quot; (else default trigger)      [.evictor(...)]            &lt;-  可选项：&quot;evictor&quot; (else no evictor)      [.allowedLateness(...)]    &lt;-  可选项：&quot;lateness&quot; (else zero)      [.sideOutputLateData(...)] &lt;-  可选项：&quot;output tag&quot; (else no side output for late data)       .reduce/aggregate/apply()      &lt;-  必填项：&quot;function&quot;      [.getSideOutput(...)]      &lt;-  可选项：&quot;output tag&quot;\n窗口分类在上面的示例中，调用 window 和 windowAll 方法时，需要传入 assigner 参数，Window Assigner 是用来定义如何将数据流中的元素划分到各个窗口中。\n下面我们再对窗口进行一次分类，首先是根据划分依据，可以分为 time window 和 count window，time window 是根据处理数据的时间来划分，count window 则是根据处理数据的数量来划分。接着，我们再根据划分规则来分类，这里又可以将窗口分为滚动窗口（Tumbling Windows）、滑动窗口（Sliding Windows）、会话窗口（Session Windows）和全局窗口（Global Windows），需要注意的是，后面两种窗口不支持 count window。我们用一张图来表示窗口的分类会更加清晰。\n\n我们在日常数据处理中，最常用的就是时间窗口，接下来就来详细了解下时间窗口的四种类型。\nSliding Windows滑动窗口是以一个固定的步长不断向前滑动的窗口，滑动过程中，窗口的大小是保持不变的。在滑动窗口中，一个元素是可以被多个窗口计算的。\n\n在代码中，可以通过 SlidingEventTimeWindows 类来定义滑动窗口，具体使用方法可以参考下面这个简单的 demo（这里多说一句，旧版本 Flink 传的是参数是 Time，新版本传的是 Duration）：\nDataStream&lt;T&gt; input = ...;// 滑动 event-time 窗口input    .keyBy(&lt;key selector&gt;)    .window(SlidingEventTimeWindows.of(Duration.ofSeconds(10), Duration.ofSeconds(5)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);// 滑动 processing-time 窗口input    .keyBy(&lt;key selector&gt;)    .window(SlidingProcessingTimeWindows.of(Duration.ofSeconds(10), Duration.ofSeconds(5)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);// 滑动 processing-time 窗口，偏移量为 -8 小时input    .keyBy(&lt;key selector&gt;)    .window(SlidingProcessingTimeWindows.of(Duration.ofHours(12), Duration.ofHours(1), Duration.ofHours(-8)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);\n上面代码中，如果不传 offset 参数的话，窗口开始时间会和 Linux 的 epoch 对齐，如果想要一些偏移量的话，就可以通过 offset 参数来控制（第二种方法）。在 demo 中，偏移量设置为 -8 小时，也就是使用东 8 区时间。\nTumbling Windows了解了滑动窗口之后，我们再来看滚动窗口。滚动窗口可以认为是一种特殊的滑动窗口（Window Size = Window Slide）。滚动窗口之间是没有重叠的，也就是说，每个元素只能落到一个窗口中。\n\n在代码中通过使用 TumblingEventTimeWindows 来定义。\nDataStream&lt;T&gt; input = ...;// 滚动 event-time 窗口input    .keyBy(&lt;key selector&gt;)    .window(TumblingEventTimeWindows.of(Duration.ofSeconds(5)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);// 滚动 processing-time 窗口input    .keyBy(&lt;key selector&gt;)    .window(TumblingProcessingTimeWindows.of(Duration.ofSeconds(5)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);// 长度为一天的滚动 event-time 窗口， 偏移量为 -8 小时。input    .keyBy(&lt;key selector&gt;)    .window(TumblingEventTimeWindows.of(Duration.ofDays(1), Duration.ofHours(-8)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);\nSession Windows会话窗口和前两种不同，它的窗口大小是不固定的，也没有固定的开始和结束时间。当一个窗口超过 Seesion gap 没有收到数据之后，窗口就会关闭。Flink 也支持动态定义判断会话窗口不活跃的条件。\n\nDataStream&lt;T&gt; input = ...;// 设置了固定间隔的 event-time 会话窗口input    .keyBy(&lt;key selector&gt;)    .window(EventTimeSessionWindows.withGap(Duration.ofMinutes(10)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);// 设置了动态间隔的 event-time 会话窗口input    .keyBy(&lt;key selector&gt;)    .window(EventTimeSessionWindows.withDynamicGap((element) -&gt; &#123;        // 决定并返回会话间隔    &#125;))    .&lt;windowed transformation&gt;(&lt;window function&gt;);// 设置了固定间隔的 processing-time session 窗口input    .keyBy(&lt;key selector&gt;)    .window(ProcessingTimeSessionWindows.withGap(Duration.ofMinutes(10)))    .&lt;windowed transformation&gt;(&lt;window function&gt;);// 设置了动态间隔的 processing-time 会话窗口input    .keyBy(&lt;key selector&gt;)    .window(ProcessingTimeSessionWindows.withDynamicGap((element) -&gt; &#123;        // 决定并返回会话间隔    &#125;))    .&lt;windowed transformation&gt;(&lt;window function&gt;);\nGlobal Windows最后是全局窗口，它将所有的key都写到一个窗口，并且必须要指定 trigger 才能触发窗口的计算。\n窗口函数现在我们已经知道如何划分窗口，或者说如何把指定元素放入对应的窗口中了。接下来的问题就是窗口中的数据要怎么处理。这就是窗口函数的职责了。Flink 支持三种窗口函数，分别是：ReduceFunction、AggregateFunction 和 ProcessWindowFunction。\nReduceFunctionReduceFunction 定义了如何把两条数据合并为一条。例如最常见的对 key 进行求和。\nDataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input    .keyBy(&lt;key selector&gt;)    .window(&lt;window assigner&gt;)    .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;      public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2) &#123;        return new Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1);      &#125;    &#125;);\n窗口输出的数据就是将多条相同 key 的数据求和后的数据。\nAggregateFunctionReduceFunction 其实是一种特殊的 AggregateFunction，AggregateFunction 的定义更加宽泛。它接收三个类型：IN（输入数据的类型）、ACC（累加器的类型）、OUT（输出数据的类型）。同时定义了四个方法：createAccumulator（创建一个累加器）、add（将一条数据加进累加器）、getResult（获取累加器结果）、merge（将两个累加器合并）\n下面这个例子展示了如何对输入数据进行求平均值。\nprivate static class AverageAggregate    implements AggregateFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;Long, Long&gt;, Double&gt; &#123;  @Override  public Tuple2&lt;Long, Long&gt; createAccumulator() &#123;    return new Tuple2&lt;&gt;(0L, 0L);  &#125;  @Override  public Tuple2&lt;Long, Long&gt; add(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator) &#123;    return new Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L);  &#125;  @Override  public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123;    return ((double) accumulator.f0) / accumulator.f1;  &#125;  @Override  public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b) &#123;    return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);  &#125;&#125;DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input    .keyBy(&lt;key selector&gt;)    .window(&lt;window assigner&gt;)    .aggregate(new AverageAggregate());\nProcessWindowFunctionProcessWindowFunction 执行效率不如前两者，因为它要获取窗口内所有的数据进行计算。但它也有另外两个窗口没有的能力，那就是在 ProcessWindowFunction 中可以通过 Context 获取到时间和状态信息。这样的能力带来的代价是大量的资源消耗，因此，为了减少不必要的资源消耗，我们通常将 ProcessWindowFunction 与 ReduceFunction 或 AggregateFunction 配合使用。我们来看一个例子。\nDataStream&lt;SensorReading&gt; input = ...;input  .keyBy(&lt;key selector&gt;)  .window(&lt;window assigner&gt;)  .reduce(new MyReduceFunction(), new MyProcessWindowFunction());// Function definitionsprivate static class MyReduceFunction implements ReduceFunction&lt;SensorReading&gt; &#123;  public SensorReading reduce(SensorReading r1, SensorReading r2) &#123;      return r1.value() &gt; r2.value() ? r2 : r1;  &#125;&#125;private static class MyProcessWindowFunction    extends ProcessWindowFunction&lt;SensorReading, Tuple2&lt;Long, SensorReading&gt;, String, TimeWindow&gt; &#123;  public void process(String key,                    Context context,                    Iterable&lt;SensorReading&gt; minReadings,                    Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out) &#123;      SensorReading min = minReadings.iterator().next();      out.collect(new Tuple2&lt;Long, SensorReading&gt;(context.window().getStart(), min));  &#125;&#125;\n在这个例子中，我们返回了窗口中最小的元素和窗口的开始时间。因为我们先做了聚合，所以在 ProcessFunction 中就不用把所有的数据都存储在 State 中，而是只存储聚合后的数据即可。\nTriggersTrigger 决定何时触发窗口计算，每个 Window Assigner 都有一个默认的 trigger。当然，Flink 也提供自定义 trigger，自定义 trigger 只需要实现 Trigger 接口，并且在 .trigger() 方法中调用即可（在文章开头的示例中提到过）。\nTrigger 接口提供了 5 个方法，分别是：\n\nonElement() 方法在接收到元素时调用\n\nonProcessingTime() 方法是在一个基于 ProcessTime 的 timer 触发时调用\n\nonEventTime() 方法是在一个基于 EventTime 的 timer 触发时调用\n\nonMerge() 方法在多个窗口合并时调用\n\nclear() 方法是在窗口被移除时调用\n\n\n前三个方法都会返回一个 TriggerResult，而这个 TriggerResult 的值就决定了窗口是否触发。TriggerResult 的值有以下四种：\n\nCONTINUE：什么也不做\n\nFIRE_AND_PURGE：触发计算并清空窗口内的元素\n\nFIRE：只触发计算，不清空窗口内元素\n\nPURGE：清空窗口内元素\n\n\nFlink 内置了多个 trigger，常见的有 EventTimeTrigger、ProcessingTimeTrigger、CountTrigger 和 PurgingTrigger。\nEvictorsEvictor 可以用于在 trigger 触发后、调用窗口函数之前或之后删除窗口内的元素。Evictor 接口提供了两个方法：\n\nevictBefore() 方法是在调用窗口函数之前调用\n\nevictAfter() 方法是在调用窗口函数之后调用\n\n\nFlink 内置了三种 Evictor：\n\nCountEvictor：记录用户设置的最大元素数量，当窗口内元素数量大于最大元素数量时，删除开头的元素\n\nDeltaEvictor：用户需要设置计算差值的方法，evictor 会计算最后一个元素与窗口内每个元素的差值差值，并将大于用户设置的 threshold 的元素删除\n\nTimeEvictor：用户需要指定窗口大小 windowSize，evictor 会计算窗口内元素最大的时间戳 maxTimestamp，将时间戳小于等于 maxTimestamp - windowSize 的元素清除\n\n\n关于 Trigger 和 Evictor，只看概念可能还比较迷惑。我们来看一个具体的例子：假设我们需要一个实时监控系统，当连续收到 5 个大于阈值的数据时，发送告警。最终窗口中只保留 10 条数据。\ntrigger 的实现应该是\npublic TriggerResult onElement(Tuple2&lt;String, Double&gt; item, long l, TimeWindow timeWindow, TriggerContext triggerContext) throws Exception &#123;    ValueState&lt;Integer&gt; state = triggerContext.getPartitionedState(new ValueStateDescriptor&lt;&gt;(stateName, Integer.class, 0));    if (item.f1 &gt; threshold) &#123;        int count = state.value() + 1;        state.update(count);        if (count &gt;= countThreshold) &#123;            state.clear();            return TriggerResult.FIRE;        &#125;    &#125; else &#123;        state.clear();    &#125;    return TriggerResult.CONTINUE;&#125;\nevictor 的实现为\npublic void evictAfter(Iterable&lt;TimestampedValue&lt;Tuple2&lt;String, Double&gt;&gt;&gt; elements, int size, TimeWindow timeWindow, EvictorContext evictorContext) &#123;    if (size &lt;= maxSize) &#123;        return;    &#125;    List&lt;TimestampedValue&lt;Tuple2&lt;String, Double&gt;&gt;&gt; elementList = new ArrayList&lt;&gt;();    elements.forEach(elementList::add);    int toEvict = size - maxSize;    List&lt;TimestampedValue&lt;Tuple2&lt;String, Double&gt;&gt;&gt; toRemove = elementList.subList(maxSize, size);    // 4. 通过迭代器删除需要移除的元素（关键：遍历原始迭代器，匹配并删除）    Iterator&lt;TimestampedValue&lt;Tuple2&lt;String, Double&gt;&gt;&gt; iterator = elements.iterator();    while (iterator.hasNext()) &#123;        TimestampedValue&lt;Tuple2&lt;String, Double&gt;&gt; element = iterator.next();        if (toRemove.contains(element)) &#123; // 匹配需要删除的元素            iterator.remove(); // 实际删除            toEvict--;            if (toEvict &lt;= 0) &#123;                break;            &#125;        &#125;    &#125;    System.out.println(&quot;Evictor finished: 保留了 &quot; + maxSize + &quot; 个元素&quot;);&#125;\n完整代码我放到 GitHub 了，感兴趣的可以看一下。\n总结最后总结一下，今天我们了解了 Flink 中窗口相关的概念，首先是窗口分类，然后是 Window Assginer（分别介绍了滑动窗口、滚动窗口、会话窗口和全局窗口）。接着又了解窗口处理函数，即怎么计算窗口中的数据。最后学习的 trigger 和 evictor 的作用和用法。\n","tags":["Flink"]},{"title":"Flink源码阅读：Checkpoint机制（下）","url":"/2025/12/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ACheckpoint%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"书接上回，前文我们梳理的 Checkpoint 机制的源码，但是对于如何写入状态数据并没有深入了解。今天就一起来梳理一下这部分代码。\n写在前面前面我们了解到在 StreamOperatorStateHandler.snapshotState 方法中会创建四个 Future，用来支持不同类型的状态写入。\nsnapshotInProgress.setKeyedStateRawFuture(snapshotContext.getKeyedStateStreamFuture());snapshotInProgress.setOperatorStateRawFuture(        snapshotContext.getOperatorStateStreamFuture());if (null != operatorStateBackend) &#123;    snapshotInProgress.setOperatorStateManagedFuture(            operatorStateBackend.snapshot(                    checkpointId, timestamp, factory, checkpointOptions));&#125;if (useAsyncState &amp;&amp; null != asyncKeyedStateBackend) &#123;    if (isCanonicalSavepoint(checkpointOptions.getCheckpointType())) &#123;        throw new UnsupportedOperationException(&quot;Not supported yet.&quot;);    &#125; else &#123;        snapshotInProgress.setKeyedStateManagedFuture(                asyncKeyedStateBackend.snapshot(                        checkpointId, timestamp, factory, checkpointOptions));    &#125;&#125;\n我们主要关心 ManagedState，ManagedState 都是调用 Snapshotable.snapshot 方法来写入数据的，下面具体看 KeyedState 和 OperatorState 的具体实现。\nKeyedStateKeyedState 我们以 HeapKeyedStateBackend 为例，这里先是创建了一个 SnapshotStrategyRunner 实例，SnapshotStrategyRunner 是一个快照策略的一个执行类，创建完成后就会调用 snapshot 方法。在这个 snapshot 方法中主要做了做了下面几件事：\n\n同步拷贝状态数据的引用。\n\n创建 Checkpoint 输出流 CheckpointStateOutputStream\n\n完成 Checkpoint 持久化\n\n返回元信息结果\n\n\n状态数据引用拷贝在 HeapSnapshotStrategy 的 syncPrepareResources 方法中调用了 HeapSnapshotResources.create 方法。这里有一个比较重要的参数是 registeredKVStates，它代表我们在业务代码中注册的状态数据表。\nValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =        new ValueStateDescriptor&lt;&gt;(                &quot;average&quot;,                TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));\n例如我们这样注册状态数据表，那么 registeredKVStates 的 key 就是 average，value 就是状态表，它通常是一个 CopyOnWriteStateTable。具体的状态数据引用拷贝的逻辑在 processSnapshotMetaInfoForAllStates 方法中。\nprivate static void processSnapshotMetaInfoForAllStates(        List&lt;StateMetaInfoSnapshot&gt; metaInfoSnapshots,        Map&lt;StateUID, StateSnapshot&gt; cowStateStableSnapshots,        Map&lt;StateUID, Integer&gt; stateNamesToId,        Map&lt;String, ? extends StateSnapshotRestore&gt; registeredStates,        StateMetaInfoSnapshot.BackendStateType stateType) &#123;    for (Map.Entry&lt;String, ? extends StateSnapshotRestore&gt; kvState :            registeredStates.entrySet()) &#123;        final StateUID stateUid = StateUID.of(kvState.getKey(), stateType);        stateNamesToId.put(stateUid, stateNamesToId.size());        StateSnapshotRestore state = kvState.getValue();        if (null != state) &#123;            final StateSnapshot stateSnapshot = state.stateSnapshot();            metaInfoSnapshots.add(stateSnapshot.getMetaInfoSnapshot());            cowStateStableSnapshots.put(stateUid, stateSnapshot);        &#125;    &#125;&#125;\n针对每个 State，这里都创建一个 CopyOnWriteStateTableSnapshot，然后存在 cowStateStableSnapshots 里。这里 CopyOnWriteStateTableSnapshot 就是拷贝数据的引用，因此可以同步执行。\n创建 CheckpointStateOutputStream创建 CheckpointStateOutputStream 的方法是 CheckpointStreamWithResultProvider.createSimpleStream，生产环境通常使用的是 FsCheckpointStateOutputStream。FsCheckpointStateOutputStream 中的参数如下：\n// 状态数据写入缓冲数组，数据先写到内存中，然后 flush 到磁盘private final byte[] writeBuffer;// 缓冲数组当前写入位置private int pos;// 文件输出流private volatile FSDataOutputStream outStream;// 内存中状态大小阈值，超过阈值会 flush 到磁盘，默认20KB，最大1MB// 目的是为了减少小文件数量private final int localStateThreshold;// checkpoint 基础路径private final Path basePath;// Flink 自己封装的文件系统private final FileSystem fs;// 状态数据完整路径private volatile Path statePath;// 相对路径private String relativeStatePath;// 是否已关闭private volatile boolean closed;// 是否允许使用相对路径private final boolean allowRelativePaths;\nCheckpoint 持久化创建完 CheckpointStateOutputStream 之后，会调用 serializationProxy.write(outView) 写入状态的元数据。元数据包括状态的名称、类型、序列化器等一些配置。\n元数据写完之后，就开始分组写入状态数据。在写入时，先写 keyGroupId，然后再写当前分组的状态数据\nfor (int keyGroupPos = 0;        keyGroupPos &lt; keyGroupRange.getNumberOfKeyGroups();        ++keyGroupPos) &#123;    int keyGroupId = keyGroupRange.getKeyGroupId(keyGroupPos);    keyGroupRangeOffsets[keyGroupPos] = localStream.getPos();    // 写 keyGroupId    outView.writeInt(keyGroupId);    for (Map.Entry&lt;StateUID, StateSnapshot&gt; stateSnapshot :            cowStateStableSnapshots.entrySet()) &#123;        StateSnapshot.StateKeyGroupWriter partitionedSnapshot =                stateSnapshot.getValue().getKeyGroupWriter();        try (OutputStream kgCompressionOut =                keyGroupCompressionDecorator.decorateWithCompression(localStream)) &#123;            DataOutputViewStreamWrapper kgCompressionView =                    new DataOutputViewStreamWrapper(kgCompressionOut);            kgCompressionView.writeShort(stateNamesToId.get(stateSnapshot.getKey()));            // 写状态数据            partitionedSnapshot.writeStateInKeyGroup(kgCompressionView, keyGroupId);        &#125; // this will just close the outer compression stream    &#125;&#125;\n状态数据写入的调用链路如下\n\npublic void writeState(        TypeSerializer&lt;K&gt; keySerializer,        TypeSerializer&lt;N&gt; namespaceSerializer,        TypeSerializer&lt;S&gt; stateSerializer,        @Nonnull DataOutputView dov,        @Nullable StateSnapshotTransformer&lt;S&gt; stateSnapshotTransformer)        throws IOException &#123;    SnapshotIterator&lt;K, N, S&gt; snapshotIterator =            getIterator(                    keySerializer,                    namespaceSerializer,                    stateSerializer,                    stateSnapshotTransformer);    int size = snapshotIterator.size();    dov.writeInt(size);    while (snapshotIterator.hasNext()) &#123;        StateEntry&lt;K, N, S&gt; stateEntry = snapshotIterator.next();        namespaceSerializer.serialize(stateEntry.getNamespace(), dov);        keySerializer.serialize(stateEntry.getKey(), dov);        stateSerializer.serialize(stateEntry.getState(), dov);    &#125;&#125;\n返回结果最后一步就是封装并返回元信息，这里收集的信息包括了每个 keyGroup 的状态数据在状态文件中的存储位置，状态数据存储的文件路径、文件大小等。\nOperatorStateOperatorState 的处理逻辑比 KeyedState 更简单一些，流程上都是先做状态数据的引用快照，然后写入状态数据和返回结果。在写入数据时，没有了分组写入的逻辑。直接处理 operatorState 和 broadcastState。这里就只贴一下调用流程，不做过多赘述了。\n\n总结本文我们重点梳理了 KeyedState 数据写入的代码。其主要步骤包括：同步拷贝状态数据的引用，创建 Checkpoint 输出流 CheckpointStateOutputStream 并完成 Checkpoint 持久化，最后返回元信息结果。OperatorState 的处理过程和 KeyedState 的过程类似，只是少了分组的逻辑。\n","tags":["Flink"]},{"title":"Flink源码阅读：JobManager的HA机制","url":"/2026/01/06/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AJobManager%E7%9A%84HA%E6%9C%BA%E5%88%B6/","content":"JobManager 在 Flink 集群中发挥着重要的作用，包括任务调度和资源管理等工作。如果 JobManager 宕机，那么整个集群的任务都将失败。为了解决 JobManager 的单点问题，Flink 也设计了 HA 机制来保障整个集群的稳定性。\n基本概念在 JobManager 启动时，调用 HighAvailabilityServicesUtils.createHighAvailabilityServices 来创建 HA 服务，HA 依赖的服务都被封装在 HighAvailabilityServices 中。当前 Flink 内部支持两种高可用模式，分别是 ZooKeeper 和 KUBERNETES。\ncase ZOOKEEPER:    return createZooKeeperHaServices(configuration, executor, fatalErrorHandler);case KUBERNETES:    return createCustomHAServices(            &quot;org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory&quot;,            configuration,            executor);\nHighAvailabilityServices 中提供的关键组件包括：\n\nLeaderRetrievalService：服务发现，用于获取当前 leader 的地址。目前用到服务发现的组件有 ResourceManager、Dispatcher、JobManager、ClusterRestEndpoint。\n\nLeaderElection：选举服务，从多个候选者中选出一个作为 leader。用到选举服务的同样是 ResourceManager、Dispatcher、JobManager、ClusterRestEndpoint 这四个。\n\nCheckpointRecoveryFactory：Checkpoint 恢复组件的工厂类，提供了创建 CompletedCheckpointStore 和 CheckpointIDCounter 的方法。CompletedCheckpointStore 是用于存储已完成的 checkpoint 的元信息，CheckpointIDCounter 是用于生成 checkpoint ID。\n\nExecutionPlanStore：用于存储执行计划。\n\nJobResultStore：用于存储作业结果，这里有两种状态，一种是 dirty，表示作业没有被完全清理，另一种是 clean，表示作业清理工作已经执行完成了。\n\nBlobStore：存储作业运行期间的一些二进制文件。\n\n\n选举服务Flink 的选举是依靠 LeaderElection 和 LeaderContender 配合完成的。LeaderElection 是 LeaderElectionService 的代理接口，提供了注册候选者、确认 leader 和 判断候选者是否是 leader 三个接口。LeaderContender 则是用来表示候选者对象。当一个 LeaderContender 当选 leader 后，LeaderElectionService 会为其生成一个 leaderSessionId，LeaderContender 会调用 confirmLeadershipAsync 发布自己的地址。选举服务的具体实现在 LeaderElectionDriver 接口中。\n服务发现服务发现的作用是获取各组件的 leader 地址。服务发现依赖 LeaderRetrievalService 和 LeaderRetrievalListener。LeaderRetrievalService 可以启动一个监听，当有新的 leader 当选时，会调用 LeaderRetrievalListener 的 notifyLeaderAddress 方法。\n信息保存当 leader 发生切换时，新的 leader 需要获取到旧 leader 存储的信息，这就需要旧 leader 把这些信息存在一个公共的存储上。它可以是 ZooKeeper 或 Kubernetes 的存储，也可以是分布式文件系统的存储。\n基于 ZooKeeper 的 HA选举服务前面我们提到了选举服务主要依赖 LeaderElection 和 LeaderContender 配合完成。我们就以 JobManager 为例，看一下机遇 ZooKeeper 的选举流程的具体实现。\n\n图中 JobMasterServiceLeadershipRunner 是 LeaderContender 的实现类。在启动服务时，会向 LeaderElection 注册自己的信息，实际执行者是 DefaultLeaderElectionService。它先创建了 LeaderElectionDriver，然后将 LeaderContender 保存在 leaderContenderRegistry 中。选举的核心逻辑封装在 LeaderElectionDriver 中。\n在创建 LeaderElectionDriver 时，会创建 LeaderLatch 对象和 TreeCache 对象， LeaderLatch 封装了与 ZooKeeper 关联的回调，会接收一个 LeaderElectionDriver 作为监听。TreeCache 主要用于监听 ZooKeeper 中 leader 节点的变更。\npublic ZooKeeperLeaderElectionDriver(        CuratorFramework curatorFramework, LeaderElectionDriver.Listener leaderElectionListener)        throws Exception &#123;    ...    this.leaderLatch = new LeaderLatch(curatorFramework, ZooKeeperUtils.getLeaderLatchPath());    this.treeCache =            ZooKeeperUtils.createTreeCache(                    curatorFramework,                    &quot;/&quot;,                    new ZooKeeperLeaderElectionDriver.ConnectionInfoNodeSelector());    treeCache            .getListenable()            .addListener(                    (client, event) -&gt; &#123;                        switch (event.getType()) &#123;                            case NODE_ADDED:                            case NODE_UPDATED:                                Preconditions.checkNotNull(                                        event.getData(),                                        &quot;The ZooKeeper event data must not be null.&quot;);                                handleChangedLeaderInformation(event.getData());                                break;                            case NODE_REMOVED:                                Preconditions.checkNotNull(                                        event.getData(),                                        &quot;The ZooKeeper event data must not be null.&quot;);                                handleRemovedLeaderInformation(event.getData().getPath());                                break;                        &#125;                    &#125;);    leaderLatch.addListener(this);    ...    leaderLatch.start();    treeCache.start();&#125;\n我们进入到 LeaderLatch 的 start 方法。它的内部是在 ZooKeeper 上创建 latch-xxx 节点。xxx 是当前 LeaderLatch 的 ID，它由 ZooKeeper 生成，ID 最小的当选 Leader。\nprivate void checkLeadership(List&lt;String&gt; children) throws Exception &#123;    if (this.debugCheckLeaderShipLatch != null) &#123;        this.debugCheckLeaderShipLatch.await();    &#125;    String localOurPath = (String)this.ourPath.get();    List&lt;String&gt; sortedChildren = LockInternals.getSortedChildren(&quot;latch-&quot;, sorter, children);    int ourIndex = localOurPath != null ? sortedChildren.indexOf(ZKPaths.getNodeFromPath(localOurPath)) : -1;    this.log.debug(&quot;checkLeadership with id: &#123;&#125;, ourPath: &#123;&#125;, children: &#123;&#125;&quot;, new Object[]&#123;this.id, localOurPath, sortedChildren&#125;);    if (ourIndex &lt; 0) &#123;        this.log.error(&quot;Can&#x27;t find our node. Resetting. Index: &quot; + ourIndex);        this.reset();    &#125; else if (ourIndex == 0) &#123;        this.lastPathIsLeader.set(localOurPath);        this.setLeadership(true);    &#125; else &#123;        this.setLeadership(false);        String watchPath = (String)sortedChildren.get(ourIndex - 1);        Watcher watcher = new Watcher() &#123;            public void process(WatchedEvent event) &#123;                if (LeaderLatch.this.state.get() == LeaderLatch.State.STARTED &amp;&amp; event.getType() == EventType.NodeDeleted) &#123;                    try &#123;                        LeaderLatch.this.getChildren();                    &#125; catch (Exception ex) &#123;                        ThreadUtils.checkInterrupted(ex);                        LeaderLatch.this.log.error(&quot;An error occurred checking the leadership.&quot;, ex);                    &#125;                &#125;            &#125;        &#125;;        BackgroundCallback callback = new BackgroundCallback() &#123;            public void processResult(CuratorFramework client, CuratorEvent event) throws Exception &#123;                if (event.getResultCode() == Code.NONODE.intValue()) &#123;                    LeaderLatch.this.getChildren();                &#125;            &#125;        &#125;;        ((ErrorListenerPathable)((BackgroundPathable)this.client.getData().usingWatcher(watcher)).inBackground(callback)).forPath(ZKPaths.makePath(this.latchPath, watchPath));    &#125;&#125;\n当选 Leader 后，会回调 LeaderElectionDriver 的 isLeader 方法，如果未当选，则继续监听 latch 节点的变更。isLeader 会继续回调 LeaderElection 的 onGrantLeadership 方法，接着调用 LeaderContender 的 grantLeadership。这时会启动 JobMaster 服务，然后调用 LeaderElection 的 confirmLeadershipAsync 来确认当选成功。确认的过程是由 LeaderElectionDriver 来执行的。主要作用是把当前 leader 的信息写回到 ZooKeeper 的 connection_info 节点。\npublic void publishLeaderInformation(String componentId, LeaderInformation leaderInformation) &#123;    Preconditions.checkState(running.get());    if (!leaderLatch.hasLeadership()) &#123;        return;    &#125;    final String connectionInformationPath =            ZooKeeperUtils.generateConnectionInformationPath(componentId);    LOG.debug(            &quot;Write leader information &#123;&#125; for component &#x27;&#123;&#125;&#x27; to &#123;&#125;.&quot;,            leaderInformation,            componentId,            ZooKeeperUtils.generateZookeeperPath(                    curatorFramework.getNamespace(), connectionInformationPath));    try &#123;        ZooKeeperUtils.writeLeaderInformationToZooKeeper(                leaderInformation,                curatorFramework,                leaderLatch::hasLeadership,                connectionInformationPath);    &#125; catch (Exception e) &#123;        leaderElectionListener.onError(e);    &#125;&#125;\n服务发现梳理完选举服务的源码后，我们再来看一下服务发现的过程。我们以 TaskManager 获取 JobManager 的 leader 为例。\n\n当我们往 TaskManager 添加任务时，会调用 JobLeaderService 的 addJob 方法。这里会先获取 LeaderRetrieval，然后调用 start 方法注册 LeaderRetrievalListener 监听，并创建 LeaderRetrievalDriver。在 LeaderRetrievalDriver 中主要是向 ZooKeeper 注册 connection_info 节点的变更。\n如果发生变更，ZooKeeper 会回调 LeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper 方法。我们从 ZooKeeper 获取到 leader 的地址和 sessionId 后，就回调 LeaderRetrievalService.notifyLeaderAddress 方法。最终调用到 JobLeaderService 的 notifyLeaderAddress 方法，这个方法中就是断开与旧 leader 的连接，增加与新 leader 的连接。\n信息保存最后我们再来看信息保存相关的源码。在 JobManager 完成一次 Checkpoint 时，会执行 CheckpointCoordinator.completePendingCheckpoint 方法，跟随调用链路可以找到 ZooKeeperStateHandleStore.addAndLock 方法，这里会把状态写入到文件系统中，然后把文件路径保存在 ZooKeeper 中。\npublic RetrievableStateHandle&lt;T&gt; addAndLock(String pathInZooKeeper, T state)        throws PossibleInconsistentStateException, Exception &#123;    checkNotNull(pathInZooKeeper, &quot;Path in ZooKeeper&quot;);    checkNotNull(state, &quot;State&quot;);    final String path = normalizePath(pathInZooKeeper);    final Optional&lt;Stat&gt; maybeStat = getStat(path);    if (maybeStat.isPresent()) &#123;        if (isNotMarkedForDeletion(maybeStat.get())) &#123;            throw new AlreadyExistException(                    String.format(&quot;ZooKeeper node %s already exists.&quot;, path));        &#125;        Preconditions.checkState(                releaseAndTryRemove(path),                &quot;The state is marked for deletion and, therefore, should be deletable.&quot;);    &#125;    final RetrievableStateHandle&lt;T&gt; storeHandle = storage.store(state);    final byte[] serializedStoreHandle = serializeOrDiscard(storeHandle);    try &#123;        writeStoreHandleTransactionally(path, serializedStoreHandle);        return storeHandle;    &#125; catch (KeeperException.NodeExistsException e) &#123;        // Transactions are not idempotent in the curator version we&#x27;re currently using, so it        // is actually possible that we&#x27;ve re-tried a transaction that has already succeeded.        // We&#x27;ve ensured that the node hasn&#x27;t been present prior executing the transaction, so        // we can assume that this is a result of the retry mechanism.        return storeHandle;    &#125; catch (Exception e) &#123;        if (indicatesPossiblyInconsistentState(e)) &#123;            throw new PossibleInconsistentStateException(e);        &#125;        // In case of any other failure, discard the state and rethrow the exception.        storeHandle.discardState();        throw e;    &#125;&#125;\n至此，基于 ZooKeeper 的 HA 逻辑我们就梳理完了。从 1.12 版本开始，Flink 还支持了 Kubernetes 高可用，下面我们再来一下它是如何实现的。\n基于 Kubernetes 的 HA选举服务通过前面的学习，我们已经了解到，选举的主要逻辑是在 LeaderElectionDriver 中，因此，我们直接来看 KubernetesLeaderElectionDriver 的逻辑即可。创建 KubernetesLeaderElectionDriver 时，创建并启动了 KubernetesLeaderElector。这个类似于 ZooKeeper 逻辑中 LeaderLatch，会跟 Kubernetes 底层的选举逻辑交互，同时注册监听。\npublic KubernetesLeaderElector(        NamespacedKubernetesClient kubernetesClient,        KubernetesLeaderElectionConfiguration leaderConfig,        LeaderCallbackHandler leaderCallbackHandler,        ExecutorService executorService) &#123;    this.kubernetesClient = kubernetesClient;    this.leaderElectionConfig =            new LeaderElectionConfigBuilder()                    .withName(leaderConfig.getConfigMapName())                    .withLeaseDuration(leaderConfig.getLeaseDuration())                    .withLock(                            new ConfigMapLock(                                    new ObjectMetaBuilder()                                            .withNamespace(kubernetesClient.getNamespace())                                            .withName(leaderConfig.getConfigMapName())                                            // Labels will be used to clean up the ha related                                            // ConfigMaps.                                            .withLabels(                                                    KubernetesUtils.getConfigMapLabels(                                                            leaderConfig.getClusterId()))                                            .build(),                                    leaderConfig.getLockIdentity()))                    .withRenewDeadline(leaderConfig.getRenewDeadline())                    .withRetryPeriod(leaderConfig.getRetryPeriod())                    .withReleaseOnCancel(true)                    .withLeaderCallbacks(                            new LeaderCallbacks(                                    leaderCallbackHandler::isLeader,                                    leaderCallbackHandler::notLeader,                                    newLeader -&gt;                                            LOG.info(                                                    &quot;New leader elected &#123;&#125; for &#123;&#125;.&quot;,                                                    newLeader,                                                    leaderConfig.getConfigMapName())))                    .build();    this.executorService = executorService;    LOG.info(            &quot;Create KubernetesLeaderElector on lock &#123;&#125;.&quot;,            leaderElectionConfig.getLock().describe());&#125;\n选举成功后，会回调 LeaderElectionListener.onGrantLeadership 方法。后续的调用链路还是会调用到 KubernetesLeaderElectionDriver.publishLeaderInformation 方法。这个方法是把 leader 信息写到 Kubernetes 的 configMap 中。\npublic void publishLeaderInformation(String componentId, LeaderInformation leaderInformation) &#123;    Preconditions.checkState(running.get());    try &#123;        kubeClient                .checkAndUpdateConfigMap(                        configMapName,                        updateConfigMapWithLeaderInformation(componentId, leaderInformation))                .get();    &#125; catch (InterruptedException | ExecutionException e) &#123;        leaderElectionListener.onError(e);    &#125;    LOG.debug(            &quot;Successfully wrote leader information &#123;&#125; for leader &#123;&#125; into the config map &#123;&#125;.&quot;,            leaderInformation,            componentId,            configMapName);&#125;\n服务发现服务发现的逻辑在 KubernetesLeaderRetrievalDriver 类中，在创建时，会将内部类 ConfigMapCallbackHandlerImpl 注册为监听回调类。\n当 configMap 有新增或变更后，会回调 LeaderRetrievalService.notifyLeaderAddress 方法。\nprivate class ConfigMapCallbackHandlerImpl        implements FlinkKubeClient.WatchCallbackHandler&lt;KubernetesConfigMap&gt; &#123;    @Override    public void onAdded(List&lt;KubernetesConfigMap&gt; configMaps) &#123;        // The ConfigMap is created by KubernetesLeaderElectionDriver with        // empty data. We don&#x27;t really need to process anything unless the retriever was started        // after the leader election has already succeeded.        final KubernetesConfigMap configMap = getOnlyConfigMap(configMaps, configMapName);        final LeaderInformation leaderInformation = leaderInformationExtractor.apply(configMap);        if (!leaderInformation.isEmpty()) &#123;            leaderRetrievalEventHandler.notifyLeaderAddress(leaderInformation);        &#125;    &#125;    @Override    public void onModified(List&lt;KubernetesConfigMap&gt; configMaps) &#123;        final KubernetesConfigMap configMap = getOnlyConfigMap(configMaps, configMapName);        leaderRetrievalEventHandler.notifyLeaderAddress(                leaderInformationExtractor.apply(configMap));    &#125;    ...&#125;\n信息保存信息保存的逻辑和 ZooKeeper 也非常类似。即先把 state 保存在文件系统，然后把存储路径写到 Kubernetes 写到 configMap 中。具体可以看 KubernetesStateHandleStore.addAndLock 方法。\n总结本文我们一起梳理了 Flink 中 JobManager 的 HA 机制相关源码。目前 Flink 支持 ZooKeeper 和 Kubernetes 两种实现。在梳理过程中，我们以 JobManager 为例，其他几个用到高可用的服务的选举逻辑也是一样的。\n","tags":["Flink"]},{"title":"Flink源码阅读：Netty通信","url":"/2026/01/05/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ANetty%E9%80%9A%E4%BF%A1/","content":"前文中我们了解了 Flink 的数据交互过程，上游的 Task 将数据写入到 ResultSubpartition 的 buffers 队列中。下游的 Task 通过 LocalInputChannel 和 RemoteInputChannel 消费上游的数据。\nLocalInputChannel 是上下游的 Task 部署在同一个 TaskManager 时使用的，在本地即可完成数据交换，无需网络通信。当上下游的 Task 部署在不同的 TaskManager 时，就需要用到 RemoteInputChannel，Flink 利用 Netty 来进行数据交互。本文我们来一起梳理一下 Netty 相关的源码。\n初始化我们先来看 NettyServer 和 NettyClient 的初始化过程。\n\nNetty 的初始化阶段是在 TaskManager 启动的过程中执行的。在 TaskManagerServices.fromConfiguration 方法中，会创建并启动 ShuffleEnvironment。\npublic static TaskManagerServices fromConfiguration(        TaskManagerServicesConfiguration taskManagerServicesConfiguration,        PermanentBlobService permanentBlobService,        MetricGroup taskManagerMetricGroup,        ExecutorService ioExecutor,        ScheduledExecutor scheduledExecutor,        FatalErrorHandler fatalErrorHandler,        WorkingDirectory workingDirectory)        throws Exception &#123;    ...    final ShuffleEnvironment&lt;?, ?&gt; shuffleEnvironment =            createShuffleEnvironment(                    taskManagerServicesConfiguration,                    taskEventDispatcher,                    taskManagerMetricGroup,                    ioExecutor,                    scheduledExecutor);    final int listeningDataPort = shuffleEnvironment.start();    ...&#125;\n我们顺着调用链路可以一直找到 NettyShuffleServiceFactory.createNettyShuffleEnvironment 方法，这个方法中创建了 NettyConnectionManager，在 NettyConnectionManager 中有几个很重要的对象。\npublic NettyConnectionManager(        NettyBufferPool bufferPool,        ResultPartitionProvider partitionProvider,        TaskEventPublisher taskEventPublisher,        NettyConfig nettyConfig,        boolean connectionReuseEnabled) &#123;    this.server = new NettyServer(nettyConfig);    this.client = new NettyClient(nettyConfig);    this.bufferPool = checkNotNull(bufferPool);    this.partitionRequestClientFactory =            new PartitionRequestClientFactory(                    client, nettyConfig.getNetworkRetries(), connectionReuseEnabled);    this.nettyProtocol =            new NettyProtocol(                    checkNotNull(partitionProvider), checkNotNull(taskEventPublisher));&#125;\nserver 和 client 不需要多介绍，就是 Netty 的服务端和客户端。bufferPool 是缓冲池，用于存储要传输的数据。nettyProtocol 提供了 NettyClient 和 NettyServer 引导启动注册的 Channel Handler。\n后面就是创建 NettyShuffleEnvironment 及其需要的对象了。在创建完成后，会调用它的 start 方法启动。这个启动方法就是调用了 connectionManager.start，在 NettyConnectionManager 中，就是初始化客户端和服务端。\npublic int start() throws IOException &#123;    client.init(nettyProtocol, bufferPool);    return server.init(nettyProtocol, bufferPool);&#125;\nclient 初始化client 的初始化过程是先创建并初始化 Bootstrap。\nprivate void initEpollBootstrap() &#123;    // Add the server port number to the name in order to distinguish    // multiple clients running on the same host.    String name =            NettyConfig.CLIENT_THREAD_GROUP_NAME + &quot; (&quot; + config.getServerPortRange() + &quot;)&quot;;    EpollEventLoopGroup epollGroup =            new EpollEventLoopGroup(                    config.getClientNumThreads(), NettyServer.getNamedThreadFactory(name));    bootstrap.group(epollGroup).channel(EpollSocketChannel.class);    config.getTcpKeepIdleInSeconds()            .ifPresent(idle -&gt; bootstrap.option(EpollChannelOption.TCP_KEEPIDLE, idle));    config.getTcpKeepInternalInSeconds()            .ifPresent(                    interval -&gt; bootstrap.option(EpollChannelOption.TCP_KEEPINTVL, interval));    config.getTcpKeepCount()            .ifPresent(count -&gt; bootstrap.option(EpollChannelOption.TCP_KEEPCNT, count));&#125;\n初始化过程重要设置 EventLoopGroup 和 channel，可以用 epoll 的话就用 epoll，否则就用 nio。设置好这些后就是设置了一些通道参数（连接超时时间、Bufffer 池等）。\n到这里 client 的初始化其实并没有结束，还需要设置 Handler 流水线，这些工作是在 Task 启动时执行了。\nserver 初始化server 的初始化过程是先创建并初始化了 ServerBootstrap。之后同样也是设置 EventLoopGroup 和 channel，以及通道相关的各种参数。\n设置好之后，会添加 ChannelHandler 流水线，这里的 ChannelHandler 流水线就是我们前面创建的 NettyProtocol 提供的。\npublic ChannelHandler[] getServerChannelHandlers() &#123;    PartitionRequestQueue queueOfPartitionQueues = new PartitionRequestQueue();    PartitionRequestServerHandler serverHandler =            new PartitionRequestServerHandler(                    partitionProvider, taskEventPublisher, queueOfPartitionQueues);    return new ChannelHandler[] &#123;        messageEncoder,        new NettyMessage.NettyMessageDecoder(),        serverHandler,        queueOfPartitionQueues    &#125;;&#125;\n流水线上包含了消息编码器、解码器、PartitionRequestServerHandler 请求服务端处理器和 PartitionRequestQueue 分区请求队列。\n这些都设置好之后，就开始启动 NettyServer 服务了。\nIterator&lt;Integer&gt; portsIterator = config.getServerPortRange().getPortsIterator();while (portsIterator.hasNext() &amp;&amp; bindFuture == null) &#123;    Integer port = portsIterator.next();    LOG.debug(&quot;Trying to bind Netty server to port: &#123;&#125;&quot;, port);    bootstrap.localAddress(config.getServerAddress(), port);    try &#123;        bindFuture = bootstrap.bind().syncUninterruptibly();    &#125; catch (Exception e) &#123;        // syncUninterruptibly() throws checked exceptions via Unsafe        // continue if the exception is due to the port being in use, fail early        // otherwise        if (isBindFailure(e)) &#123;            LOG.debug(&quot;Failed to bind Netty server&quot;, e);        &#125; else &#123;            throw e;        &#125;    &#125;&#125;if (bindFuture == null) &#123;    throw new BindException(            &quot;Could not start rest endpoint on any port in port range &quot;                    + config.getServerPortRange());&#125;localAddress = (InetSocketAddress) bindFuture.channel().localAddress();\n客户端请求远端子分区服务端和客户端初始化之后，在 Task 运行时，会先完成 Client 的 ChannelHandler 的配置，然后请求 Netty 远端服务。\n\n我们来看具体过程，在 Task 初始化时，会调用 StreamTask.invoke 方法，其内部会调用 StreamTask.restoreStateAndGate 方法，这里会便利 Task 的所有 InputGate，然后调用 requestPartitions。在 InputGate 的 requestPartitions 逻辑中，又便利所有的 InputChannel，调用 requestSubpartitions。\nfor (InputGate inputGate : inputGates) &#123;    recoveredFutures.add(inputGate.getStateConsumedFuture());    inputGate            .getStateConsumedFuture()            .thenRun(                    () -&gt;                            mainMailboxExecutor.execute(                                    inputGate::requestPartitions,                                    &quot;Input gate request partitions&quot;));&#125;private void internalRequestPartitions() &#123;    for (InputChannel inputChannel : inputChannels()) &#123;        try &#123;            inputChannel.requestSubpartitions();        &#125; catch (Throwable t) &#123;            inputChannel.setError(t);            return;        &#125;    &#125;&#125;\n我们来看 RemoteInputChannel.requestSubpartitions 的逻辑。\npublic void requestSubpartitions() throws IOException, InterruptedException &#123;    if (partitionRequestClient == null) &#123;        LOG.debug(                &quot;&#123;&#125;: Requesting REMOTE subpartitions &#123;&#125; of partition &#123;&#125;. &#123;&#125;&quot;,                this,                consumedSubpartitionIndexSet,                partitionId,                channelStatePersister);        // Create a client and request the partition        try &#123;            partitionRequestClient =                    connectionManager.createPartitionRequestClient(connectionId);        &#125; catch (IOException e) &#123;            // IOExceptions indicate that we could not open a connection to the remote            // TaskExecutor            throw new PartitionConnectionException(partitionId, e);        &#125;        partitionRequestClient.requestSubpartition(                partitionId, consumedSubpartitionIndexSet, this, 0);    &#125;&#125;\n这里主要有两个步骤，先是创建 partitionRequestClient，然后调用 requestSubpartition。\n创建请求客户端PartitionRequestClient 是在 PartitionRequestClientFactory.connect 中创建的。先调用了 NettyClient.connect，同步等待客户端连接到服务端，这个过程中会进行 ChannelHandler 配置，也就是我们在初始化的过程中介绍的，NettyClient 没有完成的步骤。\npublic ChannelHandler[] getClientChannelHandlers() &#123;    NetworkClientHandler networkClientHandler = new CreditBasedPartitionRequestClientHandler();    return new ChannelHandler[] &#123;        messageEncoder,        new NettyMessageClientDecoderDelegate(networkClientHandler),        networkClientHandler    &#125;;&#125;\nHandler 包括了消息编码器、解码器和 CreditBasedPartitionRequestClientHandler 这个基于 Credit 的分区请求客户端处理器。Handler 配置好之后会利用 bootstrap 连接到服务端。\n在获取到 Channel 和 NetworkClientHandler 之后，就直接创建了 NettyPartitionRequestClient。\n请求子分区数据让我们再回到 RemoteInputChannel 的 requestSubpartitions 方法中，现在我们创建好了 NettyPartitionRequestClient，接下来就是调用它的 requestSubpartition 方法来发起请求。\n这里逻辑也比较简单：\n\n向 NetworkClientHandler 注册当前 RemoteInputChannel。\n\n构造请求对象 PartitionRequest，这里包含了分区 ID、子分区索引、inputChannel ID 以及初识的 Credit。\n\n调用 tcpChannel.writeAndFlush 发起请求，并添加请求失败的监听。\n\n如果请求失败，移除当前 inputChannel。\n\n\n服务端响应现在数据到了服务端，我们来看服务端处理的具体过程。\n\n在 NettyServer 初始化的过程中，我们添加了两个重要的 Handler，分别是 PartitionRequestServerHandler 和 PartitionRequestQueue。服务端响应数据的过程就是这两个 Handler 在发挥作用。\nPartitionRequestServerHandler 负责处理 Client 端通过 PartitionRequestClient 发送的请求，处理过程是先创建 CreditBasedSequenceNumberingViewReader 类型的 reader，然后将它放入 PartitionRequestQueue 维护的 reader 队列中。PartitionRequestQueue 会监听 Netty Channel 的可写入状态，当 Netty Channel 可写入时，会消费数据并写入网络。\n下面我们来看具体的源码。\n服务端的响应入口在 PartitionRequestServerHandler.channelRead0 方法，这里在处理 PartitionRequest 请求时，先是创建 CreditBasedSequenceNumberingViewReader，然后调用 requestSubpartitionViewOrRegisterListener。\nrequestSubpartitionViewOrRegisterListener 的逻辑是创建 ResultSubpartitionView，并提醒 PartitionRequestQueue 有数据可用。\nOptional&lt;ResultSubpartitionView&gt; subpartitionViewOptional =        partitionProvider.createSubpartitionViewOrRegisterListener(                resultPartitionId,                subpartitionIndexSet,                this,                partitionRequestListener);...notifyDataAvailable(subpartitionView);\nResultSubpartitionView 就是用来消费 ResultSubpartition 的数据。\nnotifyDataAvailable 内部调用了 notifyReaderNonEmpty，notifyReaderNonEmpty 又触发了 userEventTriggered，这里调用 enqueueAvailableReader 将 reader 放入到可用队列 availableReaders 中。\nprivate void enqueueAvailableReader(final NetworkSequenceViewReader reader) throws Exception &#123;    if (reader.isRegisteredAsAvailable()) &#123;        return;    &#125;    ResultSubpartitionView.AvailabilityWithBacklog availabilityWithBacklog =            reader.getAvailabilityAndBacklog();    if (!availabilityWithBacklog.isAvailable()) &#123;        int backlog = availabilityWithBacklog.getBacklog();        if (backlog &gt; 0 &amp;&amp; reader.needAnnounceBacklog()) &#123;            announceBacklog(reader, backlog);        &#125;        return;    &#125;    // Queue an available reader for consumption. If the queue is empty,    // we try trigger the actual write. Otherwise this will be handled by    // the writeAndFlushNextMessageIfPossible calls.    boolean triggerWrite = availableReaders.isEmpty();    registerAvailableReader(reader);    if (triggerWrite) &#123;        writeAndFlushNextMessageIfPossible(ctx.channel());    &#125;&#125;\n如果 reader 是队列中的第一个元素，会触发数据写入网络。\nwriteAndFlushNextMessageIfPossible 的处理步骤如下：\n\n取出可用 reader。\n\n调用 reader.getNextBuffer 获取数据。\n\n如果 reader 仍然可用，将其加回队列。\n\n向下游写入数据并添加下次写入的监听。\n\n\npublic BufferAndAvailability getNextBuffer() throws IOException &#123;    BufferAndBacklog next = subpartitionView.getNextBuffer();    if (next != null) &#123;        if (next.buffer().isBuffer() &amp;&amp; --numCreditsAvailable &lt; 0) &#123;            throw new IllegalStateException(&quot;no credit available&quot;);        &#125;        final Buffer.DataType nextDataType = getNextDataType(next);        return new BufferAndAvailability(                next.buffer(), nextDataType, next.buffersInBacklog(), next.getSequenceNumber());    &#125; else &#123;        return null;    &#125;&#125;\n在 getNextBuffer 方法中，会将 credit 值减 1，并判断是否小于 0。如果小于 0 会抛出异常，reader 是否可用也是根据 numCreditsAvailable 是否大于 0 来判断的。\n客户端接收数据NettyClient 在消费数据时，同样也是以 ChannelHandler 作为入口。这里的入口方法是 CreditBasedPartitionRequestClientHandler.channelRead 。\n\n在 decodeMsg 方法中，先解码 msg，判断 InputChannel 是否可用，如果不可用，则取消当前 InputChannel 的订阅。如果可用，继续调用 decodeBufferOrEvent 进行处理。decodeBufferOrEvent 的核心逻辑是调用 RemoteInputChannel.onBuffer 方法，将数据加入到 receivedBuffers 队列。\n如果 receivedBuffers 队列在此之前处于空闲状态，会调用 notifyChannelNonEmpty，将当前 RemoteInputChannel 加入到 inputChannelsWithData 队列中，同时还会唤醒 inputChannelsWithData 上的阻塞线程，让 inputGate 可以消费 RemoteInputChannel 的数据。\nprivate boolean queueChannelUnsafe(InputChannel channel, boolean priority) &#123;    assert Thread.holdsLock(inputChannelsWithData);    if (channelsWithEndOfPartitionEvents.get(channel.getChannelIndex())) &#123;        return false;    &#125;    final boolean alreadyEnqueued =            enqueuedInputChannelsWithData.get(channel.getChannelIndex());    if (alreadyEnqueued            &amp;&amp; (!priority || inputChannelsWithData.containsPriorityElement(channel))) &#123;        // already notified / prioritized (double notification), ignore        return false;    &#125;    // 当前 inputChannel 加入到 inputChannelsWithData    inputChannelsWithData.add(channel, priority, alreadyEnqueued);    if (!alreadyEnqueued) &#123;        enqueuedInputChannelsWithData.set(channel.getChannelIndex());    &#125;    return true;&#125;// 唤醒线程public void notifyDataAvailable() &#123;    availabilityMonitor.notifyAll();    toNotify = inputGate.availabilityHelper.getUnavailableToResetAvailable();&#125;\n如果客户端有积压，还需要根据积压申请 Buffer 并更新 Credit 值。这里申请的 buffer 数量为积压数量+初识 Credit 值。\npublic void onSenderBacklog(int backlog) throws IOException &#123;    notifyBufferAvailable(bufferManager.requestFloatingBuffers(backlog + initialCredit));&#125;\n如果 RemoteInputChannel 没有足够的 buffer，则会向 LocalBufferPool 申请新的 buffer，如果申请不到，会加一个监听，等 LocalBufferPool 有空闲时再触发申请 buffer。\nprivate int tryRequestBuffers() &#123;    assert Thread.holdsLock(bufferQueue);    int numRequestedBuffers = 0;    while (bufferQueue.getAvailableBufferSize() &lt; numRequiredBuffers            &amp;&amp; !isWaitingForFloatingBuffers) &#123;        BufferPool bufferPool = inputChannel.inputGate.getBufferPool();        Buffer buffer = bufferPool.requestBuffer();        if (buffer != null) &#123;            bufferQueue.addFloatingBuffer(buffer);            numRequestedBuffers++;        &#125; else if (bufferPool.addBufferListener(this)) &#123;            isWaitingForFloatingBuffers = true;            break;        &#125;    &#125;    return numRequestedBuffers;&#125;\n当 RemoteInputChannel 申请到了需要的 buffer 之后，就会向 NettyServer 发送 AddCredit 消息，请求更新 Credit 值。\nprivate void notifyCreditAvailable() throws IOException &#123;    checkPartitionRequestQueueInitialized();    partitionRequestClient.notifyCreditAvailable(this);&#125;public void notifyCreditAvailable(RemoteInputChannel inputChannel) &#123;    sendToChannel(new AddCreditMessage(inputChannel));&#125;\nNettyServer 收到请求后，会将对应的 Credit 值进行更新。\nelse if (msgClazz == AddCredit.class) &#123;    AddCredit request = (AddCredit) msg;    outboundQueue.addCreditOrResumeConsumption(            request.receiverId, reader -&gt; reader.addCredit(request.credit));&#125;void addCreditOrResumeConsumption(        InputChannelID receiverId, Consumer&lt;NetworkSequenceViewReader&gt; operation)        throws Exception &#123;    if (fatalError) &#123;        return;    &#125;    NetworkSequenceViewReader reader = obtainReader(receiverId);    operation.accept(reader);    enqueueAvailableReader(reader);&#125;public void addCredit(int creditDeltas) &#123;    numCreditsAvailable += creditDeltas;&#125;\n此外，Flink 还有两种场景会更新 Credit 值。\n一种是 LocalBufferPool 回收空闲 buffer 时，会将 buffer 分配给申请者，分配之后会调用对应 InputChannel 的 notifyBufferAvailable 方法通知更新 Credit。\n\n另一种是 RemoteInputChannel 独占的 buffer 队列释放 buffer 时，会触发 Credit 更新。\n\n反压通过前面的学习，我们其实已经理解了反压的机制了。当前 Flink 的反压就是通过 Credit 来实现反压的，如果下游数据处理速度慢，Credit 会被耗尽，上游也就不会继续处理和下发数据了。直到下游处理完成，有了空闲的 buffer，此时向上游反馈更新 Credit 值，上游就会继续处理数据。\n总结最后我们总结一下，本文我们一起梳理了 Flink Netty 相关的源码。包括 NettyClient 和 NettyServer 的初始化，初始化过程中会创建一系列 ChannelHandler，之后利用这些Handler 处理数据，数据处理包括 Client 端的发送和接收消息，Server 端处理消息的过程。中间还穿插着 Credit 的处理。Flink 的反压逻辑就是依赖于 Credit 来实现的。\n","tags":["Flink"]},{"title":"Flink源码阅读：Mailbox线程模型","url":"/2026/01/09/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AMailbox%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","content":"本文我们来梳理 Flink 的线程模型——Mailbox。\n写在前面在以前的线程模型中，Flink 通过 checkpointLock 来隔离保证不同线程在修改内部状态时的正确性。通过 checkpointLock 控制并发会在代码中出现大量的 synchronize(lock) 这样非常不利于阅读和调试。Flink 也提供了一些 API 将锁对象暴露给用户，如果没有正确使用锁，很容易导致线程安全问题。\n为了解决这些问题，Flink 社区提出了基于 Mailbox 的线程模型。它是通过单线程加阻塞队列来实现。这样内部状态的修改就由单线程来完成了。\n旧的线程模型中，checkpointLock 主要用在三个地方：\n\nEvent Process：包括 event、watermark、barrier 的处理和发送\n\nCheckpoint：包括 Checkpoint 的触发和完成通知\n\nProcessTime Timer：ProcessTime 的回调通常涉及对状态的修改\n\n\n在 Mailbox 模型中，将所有需要处理的事件都封装成 Mail 投递到 Mailbox 中，然后由单线程按照顺序处理。\n相关定义下面我们来看 Mailbox 的具体实现，具体涉及到以下这些类。\n\n我们来逐个看一下这些类的定义和作用。\nMail在 Mailbox 线程模型中，Mail 是最基础的一个类，它用来封装需要处理的消息和执行的动作。Checkpoint Trigger 和 ProcessTime Trigger 都是通过 Mail 来触发的。Mail 中包含以下属性：\n// 选项，包括两个选项：isUrgent 和 deferrableprivate final MailOptionsImpl mailOptions;// 要执行的动作private final ThrowingRunnable&lt;? extends Exception&gt; runnable;// 优先级，这里的优先级不决定执行顺序，而是避免上下游之间的死锁问题private final int priority;// 描述信息private final String descriptionFormat;private final Object[] descriptionArgs;// 用于执行 runnable 的执行器private final StreamTaskActionExecutor actionExecutor;\nTaskMailbox有了 Mail 之后，Flink 用 TaskMailbox 来存储它，在需要执行时，再从 TaskMailbox 中取出。具体的处理逻辑在 TaskMailboxImpl 中。\n// 内部对于 queue 和 state 的并发访问都需要被这个锁保护private final ReentrantLock lock = new ReentrantLock();// 实际存储 Mail 的队列@GuardedBy(&quot;lock&quot;)private final Deque&lt;Mail&gt; queue = new ArrayDeque&lt;&gt;();// 与 lock 关联的 Condition，主要用于队列从空变为非空时唤醒等待获取 Mail 的线程@GuardedBy(&quot;lock&quot;)private final Condition notEmpty = lock.newCondition();// 状态，包括 OPEN/QUIESCED/CLOSED@GuardedBy(&quot;lock&quot;)private State state = OPEN;// 指定的邮箱线程的引用@Nonnull private final Thread taskMailboxThread;// 用于性能优化的设计private final Deque&lt;Mail&gt; batch = new ArrayDeque&lt;&gt;();// queue队列是否为空，用于性能优化，避免频繁访问主队列private volatile boolean hasNewMail = false;// 是否有紧急邮件，同样用于性能优化，减少检查队列中是否有紧急邮件的次数private volatile boolean hasNewUrgentMail = false;\n通过上面的属性，我们知道 TaskMailbox 底层是用 ArrayDeque 来存储 Mail 的，它内部包含了一个状态字段 state，state 的状态包括：\n\nOPEN：可以正常访问，接收和发送 Mail。\n\nQUIESCED：处于静默状态，不接收新的 Mail，已有的 Mail 仍然可以被取出。\n\nCLOSED：关闭状态，不能进行任何操作。\n\n\n在 TaskMailbox 内部，并发访问 queue 队列和 state 状态都需要 lock 这个锁的保护。此外 TaskMailbox 内部还做了一些性能优化，比如增加了 batch 队列，在处理 Mail 时，先将一批 Mail 从 queue 队列转移到 batch，之后会优先从 batch 队列中取，这样就减少了访问 queue 队列的次数，缓解了锁竞争压力。\nMailboxProcessorMailboxProcessor 可以认为是 Mailbox 相关的核心入口，MailboxProcessor 的核心方法就是事件循环，这个循环中主要是从 TaskMailbox 中取出 Mail 执行相应动作和执行默认动作（MailboxDefaultAction）。\nMailboxProcessor 还对外提供了 MailboxExecutor，其他组件可以利用 MailboxExecutor 来提交事件。\nMailboxExecutor我们接着来看 MailboxExecutor，它的实现类是 MailboxExecutorImpl。包括以下属性：\n// 实际存储的 mailbox 实例@Nonnull private final TaskMailbox mailbox;// 优先级，MailboxExecutor 提供的默认优先级，提交 mail 时会带上这个字段private final int priority;// 执行器，运行 mail 的动作private final StreamTaskActionExecutor actionExecutor;// 执行 MailboxProcessor，主要用于 isIdle 方法private final MailboxProcessor mailboxProcessor;\nMailboxExecutor 的主要作用是向 TaskMailbox 中投递 mail，核心方法是 execute。这个方法可以在任意线程中执行，因为 mailbox 内部控制了并发。\npublic void execute(        MailOptions mailOptions,        final ThrowingRunnable&lt;? extends Exception&gt; command,        final String descriptionFormat,        final Object... descriptionArgs) &#123;    try &#123;        mailbox.put(                new Mail(                        mailOptions,                        command,                        priority,                        actionExecutor,                        descriptionFormat,                        descriptionArgs));    &#125; catch (MailboxClosedException mbex) &#123;        throw new RejectedExecutionException(mbex);    &#125;&#125;\n除了 execute 方法以外，MailboxExecutor 中还有一个重要的方法，就是 yield。\npublic void yield() throws InterruptedException &#123;    Mail mail = mailbox.take(priority);    try &#123;        mail.run();    &#125; catch (Exception ex) &#123;        throw WrappingRuntimeException.wrapIfNecessary(ex);    &#125;&#125;\n这个方法的主要目的是为了让出对当前事件的处理。这么做的原因有二：\n\n如果不考虑优先级的因素，Mailbox 队列是 FIFO 的顺序处理，如果当前事件依赖后面的事件完成，则有可能造成”死锁“。\n\n当前事件处理事件较长，会阻塞其他事件。因此需要让出执行权，让相同或更高优先级的事件有机会执行。\n\n\n需要注意的是 yield 方法只能有 mailbox 线程自身调用。另外，Flink 也提供了非阻塞版本的方法，就是 tryYield。\n执行流程主流程在创建 StreamTask 时，会创建 mailboxProcessor，同时也会持有 mainMailboxExecutor。\nnew TaskMailboxImpl(Thread.currentThread()));...this.mailboxProcessor =        new MailboxProcessor(                this::processInput, mailbox, actionExecutor, mailboxMetricsControl);...this.mainMailboxExecutor = mailboxProcessor.getMainMailboxExecutor();\n可以看到这里将 processInput 作为 MailboxDefaultAction 传入 MailboxProcessor。在 StreamTask 启动时，会调用 MailboxProcessor 的核心方法。\npublic final void invoke() throws Exception &#123;    // Allow invoking method &#x27;invoke&#x27; without having to call &#x27;restore&#x27; before it.    if (!isRunning) &#123;        LOG.debug(&quot;Restoring during invoke will be called.&quot;);        restoreInternal();    &#125;    // final check to exit early before starting to run    ensureNotCanceled();    scheduleBufferDebloater();    // let the task do its work    getEnvironment().getMetricGroup().getIOMetricGroup().markTaskStart();    runMailboxLoop();    // if this left the run() method cleanly despite the fact that this was canceled,    // make sure the &quot;clean shutdown&quot; is not attempted    ensureNotCanceled();    afterInvoke();&#125;public void runMailboxLoop() throws Exception &#123;    mailboxProcessor.runMailboxLoop();&#125;\nrunMailboxLoop 的核心逻辑是一个 while 循环，在循环中处理 mail 并执行默认动作。\npublic void runMailboxLoop() throws Exception &#123;    suspended = !mailboxLoopRunning;    final TaskMailbox localMailbox = mailbox;    checkState(            localMailbox.isMailboxThread(),            &quot;Method must be executed by declared mailbox thread!&quot;);    assert localMailbox.getState() == TaskMailbox.State.OPEN : &quot;Mailbox must be opened!&quot;;    final MailboxController mailboxController = new MailboxController(this);    while (isNextLoopPossible()) &#123;        // The blocking `processMail` call will not return until default action is available.        processMail(localMailbox, false);        if (isNextLoopPossible()) &#123;            mailboxDefaultAction.runDefaultAction(                    mailboxController); // lock is acquired inside default action as needed        &#125;    &#125;&#125;private boolean isNextLoopPossible() &#123;    // &#x27;Suspended&#x27; can be false only when &#x27;mailboxLoopRunning&#x27; is true.    return !suspended;&#125;\n首先是做了前置检查，包括确保 TaskMailbox 是指定的 mailbox 线程，TaskMailbox 的状态是 OPEN。接着创建了 MailboxController，它用于 MailboxDefaultAction 与 MailboxProcessor 的交互。\n然后就进入到 while (isNextLoopPossible()) 循环了，循环中调用了 processMail，在这个方法中对 mail 进行处理。\nprivate boolean processMail(TaskMailbox mailbox, boolean singleStep) throws Exception &#123;    // Doing this check is an optimization to only have a volatile read in the expected hot    // path, locks are only    // acquired after this point.    boolean isBatchAvailable = mailbox.createBatch();    // Take mails in a non-blockingly and execute them.    boolean processed = isBatchAvailable &amp;&amp; processMailsNonBlocking(singleStep);    if (singleStep) &#123;        return processed;    &#125;    // If the default action is currently not available, we can run a blocking mailbox execution    // until the default action becomes available again.    processed |= processMailsWhenDefaultActionUnavailable();    return processed;&#125;\nprocessMail 方法中先创建 batch，然后非阻塞的处理这批 mail。\nprivate boolean processMailsNonBlocking(boolean singleStep) throws Exception &#123;    long processedMails = 0;    Optional&lt;Mail&gt; maybeMail;    while (isNextLoopPossible() &amp;&amp; (maybeMail = mailbox.tryTakeFromBatch()).isPresent()) &#123;        if (processedMails++ == 0) &#123;            maybePauseIdleTimer();        &#125;        runMail(maybeMail.get());        if (singleStep) &#123;            break;        &#125;    &#125;    if (processedMails &gt; 0) &#123;        maybeRestartIdleTimer();        return true;    &#125; else &#123;        return false;    &#125;&#125;private void runMail(Mail mail) throws Exception &#123;    mailboxMetricsControl.getMailCounter().inc();    mail.run();    if (!suspended) &#123;        // start latency measurement on first mail that is not suspending mailbox execution,        // i.e., on first non-poison mail, otherwise latency measurement is not started to avoid        // overhead        if (!mailboxMetricsControl.isLatencyMeasurementStarted()                &amp;&amp; mailboxMetricsControl.isLatencyMeasurementSetup()) &#123;            mailboxMetricsControl.startLatencyMeasurement();        &#125;    &#125;&#125;\nprocessMailsNonBlocking 直接调用 runMail 方法，最终是调用 mail.run 执行具体动作。\nprocessMailsWhenDefaultActionUnavailable 的逻辑是如果当前默认动作不可用，会接着调用 runMail 尝试处理 Mail，这里会阻塞的等待，直到有新的需要处理的 Mail 或者默认动作可用。\n当默认动作可用时，就会执行默认动作，也就是 Stream.processInput，这里就是处理 StreamRecord 了。\nprotected void processInput(MailboxDefaultAction.Controller controller) throws Exception &#123;    DataInputStatus status = inputProcessor.processInput();    switch (status) &#123;        case MORE_AVAILABLE:            if (taskIsAvailable()) &#123;                return;            &#125;            break;        case NOTHING_AVAILABLE:            break;        case END_OF_RECOVERY:            throw new IllegalStateException(&quot;We should not receive this event here.&quot;);        case STOPPED:            endData(StopMode.NO_DRAIN);            return;        case END_OF_DATA:            endData(StopMode.DRAIN);            notifyEndOfData();            return;        case END_OF_INPUT:            // Suspend the mailbox processor, it would be resumed in afterInvoke and finished            // after all records processed by the downstream tasks. We also suspend the default            // actions to avoid repeat executing the empty default operation (namely process            // records).            controller.suspendDefaultAction();            mailboxProcessor.suspend();            return;    &#125;    ...&#125;\n当 status 是 MORE_AVAILABLE，表示还有更多数据可用立即处理，判断当前任务可用就立即返回。当 status 是 END_OF_INPUT 时，表示所有的输入都结束了，这时就会暂停循环事件的调用。\nCheckpoint 流程触发 Checkpoint 的流程是调用 Stream.triggerCheckpointAsync 方法。\npublic CompletableFuture&lt;Boolean&gt; triggerCheckpointAsync(        CheckpointMetaData checkpointMetaData, CheckpointOptions checkpointOptions) &#123;    checkForcedFullSnapshotSupport(checkpointOptions);    MailboxExecutor.MailOptions mailOptions =            CheckpointOptions.AlignmentType.UNALIGNED == checkpointOptions.getAlignment()                    ? MailboxExecutor.MailOptions.urgent()                    : MailboxExecutor.MailOptions.options();    CompletableFuture&lt;Boolean&gt; result = new CompletableFuture&lt;&gt;();    mainMailboxExecutor.execute(            mailOptions,            () -&gt; &#123;                try &#123;                    boolean noUnfinishedInputGates =                            Arrays.stream(getEnvironment().getAllInputGates())                                    .allMatch(InputGate::isFinished);                    if (noUnfinishedInputGates) &#123;                        result.complete(                                triggerCheckpointAsyncInMailbox(                                        checkpointMetaData, checkpointOptions));                    &#125; else &#123;                        result.complete(                                triggerUnfinishedChannelsCheckpoint(                                        checkpointMetaData, checkpointOptions));                    &#125;                &#125; catch (Exception ex) &#123;                    // Report the failure both via the Future result but also to the mailbox                    result.completeExceptionally(ex);                    throw ex;                &#125;            &#125;,            &quot;checkpoint %s with %s&quot;,            checkpointMetaData,            checkpointOptions);    return result;&#125;\n通过调用 mainMailboxExecutor.execute 方法来向 Mailbox 中提交 Mail。Checkpoint 完成的通知也是一样放在 Mailbox 中执行的，不过这里提交的是一个高优先级的操作。\nprivate Future&lt;Void&gt; notifyCheckpointOperation(        RunnableWithException runnable, String description) &#123;    CompletableFuture&lt;Void&gt; result = new CompletableFuture&lt;&gt;();    mailboxProcessor            .getMailboxExecutor(TaskMailbox.MAX_PRIORITY)            .execute(                    () -&gt; &#123;                        try &#123;                            runnable.run();                        &#125; catch (Exception ex) &#123;                            result.completeExceptionally(ex);                            throw ex;                        &#125;                        result.complete(null);                    &#125;,                    description);    return result;&#125;\n总结本文我们梳理了 Mailbox 相关的源码。Flink 通过 Mailbox 线程模型来简化相关代码逻辑。我们先了解了几个核心类：Mail、TaskMailbox、MailboxProcessor、MailboxExecutor。然后梳理了具体的事件处理和触发的流程。\n","tags":["Flink"]},{"title":"Flink源码阅读：Task数据交互","url":"/2025/12/29/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ATask%E6%95%B0%E6%8D%AE%E4%BA%A4%E4%BA%92/","content":"经过前面的学习，Flink 的几个核心概念相关的源码实现我们已经了解了。本文我们来梳理 Task 的数据交互相关的源码。\n数据输出话不多说，我们直接进入正题。首先来看 Task 的数据输出，在进入流程之前，我们先介绍几个基本概念。\n基本概念\nRecordWriterOutput：它是 Output 接口的一个具体实现类，底层使用 RecordWriter 来发送数据。\n\nRecordWriter：数据写入的执行者，负责将数据写到 ResultPartition。\n\nResultPartition 和 ResultSubpartition：ResultPartition 是 ExecutionGraph 中一个节点的输出结果，下游的每个需要从当前 ResultPartition 消费数据的 Task 都会有一个 ResultSubpartition。\n\nChannelSelector：用来决定一个 Record 要被写到哪个 Subpartition 中。\n\nLocalBufferPool：用来管理 Buffer 的缓冲池。在介绍反压的原理时，我们提到过。\n\n\n对这些基本概念有了一定的了解之后，我们来看数据输出的具体流程。\n执行流程我们以 map 为例，看一下数据的输出过程。\n\n在 StreamMap.processElement 方法中，调用完 map 方法之后，就会调用 output.collect 方法将数据输出，这里的 output 就是 RecordWriterOutput。在 RecordWriterOutput 中，会调用 RecordWriter 的 emit 方法。\nprivate &lt;X&gt; void pushToRecordWriter(StreamRecord&lt;X&gt; record) &#123;    serializationDelegate.setInstance(record);    try &#123;        recordWriter.emit(serializationDelegate);    &#125; catch (IOException e) &#123;        throw new UncheckedIOException(e.getMessage(), e);    &#125;&#125;\n这里的 serializationDelegate 是用来对 record 进行序列化的。RecordWriter 有两个实现类，一个是 ChannelSelectorRecordWriter，另一个是 BroadcastRecordWriter。ChannelSelectorRecordWriter 需要先调用 ChannelSelector 选择对应的 subparition，然后进行写入。BroadcastRecordWriter 则是写到所有的 subparition。\n接下来就是调用 BufferWritingResultPartition.emitRecord 来写入数据。\npublic void emitRecord(ByteBuffer record, int targetSubpartition) throws IOException &#123;    totalWrittenBytes += record.remaining();    BufferBuilder buffer = appendUnicastDataForNewRecord(record, targetSubpartition);    while (record.hasRemaining()) &#123;        // full buffer, partial record        finishUnicastBufferBuilder(targetSubpartition);        buffer = appendUnicastDataForRecordContinuation(record, targetSubpartition);    &#125;    if (buffer.isFull()) &#123;        // full buffer, full record        finishUnicastBufferBuilder(targetSubpartition);    &#125;    // partial buffer, full record&#125;\n这里把 record 写入到 buffer 中，如果 buffer 不够，则会从 LocalBufferPool 中申请新的 buffer，申请到之后就会继续写入。下面是具体的申请过程。\nprivate MemorySegment requestMemorySegment(int targetChannel) &#123;    MemorySegment segment = null;    synchronized (availableMemorySegments) &#123;        checkDestroyed();        if (!availableMemorySegments.isEmpty()) &#123;            segment = availableMemorySegments.poll();        &#125; else if (isRequestedSizeReached()) &#123;            // Only when the buffer request reaches the upper limit(i.e. current pool size),            // requests an overdraft buffer.            segment = requestOverdraftMemorySegmentFromGlobal();        &#125;        if (segment == null) &#123;            return null;        &#125;        if (targetChannel != UNKNOWN_CHANNEL) &#123;            if (++subpartitionBuffersCount[targetChannel] == maxBuffersPerChannel) &#123;                unavailableSubpartitionsCount++;            &#125;        &#125;        checkAndUpdateAvailability();    &#125;    return segment;&#125;\n如果有可用内存，就直接从队列中出队。如果达到了本地 BufferPool 的上限，就从全局的 NetworkBufferPool 中申请，申请不到就会阻塞写入过程，等待申请。最后还会检查并更新可用内存状态。\n有了可用的 buffer 之后，就会调用 addToSubpartition，最终数据存储在 PipelinedSubpartition 的 buffers 队列中。\nprivate void addToSubpartition(        BufferBuilder buffer,        int targetSubpartition,        int partialRecordLength,        int minDesirableBufferSize)        throws IOException &#123;    int desirableBufferSize =            subpartitions[targetSubpartition].add(                    buffer.createBufferConsumerFromBeginning(), partialRecordLength);    resizeBuffer(buffer, desirableBufferSize, minDesirableBufferSize);&#125;\n数据输入看完了数据输出的过程之后，我们再来看一下数据输入的过程。首先还是了解几个基本概念。\n基本概念\nInputGate：InputGate 是对输入的封装，与 JobGraph 中的 JobEdge 一一对应，每个 InputGate 消费上游一个或多个 Resultpartition。\n\nInputChannel：InputChannel 是和 ExecutionGraph 中的 ExecutionEdge 一一对应的。每个 InputChannel 接收一个 ResultSubpartition 的输出，InputChannel 主要关注 LocalInputChannel 和 RemoteInputChannel 两种实现。\n\n\n执行流程了解了具体概念之后，我们再看数据输入的具体流程。\n\n数据输入的入口是 StreamTask.processInput 方法，这个方法中主要是调用 inputProcessor.processInput 方法，我们以 StreamOneInputProcessor 为例。这个方法就是调用 input.emitNext 方法。\npublic DataInputStatus emitNext(DataOutput&lt;T&gt; output) throws Exception &#123;    while (true) &#123;        // get the stream element from the deserializer        if (currentRecordDeserializer != null) &#123;            RecordDeserializer.DeserializationResult result;            try &#123;                result = currentRecordDeserializer.getNextRecord(deserializationDelegate);            &#125; catch (IOException e) &#123;                throw new IOException(                        String.format(&quot;Can&#x27;t get next record for channel %s&quot;, lastChannel), e);            &#125;            if (result.isBufferConsumed()) &#123;                currentRecordDeserializer = null;            &#125;            if (result.isFullRecord()) &#123;                final boolean breakBatchEmitting =                        processElement(deserializationDelegate.getInstance(), output);                if (canEmitBatchOfRecords.check() &amp;&amp; !breakBatchEmitting) &#123;                    continue;                &#125;                return DataInputStatus.MORE_AVAILABLE;            &#125;        &#125;        Optional&lt;BufferOrEvent&gt; bufferOrEvent = checkpointedInputGate.pollNext();        if (bufferOrEvent.isPresent()) &#123;            // return to the mailbox after receiving a checkpoint barrier to avoid processing of            // data after the barrier before checkpoint is performed for unaligned checkpoint            // mode            if (bufferOrEvent.get().isBuffer()) &#123;                processBuffer(bufferOrEvent.get());            &#125; else &#123;                DataInputStatus status = processEvent(bufferOrEvent.get(), output);                if (status == DataInputStatus.MORE_AVAILABLE &amp;&amp; canEmitBatchOfRecords.check()) &#123;                    continue;                &#125;                return status;            &#125;        &#125; else &#123;            if (checkpointedInputGate.isFinished()) &#123;                checkState(                        checkpointedInputGate.getAvailableFuture().isDone(),                        &quot;Finished BarrierHandler should be available&quot;);                return DataInputStatus.END_OF_INPUT;            &#125;            return DataInputStatus.NOTHING_AVAILABLE;        &#125;    &#125;&#125;\n这里是调用 checkpointedInputGate.pollNext 来获取输入的数据。它的内部就是调用 InputGate 的 pollNext 方法来获取数据。当获取到完整数据之后，就会调用 processElement 来处理数据。\n我们以 SingleInputGate 为例看 InputGate 的 pollNext 方法。它的内部调用链路可用一直追踪到 readBufferFromInputChannel 方法，这个方法内会调用 inputChannel.getNextBuffer，这里交给 InputChannel 来具体执行数据读取。\npublic Optional&lt;BufferAndAvailability&gt; getNextBuffer() throws IOException &#123;    checkError();    if (!toBeConsumedBuffers.isEmpty()) &#123;        return getBufferAndAvailability(toBeConsumedBuffers.removeFirst());    &#125;    ResultSubpartitionView subpartitionView = this.subpartitionView;    if (subpartitionView == null) &#123;        // There is a possible race condition between writing a EndOfPartitionEvent (1) and        // flushing (3) the Local        // channel on the sender side, and reading EndOfPartitionEvent (2) and processing flush        // notification (4). When        // they happen in that order (1 - 2 - 3 - 4), flush notification can re-enqueue        // LocalInputChannel after (or        // during) it was released during reading the EndOfPartitionEvent (2).        if (isReleased) &#123;            return Optional.empty();        &#125;        // this can happen if the request for the partition was triggered asynchronously        // by the time trigger        // would be good to avoid that, by guaranteeing that the requestPartition() and        // getNextBuffer() always come from the same thread        // we could do that by letting the timer insert a special &quot;requesting channel&quot; into the        // input gate&#x27;s queue        subpartitionView = checkAndWaitForSubpartitionView();    &#125;    BufferAndBacklog next = subpartitionView.getNextBuffer();    // ignore the empty buffer directly    while (next != null &amp;&amp; next.buffer().readableBytes() == 0) &#123;        next.buffer().recycleBuffer();        next = subpartitionView.getNextBuffer();        numBuffersIn.inc();    &#125;    if (next == null) &#123;        if (subpartitionView.isReleased()) &#123;            throw new CancelTaskException(                    &quot;Consumed partition &quot; + subpartitionView + &quot; has been released.&quot;);        &#125; else &#123;            return Optional.empty();        &#125;    &#125;    Buffer buffer = next.buffer();    if (buffer instanceof FullyFilledBuffer) &#123;        List&lt;Buffer&gt; partialBuffers = ((FullyFilledBuffer) buffer).getPartialBuffers();        int seq = next.getSequenceNumber();        for (Buffer partialBuffer : partialBuffers) &#123;            toBeConsumedBuffers.add(                    new BufferAndBacklog(                            partialBuffer,                            next.buffersInBacklog(),                            buffer.getDataType(),                            seq++));        &#125;        return getBufferAndAvailability(toBeConsumedBuffers.removeFirst());    &#125;    return getBufferAndAvailability(next);&#125;\n我们先来看 LocalInputChannel，先获取到了 subpartitionView，并调用 getNextBuffer，这里其实就是从 PipelinedSubpartition 的 buffers 队列中读取数据。\nRemoteInputChannel 则需要从 receivedBuffers 中读取数据，这个队列的数据就是消费上游数据后保存的。\n至此，Flink 中 Task 的数据输入和输出过程的源码就梳理完了，更加底层的 Netty 相关代码我们在后面继续梳理。\n总结最后简单总结一下，本文我们梳理了 Task 的数据输出和输入的过程。输出过程主要是利用 RecordWriter 将数据写入到 Buffer 中，输入过程则是利用 InputChannel 从 Buffer 消费的过程。如果你的 Flink 任务数据量特别大，并且没什么复杂的逻辑，可以考虑适当调整 localBufferPool 的大小来调优任务的吞吐。\n","tags":["Flink"]},{"title":"Flink源码阅读：双流操作","url":"/2025/12/24/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%8F%8C%E6%B5%81%E6%93%8D%E4%BD%9C/","content":"今天来梳理一下 Flink 双流操作相关的源码。\n写在前面通过Flink学习笔记：多流 Join一文的介绍，我们知道 Flink 有三种数据关联的方式，分别是 Window Join、Interval Join 和 CoGroup。下面我们分别看下这三种关联方式的源码实现。\nWindow Join我们先回顾一下 window join 的使用方法。\nDataStream&lt;Tuple2&lt;String, Double&gt;&gt; result = source1.join(source2)        .where(record -&gt; record.f0)        .equalTo(record -&gt; record.f0)        .window(TumblingEventTimeWindows.of(Time.seconds(2L)))        .apply(new JoinFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;() &#123;            @Override            public Tuple2&lt;String, Double&gt; join(Tuple2&lt;String, Double&gt; record1, Tuple2&lt;String, Double&gt; record2) throws Exception &#123;                return Tuple2.of(record1.f0, record1.f1);            &#125;        &#125;);\n上述调用链路类的流转如下：\n\n在 WithWindow 的 apply 方法中，是构建了一个 coGroupedWindowedStream，然后调用它的 apply 方法。\npublic &lt;T&gt; SingleOutputStreamOperator&lt;T&gt; apply(        JoinFunction&lt;T1, T2, T&gt; function, TypeInformation&lt;T&gt; resultType) &#123;    // clean the closure    function = input1.getExecutionEnvironment().clean(function);    coGroupedWindowedStream =            input1.coGroup(input2)                    .where(keySelector1)                    .equalTo(keySelector2)                    .window(windowAssigner)                    .trigger(trigger)                    .evictor(evictor)                    .allowedLateness(allowedLateness);    return coGroupedWindowedStream.apply(new JoinCoGroupFunction&lt;&gt;(function), resultType);&#125;\n这里可以看出，Window Join 的底层是转换成 coGroup 进行处理的。\n在 JoinCoGroupFunction 中，coGroup 方法就是对两个流进行两层遍历，然后将其应用到我们自定义的 JoinFunction 上。\nprivate static class JoinCoGroupFunction&lt;T1, T2, T&gt;        extends WrappingFunction&lt;JoinFunction&lt;T1, T2, T&gt;&gt;        implements CoGroupFunction&lt;T1, T2, T&gt; &#123;    private static final long serialVersionUID = 1L;    public JoinCoGroupFunction(JoinFunction&lt;T1, T2, T&gt; wrappedFunction) &#123;        super(wrappedFunction);    &#125;    @Override    public void coGroup(Iterable&lt;T1&gt; first, Iterable&lt;T2&gt; second, Collector&lt;T&gt; out)            throws Exception &#123;        for (T1 val1 : first) &#123;            for (T2 val2 : second) &#123;                out.collect(wrappedFunction.join(val1, val2));            &#125;        &#125;    &#125;&#125;\nCoGroupCoGroup 的整体用法和流程与 Join 都类似，我们就不逐个介绍了。我们直接来看 apply 方法。\npublic &lt;T&gt; SingleOutputStreamOperator&lt;T&gt; apply(        CoGroupFunction&lt;T1, T2, T&gt; function, TypeInformation&lt;T&gt; resultType) &#123;    // clean the closure    function = input1.getExecutionEnvironment().clean(function);    UnionTypeInfo&lt;T1, T2&gt; unionType =            new UnionTypeInfo&lt;&gt;(input1.getType(), input2.getType());    UnionKeySelector&lt;T1, T2, KEY&gt; unionKeySelector =            new UnionKeySelector&lt;&gt;(keySelector1, keySelector2);    SingleOutputStreamOperator&lt;TaggedUnion&lt;T1, T2&gt;&gt; taggedInput1 =            input1.map(new Input1Tagger&lt;T1, T2&gt;());    taggedInput1.getTransformation().setParallelism(input1.getParallelism(), false);    taggedInput1.returns(unionType);    SingleOutputStreamOperator&lt;TaggedUnion&lt;T1, T2&gt;&gt; taggedInput2 =            input2.map(new Input2Tagger&lt;T1, T2&gt;());    taggedInput2.getTransformation().setParallelism(input2.getParallelism(), false);    taggedInput2.returns(unionType);    DataStream&lt;TaggedUnion&lt;T1, T2&gt;&gt; unionStream = taggedInput1.union(taggedInput2);    // we explicitly create the keyed stream to manually pass the key type information in    windowedStream =            new KeyedStream&lt;TaggedUnion&lt;T1, T2&gt;, KEY&gt;(                            unionStream, unionKeySelector, keyType)                    .window(windowAssigner);    if (trigger != null) &#123;        windowedStream.trigger(trigger);    &#125;    if (evictor != null) &#123;        windowedStream.evictor(evictor);    &#125;    if (allowedLateness != null) &#123;        windowedStream.allowedLateness(allowedLateness);    &#125;    return windowedStream.apply(            new CoGroupWindowFunction&lt;T1, T2, T, KEY, W&gt;(function), resultType);&#125;\n在 apply 方法中，先把两个流进行合并，然后创建了 windowedStream，并把窗口相关的属性设置好，最后是调用 windowedStream 的 apply 方法。\n在调用 windowedStream.apply 方法时，又将 function 包装成了 CoGroupWindowFunction。\nprivate static class CoGroupWindowFunction&lt;T1, T2, T, KEY, W extends Window&gt;        extends WrappingFunction&lt;CoGroupFunction&lt;T1, T2, T&gt;&gt;        implements WindowFunction&lt;TaggedUnion&lt;T1, T2&gt;, T, KEY, W&gt; &#123;    private static final long serialVersionUID = 1L;    public CoGroupWindowFunction(CoGroupFunction&lt;T1, T2, T&gt; userFunction) &#123;        super(userFunction);    &#125;    @Override    public void apply(KEY key, W window, Iterable&lt;TaggedUnion&lt;T1, T2&gt;&gt; values, Collector&lt;T&gt; out)            throws Exception &#123;        List&lt;T1&gt; oneValues = new ArrayList&lt;&gt;();        List&lt;T2&gt; twoValues = new ArrayList&lt;&gt;();        for (TaggedUnion&lt;T1, T2&gt; val : values) &#123;            if (val.isOne()) &#123;                oneValues.add(val.getOne());            &#125; else &#123;                twoValues.add(val.getTwo());            &#125;        &#125;        wrappedFunction.coGroup(oneValues, twoValues, out);    &#125;&#125;\n在 CoGroupWindowFunction 的 apply 方法中是将主键为 key 的流分开两个流，再去调用 JoinCoGroupFunction 的 coGroup 方法。这里的 values 都是相同的 key，原因是在 window 中维护的 windowState，它内部是一个 stateTable，窗口的 namespace 和 key 共同维护一个 state，当窗口触发时，就会对相同 key 的数据调用 apply 方法。\nInterval Join梳理完了 Window Join 和 CoGroup 之后，我们再接着看 Interval Join。还是先来回顾一下用法。\nDataStream&lt;Tuple2&lt;String, Double&gt;&gt; intervalJoinResult = source1.keyBy(record -&gt; record.f0)        .intervalJoin(source2.keyBy(record -&gt; record.f0))        .between(Time.seconds(-2), Time.seconds(2))        .process(new ProcessJoinFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;() &#123;            @Override            public void processElement(Tuple2&lt;String, Double&gt; record1, Tuple2&lt;String, Double&gt; record2, ProcessJoinFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;.Context context, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out) throws Exception &#123;                out.collect(Tuple2.of(record1.f0, record1.f1 + record2.f1));            &#125;        &#125;);\n通过用法可以看出，interval join 传入的对象是两个 KeyedStream，接着使用 between 方法定义 interval join 的上下边界，最后调用 process 方法执行计算逻辑。\n在调用过程中，类型的转换如下图。\n\n我们主要关注 process 的逻辑。\npublic &lt;OUT&gt; SingleOutputStreamOperator&lt;OUT&gt; process(        ProcessJoinFunction&lt;IN1, IN2, OUT&gt; processJoinFunction,        TypeInformation&lt;OUT&gt; outputType) &#123;    Preconditions.checkNotNull(processJoinFunction);    Preconditions.checkNotNull(outputType);    final ProcessJoinFunction&lt;IN1, IN2, OUT&gt; cleanedUdf =            left.getExecutionEnvironment().clean(processJoinFunction);    if (isEnableAsyncState) &#123;        final AsyncIntervalJoinOperator&lt;KEY, IN1, IN2, OUT&gt; operator =                new AsyncIntervalJoinOperator&lt;&gt;(                        lowerBound,                        upperBound,                        lowerBoundInclusive,                        upperBoundInclusive,                        leftLateDataOutputTag,                        rightLateDataOutputTag,                        left.getType()                                .createSerializer(                                        left.getExecutionConfig().getSerializerConfig()),                        right.getType()                                .createSerializer(                                        right.getExecutionConfig().getSerializerConfig()),                        cleanedUdf);        return left.connect(right)                .keyBy(keySelector1, keySelector2)                .transform(&quot;Interval Join [Async]&quot;, outputType, operator);    &#125; else &#123;        final IntervalJoinOperator&lt;KEY, IN1, IN2, OUT&gt; operator =                new IntervalJoinOperator&lt;&gt;(                        lowerBound,                        upperBound,                        lowerBoundInclusive,                        upperBoundInclusive,                        leftLateDataOutputTag,                        rightLateDataOutputTag,                        left.getType()                                .createSerializer(                                        left.getExecutionConfig().getSerializerConfig()),                        right.getType()                                .createSerializer(                                        right.getExecutionConfig().getSerializerConfig()),                        cleanedUdf);        return left.connect(right)                .keyBy(keySelector1, keySelector2)                .transform(&quot;Interval Join&quot;, outputType, operator);    &#125;&#125;\nInterval join 是基于 ConnectedStream 实现的，ConnectedStream 提供了更加通用的双流操作，它将两个流组合成一个 TwoInputTransformation，然后加入执行图中。\n具体的 Operator 是 IntervalJoinOperator 或 AsyncIntervalJoinOperator，它们都是 TwoInputStreamOperator 的实现类，提供 processElement1  和 processElement2 两个方法分别处理两个输入源的数据，最终都调用的是 processElement。\nprivate &lt;THIS, OTHER&gt; void processElement(        final StreamRecord&lt;THIS&gt; record,        final MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;THIS&gt;&gt;&gt; ourBuffer,        final MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;OTHER&gt;&gt;&gt; otherBuffer,        final long relativeLowerBound,        final long relativeUpperBound,        final boolean isLeft)        throws Exception &#123;    final THIS ourValue = record.getValue();    final long ourTimestamp = record.getTimestamp();    if (ourTimestamp == Long.MIN_VALUE) &#123;        throw new FlinkException(                &quot;Long.MIN_VALUE timestamp: Elements used in &quot;                        + &quot;interval stream joins need to have timestamps meaningful timestamps.&quot;);    &#125;    if (isLate(ourTimestamp)) &#123;        sideOutput(ourValue, ourTimestamp, isLeft);        return;    &#125;    addToBuffer(ourBuffer, ourValue, ourTimestamp);    for (Map.Entry&lt;Long, List&lt;BufferEntry&lt;OTHER&gt;&gt;&gt; bucket : otherBuffer.entries()) &#123;        final long timestamp = bucket.getKey();        if (timestamp &lt; ourTimestamp + relativeLowerBound                || timestamp &gt; ourTimestamp + relativeUpperBound) &#123;            continue;        &#125;        for (BufferEntry&lt;OTHER&gt; entry : bucket.getValue()) &#123;            if (isLeft) &#123;                collect((T1) ourValue, (T2) entry.element, ourTimestamp, timestamp);            &#125; else &#123;                collect((T1) entry.element, (T2) ourValue, timestamp, ourTimestamp);            &#125;        &#125;    &#125;    long cleanupTime =            (relativeUpperBound &gt; 0L) ? ourTimestamp + relativeUpperBound : ourTimestamp;    if (isLeft) &#123;        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_LEFT, cleanupTime);    &#125; else &#123;        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_RIGHT, cleanupTime);    &#125;&#125;\n在 IntervalJoinOperator 中维护了两个 MapState，每个消息进来的时候，都会加入到 MapState 中，key 是 timestamp，value 是一个元素的列表。然后遍历另一个 MapState，得到符合条件的数据。最后是为每条数据注册一个定时器，当时间超过有效范围后，会从 MapState 中清除这个时间戳的数据。\n总结本文我们梳理了 Flink 的三种双流操作的源码，我们了解到 Window Join 底层是通过 CoGroup 实现的。CoGroup 本身是将两个流合并成 WindowedStream 并依赖于 WindowState 进行数据 join。最后 Interval Join 是通过 ConnectedStreams 实现的，内部的 IntervalJoinOperator 会维护两个 MapState，通过 MapState 进行数据关联。\n","tags":["Flink"]},{"title":"Flink源码阅读：Watermark机制","url":"/2025/12/17/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AWatermark%E6%9C%BA%E5%88%B6/","content":"前面我们已经梳理了 Flink 状态和 Checkpoint 相关的源码。从本文开始，我们再来关注另外几个核心概念，即时间、Watermark 和窗口。\n写在前面在 Flink 中 Watermark 是用来解决数据乱序问题的，它也是窗口关闭的触发条件。对于 Watermark 的概念和用法还不熟悉的同学可以先阅读Flink学习笔记：时间与Watermark一文。下面我们进入正题，开始梳理 Watermark 相关的源码。\nWatermark 定义Watermark 的定义非常简单，它继承了 StreamElement 类，内部只有一个 timestamp 变量。\n@PublicEvolvingpublic class Watermark extends StreamElement &#123;    /** The watermark that signifies end-of-event-time. */    public static final Watermark MAX_WATERMARK = new Watermark(Long.MAX_VALUE);    /** The watermark that signifies is used before any actual watermark has been generated. */    public static final Watermark UNINITIALIZED = new Watermark(Long.MIN_VALUE);    // ------------------------------------------------------------------------    /** The timestamp of the watermark in milliseconds. */    protected final long timestamp;    /** Creates a new watermark with the given timestamp in milliseconds. */    public Watermark(long timestamp) &#123;        this.timestamp = timestamp;    &#125;    /** Returns the timestamp associated with this &#123;@link Watermark&#125; in milliseconds. */    public long getTimestamp() &#123;        return timestamp;    &#125;    // ------------------------------------------------------------------------    @Override    public boolean equals(Object o) &#123;        return this == o                || o != null                        &amp;&amp; o.getClass() == this.getClass()                        &amp;&amp; ((Watermark) o).timestamp == timestamp;    &#125;    @Override    public int hashCode() &#123;        return (int) (timestamp ^ (timestamp &gt;&gt;&gt; 32));    &#125;    @Override    public String toString() &#123;        return &quot;Watermark @ &quot; + timestamp;    &#125;&#125;\nWatermark 处理过程我们先来回顾一下 Watermark 的生成方法。\nSingleOutputStreamOperator&lt;Event&gt; withTimestampsAndWatermarks = source        .assignTimestampsAndWatermarks(                WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(20))        );\n初始化在定义 Watermark 的时候，我们调用 assignTimestampsAndWatermarks 方法。\npublic SingleOutputStreamOperator&lt;T&gt; assignTimestampsAndWatermarks(        WatermarkStrategy&lt;T&gt; watermarkStrategy) &#123;    final WatermarkStrategy&lt;T&gt; cleanedStrategy = clean(watermarkStrategy);    // match parallelism to input, to have a 1:1 source -&gt; timestamps/watermarks relationship    // and chain    final int inputParallelism = getTransformation().getParallelism();    final TimestampsAndWatermarksTransformation&lt;T&gt; transformation =            new TimestampsAndWatermarksTransformation&lt;&gt;(                    &quot;Timestamps/Watermarks&quot;,                    inputParallelism,                    getTransformation(),                    cleanedStrategy,                    false);    getExecutionEnvironment().addOperator(transformation);    return new SingleOutputStreamOperator&lt;&gt;(getExecutionEnvironment(), transformation);&#125;\n这个方法接收了一个 WatermarkStrategy 参数，把它封装到 TimestampsAndWatermarksTransformation 中之后，就添加到 transformations 列表中了。在生成 StreamGraph 的过程中，会调用每个 transformation 的 transform 方法。\n\n通过这个调用链路，创建出了 TimestampsAndWatermarksOperatorFactory，在初始化 StreamTask 时，会调用 TimestampsAndWatermarksOperatorFactory.createStreamOperator 方法来创建 TimestampsAndWatermarksOperator，并调用它的 open 方法。\n在这个 open 方法中，主要是生成 timestampAssigner 和 watermarkGenerator。timestampAssigner 是用于提取时间戳，watermarkGenerator 是用于生成 Watermark。\n生成完成之后注册了一个定时器，到指定时间后会调用 onProcessingTime 方法。\npublic void onProcessingTime(long timestamp) throws Exception &#123;    watermarkGenerator.onPeriodicEmit(wmOutput);    final long now = getProcessingTimeService().getCurrentProcessingTime();    getProcessingTimeService().registerTimer(now + watermarkInterval, this);&#125;\n这个方法的逻辑也很简单，先发送创建并发送 Watermark，然后再注册一个定时器。\n发送 Watermark\n我们以 BoundedOutOfOrdernessWatermarks 为例，它向下游发送了一个 Watermark，时间戳为 maxTimestamp - outOfOrdernessMillis - 1（maxTimestamp 是当前最大的事件时间戳，outOfOrdernessMillis 是我们定义的周期时间毫秒值）。随后在 WatermarkEmitter.emitWatermark 方法中，更新了当前 Watermark 的值。最后 RecordWriterOutput.emitWatermark 则是向下游广播当前的 Watermark。\n下游处理下游处理方法我们从 StreamOneInputProcessor.processInput 入手，先来看具体的调用链路。\n\n在 inputWatermark 方法中，先是对 alignedSubpartitionStatuses 进行调整，alignedSubpartitionStatuses 这个变量主要是用来获取最小的 Watermark。最后调用了 findAndOutputNewMinWatermarkAcrossAlignedSubpartitions 方法。这个方法中，会获取到所有上游最小的 Watermark，如果它大于最近发送的一个 Watermark，就会向下游发送。\npublic void emitWatermark(Watermark watermark) throws Exception &#123;    watermarkGauge.setCurrentWatermark(watermark.getTimestamp());    operator.processWatermark(watermark);&#125;\n这个发送方法中，调用了 operator.processWatermark，我们接着看这个处理方法。\n\n在 tryAdvanceWatermark 方法中如果 Watermark 的时间大于 eventTimeTimersQueue 队列中头节点的时间，那么对 eventTimeTimersQueue 这个队列进行出队操作，这个操作意味着触发了窗口计算。\npublic boolean tryAdvanceWatermark(        long time, InternalTimeServiceManager.ShouldStopAdvancingFn shouldStopAdvancingFn)        throws Exception &#123;    currentWatermark = time;    InternalTimer&lt;K, N&gt; timer;    boolean interrupted = false;    while ((timer = eventTimeTimersQueue.peek()) != null            &amp;&amp; timer.getTimestamp() &lt;= time            &amp;&amp; !cancellationContext.isCancelled()            &amp;&amp; !interrupted) &#123;        keyContext.setCurrentKey(timer.getKey());        eventTimeTimersQueue.poll();        triggerTarget.onEventTime(timer);        taskIOMetricGroup.getNumFiredTimers().inc();        // Check if we should stop advancing after at least one iteration to guarantee progress        // and prevent a potential starvation.        interrupted = shouldStopAdvancingFn.test();    &#125;    return !interrupted;&#125;\n之后 Watermark 就随着数据流一直到 sink 节点，在 StreamSink 中，支持用户自己实现方法向 sink 中写入 Watermark，除此之外什么也不做。\n总结本文我们一起梳理了 Watermark 相关的源码，从 Watermark 的定义，到 Watermark 的处理过程。处理过程分成了初始化、上游发送和下游处理三部分。在下游处理部分，关于触发窗口计算的部分我们简单带过了，后面会再详细介绍这部分。\n","tags":["Flink"]},{"title":"Flink源码阅读：状态管理","url":"/2025/12/03/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/","content":"前面我们介绍了 Flink 状态的分类和应用。今天从源码层面再看一下 Flink 是如何管理状态的。\nState 概述关于 State 的详细介绍可以参考 Flink学习笔记：状态类型和应用 和 Flink学习笔记：状态后端这两篇文章，为了方面阅读，这里我们再简单介绍一下。\nState 使用State 是 Flink 做复杂逻辑所依赖的核心组件。它的分类如下\n\n常见的是 Keyed State 和 Operator State，Keyed State 作用于 KeyedStream 上，Operator State 可以作用于所有的 Operator 上。Keyed State 使用时，需要先创建 StateDescriptor，然后再调用 getState 获取。\nValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =        new ValueStateDescriptor&lt;&gt;(                &quot;average&quot;,                TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum = getRuntimeContext().getState(descriptor);\nOpeartor State 的获取方式与 Keyed State 类似，都需要 StateDescriptor。Operator State 在定义时需要实现 CheckpointedFunction。\nState 存储State Backend 用来管理 State 存储，根据存储格式和存储类型的组合，可以分为三类：\n\nMemoryStateBackend：HashMapStateBackend 和 JobManagerCheckpointStorage 的组合，即将 State 以 Java 对象的形式存储在 JobManager 内存中。\n\nFsStateBackend：HashMapStateBackend 和 FileSystemCheckpointStorage 的组合，将 State 以 Java 对象的形式存储在远端文件系统中。\n\nRocksDBStateBackend：EmbeddedRocksDBStateBackend 和 FileSystemCheckpointStorage 的组合，State 序列化后存储在 RocksDB。\n\n\n创建 State Backend创建 State Backend 的入口在 StreamTask，StreamTask 是 Flink 部署和运行在 TaskManager 的基本单元。\n在 StreamTask 的 invoke 方法中，会先调用 restoreStateAndGates 方法去创建 State Backend。完整的调用链路如下图所示。\n\n在 streamOperatorStateContext 方法中，分别调用了 keyedStatedBackend 和 operatorStateBackend 来创建两种 State Backend。\n我们先来看 keyedStateBackend 的逻辑。\nprotected &lt;K, R extends Disposable &amp; Closeable&gt; R keyedStatedBackend(        TypeSerializer&lt;K&gt; keySerializer,        String operatorIdentifierText,        PrioritizedOperatorSubtaskState prioritizedOperatorSubtaskStates,        CloseableRegistry backendCloseableRegistry,        MetricGroup metricGroup,        double managedMemoryFraction,        StateObject.StateObjectSizeStatsCollector statsCollector,        KeyedStateBackendCreator&lt;K, R&gt; keyedStateBackendCreator)        throws Exception &#123;    if (keySerializer == null) &#123;        return null;    &#125;    ......    final KeyGroupRange keyGroupRange =            KeyGroupRangeAssignment.computeKeyGroupRangeForOperatorIndex(                    taskInfo.getMaxNumberOfParallelSubtasks(),                    taskInfo.getNumberOfParallelSubtasks(),                    taskInfo.getIndexOfThisSubtask());    // Now restore processing is included in backend building/constructing process, so we need    // to make sure    // each stream constructed in restore could also be closed in case of task cancel, for    // example the data    // input stream opened for serDe during restore.    CloseableRegistry cancelStreamRegistryForRestore = new CloseableRegistry();    backendCloseableRegistry.registerCloseable(cancelStreamRegistryForRestore);    BackendRestorerProcedure&lt;R, KeyedStateHandle&gt; backendRestorer =            new BackendRestorerProcedure&lt;&gt;(                    (stateHandles) -&gt; &#123;                        KeyedStateBackendParametersImpl&lt;K&gt; parameters =                                new KeyedStateBackendParametersImpl&lt;&gt;(...);                        return keyedStateBackendCreator.create(...),                                parameters);                    &#125;,                    backendCloseableRegistry,                    logDescription);    try &#123;        return backendRestorer.createAndRestore(                prioritizedOperatorSubtaskStates.getPrioritizedManagedKeyedState(),                statsCollector);    &#125; finally &#123;        if (backendCloseableRegistry.unregisterCloseable(cancelStreamRegistryForRestore)) &#123;            IOUtils.closeQuietly(cancelStreamRegistryForRestore);        &#125;    &#125;&#125;\n这里的创建过程也比较简单，先是获取 KeyGroupRange，它表示的是当前 Operator 上处理的 key 的范围。然后就是创建 StateBackend 实例，这里通过 BackendRestorerProcedure 封装统一的恢复、异常处理和资源清理逻辑。operatorStateBackend 方法的逻辑相比较来说，只是少了 KeyGroupRange 的处理，直接创建 StateBackend 实例。\n创建和使用 State创建 KeyedStateKeyedState 是通过调用 StreamingRuntimeContext.getState 方法获取的。我们先来看完整的调用流程。\n\n在调用 getState 这些方法时，都会先调用 keyedStateStore 提供的方法，它是 Flink 提供的一个封装 keyedStateBackend 的接口。调用流程的最后，是调用 keyedStateBackend 中的 createOrUpdateInternalState 方法（这里我们以 HeapStateBackend 为例）。\npublic &lt;N, SV, SEV, S extends State, IS extends S&gt; IS createOrUpdateInternalState(        @Nonnull TypeSerializer&lt;N&gt; namespaceSerializer,        @Nonnull StateDescriptor&lt;S, SV&gt; stateDesc,        @Nonnull StateSnapshotTransformFactory&lt;SEV&gt; snapshotTransformFactory,        boolean allowFutureMetadataUpdates)        throws Exception &#123;    StateTable&lt;K, N, SV&gt; stateTable =            tryRegisterStateTable(                    namespaceSerializer,                    stateDesc,                    getStateSnapshotTransformFactory(stateDesc, snapshotTransformFactory),                    allowFutureMetadataUpdates);    @SuppressWarnings(&quot;unchecked&quot;)    IS createdState = (IS) createdKVStates.get(stateDesc.getName());    if (createdState == null) &#123;        StateCreateFactory stateCreateFactory = STATE_CREATE_FACTORIES.get(stateDesc.getType());        if (stateCreateFactory == null) &#123;            throw new FlinkRuntimeException(stateNotSupportedMessage(stateDesc));        &#125;        createdState =                stateCreateFactory.createState(stateDesc, stateTable, getKeySerializer());    &#125; else &#123;        StateUpdateFactory stateUpdateFactory = STATE_UPDATE_FACTORIES.get(stateDesc.getType());        if (stateUpdateFactory == null) &#123;            throw new FlinkRuntimeException(stateNotSupportedMessage(stateDesc));        &#125;        createdState = stateUpdateFactory.updateState(stateDesc, stateTable, createdState);    &#125;    createdKVStates.put(stateDesc.getName(), createdState);    return createdState;&#125;private static final Map&lt;StateDescriptor.Type, StateCreateFactory&gt; STATE_CREATE_FACTORIES =        Stream.of(                        Tuple2.of(                                StateDescriptor.Type.VALUE,                                (StateCreateFactory) HeapValueState::create),                        Tuple2.of(                                StateDescriptor.Type.LIST,                                (StateCreateFactory) HeapListState::create),                        Tuple2.of(                                StateDescriptor.Type.MAP,                                (StateCreateFactory) HeapMapState::create),                        Tuple2.of(                                StateDescriptor.Type.AGGREGATING,                                (StateCreateFactory) HeapAggregatingState::create),                        Tuple2.of(                                StateDescriptor.Type.REDUCING,                                (StateCreateFactory) HeapReducingState::create))                .collect(Collectors.toMap(t -&gt; t.f0, t -&gt; t.f1));\n这里首先是注册了一个 StateTable，这个是 State 中一个非常重要的成员变量，它内部是一个类似 Map 的结构，用来保存 key 和 key 的状态。\nSTATE_CREATE_FACTORIES 这个变量保存了不同类型的 State 和它对应的创建方法，同理 STATE_UPDATE_FACTORIES 保存的是不同 State 对应的 更新方法。\n创建 OperatorState看完了 KeyedState 的创建过程后，我们再来看下 OperatorState 的创建过程。\nOperatorState 的创建方法是通过 FunctionInitializationContext 先获取到 OperatorStateStore，它与 KeyedStateStore 类似，都是对 StateBackend 的方法进行了封装。\n@Overridepublic void initializeState(FunctionInitializationContext context) throws Exception &#123;    ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =            new ListStateDescriptor&lt;&gt;(                    &quot;buffered-elements&quot;,                    TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));    checkpointedState = context.getOperatorStateStore().getListState(descriptor);    if (context.isRestored()) &#123;        for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;            bufferedElements.add(element);        &#125;    &#125;&#125;\nOperatorStateStore 的 getListState 方法中，直接创建出了 PartitionableListState，同时也做了一些缓存操作。\nprivate &lt;S&gt; ListState&lt;S&gt; getListState(        ListStateDescriptor&lt;S&gt; stateDescriptor, OperatorStateHandle.Mode mode)        throws StateMigrationException &#123;    ......    PartitionableListState&lt;S&gt; partitionableListState =            (PartitionableListState&lt;S&gt;) registeredOperatorStates.get(name);    if (null == partitionableListState) &#123;        // no restored state for the state name; simply create new state holder        partitionableListState =                new PartitionableListState&lt;&gt;(                        new RegisteredOperatorStateBackendMetaInfo&lt;&gt;(                                name, partitionStateSerializer, mode));        registeredOperatorStates.put(name, partitionableListState);    &#125; else &#123;        ......    &#125;    accessedStatesByName.put(name, partitionableListState);    return partitionableListState;&#125;\nPartitionableListState 内部有一个 ArrayList 用于保存数据。\n使用 KeyedState了解完 State 的创建之后，接下来就是 State 的使用了。我们以 HeapValueState 为例来看如何获取 State。\n// HeapValueState 类public V value() &#123;    final V result = stateTable.get(currentNamespace);    if (result == null) &#123;        return getDefaultValue();    &#125;    return result;&#125;\n在 HeapValueState 类的 value 方法中，直接调用 StateTable 的 get 方法，最终调用的是 CopyOnWriteStateMap 的 get 方法，这个方法与 HashMap 的 get 方法比较类似。\npublic S get(K key, N namespace) &#123;    final int hash = computeHashForOperationAndDoIncrementalRehash(key, namespace);    final int requiredVersion = highestRequiredSnapshotVersion;    final StateMapEntry&lt;K, N, S&gt;[] tab = selectActiveTable(hash);    int index = hash &amp; (tab.length - 1);    for (StateMapEntry&lt;K, N, S&gt; e = tab[index]; e != null; e = e.next) &#123;        final K eKey = e.key;        final N eNamespace = e.namespace;        if ((e.hash == hash &amp;&amp; key.equals(eKey) &amp;&amp; namespace.equals(eNamespace))) &#123;            // copy-on-write check for state            if (e.stateVersion &lt; requiredVersion) &#123;                // copy-on-write check for entry                if (e.entryVersion &lt; requiredVersion) &#123;                    e = handleChainedEntryCopyOnWrite(tab, hash &amp; (tab.length - 1), e);                &#125;                e.stateVersion = stateMapVersion;                e.state = getStateSerializer().copy(e.state);            &#125;            return e.state;        &#125;    &#125;    return null;&#125;\n使用 OperatorStateOperatorState 底层使用的是 PartitionableListState，前面也提到了，它的内部用了一个 ArrayList 来保存数据，对于 OperatorState 的各种操作也都是来操作这个 ArrayList。\n@Overridepublic void clear() &#123;    internalList.clear();&#125;@Overridepublic Iterable&lt;S&gt; get() &#123;    return internalList;&#125;@Overridepublic void add(S value) &#123;    Preconditions.checkNotNull(value, &quot;You cannot add null to a ListState.&quot;);    internalList.add(value);&#125;@Overridepublic void update(List&lt;S&gt; values) &#123;    internalList.clear();    addAll(values);&#125;@Overridepublic void addAll(List&lt;S&gt; values) &#123;    Preconditions.checkNotNull(values, &quot;List of values to add cannot be null.&quot;);    if (!values.isEmpty()) &#123;        for (S value : values) &#123;            checkNotNull(value, &quot;Any value to add to a list cannot be null.&quot;);            add(value);        &#125;    &#125;&#125;\n总结本文对 State 的相关代码进行了梳理。包括 StateBackend 的创建，KeyedState 和 OperatorState 的创建和使用。State 和 Checkpoint 两者需要结合使用，因此后面我们会再梳理 Checkpoint 的相关代码。\n","tags":["Flink"]},{"title":"Flink源码阅读：如何生成ExecutionGraph","url":"/2025/11/08/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90ExecutionGraph/","content":"今天我们一起来了解 Flink 最后一种执行图，ExecutionGraph 的执行过程。\n基本概念在阅读源码之前，我们先来了解一下 ExecutionGraph 中的一些基本概念。\n\nExecutionJobVertex:  ExecutionJobVertex 是 ExecutionGraph 中的节点，对应的是 JobGraph 中的 JobVertex。\n\nExecutionVertex: 每个 ExecutionJobVertex 都包含了一组 ExecutionVertex，ExecutionVertex 的数量就是节点对应的并行度。\n\nIntermediateResult: IntermediateResult 表示节点的输出结果，与之对应的是 JobGraph 中的 IntermediateDataSet。\n\nIntermediateResultPartition: IntermediateResultPartition 是每个 ExecutionVertex 的输出。\n\nEdgeManager: EdgeManager 主要负责存储 ExecutionGraph 中所有之间的连接，包括其并行度。\n\nExecution: Execution 可以认为是一次实际的运行尝试。每次执行时，Flink 都会将ExecutionVertex 封装成一个 Execution，并通过一个 ExecutionAttemptID 来做唯一标识。\n\n\nExecutionGraph 生成过程了解了这些基本概念之后，我们一起来看一下 ExecutionGraph 的具体生成过程。生成 ExecutionGraph 的代码入口是 DefaultExecutionGraphBuilder.build 方法。\n首先是获取一些基本信息，包括 jobInformation、jobStatusChangedListeners 等。\n接下来就是创建一个 DefaultExecutionGraph 和生成执行计划。\n// create a new execution graph, if none exists so farfinal DefaultExecutionGraph executionGraph =        new DefaultExecutionGraph(                jobInformation,                futureExecutor,                ioExecutor,                rpcTimeout,                executionHistorySizeLimit,                classLoader,                blobWriter,                partitionGroupReleaseStrategyFactory,                shuffleMaster,                partitionTracker,                executionDeploymentListener,                executionStateUpdateListener,                initializationTimestamp,                vertexAttemptNumberStore,                vertexParallelismStore,                isDynamicGraph,                executionJobVertexFactory,                jobGraph.getJobStatusHooks(),                markPartitionFinishedStrategy,                taskDeploymentDescriptorFactory,                jobStatusChangedListeners,                executionPlanSchedulingContext);try &#123;    executionGraph.setPlan(JsonPlanGenerator.generatePlan(jobGraph));&#125; catch (Throwable t) &#123;    log.warn(&quot;Cannot create plan for job&quot;, t);    // give the graph an empty plan    executionGraph.setPlan(new JobPlanInfo.Plan(&quot;&quot;, &quot;&quot;, &quot;&quot;, new ArrayList&lt;&gt;()));&#125;\n下面就是两个比较核心的方法 getVerticesSortedTopologicallyFromSources 和 attachJobGraph。\n// topologically sort the job vertices and attach the graph to the existing oneList&lt;JobVertex&gt; sortedTopology = jobGraph.getVerticesSortedTopologicallyFromSources();executionGraph.attachJobGraph(sortedTopology, jobManagerJobMetricGroup);\n这两个方法是先将 JobVertex 进行排序，然后构建 ExecutionGraph 的拓扑图。\ngetVerticesSortedTopologicallyFromSourcespublic List&lt;JobVertex&gt; getVerticesSortedTopologicallyFromSources()        throws InvalidProgramException &#123;    // early out on empty lists    if (this.taskVertices.isEmpty()) &#123;        return Collections.emptyList();    &#125;    List&lt;JobVertex&gt; sorted = new ArrayList&lt;JobVertex&gt;(this.taskVertices.size());    Set&lt;JobVertex&gt; remaining = new LinkedHashSet&lt;JobVertex&gt;(this.taskVertices.values());    // start by finding the vertices with no input edges    // and the ones with disconnected inputs (that refer to some standalone data set)    &#123;        Iterator&lt;JobVertex&gt; iter = remaining.iterator();        while (iter.hasNext()) &#123;            JobVertex vertex = iter.next();            if (vertex.isInputVertex()) &#123;                sorted.add(vertex);                iter.remove();            &#125;        &#125;    &#125;    int startNodePos = 0;    // traverse from the nodes that were added until we found all elements    while (!remaining.isEmpty()) &#123;        // first check if we have more candidates to start traversing from. if not, then the        // graph is cyclic, which is not permitted        if (startNodePos &gt;= sorted.size()) &#123;            throw new InvalidProgramException(&quot;The job graph is cyclic.&quot;);        &#125;        JobVertex current = sorted.get(startNodePos++);        addNodesThatHaveNoNewPredecessors(current, sorted, remaining);    &#125;    return sorted;&#125;\n这段代码是将所有的节点进行排序，先将所有的 Source 节点筛选出来，然后再将剩余节点假如列表。这样就能构建出最终的拓扑图。\nattachJobGraph@Overridepublic void attachJobGraph(        List&lt;JobVertex&gt; verticesToAttach, JobManagerJobMetricGroup jobManagerJobMetricGroup)        throws JobException &#123;    assertRunningInJobMasterMainThread();    LOG.debug(            &quot;Attaching &#123;&#125; topologically sorted vertices to existing job graph with &#123;&#125; &quot;                    + &quot;vertices and &#123;&#125; intermediate results.&quot;,            verticesToAttach.size(),            tasks.size(),            intermediateResults.size());    attachJobVertices(verticesToAttach, jobManagerJobMetricGroup);    if (!isDynamic) &#123;        initializeJobVertices(verticesToAttach);    &#125;    // the topology assigning should happen before notifying new vertices to failoverStrategy    executionTopology = DefaultExecutionTopology.fromExecutionGraph(this);    partitionGroupReleaseStrategy =            partitionGroupReleaseStrategyFactory.createInstance(getSchedulingTopology());&#125;\nattachJobGraph 方法主要包含两步逻辑，第一步是调用 attachJobVertices 方法创建 ExecutionJobVertex 实例，第二步是调用 fromExecutionGraph 创建一些其他的核心对象。\nattachJobVertices\nattachJobVertices 方法中就是遍历所有的 JobVertex，然后利用 JobVertex 生成 ExecutionJobVertex。\n/** Attach job vertices without initializing them. */private void attachJobVertices(        List&lt;JobVertex&gt; topologicallySorted, JobManagerJobMetricGroup jobManagerJobMetricGroup)        throws JobException &#123;    for (JobVertex jobVertex : topologicallySorted) &#123;        if (jobVertex.isInputVertex() &amp;&amp; !jobVertex.isStoppable()) &#123;            this.isStoppable = false;        &#125;        VertexParallelismInformation parallelismInfo =                parallelismStore.getParallelismInfo(jobVertex.getID());        // create the execution job vertex and attach it to the graph        ExecutionJobVertex ejv =                executionJobVertexFactory.createExecutionJobVertex(                        this,                        jobVertex,                        parallelismInfo,                        coordinatorStore,                        jobManagerJobMetricGroup);        ExecutionJobVertex previousTask = this.tasks.putIfAbsent(jobVertex.getID(), ejv);        if (previousTask != null) &#123;            throw new JobException(                    String.format(                            &quot;Encountered two job vertices with ID %s : previous=[%s] / new=[%s]&quot;,                            jobVertex.getID(), ejv, previousTask));        &#125;        this.verticesInCreationOrder.add(ejv);        this.numJobVerticesTotal++;    &#125;&#125;\ninitializeJobVertices\n在 DefaultExecutionGraph.initializeJobVertices 中是遍历了刚刚排好序的 JobVertex，获取了 ExecutionJobVertex 之后调用了 ExecutionGraph.initializeJobVertex 方法。\n我们直接来看 ExecutionGraph.initializeJobVertex 的逻辑。\ndefault void initializeJobVertex(ExecutionJobVertex ejv, long createTimestamp)        throws JobException &#123;    initializeJobVertex(            ejv,            createTimestamp,            VertexInputInfoComputationUtils.computeVertexInputInfos(                    ejv, getAllIntermediateResults()::get));&#125;\n这里先是调用了 VertexInputInfoComputationUtils.computeVertexInputInfos 方法，生成了 Map&lt;IntermediateDataSetID, JobVertexInputInfo&gt; jobVertexInputInfos。它表示的是每个 ExecutionVertex 消费上游 IntermediateResultPartition 的范围。\n这里有两种模式，分别是 POINTWISE （点对点）和 ALL_TO_ALL（全对全）\n在 POINTWISE 模式中，会按照尽量均匀分布的方式处理。\n\n例如上游并发度是4，下游并发度是2时，那么前两个 IntermediateResultPartition 就会被第一个 ExecutionVertex 消费，后两个 IntermediateResultPartition 就会被第二个 ExecutionVertex 消费。\n\n如果上游并发度是2，下游是3时，那么下游前两个 IntermediateResultPartition 会被第一个 ExecutionVertex 消费，第三个 IntermediateResultPartition 则会被第二个 ExecutionVertex 消费。\n\n\npublic static JobVertexInputInfo computeVertexInputInfoForPointwise(        int sourceCount,        int targetCount,        Function&lt;Integer, Integer&gt; numOfSubpartitionsRetriever,        boolean isDynamicGraph) &#123;    final List&lt;ExecutionVertexInputInfo&gt; executionVertexInputInfos = new ArrayList&lt;&gt;();    if (sourceCount &gt;= targetCount) &#123;        for (int index = 0; index &lt; targetCount; index++) &#123;            int start = index * sourceCount / targetCount;            int end = (index + 1) * sourceCount / targetCount;            IndexRange partitionRange = new IndexRange(start, end - 1);            IndexRange subpartitionRange =                    computeConsumedSubpartitionRange(                            index,                            1,                            () -&gt; numOfSubpartitionsRetriever.apply(start),                            isDynamicGraph,                            false,                            false);            executionVertexInputInfos.add(                    new ExecutionVertexInputInfo(index, partitionRange, subpartitionRange));        &#125;    &#125; else &#123;        for (int partitionNum = 0; partitionNum &lt; sourceCount; partitionNum++) &#123;            int start = (partitionNum * targetCount + sourceCount - 1) / sourceCount;            int end = ((partitionNum + 1) * targetCount + sourceCount - 1) / sourceCount;            int numConsumers = end - start;            IndexRange partitionRange = new IndexRange(partitionNum, partitionNum);            // Variable used in lambda expression should be final or effectively final            final int finalPartitionNum = partitionNum;            for (int i = start; i &lt; end; i++) &#123;                IndexRange subpartitionRange =                        computeConsumedSubpartitionRange(                                i,                                numConsumers,                                () -&gt; numOfSubpartitionsRetriever.apply(finalPartitionNum),                                isDynamicGraph,                                false,                                false);                executionVertexInputInfos.add(                        new ExecutionVertexInputInfo(i, partitionRange, subpartitionRange));            &#125;        &#125;    &#125;    return new JobVertexInputInfo(executionVertexInputInfos);&#125;\n在 ALL_TO_ALL 模式中，每个下游都会消费所有上游的数据。\npublic static JobVertexInputInfo computeVertexInputInfoForAllToAll(        int sourceCount,        int targetCount,        Function&lt;Integer, Integer&gt; numOfSubpartitionsRetriever,        boolean isDynamicGraph,        boolean isBroadcast,        boolean isSingleSubpartitionContainsAllData) &#123;    final List&lt;ExecutionVertexInputInfo&gt; executionVertexInputInfos = new ArrayList&lt;&gt;();    IndexRange partitionRange = new IndexRange(0, sourceCount - 1);    for (int i = 0; i &lt; targetCount; ++i) &#123;        IndexRange subpartitionRange =                computeConsumedSubpartitionRange(                        i,                        targetCount,                        () -&gt; numOfSubpartitionsRetriever.apply(0),                        isDynamicGraph,                        isBroadcast,                        isSingleSubpartitionContainsAllData);        executionVertexInputInfos.add(                new ExecutionVertexInputInfo(i, partitionRange, subpartitionRange));    &#125;    return new JobVertexInputInfo(executionVertexInputInfos);&#125;\n生成好了 jobVertexInputInfos 之后，我们再回到 DefaultExecutionGraph.initializeJobVertex 方法中。\n@Overridepublic void initializeJobVertex(        ExecutionJobVertex ejv,        long createTimestamp,        Map&lt;IntermediateDataSetID, JobVertexInputInfo&gt; jobVertexInputInfos)        throws JobException &#123;    checkNotNull(ejv);    checkNotNull(jobVertexInputInfos);    jobVertexInputInfos.forEach(            (resultId, info) -&gt;                    this.vertexInputInfoStore.put(ejv.getJobVertexId(), resultId, info));    ejv.initialize(            executionHistorySizeLimit,            rpcTimeout,            createTimestamp,            this.initialAttemptCounts.getAttemptCounts(ejv.getJobVertexId()),            executionPlanSchedulingContext);    ejv.connectToPredecessors(this.intermediateResults);    for (IntermediateResult res : ejv.getProducedDataSets()) &#123;        IntermediateResult previousDataSet =                this.intermediateResults.putIfAbsent(res.getId(), res);        if (previousDataSet != null) &#123;            throw new JobException(                    String.format(                            &quot;Encountered two intermediate data set with ID %s : previous=[%s] / new=[%s]&quot;,                            res.getId(), res, previousDataSet));        &#125;    &#125;    registerExecutionVerticesAndResultPartitionsFor(ejv);    // enrich network memory.    SlotSharingGroup slotSharingGroup = ejv.getSlotSharingGroup();    if (areJobVerticesAllInitialized(slotSharingGroup)) &#123;        SsgNetworkMemoryCalculationUtils.enrichNetworkMemory(                slotSharingGroup, this::getJobVertex, shuffleMaster);    &#125;&#125;\n首先来看 ExecutionJobVertex.initialize 方法。这个方法主要是生成 IntermediateResult 和 ExecutionVertex。\nprotected void initialize(        int executionHistorySizeLimit,        Duration timeout,        long createTimestamp,        SubtaskAttemptNumberStore initialAttemptCounts,        ExecutionPlanSchedulingContext executionPlanSchedulingContext)        throws JobException &#123;    checkState(parallelismInfo.getParallelism() &gt; 0);    checkState(!isInitialized());    this.taskVertices = new ExecutionVertex[parallelismInfo.getParallelism()];    this.inputs = new ArrayList&lt;&gt;(jobVertex.getInputs().size());    // create the intermediate results    this.producedDataSets =            new IntermediateResult[jobVertex.getNumberOfProducedIntermediateDataSets()];    for (int i = 0; i &lt; jobVertex.getProducedDataSets().size(); i++) &#123;        final IntermediateDataSet result = jobVertex.getProducedDataSets().get(i);        this.producedDataSets[i] =                new IntermediateResult(                        result,                        this,                        this.parallelismInfo.getParallelism(),                        result.getResultType(),                        executionPlanSchedulingContext);    &#125;    // create all task vertices    for (int i = 0; i &lt; this.parallelismInfo.getParallelism(); i++) &#123;        ExecutionVertex vertex =                createExecutionVertex(                        this,                        i,                        producedDataSets,                        timeout,                        createTimestamp,                        executionHistorySizeLimit,                        initialAttemptCounts.getAttemptCount(i));        this.taskVertices[i] = vertex;    &#125;    // sanity check for the double referencing between intermediate result partitions and    // execution vertices    for (IntermediateResult ir : this.producedDataSets) &#123;        if (ir.getNumberOfAssignedPartitions() != this.parallelismInfo.getParallelism()) &#123;            throw new RuntimeException(                    &quot;The intermediate result&#x27;s partitions were not correctly assigned.&quot;);        &#125;    &#125;    // set up the input splits, if the vertex has any    try &#123;        @SuppressWarnings(&quot;unchecked&quot;)        InputSplitSource&lt;InputSplit&gt; splitSource =                (InputSplitSource&lt;InputSplit&gt;) jobVertex.getInputSplitSource();        if (splitSource != null) &#123;            Thread currentThread = Thread.currentThread();            ClassLoader oldContextClassLoader = currentThread.getContextClassLoader();            currentThread.setContextClassLoader(graph.getUserClassLoader());            try &#123;                inputSplits =                        splitSource.createInputSplits(this.parallelismInfo.getParallelism());                if (inputSplits != null) &#123;                    splitAssigner = splitSource.getInputSplitAssigner(inputSplits);                &#125;            &#125; finally &#123;                currentThread.setContextClassLoader(oldContextClassLoader);            &#125;        &#125; else &#123;            inputSplits = null;        &#125;    &#125; catch (Throwable t) &#123;        throw new JobException(                &quot;Creating the input splits caused an error: &quot; + t.getMessage(), t);    &#125;&#125;\n在创建 ExecutionVertex 时，会创建 IntermediateResultPartition 和 Execution，创建 Execution 时，会设置 attemptNumber，这个值默认是0，如果 ExecutionVertex 是重新调度的，那么 attemptNumber 会自增加1。\nExecutionJobVertex.connectToPredecessors 方法主要是生成 ExecutionVertex 与 IntermediateResultPartition 的关联关系。这里设置关联关系也分成了点对点和全对全两种模式处理，点对点模式需要计算 ExecutionVertex 对应的 IntermediateResultPartition index 的范围。两种模式最终都调用了 connectInternal 方法。\n/** Connect all execution vertices to all partitions. */private static void connectInternal(        List&lt;ExecutionVertex&gt; taskVertices,        List&lt;IntermediateResultPartition&gt; partitions,        ResultPartitionType resultPartitionType,        EdgeManager edgeManager) &#123;    checkState(!taskVertices.isEmpty());    checkState(!partitions.isEmpty());    ConsumedPartitionGroup consumedPartitionGroup =            createAndRegisterConsumedPartitionGroupToEdgeManager(                    taskVertices.size(), partitions, resultPartitionType, edgeManager);    for (ExecutionVertex ev : taskVertices) &#123;        ev.addConsumedPartitionGroup(consumedPartitionGroup);    &#125;    List&lt;ExecutionVertexID&gt; consumerVertices =            taskVertices.stream().map(ExecutionVertex::getID).collect(Collectors.toList());    ConsumerVertexGroup consumerVertexGroup =            ConsumerVertexGroup.fromMultipleVertices(consumerVertices, resultPartitionType);    for (IntermediateResultPartition partition : partitions) &#123;        partition.addConsumers(consumerVertexGroup);    &#125;    consumedPartitionGroup.setConsumerVertexGroup(consumerVertexGroup);    consumerVertexGroup.setConsumedPartitionGroup(consumedPartitionGroup);&#125;\n这个方法中 ev.addConsumedPartitionGroup(consumedPartitionGroup); 负责将 ExecutionVertex 到 IntermediateResultPartition 的关联关系保存在 EdgeManager.vertexConsumedPartitions 中。\n而 partition.addConsumers(consumerVertexGroup); 则负责将 IntermediateResultPartition 到 ExecutionVertex 的关系保存在 EdgeManager.partitionConsumers 中。\n总结通过本文，我们了解了 Flink 是如何将 JobGraph 转换成 ExecutionGraph 的。其中涉及到的一些核心概念名称比较类似，建议认真学习和理解透彻之后再研究其生成方法和对应关系，也可以借助前文中 ExecutionGraph 示意图辅助学习。\n","tags":["Flink"]},{"title":"Flink源码阅读：窗口","url":"/2025/12/22/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E7%AA%97%E5%8F%A3/","content":"前文我们梳理了 Watermark 相关的源码，Watermark 的作用就是用来触发窗口，本文我们就一起看一下窗口相关的源码。\n写在前面在Flink学习笔记：窗口一文中，我们介绍了窗口的分类以及基本的用法。按照处理数据流的类型划分，Flink 可以分为 Keyed Window 和 Non-Keyed Window，它们的用法如下：\nstream       .keyBy(...)               &lt;-  仅 keyed 窗口需要       .window(...)              &lt;-  必填项：&quot;assigner&quot;      [.trigger(...)]            &lt;-  可选项：&quot;trigger&quot; (省略则使用默认 trigger)      [.evictor(...)]            &lt;-  可选项：&quot;evictor&quot; (省略则不使用 evictor)      [.allowedLateness(...)]    &lt;-  可选项：&quot;lateness&quot; (省略则为 0)      [.sideOutputLateData(...)] &lt;-  可选项：&quot;output tag&quot; (省略则不对迟到数据使用 side output)       .reduce/aggregate/apply()      &lt;-  必填项：&quot;function&quot;      [.getSideOutput(...)]      &lt;-  可选项：&quot;output tag&quot;stream       .windowAll(...)           &lt;-  必填项：&quot;assigner&quot;      [.trigger(...)]            &lt;-  可选项：&quot;trigger&quot; (else default trigger)      [.evictor(...)]            &lt;-  可选项：&quot;evictor&quot; (else no evictor)      [.allowedLateness(...)]    &lt;-  可选项：&quot;lateness&quot; (else zero)      [.sideOutputLateData(...)] &lt;-  可选项：&quot;output tag&quot; (else no side output for late data)       .reduce/aggregate/apply()      &lt;-  必填项：&quot;function&quot;      [.getSideOutput(...)]      &lt;-  可选项：&quot;output tag&quot;\n下面我们根据用法，分别来看两种窗口的源码。\nKeyed Window\nWindowAssigner在示例代码中，数据流类型流转过程如图。我们聚焦于 WindowedStream，它是在调用 KeyedStream.window 方法之后生成的。window 方法需要传入一个 WindowAssigner，用来确定一条消息属于哪几个窗口，各个类型的窗口都有不同的实现。\n\n我们以 TumblingEventTimeWindows 为例，看一下它具体的分配逻辑。\npublic Collection&lt;TimeWindow&gt; assignWindows(        Object element, long timestamp, WindowAssignerContext context) &#123;    if (timestamp &gt; Long.MIN_VALUE) &#123;        if (staggerOffset == null) &#123;            staggerOffset =                    windowStagger.getStaggerOffset(context.getCurrentProcessingTime(), size);        &#125;        // Long.MIN_VALUE is currently assigned when no timestamp is present        long start =                TimeWindow.getWindowStartWithOffset(                        timestamp, (globalOffset + staggerOffset) % size, size);        return Collections.singletonList(new TimeWindow(start, start + size));    &#125; else &#123;        throw new RuntimeException(                &quot;Record has Long.MIN_VALUE timestamp (= no timestamp marker). &quot;                        + &quot;Did you forget to call &#x27;DataStream.assignTimestampsAndWatermarks(...)&#x27;?&quot;);    &#125;&#125;\n这里就是根据消息的 timestamp 来确定窗口的开始和结束时间，然后返回消息所属的窗口。这里还有个 windowStagger 变量，它是窗口触发是否错峰的配置，如果你的任务有成千上万个子任务，同时触发窗口计算带来的瞬时流量可能会对服务器本身和下游造成稳定性的影响，这时就可以通过修改 WindowStagger 配置将流量打散。\n将我们自己定义好的 WindowAssigner 传入 window 方法后，会创建一个 WindowOperatorBuilder，它负责创建一个 WindowOperator 对象，WindowOperator 来执行窗口具体的计算逻辑。\npublic WindowedStream(KeyedStream&lt;T, K&gt; input, WindowAssigner&lt;? super T, W&gt; windowAssigner) &#123;    this.input = input;    this.isEnableAsyncState = input.isEnableAsyncState();    this.builder =            new WindowOperatorBuilder&lt;&gt;(                    windowAssigner,                    windowAssigner.getDefaultTrigger(),                    input.getExecutionConfig(),                    input.getType(),                    input.getKeySelector(),                    input.getKeyType());&#125;\nTrigger有了 WindowOperatorBuilder 之后，我们可以对它进行一些设置，如 trigger、evictor 等，trigger 中提供了一些回调函数，这些回调函数的返回结果 TriggerResult 决定了是否触发窗口计算。\npublic abstract class Trigger&lt;T, W extends Window&gt; implements Serializable &#123;    private static final long serialVersionUID = -4104633972991191369L;    public abstract TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx)            throws Exception;    public abstract TriggerResult onProcessingTime(long time, W window, TriggerContext ctx)            throws Exception;    public abstract TriggerResult onEventTime(long time, W window, TriggerContext ctx)            throws Exception;    public boolean canMerge() &#123;        return false;    &#125;    public void onMerge(W window, OnMergeContext ctx) throws Exception &#123;        throw new UnsupportedOperationException(&quot;This trigger does not support merging.&quot;);    &#125;    public abstract void clear(W window, TriggerContext ctx) throws Exception;&#125;\n回调函数有三个，分别是 onElement、onProcessingTime、onEventTime，onElement 是在处理每条消息的时候触发，onProcessingTime 和 onEventTime 都是与定时器配合触发，上一篇文章我们提到过，在处理 Watermark 的时候会注册定时器，触发时就会回调这两个方法。\n此外，Trigger 类中还有三个方法，我们简单介绍一下。canMerge 是用来判断窗口是否可以被合并，onMerge 则是在合并窗口时的回调方法。clear 方法用于清除窗口的状态数据。\npublic enum TriggerResult &#123;    /** No action is taken on the window. */    CONTINUE(false, false),    /** &#123;@code FIRE_AND_PURGE&#125; evaluates the window function and emits the window result. */    FIRE_AND_PURGE(true, true),    /**     * On &#123;@code FIRE&#125;, the window is evaluated and results are emitted. The window is not purged,     * though, all elements are retained.     */    FIRE(true, false),    /**     * All elements in the window are cleared and the window is discarded, without evaluating the     * window function or emitting any elements.     */    PURGE(false, true);&#125;\n说回 TriggerResult，它有四种枚举：\n\nCONTINUE：什么也不做\n\nFIRE_AND_PURGE：触发窗口计算并清除窗口中的元素\n\nFIRE：只触发窗口计算\n\nPURGE：清除窗口中的元素，不触发计算\n\n\nEvictorEvictor 是用来自定义删除窗口中元素的的接口，如果设置了 evictor，WindowOperatorBuilder 就会创建 EvictingWindowOperator。在执行窗口计算逻辑前后，都会调用 evictBefore 和 evictAfter。\nprivate void emitWindowContents(        W window, Iterable&lt;StreamRecord&lt;IN&gt;&gt; contents, ListState&lt;StreamRecord&lt;IN&gt;&gt; windowState)        throws Exception &#123;    ...    evictorContext.evictBefore(recordsWithTimestamp, Iterables.size(recordsWithTimestamp));    FluentIterable&lt;IN&gt; projectedContents =            recordsWithTimestamp.transform(                    new Function&lt;TimestampedValue&lt;IN&gt;, IN&gt;() &#123;                        @Override                        public IN apply(TimestampedValue&lt;IN&gt; input) &#123;                            return input.getValue();                        &#125;                    &#125;);    processContext.window = triggerContext.window;    userFunction.process(            triggerContext.key,            triggerContext.window,            processContext,            projectedContents,            timestampedCollector);    evictorContext.evictAfter(recordsWithTimestamp, Iterables.size(recordsWithTimestamp));    ...&#125;\nallowedLateness &amp; sideOutputLateDataallowedLateness 和 sideOutputLateData 都是针对迟到数据的，allowedLateness 是用来指定允许的最大迟到时长，sideOutputLateData 则是将迟到数据输出到指定 outputTag。\n判断是否迟到的方法如下：\nprotected boolean isElementLate(StreamRecord&lt;IN&gt; element) &#123;    return (windowAssigner.isEventTime())            &amp;&amp; (element.getTimestamp() + allowedLateness                    &lt;= internalTimerService.currentWatermark());&#125;\n如果是迟到数据，则进行如下处理：\nif (isSkippedElement &amp;&amp; isElementLate(element)) &#123;    if (lateDataOutputTag != null) &#123;        sideOutput(element);    &#125; else &#123;        this.numLateRecordsDropped.inc();    &#125;&#125;\nWindowOperator设置好 WindowOperatorBuilder 之后，接着就可以调用 process/aggregate/reduce 等方法进行数据计算。\n我们以 process 方法为例，来看下具体的处理逻辑。\npublic &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; process(        ProcessWindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType) &#123;    function = input.getExecutionEnvironment().clean(function);    final String opName = builder.generateOperatorName();    final String opDesc = builder.generateOperatorDescription(function, null);    OneInputStreamOperator&lt;T, R&gt; operator =            isEnableAsyncState ? builder.asyncProcess(function) : builder.process(function);    return input.transform(opName, resultType, operator).setDescription(opDesc);&#125;\n在 WindowedStream.process 方法中，就是调用 WindowOperatorBuilder 的 process 方法（如果是异步则调用异步方法）生成 WindowOperator，再将 WindowOperator 加入到执行图中。\n下面我们来看 WindowOperator 中几个重要的方法。\nopen首先是 open 方法，它主要负责进行初始化，包括创建 timerService，创建 windowState 等。\npublic void open() throws Exception &#123;    super.open();    this.numLateRecordsDropped = metrics.counter(LATE_ELEMENTS_DROPPED_METRIC_NAME);    timestampedCollector = new TimestampedCollector&lt;&gt;(output);    internalTimerService = getInternalTimerService(&quot;window-timers&quot;, windowSerializer, this);    triggerContext = new Context(null, null);    processContext = new WindowContext(null);    windowAssignerContext =            new WindowAssigner.WindowAssignerContext() &#123;                @Override                public long getCurrentProcessingTime() &#123;                    return internalTimerService.currentProcessingTime();                &#125;            &#125;;    // create (or restore) the state that hold the actual window contents    // NOTE - the state may be null in the case of the overriding evicting window operator    if (windowStateDescriptor != null) &#123;        windowState =                (InternalAppendingState&lt;K, W, IN, ACC, ACC&gt;)                        getOrCreateKeyedState(windowSerializer, windowStateDescriptor);    &#125;    // create the typed and helper states for merging windows    if (windowAssigner instanceof MergingWindowAssigner) &#123;        ...    &#125;&#125;\nprocessElementprocessElement 是负责处理进入窗口的数据，这里首先调用 WindowAssigner.assignWindows 方法确认元素属于哪些窗口。然后遍历窗口进行处理，包括向 windowState 中添加元素，调用 trigger 的 onElement 方法获取 TriggerResult。如果触发了窗口计算，调用 emitWindowContents 执行计算逻辑。最后是处理迟到数据，我们前面提到过。\npublic void processElement(StreamRecord&lt;IN&gt; element) throws Exception &#123;    final Collection&lt;W&gt; elementWindows =            windowAssigner.assignWindows(                    element.getValue(), element.getTimestamp(), windowAssignerContext);    // if element is handled by none of assigned elementWindows    boolean isSkippedElement = true;    final K key = this.&lt;K&gt;getKeyedStateBackend().getCurrentKey();    if (windowAssigner instanceof MergingWindowAssigner) &#123;        ...    &#125; else &#123;        for (W window : elementWindows) &#123;            // drop if the window is already late            if (isWindowLate(window)) &#123;                continue;            &#125;            isSkippedElement = false;            windowState.setCurrentNamespace(window);            windowState.add(element.getValue());            triggerContext.key = key;            triggerContext.window = window;            TriggerResult triggerResult = triggerContext.onElement(element);            if (triggerResult.isFire()) &#123;                ACC contents = windowState.get();                if (contents != null) &#123;                    emitWindowContents(window, contents);                &#125;            &#125;            if (triggerResult.isPurge()) &#123;                windowState.clear();            &#125;            registerCleanupTimer(window);        &#125;    &#125;    // side output input event if    // element not handled by any window    // late arriving tag has been set    // windowAssigner is event time and current timestamp + allowed lateness no less than    // element timestamp    if (isSkippedElement &amp;&amp; isElementLate(element)) &#123;        if (lateDataOutputTag != null) &#123;            sideOutput(element);        &#125; else &#123;            this.numLateRecordsDropped.inc();        &#125;    &#125;&#125;\nonEventTimeonEventTime 方法是 eventTime 触发窗口计算时调用的。主要逻辑就是获取 TriggerResult，然后触发计算逻辑，以及对 windowState 的处理。\npublic void onEventTime(InternalTimer&lt;K, W&gt; timer) throws Exception &#123;    triggerContext.key = timer.getKey();    triggerContext.window = timer.getNamespace();    MergingWindowSet&lt;W&gt; mergingWindows;    if (windowAssigner instanceof MergingWindowAssigner) &#123;        mergingWindows = getMergingWindowSet();        W stateWindow = mergingWindows.getStateWindow(triggerContext.window);        if (stateWindow == null) &#123;            // Timer firing for non-existent window, this can only happen if a            // trigger did not clean up timers. We have already cleared the merging            // window and therefore the Trigger state, however, so nothing to do.            return;        &#125; else &#123;            windowState.setCurrentNamespace(stateWindow);        &#125;    &#125; else &#123;        windowState.setCurrentNamespace(triggerContext.window);        mergingWindows = null;    &#125;    TriggerResult triggerResult = triggerContext.onEventTime(timer.getTimestamp());    if (triggerResult.isFire()) &#123;        ACC contents = windowState.get();        if (contents != null) &#123;            emitWindowContents(triggerContext.window, contents);        &#125;    &#125;    if (triggerResult.isPurge()) &#123;        windowState.clear();    &#125;    if (windowAssigner.isEventTime()            &amp;&amp; isCleanupTime(triggerContext.window, timer.getTimestamp())) &#123;        clearAllState(triggerContext.window, windowState, mergingWindows);    &#125;    if (mergingWindows != null) &#123;        // need to make sure to update the merging state in state        mergingWindows.persist();    &#125;&#125;\nonProcessingTimeonProcessingTime 和 onEventTime 逻辑基本一致，只是触发条件不同，这里就不再赘述了。\n至此，Keyed Window 从设置到使用的源码我们就梳理完成了，下面再来看另外一种窗口 Non-Keyed Window。\nNon-Keyed Window\n我们调用 windowAll 得到 AllWindowedStream，在构造函数中，会给对 input 调用 keyBy 方法，传入 NullByteKeySelector， NullByteKeySelector 对每个 key 都返回0，因此所有的 key 都会被分配到同一个节点。\npublic class NullByteKeySelector&lt;T&gt; implements KeySelector&lt;T, Byte&gt; &#123;    private static final long serialVersionUID = 614256539098549020L;    @Override    public Byte getKey(T value) throws Exception &#123;        return 0;    &#125;&#125;\nNon-Keyed Window 后续的逻辑都和 Keyed Window 比较类似。\n总结本文我们梳理了窗口相关的源码，几个重点概念包括 WindowAssginer、WindowOperator、Trigger、Evictor。其中 WindowAssigner 是用来确定一条消息属于哪些窗口，WindowOperator 则是窗口计算逻辑的具体执行层。Trigger 和 Evictor 分别用于触发窗口和清理窗口中数据。\n","tags":["Flink"]},{"title":"Flink源码阅读：如何生成JobGraph","url":"/2025/09/18/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90JobGraph/","content":"前文我们介绍了 Flink 的四种执行图，并且通过源码了解了 Flink 的 StreamGraph 是怎么生成的，本文我们就一起来看下 Flink 的另一种执行图——JobGraph 是如何生成的。\nStreamGraph 和 JobGraph 的区别在正式开始之前，我们再来回顾一下 StreamGraph 和 JobGraph 的区别。假设我们的任务是建造一座大楼，StreamGraph 就像是设计蓝图，它描述了每个窗户、每根水管的位置和规格，而 JobGraph 像是给到施工队的施工流程图，它描述了每个任务模块，例如先把地基浇筑好，再铺设管线等。总的来说，JobGraph 更偏向执行层面，它是由 StreamGraph 优化而来。\n回到 Flink 本身，我们通过一个表格来了解两个图的区别。\n\n\n\n\nStreamGraph\nJobGraph\n\n\n\n\n生成阶段\n客户端，执行 execute() 时\n客户端，提交前由 StreamGraph 转换生成\n\n\n抽象层级\n高层逻辑图，直接对应 API\n优化后的执行图，为调度做准备\n\n\n核心优化\n无\n主要是算子链优化\n\n\n节点\nStreamNode\nJobVertex\n\n\n边\nStreamEdge\nJobEdge\n\n\n提交对象\n无\n提交给 JobManager\n\n\n包含资源\n无\n包含执行作业所需的 Jar 包、依赖库和资源文件\n\n\n\nJobVertexJobGraph 中的节点是 JobVertex，在 StreamGraph 转换成 JobGraph 的过程中，会将多个节点串联起来，最终生成 JobVertex。\nJobVertex包含以下成员变量：\n\n我们分别看一下这些成员变量及其作用。\n1、标识符相关// JobVertex的id，在作业执行过程中的唯一标识。监控、调度和故障恢复都会使用private final JobVertexID id;// operator id列表，按照深度优先顺序存储。operator 的管理、状态分配都会用到private final List&lt;OperatorIDPair&gt; operatorIDs;\n2、输入输出相关// 定义所有的输入边private final List&lt;JobEdge&gt; inputs = new ArrayList&lt;&gt;();// 定义所有的输出数据集private final Map&lt;IntermediateDataSetID, IntermediateDataSet&gt; results = new LinkedHashMap&lt;&gt;();// 输入分片源，主要用于批处理作业，定义如何将数据分成多个片private InputSplitSource&lt;?&gt; inputSplitSource;\n3、执行配置相关// 并行度，即运行时拆分子任务数量，默认使用全局配置private int parallelism = ExecutionConfig.PARALLELISM_DEFAULT;// 最大并行度private int maxParallelism = MAX_PARALLELISM_DEFAULT;// 存储运行时实际执行的类，使 Flink 可以灵活处理不同类型的操作符// 流任务可以是&quot;org.apache.flink.streaming.runtime.tasks.StreamTask&quot;// 批任务可以是&quot;org.apache.flink.runtime.operators.BatchTask&quot;private String invokableClassName;// 自定义配置private Configuration configuration;// 是否是动态设置并发度private boolean dynamicParallelism = false;// 是否支持优雅停止private boolean isStoppable = false;\n4、资源管理相关// JobVertex 最小资源需求private ResourceSpec minResources = ResourceSpec.DEFAULT;// JobVertex 推荐资源需求private ResourceSpec preferredResources = ResourceSpec.DEFAULT;// 用于资源优化，运行不同的 JobVertex 的子任务运行在同一个 slot@Nullable private SlotSharingGroup slotSharingGroup;// 需要严格共址的 JobVertex 组，每个 JobVertex 的第 n 个子任务运行在同一个 TaskManager@Nullable private CoLocationGroupImpl coLocationGroup;\n5、协调器// 操作符协调器，用于处理全局协调逻辑private final List&lt;SerializedValue&lt;OperatorCoordinator.Provider&gt;&gt; operatorCoordinators =            new ArrayList&lt;&gt;();\n6、显示和描述信息// JobVertex 的名称private String name;// 操作符名称，比如 &#x27;Flat Map&#x27; 或 &#x27;Join&#x27;private String operatorName;// 操作符的描述，比如 &#x27;Hash Join&#x27; 或 &#x27;Sorted Group Reduce&#x27;private String operatorDescription;// 提供比 name 更友好的描述信息private String operatorPrettyName;\n7、状态和行为标志// 是否支持同一个子任务并发多次执行private boolean supportsConcurrentExecutionAttempts = true;// 标记并发度是否被显式设置private boolean parallelismConfigured = false;// 是否有阻塞型输出private boolean anyOutputBlocking = false;\n8、缓存数据集// 存储该 JobVertex 需要消费的缓存中间数据集的 ID，可提高作业执行效率private final List&lt;IntermediateDataSetID&gt; intermediateDataSetIdsToConsume = new ArrayList&lt;&gt;();\nJobEdge在 StreamGraph 中，StreamEdge 是连接 StreamNode 的桥梁。在 JobGraph 中，与之对应的是 JobEdge，不同点在于  JobEdge 中保存的是输入节点和输出结果。\n1、连接关系成员// 定义数据流向哪个 JobVertexprivate final JobVertex target;// 定义这条边的源数据private final IntermediateDataSet source;// 输入类型的编号private final int typeNumber;// 多个输入间的键是否相关，如果为 true，相同键的数据在一个输入被分割时，在其他数据对应的记录也会发送到相同的下游节点private final boolean interInputsKeysCorrelated;// 同一输入内相同的键是否必须发送到同一下游任务private final boolean intraInputKeyCorrelated;\n2、数据分发模式// 定义数据在并行任务期间的分发模式// 可能值：// ALL_TO_ALL：全连接，每个上游子任务连接所有下游任务// POINTWISE：点对点连接，一对一或一对多的本地连接private final DistributionPattern distributionPattern;\n3、数据传输策略// 是否为广播连接private final boolean isBroadcast;// 是否为 forward 连接，forward 连接最高效，直接转发，无需序列化网络传输private final boolean isForward;// 数据传输策略名称，用于显示private String shipStrategyName;\n4、状态重分布映射器// 下游状态重分布映射器，当作业扩容时，决定是否重新分配下游算子的持久化状态private SubtaskStateMapper downstreamSubtaskStateMapper = SubtaskStateMapper.ROUND_ROBIN;// 上游状态重分布映射器，当作业扩容时，决定是否重新分配上游算子的持久化状态private SubtaskStateMapper upstreamSubtaskStateMapper = SubtaskStateMapper.ROUND_ROBIN;\n5、描述和缓存信息// 预处理操作的名称private String preProcessingOperationName;// 操作符级别缓存的描述private String operatorLevelCachingDescription;\nStreamGraph 转换成 JobGraph现在我们再来看一下 StreamGraph 是如何转换成 JobGraph 的。转换逻辑的入口是 StreamGraph.getJobGraph 方法。它只是调用了 StreamingJobGraphGenerator.createJobGraph，核心逻辑在 createJobGraph 方法中。\nprivate JobGraph createJobGraph() &#123;    // 预验证，检查 StreamGraph 配置正确性    preValidate(streamGraph, userClassloader);    // 【核心】链化操作符    setChaining();    if (jobGraph.isDynamic()) &#123;        // 支持动态扩缩容场景，为动态图设置并行度        setVertexParallelismsForDynamicGraphIfNecessary();    &#125;    // Note that we set all the non-chainable outputs configuration here because the    // &quot;setVertexParallelismsForDynamicGraphIfNecessary&quot; may affect the parallelism of job    // vertices and partition-reuse    final Map&lt;Integer, Map&lt;StreamEdge, NonChainedOutput&gt;&gt; opIntermediateOutputs =            new HashMap&lt;&gt;();    // 设置不能链化的输出边    setAllOperatorNonChainedOutputsConfigs(opIntermediateOutputs, jobVertexBuildContext);    setAllVertexNonChainedOutputsConfigs(opIntermediateOutputs);    // 设置物理边连接    setPhysicalEdges(jobVertexBuildContext);    // 设置支持并发执行的 JobVertex    markSupportingConcurrentExecutionAttempts(jobVertexBuildContext);    // 验证混合 shuffle 模式只在批处理模式下使用    validateHybridShuffleExecuteInBatchMode(jobVertexBuildContext);    // 设置 Slot 共享和协同定位    setSlotSharingAndCoLocation(jobVertexBuildContext);    // 设置托管内存比例    setManagedMemoryFraction(jobVertexBuildContext);    // 为 JobVertex 名称添加前缀    addVertexIndexPrefixInVertexName(jobVertexBuildContext, new AtomicInteger(0));    // 设置操作符描述信息    setVertexDescription(jobVertexBuildContext);    // Wait for the serialization of operator coordinators and stream config.    // 序列化操作符协调器和流配置    serializeOperatorCoordinatorsAndStreamConfig(serializationExecutor, jobVertexBuildContext);    return jobGraph;&#125;\n可以看到，在 createJobGraph 方法中，调用了 setChaining 方法，即进行链化操作。这也是 JobGraph 最核心的优化之一。下面我们来看一下具体怎么做链化。\nprivate void setChaining() &#123;    // we separate out the sources that run as inputs to another operator (chained inputs)    // from the sources that needs to run as the main (head) operator.    final Map&lt;Integer, OperatorChainInfo&gt; chainEntryPoints =            buildChainedInputsAndGetHeadInputs();    final Collection&lt;OperatorChainInfo&gt; initialEntryPoints =            chainEntryPoints.entrySet().stream()                    .sorted(Comparator.comparing(Map.Entry::getKey))                    .map(Map.Entry::getValue)                    .collect(Collectors.toList());    // iterate over a copy of the values, because this map gets concurrently modified    for (OperatorChainInfo info : initialEntryPoints) &#123;        createChain(                info.getStartNodeId(),                1, // operators start at position 1 because 0 is for chained source inputs                info,                chainEntryPoints,                true,                serializationExecutor,                jobVertexBuildContext,                null);    &#125;&#125;\nsetChaining 方法中主要分为两步，第一步是处理 Source 节点，将可以链化的 Source 和不能链化的 Source 节点分开。先来看如何判断一个 Source 是否可被链化。\npublic static boolean isChainableSource(StreamNode streamNode, StreamGraph streamGraph) &#123;    // 最基本的一些判空，输出边数量为1    if (streamNode.getOperatorFactory() == null            || !(streamNode.getOperatorFactory() instanceof SourceOperatorFactory)            || streamNode.getOutEdges().size() != 1) &#123;        return false;    &#125;    final StreamEdge sourceOutEdge = streamNode.getOutEdges().get(0);    final StreamNode target = streamGraph.getStreamNode(sourceOutEdge.getTargetId());    final ChainingStrategy targetChainingStrategy =            Preconditions.checkNotNull(target.getOperatorFactory()).getChainingStrategy();    // 链化策略必须 HEAD_WITH_SOURCES，输出边是可链化的    return targetChainingStrategy == ChainingStrategy.HEAD_WITH_SOURCES            &amp;&amp; isChainableInput(sourceOutEdge, streamGraph, false);&#125;private static boolean isChainableInput(        StreamEdge edge, StreamGraph streamGraph, boolean allowChainWithDefaultParallelism) &#123;    StreamNode upStreamVertex = streamGraph.getSourceVertex(edge);    StreamNode downStreamVertex = streamGraph.getTargetVertex(edge);    if (!(streamGraph.isChainingEnabled()            // 上下游节点是否在同一个 slot 共享组            &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex)            // 操作符是否可以链化，主要做并行度检查            &amp;&amp; areOperatorsChainable(                    upStreamVertex,                    downStreamVertex,                    streamGraph,                    allowChainWithDefaultParallelism)            // 分区器和交换模式是否支持链化            &amp;&amp; arePartitionerAndExchangeModeChainable(                    edge.getPartitioner(), edge.getExchangeMode(), streamGraph.isDynamic()))) &#123;        return false;    &#125;    // check that we do not have a union operation, because unions currently only work    // through the network/byte-channel stack.    // we check that by testing that each &quot;type&quot; (which means input position) is used only once    // 检查是否为 Union 操作，Union 操作不能链化    for (StreamEdge inEdge : downStreamVertex.getInEdges()) &#123;        if (inEdge != edge &amp;&amp; inEdge.getTypeNumber() == edge.getTypeNumber()) &#123;            return false;        &#125;    &#125;    return true;&#125;\nSource 的链化条件主要就是这些，我们结合一些例子来看一下。\nSource(并行度=4) -&gt; Map(并行度=4) -&gt; Filter(并行度=4)Source -&gt; Map 边：1. isChainingEnabled() = true2. isSameSlotSharingGroup() = true (都在默认组)3. areOperatorsChainable() = true (Source可链化，Map是HEAD_WITH_SOURCES)4. arePartitionerAndExchangeModeChainable() = true (ForwardPartitioner)5. Union检查通过结果：可链化Map -&gt; Filter 边：1. isChainingEnabled() = true2. isSameSlotSharingGroup() = true3. areOperatorsChainable() = true (Map和Filter都是ALWAYS)4. arePartitionerAndExchangeModeChainable() = true (ForwardPartitioner)5. Union检查通过结果：可链化最终：Source -&gt; Map -&gt; Filter 三者链化到一个JobVertex中Source(并行度=2) -&gt; Map(并行度=4)  // 并行度不匹配Source -&gt; Map 边：1. isChainingEnabled() = true2. isSameSlotSharingGroup() = true3. areOperatorsChainable() = false (并行度不匹配)结果：不可链化，需要网络传输Source1 --\\          Union -&gt; MapSource2 --/Source1 -&gt; Union 边：虽然满足前4个条件，但Union节点有两个输入边，typeNumber相同Union检查失败，不可链化\n得到了所有入口之后，就可以进行后续节点的链化操作了，它的逻辑在 createChain 方法中。这里主要是一个递归过程，先将节点的输出边分为可链化和不可链化两个 list，之后对可链化的边进行递归调用链化。对不可链化的边，需要创建出新的链。由于篇幅原因，这里只贴一部分核心的代码\npublic static List&lt;StreamEdge&gt; createChain(        final Integer currentNodeId,        final int chainIndex,        final OperatorChainInfo chainInfo,        final Map&lt;Integer, OperatorChainInfo&gt; chainEntryPoints,        final boolean canCreateNewChain,        final Executor serializationExecutor,        final JobVertexBuildContext jobVertexBuildContext,        final @Nullable Consumer&lt;Integer&gt; visitedStreamNodeConsumer) &#123;    ......        // 拆分可链化边和不可链化边        for (StreamEdge outEdge : currentNode.getOutEdges()) &#123;            if (isChainable(outEdge, streamGraph)) &#123;                chainableOutputs.add(outEdge);            &#125; else &#123;                nonChainableOutputs.add(outEdge);            &#125;        &#125;        // 处理可链化边        for (StreamEdge chainable : chainableOutputs) &#123;            StreamNode targetNode = streamGraph.getStreamNode(chainable.getTargetId());            Attribute targetNodeAttribute = targetNode.getAttribute();            if (isNoOutputUntilEndOfInput) &#123;                if (targetNodeAttribute != null) &#123;                    targetNodeAttribute.setNoOutputUntilEndOfInput(true);                &#125;            &#125;            transitiveOutEdges.addAll(                    createChain(                            chainable.getTargetId(),                            chainIndex + 1,                            chainInfo,                            chainEntryPoints,                            canCreateNewChain,                            serializationExecutor,                            jobVertexBuildContext,                            visitedStreamNodeConsumer));            // Mark upstream nodes in the same chain as outputBlocking            if (targetNodeAttribute != null                    &amp;&amp; targetNodeAttribute.isNoOutputUntilEndOfInput()) &#123;                currentNodeAttribute.setNoOutputUntilEndOfInput(true);            &#125;        &#125;        // 处理不可链化边        for (StreamEdge nonChainable : nonChainableOutputs) &#123;            transitiveOutEdges.add(nonChainable);            // Used to control whether a new chain can be created, this value is true in the            // full graph generation algorithm and false in the progressive generation            // algorithm. In the future, this variable can be a boolean type function to adapt            // to more adaptive scenarios.            if (canCreateNewChain) &#123;                createChain(                        nonChainable.getTargetId(),                        1, // operators start at position 1 because 0 is for chained source                        // inputs                        chainEntryPoints.computeIfAbsent(                                nonChainable.getTargetId(),                                (k) -&gt; chainInfo.newChain(nonChainable.getTargetId())),                        chainEntryPoints,                        canCreateNewChain,                        serializationExecutor,                        jobVertexBuildContext,                        visitedStreamNodeConsumer);            &#125;        &#125;        // 创建 JobVertex        StreamConfig config;        if (currentNodeId.equals(startNodeId)) &#123;            JobVertex jobVertex = jobVertexBuildContext.getJobVertex(startNodeId);            if (jobVertex == null) &#123;                jobVertex =                        createJobVertex(                                chainInfo, serializationExecutor, jobVertexBuildContext);            &#125;            config = new StreamConfig(jobVertex.getConfiguration());        &#125; else &#123;            config = new StreamConfig(new Configuration());        &#125;        // 判断是否为起始节点，如果不是，将对应的配置信息存到链化起始节点的 key 中        if (currentNodeId.equals(startNodeId)) &#123;            chainInfo.setTransitiveOutEdges(transitiveOutEdges);            jobVertexBuildContext.addChainInfo(startNodeId, chainInfo);            config.setChainStart();            config.setChainIndex(chainIndex);            config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName());            config.setTransitiveChainedTaskConfigs(                    jobVertexBuildContext.getChainedConfigs().get(startNodeId));        &#125; else &#123;            config.setChainIndex(chainIndex);            StreamNode node = streamGraph.getStreamNode(currentNodeId);            config.setOperatorName(node.getOperatorName());            jobVertexBuildContext                    .getOrCreateChainedConfig(startNodeId)                    .put(currentNodeId, config);        &#125;    ......&#125;\n是否可链化依赖于 isChainable 方法的结果。它主要判断了下游的输入边数量是否为1，然后调用了 isChainableInput，这个方法我们刚刚已经看过了。\npublic static boolean isChainable(StreamEdge edge, StreamGraph streamGraph) &#123;    return isChainable(edge, streamGraph, false);&#125;public static boolean isChainable(        StreamEdge edge, StreamGraph streamGraph, boolean allowChainWithDefaultParallelism) &#123;    StreamNode downStreamVertex = streamGraph.getTargetVertex(edge);    return downStreamVertex.getInEdges().size() == 1            &amp;&amp; isChainableInput(edge, streamGraph, allowChainWithDefaultParallelism);&#125;\n总结本文我们主要介绍了生成 JobGraph 的相关代码。首先了解了 JobGraph 中的节点和边对应的类，以及它们和 StreamGraph 中的类的映射关系。然后又看了生成 JobGraph 的核心代码，其中重点学习了链化相关的代码。\n最后补充一个生成 JobGraph 的调用链路，感兴趣的同学可以看下。\nclusterClient.submitJob() → MiniCluster.submitJob() → Dispatcher.submitJob() → JobMasterServiceLeadershipRunnerFactory → DefaultJobMasterServiceFactory → JobMaster → DefaultSchedulerFactory.createInstance()→ StreamGraph.getJobGraph()\n","tags":["Flink"]},{"title":"Flink源码阅读：如何生成StreamGraph","url":"/2025/09/10/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90StreamGraph/","content":"Flink 中有四种执行图，分别是 StreamGraph、JobGraph、ExecutionGraph 和 Physical Graph。今天我们来看下我们编写的 Flink 程序代码是如何生成 StreamGraph 的。\n在开始读代码之前，我们先来简单介绍一下四种图之间的关系和区别。\n\nStreamGraph 是根据用户用 Stream API 编写的代码生成的图，用来表示整个程序的拓扑结构。\nJobGraph 是由 StreamGraph 生成的，它在 StreamGraph 的基础上，对链化了部分算子，将其合并成为一个节点，减少数据在节点之间传输时序列化和反序列化这些消耗。\nExecutionGraph 是由 JobGraph 生成的，它的主要特点是并行，将多并发的节点拆分。\nPhysicalGraph 是 ExecutionGraph 实际部署后的图，它并不是一种数据结构。\nStreamExecutionEnvironmentOK，了解了 Flink 四种执行图之后，我们就正式开始源码探索了。首先从 StreamExecutionEnvironment 入手，在编写 Flink 程序时，它是必不可少的一个类。它提供了一系列方法来配置流处理程序的执行环境（如并行度、Checkpoint 配置、时间属性等）。\n本文我们主要关注 StreamGraph 的生成，首先是数据流的入口，即 Source 节点。在 StreamExecutionEnvironment 中有 addSource 和 fromSource 等方法，它们用来定义从哪个数据源读取数据，然后返回一个 DataStreamSource （继承自 DataStream），得到 DataStream 之后，它会在各个算子之间流转，最终到 Sink 端输出。\n我们从 addSource 方法入手，addSource 方法中主要做了三件事：\n1、处理数据类型，优先使用用户执行的数据类型，也可以自动推断\n2、闭包清理，使用户传入的 function 能被序列化并发布到分布式环境执行\n3、创建 DataStreamSource 并返回\nprivate &lt;OUT&gt; DataStreamSource&lt;OUT&gt; addSource(        final SourceFunction&lt;OUT&gt; function,        final String sourceName,        @Nullable final TypeInformation&lt;OUT&gt; typeInfo,        final Boundedness boundedness) &#123;    checkNotNull(function);    checkNotNull(sourceName);    checkNotNull(boundedness);    TypeInformation&lt;OUT&gt; resolvedTypeInfo =            getTypeInfo(function, sourceName, SourceFunction.class, typeInfo);    boolean isParallel = function instanceof ParallelSourceFunction;    clean(function);    final StreamSource&lt;OUT, ?&gt; sourceOperator = new StreamSource&lt;&gt;(function);    return new DataStreamSource&lt;&gt;(            this, resolvedTypeInfo, sourceOperator, isParallel, sourceName, boundedness);&#125;\n现在我们有了 DataStream 了，那如何知道后续要进行哪些转换逻辑呢？答案在 transformations 这个变量中，它保存了后续所有的转换。\nprotected final List&lt;Transformation&lt;?&gt;&gt; transformations = new ArrayList&lt;&gt;();\nTransformation我们来看 Transformation 是如何生成和描述 DataStream 的转换流程的。以最常见的 map 方法为例。\npublic &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; map(        MapFunction&lt;T, R&gt; mapper, TypeInformation&lt;R&gt; outputType) &#123;    return transform(&quot;Map&quot;, outputType, new StreamMap&lt;&gt;(clean(mapper)));&#125;\n它调用了 transform 方法，transform 又调用了 doTransform 方法。\nprotected &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; doTransform(        String operatorName,        TypeInformation&lt;R&gt; outTypeInfo,        StreamOperatorFactory&lt;R&gt; operatorFactory) &#123;    // read the output type of the input Transform to coax out errors about MissingTypeInfo    transformation.getOutputType();    OneInputTransformation&lt;T, R&gt; resultTransform =            new OneInputTransformation&lt;&gt;(                    this.transformation,                    operatorName,                    operatorFactory,                    outTypeInfo,                    environment.getParallelism(),                    false);    @SuppressWarnings(&#123;&quot;unchecked&quot;, &quot;rawtypes&quot;&#125;)    SingleOutputStreamOperator&lt;R&gt; returnStream =            new SingleOutputStreamOperator(environment, resultTransform);    getExecutionEnvironment().addOperator(resultTransform);    return returnStream;&#125;\n在 doTransform 方法中，就是创建 Transformation 和 SingleOutputStreamOperator（DataStream 的一个子类），然后调用 addOperator 方法将 transform 存到 StreamExecutionEnviroment 中的 transformations 变量中。\n每个 Transformation 都有 id、name、parallelism 和 slotSharingGroup 等信息。其子类也记录有输入信息，如 OneInputTransformation 和 TwoInputTransformation。\nStreamOperator我们在调用 map 方法时，会传入一个自定义的处理函数，它也会保存在 Transformation 中。在 Flink 中定义了 StreamOperator 方法来抽象这类处理函数。在 map 方法中，它将我们传入的函数转成了 StreamMap，它继承了 AbstractUdfStreamOperator，同时实现了 OneInputStreamOperator 接口。\nStreamOperator 定义了对算子生命周期管理的函数。\nvoid open() throws Exception;void finish() throws Exception;void close() throws Exception;OperatorSnapshotFutures snapshotState(            long checkpointId,            long timestamp,            CheckpointOptions checkpointOptions,            CheckpointStreamFactory storageLocation)            throws Exception;void initializeState(StreamTaskStateInitializer streamTaskStateManager) throws Exception;\nOneInputStreamOperator 是 StreamOperator 的子接口。在其基础上增加了对具体元素的处理，主要是对 key 的提取。\ndefault void setKeyContextElement(StreamRecord&lt;IN&gt; record) throws Exception &#123;    setKeyContextElement1(record);&#125;\nAbstractUdfStreamOperator 则是提供了对自定义函数生命周期管理的实现。\n@Overridepublic void open() throws Exception &#123;    super.open();    FunctionUtils.openFunction(userFunction, DefaultOpenContext.INSTANCE);&#125;@Overridepublic void finish() throws Exception &#123;    super.finish();    if (userFunction instanceof SinkFunction) &#123;        ((SinkFunction&lt;?&gt;) userFunction).finish();    &#125;&#125;@Overridepublic void close() throws Exception &#123;    super.close();    FunctionUtils.closeFunction(userFunction);&#125;\n到这里，我们就知道了 Flink 中 DataStream 是如何转换的。处理逻辑保存在 Transformation 中。下面我们来看一组 Transformation 是如何生成 StreamGraph 的。\nStreamGraph生成 StreamGraph 的入口在 org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#generateStreamGraph 。\n在 generate 方法中，会遍历所有 Transformation 并调用 transform 方法。在调用节点的 transform 方法之前，会先确保它的输入节点都已经转换成功。\n目前定义了以下 Transformation：\nstatic &#123;    @SuppressWarnings(&quot;rawtypes&quot;)    Map&lt;Class&lt;? extends Transformation&gt;, TransformationTranslator&lt;?, ? extends Transformation&gt;&gt;            tmp = new HashMap&lt;&gt;();    tmp.put(OneInputTransformation.class, new OneInputTransformationTranslator&lt;&gt;());    tmp.put(TwoInputTransformation.class, new TwoInputTransformationTranslator&lt;&gt;());    tmp.put(MultipleInputTransformation.class, new MultiInputTransformationTranslator&lt;&gt;());    tmp.put(KeyedMultipleInputTransformation.class, new MultiInputTransformationTranslator&lt;&gt;());    tmp.put(SourceTransformation.class, new SourceTransformationTranslator&lt;&gt;());    tmp.put(SinkTransformation.class, new SinkTransformationTranslator&lt;&gt;());    tmp.put(GlobalCommitterTransform.class, new GlobalCommitterTransformationTranslator&lt;&gt;());    tmp.put(LegacySinkTransformation.class, new LegacySinkTransformationTranslator&lt;&gt;());    tmp.put(LegacySourceTransformation.class, new LegacySourceTransformationTranslator&lt;&gt;());    tmp.put(UnionTransformation.class, new UnionTransformationTranslator&lt;&gt;());    tmp.put(StubTransformation.class, new StubTransformationTranslator&lt;&gt;());    tmp.put(PartitionTransformation.class, new PartitionTransformationTranslator&lt;&gt;());    tmp.put(SideOutputTransformation.class, new SideOutputTransformationTranslator&lt;&gt;());    tmp.put(ReduceTransformation.class, new ReduceTransformationTranslator&lt;&gt;());    tmp.put(            TimestampsAndWatermarksTransformation.class,            new TimestampsAndWatermarksTransformationTranslator&lt;&gt;());    tmp.put(BroadcastStateTransformation.class, new BroadcastStateTransformationTranslator&lt;&gt;());    tmp.put(            KeyedBroadcastStateTransformation.class,            new KeyedBroadcastStateTransformationTranslator&lt;&gt;());    tmp.put(CacheTransformation.class, new CacheTransformationTranslator&lt;&gt;());    translatorMap = Collections.unmodifiableMap(tmp);&#125;\nFlink 会根据不同的 Transformation 类调用其 translateInternal 方法。在 translateInternal 方法中就会去添加节点和边。\nstreamGraph.addOperator(        transformationId,        slotSharingGroup,        transformation.getCoLocationGroupKey(),        operatorFactory,        inputType,        transformation.getOutputType(),        transformation.getName());for (Integer inputId : context.getStreamNodeIds(parentTransformations.get(0))) &#123;    streamGraph.addEdge(inputId, transformationId, 0);&#125;\n在 addOperator 方法中，它通过调用 addNode 来创建 StreamNode。\nprotected StreamNode addNode(        Integer vertexID,        @Nullable String slotSharingGroup,        @Nullable String coLocationGroup,        Class&lt;? extends TaskInvokable&gt; vertexClass,        @Nullable StreamOperatorFactory&lt;?&gt; operatorFactory,        String operatorName) &#123;    if (streamNodes.containsKey(vertexID)) &#123;        throw new RuntimeException(&quot;Duplicate vertexID &quot; + vertexID);    &#125;    StreamNode vertex =            new StreamNode(                    vertexID,                    slotSharingGroup,                    coLocationGroup,                    operatorFactory,                    operatorName,                    vertexClass);    streamNodes.put(vertexID, vertex);    isEmpty = false;    return vertex;&#125;\n在 addEdgeInternal 方法中，对于 sideOutput 和 partition 这类虚拟节点，会先解析出原始节点，再建立实际的边。\nprivate void addEdgeInternal(        Integer upStreamVertexID,        Integer downStreamVertexID,        int typeNumber,        StreamPartitioner&lt;?&gt; partitioner,        List&lt;String&gt; outputNames,        OutputTag outputTag,        StreamExchangeMode exchangeMode,        IntermediateDataSetID intermediateDataSetId) &#123;    if (virtualSideOutputNodes.containsKey(upStreamVertexID)) &#123;        int virtualId = upStreamVertexID;        upStreamVertexID = virtualSideOutputNodes.get(virtualId).f0;        if (outputTag == null) &#123;            outputTag = virtualSideOutputNodes.get(virtualId).f1;        &#125;        addEdgeInternal(                upStreamVertexID,                downStreamVertexID,                typeNumber,                partitioner,                null,                outputTag,                exchangeMode,                intermediateDataSetId);    &#125; else if (virtualPartitionNodes.containsKey(upStreamVertexID)) &#123;        int virtualId = upStreamVertexID;        upStreamVertexID = virtualPartitionNodes.get(virtualId).f0;        if (partitioner == null) &#123;            partitioner = virtualPartitionNodes.get(virtualId).f1;        &#125;        exchangeMode = virtualPartitionNodes.get(virtualId).f2;        addEdgeInternal(                upStreamVertexID,                downStreamVertexID,                typeNumber,                partitioner,                outputNames,                outputTag,                exchangeMode,                intermediateDataSetId);    &#125; else &#123;        createActualEdge(                upStreamVertexID,                downStreamVertexID,                typeNumber,                partitioner,                outputTag,                exchangeMode,                intermediateDataSetId);    &#125;&#125;\n最后根据两个物理节点创建 StreamEdge 进行连接。\nprivate void createActualEdge(        Integer upStreamVertexID,        Integer downStreamVertexID,        int typeNumber,        StreamPartitioner&lt;?&gt; partitioner,        OutputTag outputTag,        StreamExchangeMode exchangeMode,        IntermediateDataSetID intermediateDataSetId) &#123;    StreamNode upstreamNode = getStreamNode(upStreamVertexID);    StreamNode downstreamNode = getStreamNode(downStreamVertexID);    // If no partitioner was specified and the parallelism of upstream and downstream    // operator matches use forward partitioning, use rebalance otherwise.    if (partitioner == null            &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;        partitioner =                dynamic ? new ForwardForUnspecifiedPartitioner&lt;&gt;() : new ForwardPartitioner&lt;&gt;();    &#125; else if (partitioner == null) &#123;        partitioner = new RebalancePartitioner&lt;Object&gt;();    &#125;    if (partitioner instanceof ForwardPartitioner) &#123;        if (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;            if (partitioner instanceof ForwardForConsecutiveHashPartitioner) &#123;                partitioner =                        ((ForwardForConsecutiveHashPartitioner&lt;?&gt;) partitioner)                                .getHashPartitioner();            &#125; else &#123;                throw new UnsupportedOperationException(                        &quot;Forward partitioning does not allow &quot;                                + &quot;change of parallelism. Upstream operation: &quot;                                + upstreamNode                                + &quot; parallelism: &quot;                                + upstreamNode.getParallelism()                                + &quot;, downstream operation: &quot;                                + downstreamNode                                + &quot; parallelism: &quot;                                + downstreamNode.getParallelism()                                + &quot; You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.&quot;);            &#125;        &#125;    &#125;    if (exchangeMode == null) &#123;        exchangeMode = StreamExchangeMode.UNDEFINED;    &#125;    /**     * Just make sure that &#123;@link StreamEdge&#125; connecting same nodes (for example as a result of     * self unioning a &#123;@link DataStream&#125;) are distinct and unique. Otherwise it would be     * difficult on the &#123;@link StreamTask&#125; to assign &#123;@link RecordWriter&#125;s to correct &#123;@link     * StreamEdge&#125;.     */    int uniqueId = getStreamEdges(upstreamNode.getId(), downstreamNode.getId()).size();    StreamEdge edge =            new StreamEdge(                    upstreamNode,                    downstreamNode,                    typeNumber,                    partitioner,                    outputTag,                    exchangeMode,                    uniqueId,                    intermediateDataSetId);    getStreamNode(edge.getSourceId()).addOutEdge(edge);    getStreamNode(edge.getTargetId()).addInEdge(edge);&#125;\n通过 StreamNode 和 StreamEdge，就可以得到所有的节点和边，也就是我们的 StreamGraph 就创建完成了。\n总结本文先介绍了 Flink 的四种执行图以及它们之间的关系。接着又通过源码探索了 StreamGraph 的生成逻辑，Flink 将处理 逻辑保存在 Transformation 中，又由 Transformation 生成了 StreamGraph。\n","tags":["Flink"]},{"title":"Flink源码阅读：集群启动","url":"/2025/11/27/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8/","content":"前文中，我们已经了解了 Flink 的三种执行图是怎么生成的。今天继续看一下 Flink 集群是如何启动的。\n启动脚本集群启动脚本的位置在：\nflink-dist/src/main/flink-bin/bin/start-cluster.sh\n脚本会负责启动 JobManager 和 TaskManager，我们主要关注 standalone 启动模式，具体的流程见下图。\n\n从图中可以看出 JobManager 是通过 jobmanager.sh 文件启动的，TaskManager 是通过taskmanager.sh 启动的，两者都调用了 flink-daemon.sh，通过传递不同的参数，最终运行不同的 Java 类。\ncase $DAEMON in    (taskexecutor)        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner    ;;    (zookeeper)        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer    ;;    (historyserver)        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer    ;;    (standalonesession)        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint    ;;    (standalonejob)        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint    ;;    (sql-gateway)        CLASS_TO_RUN=org.apache.flink.table.gateway.SqlGateway        SQL_GATEWAY_CLASSPATH=&quot;`findSqlGatewayJar`&quot;:&quot;`findFlinkPythonJar`&quot;    ;;    (*)        echo &quot;Unknown daemon &#x27;$&#123;DAEMON&#125;&#x27;. $USAGE.&quot;        exit 1    ;;esac\nJobManager 启动流程在 StandaloneSessionClusterEntrypoint 的 main 方法中，主要就是加载各种配置和环境变量，然后调用 ClusterEntrypoint.runClusterEntrypoint 来启动集群。跟着调用链一直找到 ClusterEntrypoint.runCluster 方法，这里会启动 ResourceManager、DispatcherRunner 等组件。\nprivate void runCluster(Configuration configuration, PluginManager pluginManager)        throws Exception &#123;    synchronized (lock) &#123;        // 初始化各种服务        initializeServices(configuration, pluginManager);        // 创建 DispatcherResourceManagerComponentFactory，        // 包含了三个核心组件的 Factory        // DispatcherRunnerFactory、ResourceManagerFactory、RestEndpointFactory        final DispatcherResourceManagerComponentFactory                dispatcherResourceManagerComponentFactory =                        createDispatcherResourceManagerComponentFactory(configuration);        // 启动 ResourceManager、DispatcherRunner、WebMonitorEndpoint        clusterComponent =                dispatcherResourceManagerComponentFactory.create(                        configuration,                        resourceId.unwrap(),                        ioExecutor,                        commonRpcService,                        haServices,                        blobServer,                        heartbeatServices,                        delegationTokenManager,                        metricRegistry,                        executionGraphInfoStore,                        new RpcMetricQueryServiceRetriever(                                metricRegistry.getMetricQueryServiceRpcService()),                        failureEnrichers,                        this);        // 关闭服务        clusterComponent                .getShutDownFuture()                .whenComplete(                        (ApplicationStatus applicationStatus, Throwable throwable) -&gt; &#123;                            if (throwable != null) &#123;                                shutDownAsync(                                        ApplicationStatus.UNKNOWN,                                        ShutdownBehaviour.GRACEFUL_SHUTDOWN,                                        ExceptionUtils.stringifyException(throwable),                                        false);                            &#125; else &#123;                                // This is the general shutdown path. If a separate more                                // specific shutdown was                                // already triggered, this will do nothing                                shutDownAsync(                                        applicationStatus,                                        ShutdownBehaviour.GRACEFUL_SHUTDOWN,                                        null,                                        true);                            &#125;                        &#125;);    &#125;&#125;\n下面来详细看一下这几个方法， initializeServices 就是负责初始化各种服务，有几个比较重要的可以着重关注下：\n// 初始化并启动一个通用的 RPC ServicecommonRpcService = RpcUtils.createRemoteRpcService(...);// 创建一个 IO 线程池，线程数量位 CPU 核数 * 4ioExecutor = Executors.newFixedThreadPool(...);// 创建 HA 服务组件，根据配置初始化 Standalone、ZK、K8S 三种haServices = createHaServices(configuration, ioExecutor, rpcSystem);// 创建并启动 blobServer,blobServer 可以理解为是 Flink 内部的blobServer = BlobUtils.createBlobServer(...);blobServer.start();// 创建心跳服务heartbeatServices = createHeartbeatServices(configuration);// 创建一个监控服务processMetricGroup = MetricUtils.instantiateProcessMetricGroup(...);\ncreateDispatcherResourceManagerComponentFactory 这个方法就是创建了三个工厂类，不需要过多介绍。我们重点关注 dispatcherResourceManagerComponentFactory.create 方法，即 ResourceManager、DispatcherRunner、WebMonitorEndpoint 是如何启动的。\nWebMonitorEndpointWebMonitorEndpoint 的启动流程图如下，图中细箭头代表同一个方法中顺序调用，粗箭头代表进入上一个方法内部的调用。\n\nWebMonitorEndpoint 创建和启动步骤如下：\n\n通过工厂创建出了 WebMonitorEndpoint，这里就是比较常规的初始化操作。\n\n调用 WebMonitorEndpoint 的 start 方法开始启动，start 方法内部先是创建了一个 Router 并调用 initializeHandlers 创建了一大堆 handler（是真的一大堆，这个方法有接近一千行，都是在创建 handler），创建完成之后，对 handler 进行排序和去重，再把它们都注册到 Router 中。这里排序是为了确保路由匹配的正确性，排序规则是先静态路径（/jobs/overview），后动态路径（/jobs/:jobid），假如我们没有排序，先注册了 /jobs/:jobid ，后注册 /jobs/overview ，这时当我们请求 /jobs/overview 时，就会被错误的路由到 /jobs/:jobid 上去。\n\n是调用 startInternal 方法，在 startInternal 方法内部只有 leader 选举和启动缓存清理任务两个步骤。\n\n\nResourceManager\nResourceManager 创建和启动步骤如下：\n\n调用 ResourceManagerServiceImpl.create 方法创建 ResourceManagerService，这里只是创建 ResourceManager 服务，实际创建 ResourceManager 在后面的步骤中。\n\n调用 resourceManagerService.start 方法启动服务，这里就是启动选主服务，standalne 模式直接调用 grantLeadership 成为 leader。\n\n成为 leader 后，就会调用 startNewLeaderResourceManager 方法，这个方法中会调用 resourceManagerFactory.createResourceManager 正式创建 resourceManager。创建完成后，就会调用 resourceManager.start 来启动它。\n\n启动后会回调 ResourceManager.onStart 方法。这里调用 startHeartbeatServices 启动了两个心跳服务，一个是 ResourceManager 和 TaskManager 之间的心跳，一个是 ResourceManager 和 JobManager 之间的心跳，然后会启动 SlotManager。SlotManager 可以被当作 Flink 集群的资源调度中心。它会负责管理集群中的所有 Slot 资源，也需要响应 JobManager 的资源请求。\n\n\nDispatcherRunner\n\n先创建工厂，创建完成后调用 DefaultDispatcherRunner.create 创建出 DispatcherRunner，接着是调用 start 启动选主流程。\n\n选主完成后就调用 startNewDispatcherLeaderProcess 启动新的流程。启动新的流程需要先关闭旧流程，然后创建新的 dispatcherLeaderProcess，并调用 start 启动。\n\n启动时，会回调 onStart 方法。\n\n回调方法中，先启动 executionPlanStore，它主要是用于持久化 JobGraph。然后恢复执行计划，重建状态（如果是从失败中恢复），实例化 Dispatcher，完成作业启动。\n\n\nTaskManager 启动流程\nTaskManager 是 Flink 的执行节点，其最小执行单元是 slot。TaskManager 启动流程也主要是和资源管理相关，包括 slot 列表的管理和与 ResourceManager 的通信。\nTaskManager 启动流程大体分为以下几部分：\n\n构建并启动 TaskManagerRunner（蓝色部分）\n\n启动 TaskExecutor（红色部分）\n\n完成与 ResourceManager 的连接（橙色部分）\n\n\n启动 TaskManagerRunner在 TaskManagerRunner 的 start 方法中，有两个步骤：\n第一步是调用 startTaskManagerRunnerServices 创建和启动了很多服务，这一点和 JobManager 的启动流程比较像。这些服务包括了高可用服务、心跳服务、监控指标服务等，这里也创建了 taskExecutorService，它的启动在第二步。\n第二步是调用 taskExecutorService.start 方法，启动 TaskExecutorService，它内部主要负责启动 TaskExecutor。\n启动 TaskExecutorTaskExecutor 是 TaskManager 内部的一个核心组件，负责帮助 TaskManager 完成 task 的部署和执行等核心操作。\n在上一步调用 taskExecutor 的 start 方法后，会回调 onStart 方法，这里主要是三个步骤\n\n连接 ResourceManager 以及注册监听\n\n启动 taskSlotTable\n\n连接 JobMaster 以及注册监听\n\n\n第一步我们在下面详细解释。第二步启动的 TaskSlotTable 是 TaskManager 中负责资源的核心组件，它维护了一个 Slot 列表，管理每个 Slot 的状态，负责 Slot 的分配和释放。第三步主要是和 JobMaster 建立连接并保持心跳，同时也会接收 Slot 申请的请求。\n连接 ResourceManagerTaskExecutor 注册完监听之后，会收到 ResourceManagerLeaderListener.notifyLeaderAddress 方法回调。回调方法中，会创建一个 TaskExecutorToResourceManagerConnection 实例并启动它。这个类是用来将 TaskExecutor 注册到 ResourceManager，注册成功会回调 onRegistrationSuccess 方法。回调成功的方法中，TaskManager 会调用 resourceManagerGateway.sendSlotReport 将 Slot 的状态进行上报。\n总结本文介绍了 Flink 集群在 Standalone 模式下的启动过程，其中 JobManager 重点介绍了 WebMonitorEndpoint、ResourceManager 和 DispatcherRunner 这三个组件的启动过程。TaskManager 主要介绍了启动 TaskExecutor 和连接 ResourceManager 的过程。\n","tags":["Flink"]},{"title":"Java原子操作类，知多少？","url":"/2018/08/23/Java%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E7%B1%BB%EF%BC%8C%E7%9F%A5%E5%A4%9A%E5%B0%91%EF%BC%9F/","content":"前文我们介绍了Java并发编程中的两个关键字：volatile和synchronized。我们也知道了volatile虽然是轻量级，但不能保证原子性，synchronized可以保证原子性，但是比较重量级。\n那么有没有一种简单的、性能高的方法来保证Java的原子操作呢？答案当然是有的，本文就为大家揭秘一些在JDK1.5时期加入Java家族的成员——Atomic包。Atomic包下包含了12个类，分为4种类型：\n\n原子更新基本类型\n原子更新数组\n原子更新引用\n原子更新字段\n\n下面我来为大家一一引荐。\n原子基本类型原子基本类型，从名称上就可以看出，是为基本类型提供原子操作的类。它们是以下3位：\n\nAtomicBoolean\nAtomicInteger\nAtomicLong\n\n这三位属于近亲，提供的方法基本一模一样（AtomicBoolean支持方法略少）。\n\n\n\n这里我们以AtomicInteger为例介绍这些方法。\n\nvoid lazySet(int newValue)：使用此方法后最终会被设置成newValue。是线程不安全的。官方解释如下：\n\nAs probably the last little JSR166 follow-up for Mustang, we added a “lazySet” method to the Atomic classes (AtomicInteger, AtomicReference, etc). This is a niche method that is sometimes useful when fine-tuning code using non-blocking data structures. The semantics are that the write is guaranteed not to be re-ordered with any previous write, but may be reordered with subsequent operations (or equivalently, might not be visible to other threads) until some other volatile write or synchronizing action occurs).\nThe main use case is for nulling out fields of nodes in non-blocking data structures solely for the sake of avoiding long-term garbage retention; it applies when it is harmless if other threads see non-null values for a while, but you’d like to ensure that structures are eventually GCable. In such cases, you can get better performance by avoiding the costs of the null volatile-write. There are a few other use cases along these lines for non-reference-based atomics as well, so the method is supported across all of the AtomicX classes.\nFor people who like to think of these operations in terms of machine-level barriers on common multiprocessors, lazySet provides a preceeding store-store barrier (which is either a no-op or very cheap on current platforms), but no store-load barrier (which is usually the expensive part of a volatile-write).\n\n这里解释道：此方法不可与之前的写操作进行重排序，可以与之后的写操作进行重排序，知道出现volatile写或synchronizing操作。好处是比普通的set方法性能要好，前提是可以忍受其他线程在一段时间内读到的是旧数据。\n\nint getAndSet(int newValue)：以原子方式更新，并且返回旧值。\n\nboolean compareAndSet(int expect, int update)：如果输入的值等于expect的值，则以原子方式更新。\n\nint getAndIncrement()：以原子方式自增，返回的是自增前的值。\n\nint getAndDecrement()：与getAndIncrement相反，返回的是自减前的值。\n\nint getAndAdd(int delta)：以原子方式，将当前值与输入值相加，返回的是计算前的值。\n\nint incrementAndGet()：以原子方式自增，返回自增后的值。\n\nint decrementAndGet()：以原子方式自减，返回自减后的值。\n\nint addAndGet(int delta)：以原子方式，将当前值与输入值相加，返回的是计算后的值。\n\nint getAndUpdate(IntUnaryOperator updateFunction)：Java1.8新增方法，以原子方式，按照指定方法更新当前数值，返回更新前的值，需要注意的是，提供的方法应该无副作用（side-effect-free），即两次执行结果相同，原因是如果由于线程争用导致更新失败会尝试再次执行该方法。\n\nint updateAndGet(IntUnaryOperator updateFunction)：同样是Java1.8新增方法，与getAndUpdate唯一不同的是返回值是更新后的值。\n\nint getAndAccumulate(int x, IntBinaryOperator accumulatorFunction)：与上述两个方法类似，操作数由参数x提供。返回更新前的值。\n\nint accumulateAndGet(int x, IntBinaryOperator accumulatorFunction)：与getAndAccumulate方法作用相同，返回更新后的值。\n\n\n方法介绍完了，AtomicInteger是怎么实现原子操作的呢？一起来看一下getAndIncrement方法的源码。\n/** * Atomically increments by one the current value. * * @return the previous value */public final int getAndIncrement() &#123;    return unsafe.getAndAddInt(this, valueOffset, 1);&#125;\n继续看Unsafe方法里的getAndIncrement方法\npublic final int getAndAddInt(Object var1, long var2, int var4) &#123;    int var5;    do &#123;        var5 = this.getIntVolatile(var1, var2);    &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));    return var5;&#125;\n其中getIntVolatile方法是一个本地方法，根据对象以及偏移量获取对应的值。然后执行compareAndSwapInt方法，该方法根据对象和偏移量获取当当前值，与希望的值var5比较，如果相等，则将值更新为var5+var4。否则，进入循环。如果想要了解UnSafe类的其他方法，可以阅读源码或者参考这篇文章Java中Unsafe类详解。\n原子数组下面的类是为数组中某个元素的更新提供原子操作的类。\n\nAtomicIntegerArray\nAtomicLongArray\nAtomicReferenceArray\n\n这三个类中的方法也都是类似的：\n\n\n\n我们对AtomicIntegerArray中的方法进行介绍。\n\nAtomicIntegerArray(int length)：构造函数，新建一个数组，传入AtomicIntegerArray。\nAtomicIntegerArray(int[] array)：构造函数，将array克隆一份，传入AtomicIntegerArray，因此，修改AtomicIntegerArray中的元素时不会影响原数组。\nint length()：获取数组长度。\nint get(int i)：获取位置i的元素。\nvoid set(int i, int newValue)：设置对应位置的值。\nvoid lazySet(int i, int newValue)：类似AtomicInteger中的lazySet。\nint getAndSet(int i, int newValue)：更新对应位置的值，返回更新前的值。\nboolean compareAndSet(int i, int expect, int update)：比较对应位置的值与期望值，如果相等，则更新，返回true。如果不能返回false。\nint getAndIncrement(int i)：对位置i的元素以原子方式自增，返回更新前的值。\nint getAndDecrement(int i)：对位置i的元素以原子方式自减，返回更新前的值。\nint getAndAdd(int i, int delta)：对位置i的元素以原子方式计算，返回更新前的值。\nint incrementAndGet(int i)、int decrementAndGet(int i)、addAndGet(int i, int delta)：这三个方法与上面三个方法操作相同，区别是这三个方法返回的是更新后的值。\n\n下面四个方法都是1.8才加入的，根据提供的参数中的方法对位置i的元素进行操作。区别是返回值不同以及是否提供操作数。\n\nint getAndUpdate(int i, IntUnaryOperator updateFunction)\nint updateAndGet(int i, IntUnaryOperator updateFunction)\nint getAndAccumulate(int i, int x, IntBinaryOperator accumulatorFunction)\nint accumulateAndGet(int i, int x, IntBinaryOperator accumulatorFunction)\n\n原子数组类型同样也是调用Unsafe类的方法，因此原理与基本类型的原理相同，这里不做赘述。\n原子引用类型前面讲到的类型都只能以原子的方式更新一个变量，有没有办法以原子方式更新多个变量呢？我们可以利用了面向对象的封装思想，可以把多个变量封装成一个类，再以原子的方式更新一个类对象。幸运的是，Atomic为我们提供了更新引用类型的方法。一起来认识一下他们吧。\n\nAtomicReference\nAtomicReferenceFieldUpdater\nAtomicMarkableReference\n\n同样的，先来看一下这三个类提供的方法有哪些。\n\n\n\n方法的作用与AtomicInteger中的方法类似，不做过多介绍。\n/** * Atomically sets the value to the given updated value * if the current value &#123;@code ==&#125; the expected value. * @param expect the expected value * @param update the new value * @return &#123;@code true&#125; if successful. False return indicates that * the actual value was not equal to the expected value. */public final boolean compareAndSet(V expect, V update) &#123;    return unsafe.compareAndSwapObject(this, valueOffset, expect, update);&#125;\n这是compareAndSet方法的源码，同样是调用UnSafe类的CAS方法，因此，原子操作的原理也和基本类型相同。\n原子更新字段类前文提到了AtomicReferenceFieldUpdater类，它更新的是类的字段，除了这个类，Atomic还提供了另外三个类用于更新类中的字段：\n\nAtomicIntegerFieldUpdater\nAtomicLongFieldUpdater\nAtomicStampedReference\n\n使用这些类时需要注意以下几点：\n\n更新字段必须有volatile关键字修饰\n更新字段不能是类变量\n使用前需要调用newUpdater()方法创建一个Updater\n\n这三个类的方法语义也很明确，可以参考AtomicInteger。\n\n\n\n总结Atomic包提供了足够的原子类供我们使用，想要真正完全理解这些类，还需要不断的练习。\n","tags":["Java"]},{"title":"Hello Hexo!","url":"/2018/08/05/Hello-Hexo/","content":""},{"title":"Redis Lua脚本中学教程（上）","url":"/2019/06/10/Redis-Lua%E8%84%9A%E6%9C%AC%E4%B8%AD%E5%AD%A6%E6%95%99%E7%A8%8B%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"失踪人口回来啦！\n有读者问我为什么这么久都没有出Redis Lua中学教程，表示村头厕所已经好久没有纸了。其实我早就要写这篇中学教程了，奈何最近太忙了，就一拖再拖，直到今天我终于又开始动笔了。忘记Lua相关概念的同学可以先回顾一下小学教程。\n中学教程主要分为两部分：Redis Lua的相关命令详解和Lua的语法介绍。\n前面我们简单介绍了EVAL和EVALSHA命令。但是只有那点只是是没办法从中学毕业的，因此我们需要进行更深入的学习。\nEVAL最早可用版本：2.6.0\n用法：EVAL script numkeys key [key …] arg [arg …]\n关于用法我们已经演示过了，其中第一个参数是要执行的Lua脚本，第二个参数是传入脚本的参数个数。后面则是参数的key数组和value数组。\n在Lua中执行Redis命令的方法我们也介绍过，就是使用redis.call()和redis.pcall()两个函数。它们之间唯一的不同就是当Redis命令执行错误时，redis.call()会抛出这个错误，使EVAL命令抛出错误，而redis.pcall()会捕获这个错误，并返回Lua的错误表。\n通常我们约定执行命令的key都需要由参数传入，命令必须在执行之前进行分析，以确定它作用于哪个key。这样做的目的是为了在一定程度上保证EVAL执行的Lua脚本的正确性。\nLua和Redis之间数据类型的转换在Redis执行EVAL命令时，如果脚本中有call()或者pcall()命令，就会涉及到Redis和Lua之间数据类型转换的问题。转换规则要求，一个Redis的返回值转换成Lua数据类型后，再转换成Redis数据类型，其结果必须和初始值相同。所以每种类型是一一对应的。转换规则如下：\nRedis与Lua互相转换\n\n\nRedis\nLua\n\n\n\n\ninteger\nnumber\n\n\nbulk\nstring\n\n\nmulti bulk\ntable\n\n\nstatus\ntable with a single ok field\n\n\nerror\ntable with a single err field\n\n\nNil bulk &amp;Nil multi bulk\nfalse boolean type\n\n\n\n除此之外，Lua到Redis的转换还有一些其他的规则：\n\nLua boolean true -&gt; Redis integer reply with value of 1\nLua只有一种数字类型，不会区分整数和浮点数。而数字类型只能转换成Redis的integer类型，如果要返回浮点数，那么在Lua中就需要返回一个字符串。\nLua数组在转换成Redis类型时，遇到nil就停止转换\n\n来个栗子验证一下：\nEVAL &quot;return &#123;1,2,3.3333,&#x27;foo&#x27;,nil,&#x27;bar&#x27;&#125;&quot; 01) (integer) 12) (integer) 23) (integer) 34) &quot;foo&quot;\n可以看到bar没有返回，并且3.333返回了3。\n脚本的原子性Redis运行所有的Lua命令都使用相同的Lua解释器。当一个脚本正在执行时，其他的脚本或Redis命令都不能执行。这很像Redis的事务multi/exec。这意味着我们要尽量避免脚本的执行时间过长。\n脚本整体复制当脚本进行传播或者写入AOF文件时，Redis通常会将脚本本身进行传播或写入AOF，而不是使用它产生的若干命令。原因很简单，传播整个脚本要比传播一大堆生成的命令的速度要快。\n从Redis3.2开始，可以只复制影响脚本执行结果的语句，而不用复制整个脚本。这个复制整个脚本的方法有以下属性：\n\n如果输入相同，脚本必须输出相同的结果。即执行结果不能依赖于隐式的变量，或依赖于I/O输入\nLua不会导出访问系统时间或其他外部状态的命令\n如果先执行了“随机命令”（如RANDOMKEY，SRANDMEMBER，TIME），并改变了数据集，接着执行脚本时会被阻塞。\n在Redis4中，Lua脚本调用返回随机顺序的元素的命令时，会在返回之前进行排序，也就是说，调用redis.call(“smembers”,KEYS[1])，每次返回的顺序都相同。从Redis5开始就不需要排序了，因为Redis5复制的是产生影响的命令。\nLua修改了伪随机函数math.random和math.randomseed，使每次执行脚本时seed都相同，而如果不执行math.randomseed，只执行math.random时，每次的结果也都相同。\n\n复制命令队列在这种模式下，Redis在执行脚本时会收集所有影响数据集的命令，当脚本执行完毕时，命令队列会被放在事务中，发送给AOF文件。\nLua可以通过执行redis.replicate_commands()函数来检查复制模式，如果返回true表示当前是复制命令模式，如果返回false，则是复制整个脚本模式。\n可选择的复制命令脚本复制模式选择好以后，就可以对复制到副本和AOF的方式进行更多的控制。这是一种高级特性，因为滥用会切断主从备份，和AOF持久化。如果我们只需要在master上执行某些命令时，这一特性就变得很有用。例如我们需要计算一些中间值时，只需要在master上计算就好，那么这些命令就不必进行复制。\n从Redis3.2开始，有一个新的命令叫做redis.set_repl()，它可以用来控制复制方式，有如下选项（默认是REPL_ALL）：\nredis.set_repl(redis.REPL_ALL) -- Replicate to AOF and replicas.redis.set_repl(redis.REPL_AOF) -- Replicate only to AOF.redis.set_repl(redis.REPL_REPLICA) -- Replicate only to replicas (Redis &gt;= 5)redis.set_repl(redis.REPL_SLAVE) -- Used for backward compatibility, the same as REPL_REPLICA.redis.set_repl(redis.REPL_NONE) -- Don&#x27;t replicate at all.\n全局变量为了避免数据泄露，Redis脚本不允许创建全局变量。如果必须有一个公共变量，可以使用Redis的key来代替。在EVAL命令中创建一个全局变量会引起一个异常。\n&gt; eval &#x27;a=10&#x27; 0(error) ERR Error running script (call to f_933044db579a2f8fd45d8065f04a8d0249383e57): user_script:1: Script attempted to create global variable &#x27;a\n关于SELECT的使用在Lua脚本中使用SELECT就像在正常客户端中使用一样。值得一提的是，在Redis2.8.12之前，Lua脚本中执行SELECT是会影响到客户端的，而从2.8.12开始，Lua脚本中的SELECT只会在脚本执行过程中生效。这点在Redis版本升级时需要注意，因为升级前后，命令的语义会改变。\n可用的库Lua脚本中有许多库，但并不是都能在Redis中使用，其中可以使用的有：\n\nbase lib.\ntable lib.\nstring lib.\nmath lib.\nstruct lib.\ncjson lib.\ncmsgpack lib.\nbitop lib.\nredis.sha1hex function.\nredis.breakpoint and redis.debug function in the context of the Redis Lua debugger.\n\nstruct, CJSON and cmsgpack是外部库，其他的都是Lua的标准库。\n在脚本中打印Redis日志使用redis.log(loglevel,message)函数可以在Lua脚本中打印Redis日志。\nloglevel包括：\n\nredis.LOG_DEBUG\nredis.LOG_VERBOSE\nredis.LOG_NOTICE\nredis.LOG_WARNING\n\n它们与Redis的日志等级是对应的。\n沙箱和最大执行时间脚本不应该访问外部系统，包括文件系统和其他系统。脚本应该只能操作Redis数据和传入进来的参数。\n脚本默认的最大执行时间是5秒（正常脚本执行时间都是毫秒级，所以5秒已经足够长了）。可以通过修改lua-time-limit变量来控制最大执行时间。\n当脚本执行时间超过最大执行时间时，并不会被自动终止，因为这违反了脚本的原子性原则。当一个脚本执行时间过长时，Redis会有如下操作：\n\nRedis记录下这个脚本执行时间过长\n其他客户端开始接收命令，但是所有的命令都会会返回繁忙，除了SCRIPT KILL 和 SHUTDOWN NOSAVE\n如果一个脚本仅执行只读命令，则可以用SCRIPT KILL命令来停止它。\n如果脚本执行了写入命令，那么只能用SHUTDOWN NOSAVE来终止服务器，当前的所有数据都不会保存到磁盘。\n\nEVALSHA最早可用版本：2.6.0\n用法：EVALSHA sha1 numkeys key [key …] arg [arg …]\n该命令用来执行缓存在服务器上的脚本，sha1为脚本的唯一标识。\n使用EVAL命令必须每次都要把脚本从客户端传到服务器，由于Redis的内部缓存机制，它并不会每次都重新编译脚本，但是传输上仍然浪费带宽。\n另一方面，如果使用特殊命令或者通过redis.conf来定义命令会有以下问题：\n\n不同实例有不同的实现方式\n发布将会很困难，特别是分布式环境，因为要保证所有实例都包含给定的命令\n读应用程序代码时，由于它调用了服务端命令，会不清楚代码的语义\n\n为了避免这些问题，同时避免浪费带宽，Redis实现了EVALSHA命令。\n如果服务器中没有缓存指定的脚本，会返回给客户端脚本不存在的错误信息。\nSCRIPT DEBUG最早可用版本：3.2.0\n时间复杂度：O(1)\n用法：SCRIPT DEBUG YES|SYNC|NO\n该命令用于设置随后执行的EVAL命令的调试模式。Redis包含一个完整的Lua调试器，代号为LDB，可以使编写复杂脚本的任务更加简单，在调试模式下，Redis充当远程调试服务器，客户端可以逐步执行脚本，设置断点，检查变量等。想了解更多调试器内容的可以查看官方文档Redis Lua debugger。\nLDB可以设置成异步或同步模式。异步模式下，服务器会fork出一个调试会话，不会阻塞主会话，，调试会话结束后，所有数据都会回滚。同步模式则会阻塞会话，并保留调试过程中数据的改变。\nSCRIPT EXISTS最早可用版本：2.6.0\n时间复杂度：O(N)，N是脚本数量\n返回脚本是否存在于缓存中（存在返回1，不存在返回0）。这个命令适合在管道前执行，以保证管道中的所有脚本都已经加载到服务器端了，如果没有，需要用SCRIPT LOAD命令进行加载。\nSCRIPT FLUSH最早可用版本：2.6.0\n时间复杂度：O(N)，N是缓存中的脚本数\n刷新缓存中的脚本，这一命令常在云服务上被使用。\nSCRIPT KILL最早可用版本：2.6.0\n时间复杂度：O(1)\n停止当前正在执行的Lua脚本，通常用来停止执行时间过长的脚本。停止后，被阻塞的客户端会抛出一个错误。\nSCRIPT LOAD最早可用版本：2.6.0\n时间复杂度：O(N)，N是脚本的字节数\n该命令用于将脚本加载到服务器端的缓存中，但不会执行。加载后，服务器会一直缓存，因为良好的应用程序不太可能有太多不同的脚本导致内存不足。每个脚本都像一个新命令的缓存，所以即使是大型应用程序，也就有几百个，它们占用的内存是微不足道的。\n小结本文介绍了Redis Lua相关的命令。其中EVAL和EVALSHA用来执行脚本。脚本执行具有原子性。脚本的复制和传播可以根据需要设置。脚本中不能定义全局变量。\n","tags":["Redis"]},{"title":"Redis Lua脚本小学教程","url":"/2019/05/13/Redis-Lua%E8%84%9A%E6%9C%AC%E5%B0%8F%E5%AD%A6%E6%95%99%E7%A8%8B/","content":"Redis提供了丰富的指令集，但是仍然不能满足所有场景，在一些特定场景下，需要自定义一些指定来完成某些功能。因此，Redis提供了Lua脚本支持，用户可以自己编写脚本来实现想要的功能。\n什么是Lua？Lua是一种功能强大的，高效，轻量级，可嵌入的脚本语言。它是动态类型语言，通过使用基于寄存器的虚拟机解释字节码运行，并具有增量垃圾收集的自动内存管理，是配置，脚本和快速原型设计的最佳选择。\nRedis怎么执行Lua脚本EVAL命令Redis中可以使用EVAL命令执行相应的Lua脚本\n&gt; EVAL &#x27;local val=&quot;Hello Jackey&quot; return val&#x27; 0&quot;Hello Jackey&quot;\n你可以像这样在交互模式下执行Lua脚本，这样更方便处理错误。只是这样还不够，有时候，我们需要给Lua脚本传入一些参数。细心的同学一定注意到了，脚本的后面还有一个数字0，它的意思的不传入参数。\n那怎么传参数呢？\n&gt; EVAL &#x27;local val=KEYS[1] return val..&quot; &quot;..ARGV[1]&#x27; 1 Hello Redis&quot;Hello Redis&quot;\n其实也很简单，传入的参数都是kv形式的，这个数字代表传入参数的key的数量，再后面就是n个key和n个value。在脚本中，可以理解为从KEYS数组和ARGV数组中获取对应的值，下标是从1开始的。\n上面例子中的两个点是Lua脚本中字符串连接的操作符\n现在我们已经知道怎么在Redis中执行Lua脚本了，可是这样的脚本和Redis没有关系啊，怎么才能操作Redis中的数据呢？请继续看我表演\n&gt; get my_name&quot;Jackeyzhe&quot;&gt; EVAL &#x27;local val=ARGV[1]..&quot; &quot;..redis.call(&quot;get&quot;,KEYS[1]) return val&#x27; 1 my_name Hello&quot;Hello Jackeyzhe&quot;\n使用redis.call或redis.pcall（以后会提到）就可以操作redis了。\n需要注意的是，如果返回下面的错误，说明要获取的key不存在\n&gt; EVAL &#x27;local val=ARGV[1]..&quot; &quot;..redis.call(&quot;get&quot;,KEYS[1]) return val&#x27; 1 me Hello(error) ERR Error running script (call to f_eb11f8ddeeee07cc88d1f3bd103069284b83c5d8): @user_script:1: user_script:1: attempt to concatenate a boolean value\n我们可以使用上面这种方法执行一些简单的Lua脚本，如果要执行更加复杂的Lua脚本，用EVAL命令就会显得臃肿且凌乱。所以Redis又提供了一种方法。\nredis-cli –eval我们可以先写一个Lua文件，然后使用redis-cli命令来执行。\nlocal name=redis.call(&quot;get&quot;, KEYS[1])local greet=ARGV[1]local result=greet..&quot; &quot;..namereturn result\n&gt; redis-cli --eval hello.lua my_name , Hello&quot;Hello Jackey&quot;\n这样，我们就可以先写一个.lua文件，然后再使用redis-cli命令来执行了，看起来也不会很凌乱，使用这种方式传入参数时，不需要指定key的数量，而是用逗号分隔key和argv。\nEVALSHA你以为到这就结束了吗？那就too naive了。如果我们在Redis交互模式中，想要执行脚本文件怎么办？每次都退出来，执行完再连接一次？这未免太麻烦了。Redis提供了EVALSHA命令，使我们可以在交互模式执行脚本文件。\n首先，需要上传脚本文件\n$ redis-cli SCRIPT LOAD &quot;$(cat hello.lua)&quot;&quot;463ff2ca9e78e36cd66ee9d37ee0dcd59100bf46&quot;\n会得到一串十六进制的数字，这是这个脚本的唯一标识。拿到这个数字后，表示我们已经将脚本上传到服务器了，接下来就可以使用这个标识来执行脚本了。\n&gt; EVALSHA 463ff2ca9e78e36cd66ee9d37ee0dcd59100bf46 1 my_name Hello&quot;Hello Jackeyzhe&quot;\n终止脚本Redis中Lua脚本到默认执行时长是5秒，一般情况下脚本的执行时间都是毫秒级的，如果执行超时，脚本也不会停止，而是记录错误日志。\n终止脚本执行的方法有两种\n\n使用KILL SCRIPT命令\n使用SHUTDOWN NOSAVE命令关闭服务器\n\n不过不建议手动终止脚本\n总结本文简要介绍了什么是Lua，以及Redis执行和终止Lua脚本的方法。如果都掌握了，那么恭喜你已经从Lua小学毕业了。在Lua中学你会学到Redis关于Lua命令的更详细介绍。\n","tags":["Redis"]},{"title":"Redis Lua脚本中学教程（下）","url":"/2019/06/16/Redis-Lua%E8%84%9A%E6%9C%AC%E4%B8%AD%E5%AD%A6%E6%95%99%E7%A8%8B%EF%BC%88%E4%B8%8B%EF%BC%89/","content":" 在中学教程的上半部分我们介绍了Redis Lua相关的命令，没有看过或者忘记的同学可以步行前往直接使用机票Redis Lua脚本中学教程（上）。今天我们来简单学习一下Lua的语法。\n在介绍Lua语法之前，先来介绍一下Lua的身世。Lua是由简称为PUC-Rio的团队设计、开发和维护的。Lua在葡萄牙语中是月亮的意思，所以它不是简写，而是一个名词。所以只能写成Lua，而不能写成LUA或者其他什么的。接下来我们正式入门Lua。\n变量变量名可以是由字母、数字和下划线组成的字符串，但不能以数字开头。另外需要注意的是，需要尽量避免使用下划线加一个或多个大写字母格式的变量名，因为这是Lua的保留字，除了这种格式以外，还有一些普通格式的保留字：\n\n\n\nand\nbreak\ndo\nelse\nelseif\n\n\n\n\nend\nfalse\nfor\nfunction\ngoto\n\n\nif\nin\nlocal\nnil\nnot\n\n\nor\nrepeat\nreturn\nthen\ntrue\n\n\nuntil\nwhile\n\n\n\n\n\nLua是大小写敏感的，and是保留字，但And和AND不是。\n全局变量前面我们提到过Redis不支持Lua的全局变量，但Lua本身是支持全局变量的。\n全局变量不需要声明，直接一个未初始化的变量时，它的值是nil。\n&gt; b --&gt; nil&gt; b = 10&gt; b --&gt; 10\n如果显示的将nil赋值给某个全局变量，Lua会认为我们不再使用这个变量。\n局部变量Lua的变量默认是全局变量，局部变量需要显示声明。局部变量可以避免增加不必要的名称来混淆全局环境，同时也能避免两部分代码的命名冲突。另外，访问局部变量要比访问全局变量的速度更快。\n局部变量的使用范围是有限制的，只在它声明的块中可用。（块可以是控制结构体或函数体或者是整个文件中）\nx = 10local i = 1 -- local to the chunkwhile i &lt;= x dolocal x = i * 2 -- local to the while bodyprint(x) --&gt; 2, 4, 6, 8, ...i = i + 1endif i &gt; 20 thenlocal x -- local to the &quot;then&quot; bodyx = 20print(x + 2) -- (would print 22 if test succeeded)elseprint(x) --&gt; 10 (the global one)endprint(x) --&gt; 10 (the global one)\n在交互模式下，每次输入都是一块代码，当你输入local i = 1时，就定义了一个局部变量i，而当你在下一行使用i时，发现它又成了全局变量。因此上面的栗子就不能用了。为了解决这个问题，我们需要在程序中显式的使用do-end标记代码块的范围。\nlocal x1, x2do  local a2 = 2*a  local d = (b^2 - 4*a*c)^(1/2)  x1 = (-b + d)/a2  x2 = (-b - d)/a2end                      -- scope of &#x27;a2&#x27; and &#x27;d&#x27; ends hereprint(x1, x2)            -- &#x27;x1&#x27; and &#x27;x2&#x27; still in scope\n使用这种方式标记代码块范围是一种良好的习惯，而使用局部变量编程也要优于使用全局变量，因此有很多人呼吁Lua默认应该定义局部变量，但是这样也会存在问题。最好的解决方案是不要默认，使用所有的变量之前都要声明。\nLua有一个常见的习语：\nlocal foo = foo\n这里定义了一个局部变量foo，并把全局变量foo的值赋给局部变量。这一习语主要用来提升变量foo的访问速度，或者对变量进行暂存，防止其他函数改变这个变量的值。\n注释单行注释Lua的单行注释使用双横线“–”表示，双横线后的内容为注释内容。\n多行注释多行注释的一种表现是以双横线加双左中括号开始，以双右中括号结束。例如：\n--[[A multi-linelong comment]]\n不过通常我们使用另一种写法：以双横线加双左中括号开始，以双横线加双右中括号结束，这种写法看起来更加美观，同时解注释也更加方便：\n--[[print(10) -- no action (commented out)--]]\n解注释\n---[[print(10) --&gt; 10--]]\n这里稍微解释一下这种写法的原理，注释时，后一组双横线在注释内容中，因此不起作用，只为了对称，效果和普通多行注释一样。而解注释时，第一组双横线前又加了一个横线，就不能认为是多行注释了，只能当做单行注释，因此，第一行被注释掉了，这时后一组双横线就会起作用了，注释掉后面的双右中括号。\n数据类型Lua是一种动态类型语言，它有8种基本类型：nil，Boolean，number，string，userdata，function，thread和table。type函数可以返回指定值的类型：\n&gt; type(nil) --&gt; nil&gt; type(true) --&gt; boolean&gt; type(10.4 * 3) --&gt; number&gt; type(&quot;Hello world&quot;) --&gt; string&gt; type(io.stdin) --&gt; userdata&gt; type(print) --&gt; function&gt; type(type) --&gt; function&gt; type(&#123;&#125;) --&gt; table&gt; type(type(X)) --&gt; string\nNilNil类型的值只有一种，就是nil，它是一种没有值的表现。\nBooleanBoolean类型有两种取值，@false{} and @true{}。但是Boolean类型并不能囊括所有的条件值：在条件判断时，Lua会将false和nil判断为假，其他的都判断为真。\n画外音：Lua把0和空字符串也判断为真，这点感觉设计的不太好啊\nand、or和not是Lua的逻辑运算符。\nand的运算方法是，判断第一个操作数是不是false，如果不是，结果就是第二个操作数。\nor的运算方法是，判断第一个操作数是不是真，如果不是，结果就是第二个操作数。\n&gt; 4 and 5 --&gt; 5&gt; nil and 13 --&gt; nil&gt; false and 13 --&gt; false&gt; 0 or 5 --&gt; 0&gt; false or &quot;hi&quot; --&gt; &quot;hi&quot;&gt; nil or false --&gt; false\nTableTable是Lua中主要的（也是唯一的）结构化数据表现类型。它可以用来表现很多种数据类型，如数组、集合、记录等。\n每个表的key可以是不同类型的，对于未定义索引的表元素，它的默认值是nil。和其他大部分语言不同的是Lua中表的下标是从1开始的。\nTable有两种格式：record-style和list-style\nrecord-style可以直接用”.”访问，list-style可以用下标来访问。定义时可以一起定义。\npolyline = &#123;color=&quot;blue&quot;,            thickness=2,            npoints=4,            &#123;x=0,   y=0&#125;,            &#123;x=-10, y=0&#125;,            &#123;x=-10, y=1&#125;,            &#123;x=0,   y=1&#125;&#125;\n当我们访问一个可能为空的Table，往往需要先判断非空\nif lib and lib.foo then ....\n使用这种方式访问结构比较深的表示就会非常痛苦：\nzip = company and company.director and                      company.director.address and                        company.director.address.zipcode\nLua没有像C#一样提供?.这样的操作，不过我们可以使用or {}的形式来处理。\nzip = (((company or &#123;&#125;).director or &#123;&#125;).address or &#123;&#125;).zipcode\n流程控制Lua提供了一些基本的流程控制语句：\n\nif用于条件判断\nwhile、repeat和for用于便利\nend可以终止if、for和while\nuntil可以终止repeat\nbreak用于跳出循环\nreturn用于跳出函数\ngoto会跳转到指定位置\n\n函数Lua中函数可以接收的参数是list，如果没有参数，也需要写一对空的括号”()”（一句废话）。如果只有一个参数，则括号可写可不写。Lua还提供了一种特殊的函数访问方法，有兴趣的话可以参考https://www.lua.org/pil/16.html\no:foo(x)\nLua程序中既可以使用定义在Lua中的函数，也可以使用定义在C语言中的函数。\nLua函数有一个非常方便的特性：可以返回多个结果。\nfunction maximum (a)  local mi = 1  local m = a[mi]  for i = 1, #a do    if a[i] &gt; m then      mi = i; m = a[i]end end  return m, miendprint(maximum(&#123;8,10,23,12,5&#125;))     --&gt; 23   3\nLua可以自动调整返回结果的数量，当函数作为语句调用时，会舍弃所有返回值；当函数作为表达式调用时，只保留第一个返回值；如果要获得全部返回值，函数调用需要是表达式最后一个。\nLua函数也支持可变参数：\nfunction add (...)  local s = 0  for _, v in ipairs&#123;...&#125; do\t\ts=s+ v   endreturn s endprint(add(3, 4, 10, 25, 12))   --&gt; 54\n总结来简单总结一下，本文我们介绍了Lua的基本语法，包括如何定义变量（包括全局变量和局部变量），8种基本数据类型，流程控制语句以及Lua中函数的一些特性。相信看完本文，你就可以写一些简单的Lua脚本了。\n对Lua感兴趣的同学可以自行前往Lua官网继续深造。\n","tags":["Redis"]},{"title":"Redis Lua脚本大学教程","url":"/2019/06/17/Redis-Lua%E8%84%9A%E6%9C%AC%E5%A4%A7%E5%AD%A6%E6%95%99%E7%A8%8B/","content":"前面我们已经把Redis Lua相关的基础都介绍过了，如果你可以编写一些简单的Lua脚本，恭喜你已经可以从Lua中学毕业了。\n在大学课程中，我们主要学习Lua脚本调试和Redis中Lua执行原理两部分内容两部分。\nLua脚本调试Redis从3.2版本开始支持Lua脚本调试，调试器的名字叫做LDB。它有一些重要的特性：\n\n它使用的是服务器-客户端模式，所以是远程调试。Redis服务器就是调试服务器，默认的客户端是redis-cli。也可以开发遵循服务器协议的其他客户端。\n默认情况下，每个debugging session都是一个新的session。也就是说在调试的过程中，服务器不会被阻塞。仍然可以被其他客户端使用或开启新的session。同时也意味着在调试过程中所有的修改在结束时都会回滚。\n如果需要，可以把debugging模式调成同步，这样就可以保留对数据集的更改。在这种模式下，调试时服务器会处于阻塞状态。\n支持步进式执行\n支持静态和动态断点\n支持从脚本中向调试控制台打印调试日志\n检查Lua变量\n追踪Redis命令的执行\n很好的支持打印Redis和Lua的值\n无限循环和长执行检测，模拟断点\n\nLua脚本调试实战在开始调试之前，首先编写一个简单的Lua脚本script.lua：\nlocal src = KEYS[1]local dst = KEYS[2]local count = tonumber(ARGV[1])while count &gt; 0 do    local item = redis.call(&#x27;rpop&#x27;,src)    if item ~= false then        redis.call(&#x27;lpush&#x27;,dst,item)    end    count = count - 1endreturn redis.call(&#x27;llen&#x27;,dst)  \n这个脚本是把src中的元素依次插入到dst元素的头部。\n有了这个脚本之后我们就可以开始调试工作了。\n我们可以使用redis-cli —eval命令来运行这个脚本，而要调试的话，可以加上—ldb参数，因此我们先执行下面的命令：\nredis-cli --ldb --eval script.lua foo bar , 10\n页面会出现一些帮助信息，并进入到调试模式\n\n可以看到帮助页告诉我们\n\n执行quit可以退出调试模式\n执行restart可以重新调试\n执行help可以查看更多帮助信息\n\n这里我们执行help命令，查看一下帮助信息，打印出很多可以在调试模式下执行的命令，中括号”[]”内到内容表示命令的简写。\n其中常用的有：\n\nstep/next：执行一行\ncontinue：执行到西一个断点\nlist：展示源码\nprint：打印一些值\nbreak：打断点\n\n另外在脚本中还可以使用redis.breakpoint()添加动态断点。\n下面来简单演示一下\n\n现在我把代码中count = count - 1这一行删除，使程序死循环，再来调试一下\n\n可以看到我们并没有打断点，但是程序仍然会停止，这是因为执行超时，调试器模拟了一个断点使程序停止。从源码中可以看出，这里的超时时间是5s。\n/* Check if a timeout occurred. */if (ar-&gt;event == LUA_HOOKCOUNT &amp;&amp; ldb.step == 0 &amp;&amp; bp == 0) &#123;  mstime_t elapsed = mstime() - server.lua_time_start;  mstime_t timelimit = server.lua_time_limit ?    server.lua_time_limit : 5000;  if (elapsed &gt;= timelimit) &#123;    timeout = 1;    ldb.step = 1;  &#125; else &#123;    return; /* No timeout, ignore the COUNT event. */  &#125;&#125;\n由于Redis默认的debug模式是异步的，所以在调试结束后不会改变redis中的数据。\n\n当然，你也可以选择以同步模式执行，只需要把执行命令中的—ldb参数改成–ldb-sync-mode就可以了。\n解读EVAL命令前文我们已经详细介绍过EVAL命令了，不了解的同学可以再回顾一下Redis Lua脚本中学教程（上）)。今天我们结合源码继续探究EVAL命令。\n在server.c文件中，我们知道了eval命令执行的是evalCommand函数。这个函数的实现在scripting.c文件中。\n函数调用栈是\nevalCommand\t(evalGenericCommandWithDebugging)    evalGenericCommand      lua_pcall  //Lua函数\nevalCommand函数很简单，只是简单的判断是否是调试模式，如果是调试模式，调用evalGenericCommandWithDebugging函数，如果不是，直接调用evalGenericCommand函数。\n在evalGenericCommand函数中，先判断了key的数量是否正确\n/* Get the number of arguments that are keys */if (getLongLongFromObjectOrReply(c,c-&gt;argv[2],&amp;numkeys,NULL) != C_OK)    return;if (numkeys &gt; (c-&gt;argc - 3)) &#123;    addReplyError(c,&quot;Number of keys can&#x27;t be greater than number of args&quot;);    return;&#125; else if (numkeys &lt; 0) &#123;    addReplyError(c,&quot;Number of keys can&#x27;t be negative&quot;);    return;&#125;\n接着查看脚本是否已经在缓存中，如果没有，计算脚本的SHA1校验和，如果已经存在，将SHA1校验和转换为小写\n /* We obtain the script SHA1, then check if this function is already     * defined into the Lua state */funcname[0] = &#x27;f&#x27;;funcname[1] = &#x27;_&#x27;;if (!evalsha) &#123;    /* Hash the code if this is an EVAL call */    sha1hex(funcname+2,c-&gt;argv[1]-&gt;ptr,sdslen(c-&gt;argv[1]-&gt;ptr));&#125; else &#123;    /* We already have the SHA if it is a EVALSHA */    int j;    char *sha = c-&gt;argv[1]-&gt;ptr;    /* Convert to lowercase. We don&#x27;t use tolower since the function         * managed to always show up in the profiler output consuming         * a non trivial amount of time. */    for (j = 0; j &lt; 40; j++)        funcname[j+2] = (sha[j] &gt;= &#x27;A&#x27; &amp;&amp; sha[j] &lt;= &#x27;Z&#x27;) ?        sha[j]+(&#x27;a&#x27;-&#x27;A&#x27;) : sha[j];    funcname[42] = &#x27;\\0&#x27;;&#125;\n这里funcname变量存储的是f_ +SHA1校验和，Redis会将脚本定义为一个Lua函数，funcname是函数名。函数体是脚本本身。\nsds luaCreateFunction(client *c, lua_State *lua, robj *body) &#123;    char funcname[43];    dictEntry *de;    funcname[0] = &#x27;f&#x27;;    funcname[1] = &#x27;_&#x27;;    sha1hex(funcname+2,body-&gt;ptr,sdslen(body-&gt;ptr));    sds sha = sdsnewlen(funcname+2,40);    if ((de = dictFind(server.lua_scripts,sha)) != NULL) &#123;        sdsfree(sha);        return dictGetKey(de);    &#125;    sds funcdef = sdsempty();    funcdef = sdscat(funcdef,&quot;function &quot;);    funcdef = sdscatlen(funcdef,funcname,42);    funcdef = sdscatlen(funcdef,&quot;() &quot;,3);    funcdef = sdscatlen(funcdef,body-&gt;ptr,sdslen(body-&gt;ptr));    funcdef = sdscatlen(funcdef,&quot;\\nend&quot;,4);    if (luaL_loadbuffer(lua,funcdef,sdslen(funcdef),&quot;@user_script&quot;)) &#123;        if (c != NULL) &#123;            addReplyErrorFormat(c,                &quot;Error compiling script (new function): %s\\n&quot;,                lua_tostring(lua,-1));        &#125;        lua_pop(lua,1);        sdsfree(sha);        sdsfree(funcdef);        return NULL;    &#125;    sdsfree(funcdef);    if (lua_pcall(lua,0,0,0)) &#123;        if (c != NULL) &#123;            addReplyErrorFormat(c,&quot;Error running script (new function): %s\\n&quot;,                lua_tostring(lua,-1));        &#125;        lua_pop(lua,1);        sdsfree(sha);        return NULL;    &#125;    /* We also save a SHA1 -&gt; Original script map in a dictionary     * so that we can replicate / write in the AOF all the     * EVALSHA commands as EVAL using the original script. */    int retval = dictAdd(server.lua_scripts,sha,body);    serverAssertWithInfo(c ? c : server.lua_client,NULL,retval == DICT_OK);    server.lua_scripts_mem += sdsZmallocSize(sha) + getStringObjectSdsUsedMemory(body);    incrRefCount(body);    return sha;&#125;\n在执行脚本之前，还要保存传入的参数，选择正确的数据库。\n/* Populate the argv and keys table accordingly to the arguments that * EVAL received. */luaSetGlobalArray(lua,&quot;KEYS&quot;,c-&gt;argv+3,numkeys);luaSetGlobalArray(lua,&quot;ARGV&quot;,c-&gt;argv+3+numkeys,c-&gt;argc-3-numkeys);/* Select the right DB in the context of the Lua client */selectDb(server.lua_client,c-&gt;db-&gt;id);\n然后还需要设置钩子，我们之前提过的脚本执行超时自动打断点以及可以执行SCRPIT KILL命令停止脚本和通过SHUTDOWN命令停止服务器，都是通过钩子来实现的。\n/* Set a hook in order to be able to stop the script execution if it     * is running for too much time.     * We set the hook only if the time limit is enabled as the hook will     * make the Lua script execution slower.     *     * If we are debugging, we set instead a &quot;line&quot; hook so that the     * debugger is call-back at every line executed by the script. */server.lua_caller = c;server.lua_time_start = mstime();server.lua_kill = 0;if (server.lua_time_limit &gt; 0 &amp;&amp; ldb.active == 0) &#123;    lua_sethook(lua,luaMaskCountHook,LUA_MASKCOUNT,100000);    delhook = 1;&#125; else if (ldb.active) &#123;    lua_sethook(server.lua,luaLdbLineHook,LUA_MASKLINE|LUA_MASKCOUNT,100000);    delhook = 1;&#125;\n到这里已经万事俱备了，就可以直接调用lua_pcall函数来执行脚本了。执行完之后，还要删除钩子并把结果保存到缓冲中。\n上面就是脚本执行的整个过程，这个过程之后，Redis还会处理一些脚本同步的问题。这个前文我们也介绍过了《Redis Lua脚本中学教程（上）》\n总结到这里，Redis Lua脚本系列就全部结束了。文章虽然结束了，但是学习还远远没有结束。大家有问题的话欢迎和我一起探讨。共同学习，共同进步~\n对Lua感兴趣的同学可以读一下《Programming in Lua》，有条件的尽量支持正版，想先看看质量的可以在我公众号后台回复Lua获取电子书。\n","tags":["Redis"]},{"title":"Redis命令详解：Cluster","url":"/2019/08/28/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9ACluster/","content":"前文中我们介绍过了Redis的三种集群方案，没有了解过的同学可以自行前往。今天要介绍的Redis的亲儿子Cluster相关的命令。\n玩转Redis集群之Sentinel\n玩转Redis集群之Codis\n玩转Redis集群之Cluster\nCLUSTER ADDSLOTS最早可用版本：3.0.0\n时间复杂度：O(N)，N是参数中hash的slot总数\n这个命令是用来将指定的slot分配给接收命令的机器。如果执行成功，该机器就拥有这些slot，并会在集群中进行广播。\n需要注意的是：\n\n该命令只有在当所有指定的slot在接收命令的节点上没有被分配时生效。节点将拒绝接纳已经分配到其他节点的slot。\n如果相同的slot被多次指定，命令就会执行失败。\n如果一个slot作为参数被设置为importing，一旦节点向自己分配该slot，这个状态将会被清除。\n\n这个命令的应用场景有两种：\n\n创建新的集群时，ADDSLOTS用于主节点初始化分配可用的hash slots\n修复有未分配slots的坏集群\n\n如果一个节点为自己分配了一个slot集合，它会将这个信息在心跳包的header里传播出去。然而其他节点只有在他们的slot没有被其他节点绑定或者没有作为新节点传播时才会接收这个信息。\n这意味着这个命令应该仅通过redis集群应用管理客户端，例如redis-trib。如果这个命令使用了错误的上下文会导致集群处于错误的状态或者导致数据丢失，因此这个命令需要谨慎使用。\nCLUSTER COUNT-FAILURE-REPORTS最早可用版本：3.0.0\n时间复杂度：O(N)，N是故障报告的数量\n这个命令返回指定节点的故障报告。故障报告是Redis Cluster用来将节点从PFAIL状态转换到FAIL状态的方式。\n更多的细节：\n\n一个节点会用PFAIL标记一个不可达时间超过超时时间，这个超时时间是Redis Cluster配置中的基本选项\n处于PFAIL状态的节点会将状态信息提供在心跳包的gossip部分。\n每当一个节点处理来自其他节点的gossip信息时，该节点会建立故障报告，并且会记住发送消息包的节点说的其他节点是在PFAIL状态的消息。\n每个故障报告的生存时间是节点超时时间的两倍\n如果在一段时间一个节点被另一个节点标记为PFAIL状态，并且在同一时间收到大多数主节点关于该节点的故障报告，那么该节点的故障状态会从PFAIL变成FAIL，并且广播这个信息，让所有可达的节点将这个节点标记为FAIL。\n\n该节点返回当前节点的故障报告数，该计数值不包括当前节点。\nCLUSTER COUNTKEYSINSLOT最早可用版本：3.0.0\n时间复杂度：O(1)\n这个命令返回指定Redis集群的slot的key的数量。该命令只查询连接节点的本地数据集，如果指定的slot被分配在别的节点上，就会返回0。\nCLUSTER DELSLOTS最早可用版本：3.0.0\n时间复杂度：O(N)，N是slot参数的数量\n在Redis Cluster中，每个节点都会知道哪些主节点正在负责哪些slot。\nDELSLOTS命令使一个特定的节点忘记主节点负责的hash slot。在这之后，这些hash slot就被认为是未绑定状态的。需要注意的是：\n\n命令只在参数指定的hash slot和某些节点绑定时有效\n如果同一个hash slot被指定多次，该命令会失效\n节点可能因为没有覆盖全部slot而变成下线状态\n\nCLUSTER FAILOVER最早可用版本：3.0.0\n时间复杂度：O(1)\n用法：CLUSTER FAILOVER [FORCE|TAKEOVER]\n该命令只能再集群slave节点执行，让slave节点进行一次人工的故障切换。\n人工故障切换时一种常规操作，而不是真的出了故障。当我们希望当前的master和它的slave进行一次安全的主备切换时，流程如下：\n\n当前slave节点告知其master停止处理来自客户端的请求。\nmaster节点将当前replication offset回复给slave\n该slave节点在未应用至replication offset之前不做任何操作，以保证master传来的数据均被处理\n该slave节点进行故障转移，从集群中大多数master获取一个新的配置，并广播自己的最新配置\n旧的master更新配置，解除客户端阻塞，回复重定向信息，以便客户端可以和新的master通信。\n\n该命令有两个选项：FORCE和TAKEOVER，下面我们来解释一下这两个选项的作用。\nFORCE选项：slave节点不会和master做协商，直接从上述第4步开始进行故障切换\nTAKEOVER选项：忽略集群一致验证的人工故障切换。有时会出现集群中master节点不够的情况，此时我们就需要使用TAKEOVER选项将slave批量切换为master节点。\nTAKEOVER选项实现了FORCE选项的所有功能，当一个slave节点收到CLUSTER FAILOVER TAKEOVER命令时会有如下操作：\n\n生成一个新的configEpoch，如果本地配置的epoch不是最大的，就需要将其配置为最大。\n将原master节点管理的所有slot分配给自己，同时尽快分发最新的配置给所有可达节点。\n\nCLUSTER FORGET最早可用版本：3.0.0\n时间复杂度：O(1)\n这个命令用于移除指定node-id的node。如果一个node属于某个集群，那么集群中其他节点都会知道它的存在，因此CLUSTER FORGET命令会发送给集群中剩下的所有节点。\n然而这个命令并不是简单的把node从node表中删除，它还禁止这个节点再次被添加进集群。\n假设我们有4个节点：A、B、C、D，此时我们删除D，如果不把D加入禁用列表，就会发生以下情况：\n\n把D负责的slot重新分配给A、B、C\n此时D没有负责的slot了，但仍然在node表中\n给A发送命令CLUSTER FORGET D\nB给A发送一个心跳包，其中包含了D的信息\nA不知道D的信息，就又开始联系D\nD又被重新加入到A的node表中\n\n这样我们的删除命令就无效了，除非我们在各个节点没有互相发送心跳包的时候同时给他们发送CLUSTER FORGET命令。但这显然不合理，所以我们实际上应该这样做：\n\n指定的节点从node表中删除\n被删除的节点的node-id加入禁用列表一分钟\nnode在处理心跳消息时会忽略禁用列表中的所有node-id的节点\n\n在一些特殊情况下，这个命令会无法执行并返回一个错误。\n\n指定的node-id没有在node表中\n收到命令的节点是从节点，而要删除的节点是它的主节点\n收到命令的节点和待删除的节点是同一个节点\n\nCLUSTER GETKEYSINSLOT最早可用版本：3.0.0\n时间复杂度：O(log(N))，N是请求的key的数量\n用法：CLUSTER GETKEYSINSLOT slot count\n这个命令返回连接节点指定的slot里key的列表。key的最大数量由count指定。所以这个API可以用作key的批处理。这个命令的主要用途是在做rehash的过程中，把slot从一个节点移动到另外一个节点。\nCLUSTER INFO最早可用版本：3.0.0\n时间复杂度：O(1)\n提供Redis集群的相关信息。\ncluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:2cluster_stats_messages_sent:1483972cluster_stats_messages_received:1483968\n\ncluster_state：ok状态表示节点可以接收查询请求，fail表示至少有一个slot没有分配或者在error状态。\ncluster_slots_assigned：已经分配到集群节点的slot。16384个slot全部被分配到集群节点是集群节点正常运行的必要条件\ncluster_slots_ok：slot不是FAIL或PFAIL状态的数量\ncluster_slots_pfail：slot状态是PFAIL的数量\ncluster_slots_fail：slot状态是FAIL的数量，如果不是0，那么集群节点将无法提供服务，除非cluster-require-full-coverage被设置为no\ncluster_known_nodes：集群中的节点数量\ncluster_size：至少包含一个slot且能够提供服务的master节点数量\ncluster_current_epoch：集群本地Current Epoch的值\ncluster_my_epoch：当前正在使用节点的Config Epoch值\ncluster_stats_messages_sent：通过点到点总线发送消息的数量\ncluster_stats_messages_received：通过点到点总线接收消息的数量\n\nCLUSTER KEYSLOT最早可用版本：3.0.0\n时间复杂度：O(N)，N是key的字节数\n返回一个整数，用于标识指定键所散列到的slot。这个命令主要是用于调试和测试。因为它通过一个API来暴露Redis底层hash算法的实现。\nCLUSTER MEET最早可用版本：3.0.0\n时间复杂度：O(1)\nCLUSTER MEET命令用来连接不同Redis节点，以进入工作集群。\n有一点基本的共识是节点之间互相都是不信任的，并且被认为是未知节点。以避免因为系统管理错误或者网络地址被修改而导致多个集群的节点混合成一个。\n因此，为了使给定的节点能接收另一个节点到Redis Cluster中，有两种方法：\n\n系统管理员发送CLUSTER MEET命令强制一个节点会面另一个节点\n一个已知节点发送一个保存在gossip部分的节点列表，包含着未知节点。如果接收的节点已经将发送节点标记为已知节点，那么它会处理gossip中的位置节点信息，并给它发送一个握手消息。\n\nRedis Cluster是一个完整的网络，在创建网络时，并不需要给所有节点发送CLUSTER MEET命令，只要发送了足够的命令，保证每个节点都有已知节点，其他的事情就交给gossip来处理了。\nCLUSTER NODES最早可用版本：3.0.0\n时间复杂度：O(N)，N是集群中的节点数\n该命令提供了当前连接节点所属集群的配置信息。信息格式和Redis集群在磁盘上存储使用的序列化格式完全一样。\n通常，如果你想知道hash slot与节点的关联关系，你应该使用CLUSTER SLOTS命令。CLUSTER NODES主要用于管理任务，调试和配置监控。redis-trib也会使用该命令管理集群。\n命令的结构如下：\n&lt;id&gt; &lt;ip:port&gt; &lt;flags&gt; &lt;master&gt; &lt;ping-sent&gt; &lt;pong-recv&gt; &lt;config-epoch&gt; &lt;link-state&gt; &lt;slot&gt; &lt;slot&gt; ... &lt;slot&gt;\nCLUSTER REPLICAS最早可用版本：5.0.0\n时间复杂度：O(1)\n该命令会列出主节点的从节点列表。输出格式与CLUSTER NODES格式相同。\n若特定节点状态未知，或在接收命令节点不是主节点，则命令失败。\nCLUSTER REPLICATE最早可用版本：3.0.0\n时间复杂度：O(1)\n该命令重新配置一个节点成为指定master的从节点。如果收到命令的节点是empty master，那么该节点的角色将由master转换为slave。\n一旦一个节点变成另一个master的slave，不需要将这一变化告知集群内的其他节点，心跳消息会把最新配置同步给其他节点。\n一个从节点接收这个命令需要满足以下条件：\n\n指定节点存在它的节点列表中\n指定节点对接收命令的节点未知\n指定节点是master\n\n如果收到命令的节点不是slave而是master，只有在如下情况下，命令才会执行成功：\n\n该节点不保存任何hash slot\n该节点是空的，key空间没有任何key\n\nCLUSTER RESET最早可用版本：3.0.0\n时间复杂度：O(N)，N是已知节点的数量\n根据reset类型（hard或soft）重置一个集群的节点。当主节点hold住一个或多个key时，这个命令无法执行，必须先使用FLUSHALL命令删除所有的key。该命令的影响是：\n\n集群中的节点都被忽略\n所有已分配的slot会被reset，slots-to-nodes关系被完全清除\n如果节点是slave，它会被切换成空master。\nHard模式：生成新的节点ID\nHard模式：currentEpoch和configEpoch被置为0\n新配置被持久化到节点磁盘上的集群配置信息文件中\n\nCLUSTER SAVECONFIG最早可用版本：3.0.0\n时间复杂度：O(1)\n强制保存配置nodes.conf到磁盘。该命令主要用于nodes.conf文件丢失或删除时重新生成文件。\nCLUSTER SET-CONFIG-EPOCH最早可用版本：3.0.0\n时间复杂度：O(1)\n该命令为一个全新的节点设置config epoch，只在以下情况有效：\n\n节点的节点信息表是空的\n节点的config epoch是0\n\n人工修改一个节点的config epoch是不安全的，但是当epoch产生冲突时，自动解决又非常慢，这时可以使用这个命令进行人工干预。\nCLUSTER SETSLOT最早可用版本：3.0.0\n时间复杂度：O(1)\n用法：CLUSTER SETSLOTslot IMPORTING|MIGRATING|STABLE|NODE [node-id]\nCLUSTER SETSLOT根据子命令修改节点的hash slot状态：\n\nMIGRATING：将一个hash slot设置为migrating状态\nIMPORTING：将一个hash slot设置为importing状态\nSTABLE：清除migrating或importing状态\nNODE：把hash slot绑定到其他节点\n\n该命令通常在rehash时使用，将源节点的hash slot置为migrating状态，目标节点的hash slot置为importing状态。\n\nCLUSTER SETSLOT  MIGRATING \n\n该命令将slot设置为migrating状态，接下来要处理的key如果存在，命令正常执行。如果不存在，则节点发出ASK重定向，让客户端去请求destination-node节点。对于批量key处理，如果只有部分节点存在，则返回TRYAGAIN错误。\n\nCLUSTER SETSLOT  IMPORTING \n\n这是MIGRATING的反操作，接下来涉及该slot的命令都被拒绝，并产生一个MOVED重定向，除非命令跟着一个ASK重定向。\nCLUSTER SLAVES最早可用版本：3.0.0\n时间复杂度：O(1)\n该命令会列出指定master节点的所有slave节点，格式和CLUSTER NODES相同。当指定节点未知或不是master时，命令返回一个错误。\nCLUSTER SLOTS最早可用版本：3.0.0\n时间复杂度：O(N)，N是slot的总数\n该命令返回slot和Redis实例的映射关系。这个命令对客户端很有用，在执行命令时，客户端会根据这个命令返回的信息去连接正确的节点执行命令。\n每个节点的信息结构如下：\n\n起始slot编号\n结束slot编号\nslot对应的master节点，用IP/Port表示\nmaster节点的第一个副本\n第二个副本\n\nREADONLY最早可用版本：3.0.0\n时间复杂度：O(1)\n开启与Redis Cluster从节点连接的读请求\n通常从节点将重定向客户端到认证过的主节点，以获取在指定命令中所涉及的slot，然而客户端可以通过READONLY命令将从节点设置为只读模式。\nREADONLY告诉Redis Cluster从节点愿意读取可能过时的数据。\n当连接处于只读模式，只有操作涉及到该从节点的主节点不服务的键时，集群将会发送一个重定向给客户端，这可能是因为：\n\n客户端发送一个有关这个从节点的主节点不服务hash slot的命令\n集群被重新配置并且从节点不在服务给定hash slot的命令\n\nREADWRITE最早可用版本：3.0.0\n时间复杂度：O(1)\n禁止与Redis Cluster从节点连接的读请求。\n默认情况下禁止Redis Cluster从节点的读请求，但是可以使用READONLY去在每一个连接的基础上改变这个行为，READWRITE命令将连接的只读模式重置为读写模式。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Geo","url":"/2019/09/06/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AGeo/","content":"Redis Geo相关命令。\nGEOADD最早可用版本：3.2.0\n时间复杂度：O(log(N))，N是Sorted set元素数量\n用法：GEOADDkey longitude latitude member [longitude latitude member …]\n将指定的地理空间位置（纬度、经度、名称）添加到指定key中。这些数据将存储到sorted set，这样为了方便使用GEORADIUS或GEORADIUSBYMEMBER命令。\n该命令采用标准格式参数x,y，所以经度必须在纬度之前。输入的坐标有如下限制：\n\n有效的经度从-180度到180度\n有效的纬度从-85.05112878度到85.05112878度\n\n当坐标位置超出上述指定范围时，该命令返回一个错误。\nGEODIST最早可用版本：3.2.0\n时间复杂度：O(log(N))\n用法：GEODIST key member1 member2 [unit]\n返回两个给定位置之间的距离。\n如果两个位置之间的其中一个不存在，那么命令返回空值。\n指定单位的参数unit必须是以下其中一个：\n\nm表示单位为米\nkm表示单位为千米\nmi表示单位为英里\nft表示单位为英尺\n\n如果用户没有显示指定单位参数，默认使用米作为单位。\nGEODIST命令在计算距离时会假设地球为完美球形，极限情况下，这一假设最大会造成0.5%的误差。\nGEOHASH最早可用版本：3.2.0\n时间复杂度：O(log(N))\n返回一个或多个元素位置的Geohash表示。\n用法：GEOHASH key member [member …]\n返回一个或多个位置元素的Geohash表示。\n通常，Redis使用Geohash技术的变体表示元素的位置，位置使用52位整数进行编码。由于编码和解码过程的初始最大和最小坐标不同，所以编码也不是标准的编码方式。\n该命令返回11个字符的Geohash字符串，和内部的52位表示方法相比没有精度的损失。返回的Geohash有以下属性：\n\n它可以移除右边的字符以缩短长度，这只会导致精度的损失，但仍指向同一区域\n它可以在heohash.org网站使用，地址是http://geohash.org/\n前缀相似的字符串指向的位置离得很近，但这不代表前缀不同的字符串就离得很远\n\nGEOPOS最早可用版本：3.2.0\n时间复杂度：O(log(N))\n用法：GEOPOS key member [member …]\n返回指定key中的指定位置信息。\nGEORADIUS最早可用版本：3.2.0\n时间复杂度：O(N+log(M))，N是半径区域内元素数量，M是指定key中元素数量\n用法：GEORADIUS key longitude latitude radiusm|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH][COUNT count] [ASC|DESC] [STORE key][STOREDIST key]\n以给定经纬度为中心，返回键包含的位置元素与中心距离不超过最大距离的所有位置元素。\n命令额外选项：\n\nWITHDIST：在返回位置元素的同时，将位置元素与中心的距离也一并返回，单位与用户给定距离的单位一直\nWITHCOORD：将位置元素的经度和纬度也一并返回\nWITHHASH：以52位有符号整数的形式，返回位置元素经过原始geohash编码的有序集合分值。这个选项主要用于底层应用或调试。\n\n命令默认返回结果未排序，可以指定ASC或DESC按距离排序。\nCOUNT表示指定返回元素的数量，如果不指定则返回全部符合的元素。\n当GEORADIUS和GEORADIUSBYMEMBER命令有了STORE和STOREDIST参数时，这两命令被标记成了写命令。在集群中，如果设置了READONLY，它们将被重定向到主节点，即使它们没有做写操作。但为了解决这个问题，在Redis4.0引入了这两个命令的变种，分别是GEORADIUS_RO和GEORADIUSBYMEMBER_RO。\nGEORADIUSBYMEMBER最早可用版本：3.2.0\n时间复杂度：O(N+log(M))，N是半径区域内元素数量，M是指定key中元素数量\n用法：GEORADIUSBYMEMBER key member radiusm|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH][COUNT count] [ASC|DESC] [STORE key][STOREDIST key]\n这个命令和GEORADIUS命令一样，都可以找出位置范围内的元素，但指定中心点的方式不同，该命令直接指定key中的元素作为中心，而不像GEORADIUS一样指定经纬度。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Connection","url":"/2018/09/19/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AConnection/","content":"最近在学习Redis的相关知识，上一篇我们也介绍了Redis的安装方法和基本数据结构，后面就打算开一个新的系列文章：Redis命令详解。既是对基础的巩固，也是为了以后查询起来更方便。\n整个系列会分为以下几个部分：\n\nConnection\nKeys\nStrings\nHashs\nLists\nSets\nSorted Sets\nHyperLogLog\nTransactions\nServer\nStreams\nPub/Sub\nCluster\nGeo\nScripting\n\n今天我们从Redis连接的相关命令开始。\nAUTH可用版本：1.0.0\nAUTH命令用于检测密码是否与配置文件中的密码是否一致，如果一致，则服务器会返回OK，并且继续接受后面的命令，否则，Redis会拒绝执行接下来的命令。\n127.0.0.1:6379&gt; config set requirepass &quot;mypass&quot;OK127.0.0.1:6379&gt; AUTH my(error) ERR invalid password127.0.0.1:6379&gt; ping(error) NOAUTH Authentication required.127.0.0.1:6379&gt; AUTH mypassOK127.0.0.1:6379&gt; pingPONG\n需要注意的是：由于Redis的读写性能非常高，所以可以在段时间内处理许多次AUTH操作，这样使得密码被暴力破解的可能性增加，所以我们在设置密码的时候需要尽量使密码安全性更强。\nECHO可用版本：1.0.0\nECHO命令打印字符串。\n127.0.0.1:6379&gt; ECHO &quot;Hello!&quot;&quot;Hello!&quot;\nPING可用版本：1.0.0\nPING命令用于检测服务器是否在运行，或者测试延迟。正常情况下，如果没有参数，则服务器会返回一个PONG，如果有参数的话，服务器会将参数复制一份，返回为字符串。\n127.0.0.1:6379&gt; PINGPONG127.0.0.1:6379&gt; PING &quot;hi&quot;&quot;hi&quot;\nQUIT可用版本：1.0.0\nQUIT命令用于关闭当前连接，当所有等待中的回复都写入客户端后，就会立即关闭当前连接。\nSELECT可用版本：1.0.0\nSELECT命令用于切换数据库，参数为数据库索引号。一个新连接的默认数据库索引号是0，所有的数据库都持久化到一个相同的RDB或AOF文件。不同的数据库可以有相同的key。\n127.0.0.1:6379&gt; SELECT 1OK127.0.0.1:6379[1]&gt;\n切换数据库后，提示符后面会出现数据库索引号。需要注意的是：当使用Redis Cluster时，不能使用SELECT命令。\nSWAPDB可用版本：4.0.0\nSWAPDB用于交换两个数据库，连接到这个数据库的其他客户端会立即看到另一个数据库的数据。\n#client 0127.0.0.1:6379&gt; set db db_0OK127.0.0.1:6379&gt; get db&quot;db_0&quot;127.0.0.1:6379&gt; SWAPDB 0 1OK127.0.0.1:6379&gt; get db(nil)#client 1127.0.0.1:6379&gt; SELECT 1OK127.0.0.1:6379[1]&gt; get db&quot;db_0&quot;\n","tags":["Redis命令"]},{"title":"Redis命令详解：Hashs","url":"/2018/11/22/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AHashs/","content":"Hash是一种String类型的field、value的映射表，因此，它非常适合存储对象。下面我们来一一介绍与Hash相关的命令。\nHDEL最早可用版本：2.0.0\n时间复杂度：O(N)，其中N为要删除的field的个数\nHDEL命令用于删除指定key的指定的一个或多个field。如果指定的field不存在于指定的key中则会被忽略，如果指定的key不存在，会当做空的hash进行处理，向客户端返回0。\n命令的返回值是实际删除的field的个数，不包括不存在的field。\n从2.4.0版本开始，该命令支持一次删除多个field。在此之前，如果想一次性删除多个field，只能利用Redis的事务来实现。\nHEXISTS最早可用版本：2.0.0\n时间复杂度：O(1)\nHEXISTS命令用来验证指定的key是否包含指定的field，如果包含，返回1；如果不包含或者key不存在，返回0。\nHGET最早可用版本：2.0.0\n时间复杂度：O(1)\n返回指定的key中指定的field对应的value。如果field不在key中或者key不存在，则返回nil。\nHGETALL最早可用版本：2.0.0\n时间复杂度：O(N)，N为hash的大小，即key中field的个数。\n返回key所存储的所有field以及field对应的value。每个value跟在field的后面被返回，因此，返回值的长度是hash的size的2倍。如果key不存在，则返回空列表。\n127.0.0.1:6379&gt; HGETALL noexist(empty list or set)127.0.0.1:6379&gt; HSET mykey field1 &quot;follow&quot;(integer) 1127.0.0.1:6379&gt; HSET mykey field2 &quot;Jackeyzhe2018&quot;(integer) 1127.0.0.1:6379&gt; HGETALL mykey1) &quot;field1&quot;2) &quot;follow&quot;3) &quot;field2&quot;4) &quot;Jackeyzhe2018&quot;\nHINCRBY最早可用版本：2.0.0\n时间复杂度：O(1)\n用法：\nHINCRBY key field increment\n用来对指定key的指定field进行增量操作，返回计算后的结果。如果key不存在，或者key中不包含指定的field，则会先创建一个value为0的hash，如果value不是数字类型，则会报错。该命令支持的数字范围是64位有符号整数。\n127.0.0.1:6379&gt; keys * #演示使用，生产环境不要用1) &quot;mykey&quot;127.0.0.1:6379&gt; HINCRBY myhash field1 1(integer) 1127.0.0.1:6379&gt; HGET myhash field1&quot;1&quot;127.0.0.1:6379&gt; HSET myhash fieldStr &quot;follow&quot;(integer) 1127.0.0.1:6379&gt; HINCRBY myhash fieldStr 1(error) ERR hash value is not an integer127.0.0.1:6379&gt; HGETALL myhash1) &quot;field1&quot;2) &quot;1&quot;3) &quot;fieldStr&quot;4) &quot;follow&quot;127.0.0.1:6379&gt; HINCRBY myhash field2 2(integer) 2127.0.0.1:6379&gt; HGETALL myhash1) &quot;field1&quot;2) &quot;1&quot;3) &quot;fieldStr&quot;4) &quot;follow&quot;5) &quot;field2&quot;6) &quot;2&quot;\nHINCRBYFLOAT最早可用版本：2.6.0\n时间复杂度：O(1)\n用来对指定的key中指定的field进行浮点类型的加法，如果field不存在，则会先创建一个value为0的field。如果value或者increments不能解析为float类型，则会报错。通过下面的例子可以看到，浮点数的加法会存在一些偏差。\n127.0.0.1:6379&gt; HINCRBYFLOAT myhash field3 0.3&quot;0.3&quot;127.0.0.1:6379&gt; HINCRBYFLOAT myhash field3 1.0e3&quot;1000.29999999999999999&quot;127.0.0.1:6379&gt; HINCRBYFLOAT myhash field3 -1.0e3&quot;0.29999999999999999&quot;127.0.0.1:6379&gt; HINCRBYFLOAT myhash fieldStr 0.1(error) ERR hash value is not a float127.0.0.1:6379&gt; HINCRBYFLOAT myhash field3 &quot;haha&quot;(error) ERR value is not a valid float\nHKEYS最早可用版本：2.0.0\n时间复杂度：O(N)，其中N为指定key中field的个数\nHKEYS命令用于返回指定key中所包含的field列表，如果key不存在，则返回空列表。\nHLEN最早可用版本：2.0.0\n时间复杂度：O(1)\n返回指定的key所包含的field的个数。如果key不存在，则返回0。\nHMGET最早可用版本：2.0.0\n时间复杂度：O(N)：N是请求的field的个数\n返回指定key中指定的一个或多个field的值。如果field不存在，则返回nil，如果key不存在，同样会返回field数量的nil。因为不存在的key被作为空的hash处理。\n127.0.0.1:6379&gt; HMGET myhash field1 field2 no-exist1) &quot;1&quot;2) &quot;2&quot;3) (nil)127.0.0.1:6379&gt; HMGET no-exist field1 field21) (nil)2) (nil)\nHMSET最早可用版本：2.0.0\n时间复杂度：O(N)：N是需要设置的field的个数\n为指定的key设置一个或多个field。如果field已经存在，则会被覆盖。如果指定的key不存在，则会创建一个新的hash。\nHSCAN最早可用版本：2.8.0\n时间复杂度：每次请求的时间复杂度为O(1)，完成整个迭代的时间复杂度为O(N)\n该命令与SCAN命令相似，可以参考我的另外一篇文章Redis命令详解：Keys中对SCAN用法的介绍，如果你想要有更深入了了解，可以看我的另外一篇文章深入理解Redis的scan命令。\nHSET最早可用版本：2.0.0\n时间复杂度：O(1)\n为指定的key中的field设置value，如果key不存在，则会创建一个新的hash，如果field已经存在，则会覆盖旧值。如果是新增的field，设置完成后会返回1，如果是更新已有的field，设置完成后会返回0。\nHSETNX最早可用版本：2.0.0\n时间复杂度：O(1)\n同样是为指定的key中的field设置value，与HSET命令不同的是，如果field已经存在，则不会有任何操作，直接返回0。\nHSTRLEN最早可用版本：3.2.0\n时间复杂度：O(1)\n返回指定key中field对应value的字符串长度，如果key或field不存在，返回0。\nHVALS最早可用版本：2.0.0\n时间复杂度：O(N)，N为hash的size\n返回指定key的hash的所有value。如果key不存在，则会返回空列表。\n","tags":["Redis命令"]},{"title":"Redis命令详解：HyperLogLog","url":"/2019/01/15/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AHyperLogLog/","content":"HyperLogLog是Redis的高级数据结构，它在做基数统计的时候非常有用，每个HyperLogLog的键可以计算接近264不同元素的基数，而大小只需要12KB。\nHyperLogLog目前只支持3个命令，PFADD、PFCOUNT、PFMERGE。我们先来逐一介绍一下。\nPFADD最早可用版本：2.8.9\n时间复杂度：O(1)\n将参数中的元素都加入指定的HyperLogLog数据结构中，这个命令会影响基数的计算。如果执行命令之后，基数估计改变了，就返回1；否则返回0。如果指定的key不存在，那么就创建一个空的HyperLogLog数据结构。该命令也支持不指定元素而只指定键值，如果不存在，则会创建一个新的HyperLogLog数据结构，并且返回1；否则返回0。\nPFCOUNT最早可用版本：2.8.9\n时间复杂度：O(1)，对于多个比较大的key的时间复杂度是O(N)\n对于单个key，该命令返回的是指定key的近似基数，如果变量不存在，则返回0。\n对于多个key，返回的是多个HyperLogLog并集的近似基数，它是通过将多个HyperLogLog合并为一个临时的HyperLogLog，然后计算出来的。\nHyperLogLog可以用很少的内存来存储集合的唯一元素。（每个HyperLogLog只有12K加上key本身的几个字节）\nHyperLogLog的结果并不精准，错误率大概在0.81%。\n需要注意的是：该命令会改变HyperLogLog，因此使用8个字节来存储上一次计算的基数。所以，从技术角度来讲，PFCOUNT是一个写命令。\n性能问题即使理论上处理一个存储密度大的HyperLogLog需要花费较长时间，但是当指定一个key时，PFCOUNT命令仍然具有很高的性能。这是因为PFCOUNT会缓存上一次结算的基数，而多数PFADD命令不会更新寄存器。所以才可以达到每秒上百次请求的效果。\n当处理多个key时，最耗时的一步是合并操作。而通过计算出来的并集的基数是不能缓存的。所以多个key的处理速度一般在毫秒级。\nPFMERGE最早可用版本：2.8.9\n时间复杂度：O(N)，N是要合并的HyperLogLog的数量\n用法：PFMERGE destkey sourcekey [sourcekey …]\n合并多个HyperLogLog，合并后的基数近似于合并前的基数的并集（observed Sets）。计算完之后，将结果保存到指定的key。\n除了这三个命令，我们还可以像操作String类型的数据那样，对HyperLogLog数据使用SET和GET命令。关于HyperLogLog的原理以及其他细节，我将在后面介绍，敬请期待。\n","tags":["Redis"]},{"title":"Redis命令详解：Keys","url":"/2018/09/22/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AKeys/","content":"介绍完Redis连接相关命令后，再来介绍一下与Key相关的命令，Redis作为一个key-value数据库，对Key进行操作是无法避免的。\nDEL最早可用版本1.0.0\n删除指定的键值对，如果指定的key不存在，则忽略。DEL命令的时间复杂度是O(N)，对于除字符串外的其他数据类型，命令的时间复杂度为O(M)，M是值的元素的个数。所以，在生产环境尽量避免一次性删除过多复杂数据类型的操作。\n127.0.0.1:6379&gt; SET key1 &quot;jackey&quot;OK127.0.0.1:6379&gt; SET key2 &quot;zhe&quot;OK127.0.0.1:6379&gt; DEL key1 key2 key3(integer) 2\nDUMP最早可用版本2.6.0\n使用一种Redis的格式序列化指定键存储的值。可用使用RESTORE命令将这个值反序列化。\n这种序列化格式有以下3个特点：\n\n它包含有64位的校验和，用于错误检查，RESTORE命令在反序列化之前会先检查校验和\n值的编码格式和RDB文件的编码格式相同\nRDB的版本会被序列化到值中，因此，不同版本的Redis可能会因为不兼容RDB版本而拒绝反序列化\n\n序列化的值不包含过期时间的相关信息，可以使用PTTL命令获取当前值的存活时间。如果值不存在则会返回nil\n127.0.0.1:6379&gt; SET key1 &quot;jackey&quot;OK127.0.0.1:6379&gt; DUMP key1&quot;\\x00\\x06jackey\\b\\x00\\xec\\x89&#x27;G&#x27;X\\xfc:&quot;127.0.0.1:6379&gt; DUMP not-exist-key(nil)\nDUMP时间复杂度分为两部分：访问key值的时间复杂度为O(1)，而序列化值的时间复杂度为O(N*M)，N是组成值的元素的数量，M是元素的平均大小。如果序列化比较短的字符串，则该命令的时间复杂度可以看做O(1)。\nEXISTS最早可用版本1.0.0\n用于判断key是否存在。3.0.3版本以后支持多参数，即可以一次性判断多个key，返回值是存在的key的数量。对于判断单个key是否存在，会返回1或者0，因此，该命令是向后兼容的。\n需要注意的是：如果参数中有重复的存在命令，则返回结果不会去重。\n127.0.0.1:6379&gt; SET key1 &quot;jackey&quot;OK127.0.0.1:6379&gt; SET key2 &quot;zhe&quot;OK127.0.0.1:6379&gt; EXISTS key1(integer) 1127.0.0.1:6379&gt; EXISTS not-exist-key(integer) 0127.0.0.1:6379&gt; EXISTS key1 key2 not-exist-key(integer) 2127.0.0.1:6379&gt; EXISTS key1 key1 key1(integer) 3\nEXPIRE最早可用版本1.0.0\n为指定的key设置存活时间。存活时间会被DEL，SET，GETSET和所有的STORE命令删除或者覆盖。如果我们只修改key的值而不修改存活时间或者保存到一个新的key中，则原来的key的存活时间保持不变。如果使用RENAME对一个key重命名，那么原有key的存活时间会赋给新的key。\n如果想要清除存活时间，使指定的key成为一个永久的key，则可以使用PERSIST命令，我们稍后会详细介绍这个命令。\n如果使用EXPIRE/PEXPIRE为某个key设置的存活时间为非正数，或者使用EXPIREAT/PEXPIREAT设置了一个过去的时间，则这个key会直接被删除。\n127.0.0.1:6379&gt; EXPIRE key1 -1(integer) 1127.0.0.1:6379&gt; EXISTS key1(integer) 0\n对一个已经有存活时间的key再次使用EXPIRE设置存活时间，则将key的存活时间更新，在许多应用中我们都会用到这一点。\n注意：在Redis的2.1.3版本之前，如果修改一个带有存活时间的key的值，则会删除整个key。\n关于时间精度，Redis2.4版本中，一个key过期的一秒内仍可以访问，而到了2.6版本，这一时间已经被精确到了1毫秒。因为从2.6版本开始，存活时间保存的是绝对时间（Unix的时间戳），而这就意味着，你的计算机的时间需要保证可靠，如果你将RDB文件放到另一台机器上加载，当这两台机器的时间差距较大时，你就会发现可能有些key被删除了或者有些key的存活时间被延长了。\n下面我们在来讨论一下Redis究竟是如何使key过期的，Redis的过期策略有两种：一种是被动的，一种是主动的。\n被动过期就是当客户端访问某个key，服务端会去检查这个key的存活时间，判断是否过期。当然，这种过期策略存在一定的问题，如果某个key一直都不访问，就不会被发现它过期了，那么它将永远“苟活”在内存中。所以Redis会定期随机的查看被设置过存活时间的key，看它们是否过期，如果过期了，就会及时清理掉。Redis每秒会做10次下面的操作：\n\n随机查看20个设置过存活时间的key（从设置存活时间的set中取）\n删除所有过期的key\n如果过期的key超过25%，那么会从第一步开始再执行一次\n\nEXPIREAT最早可用版本1.2.0\n此命令和EXPIRE的作用相同，不同之处是它的参数需要传Unix时间戳（即从1970年1月1日起的毫秒数）。\n127.0.0.1:6379&gt; GET key2&quot;zhe&quot;127.0.0.1:6379&gt; EXPIREAT key2 1537733374(integer) 1127.0.0.1:6379&gt; TTL key2(integer) 12960\nKEYS最早可用版本1.0.0\n这个命令会返回匹配到的所有key，时间复杂度为O(N)。在官方文档中说，在入门级的笔记本电脑上，Redis扫描100万条key只需要40毫秒，但是我们仍然要避免在生产环境使用这个命令。特别是千万不要使用KEYS *这样的命令，因为你不知道生产环境存在多少key，这样的命令有可能使你的生产环境的Redis陷入很长一段时间的不可用状态。所以，请马上删除应用层代码中的KEYS命令或者抓紧时间更新自己的简历。\n如果需要查找key，可以使用SCAN命令或者sets命令。\n虽然我们非常不建议使用KEYS命令，但是它的匹配策略还是要介绍一下的：\n？是单个字符的通配符，*是任意个数的通配符，[ae]会匹配到a或e，^e表示不匹配e，a-c表示匹配a或b或c，特殊符号使用\\隔开。\n127.0.0.1:6379&gt; MSET key1hello jackey key2hello zhe age 3OK127.0.0.1:6379&gt; KEYS key?hello1) &quot;key1hello&quot;2) &quot;key2hello&quot;127.0.0.1:6379&gt; KEYS k*1) &quot;key1hello&quot;2) &quot;key2hello&quot;127.0.0.1:6379&gt; KEYS *age*1) &quot;age&quot;127.0.0.1:6379&gt; KEYS *1) &quot;age&quot;2) &quot;key1hello&quot;3) &quot;key2hello&quot;\nMIGRATE最早可用版本2.6.0\n这个命令用来将源实例的key以原子操作传输到目标实例，然后将源实例的key删除。相当于在源实例执行了DUMP+DEL操作，在目标实例执行了RESTORE操作。这一操作会阻塞进行传输的两个实例，在传输过程中，key总会存在于一个实例中，除非发生超时错误。在3.2版本以后，MIGRATE可以将多个key作为管线一次性传输。\n在执行MIGRATE命令时，必须要设置一个超时时间，如果到了超时时间命令仍未执行完，则会抛出一个IOERR。但返回这个错误时，两个实例的状态可能有两种：要么两个实例都存在指定的key，要么只有源实例存在指定的key。总之，key是不会丢失的。\n从3.0.6版本开始，MIGRATE支持一次传输多个key，为了保证不过载或者出现环形操作，MIGRATE需要使用KEYS参数，而原来指定的key的参数要被设置为空字符串。\nMIGRATE 192.168.1.34 6379 &quot;&quot; 0 5000 KEYS key1 key2 key3\n这里还有两个选填参数需要介绍：一个是COPY，加上这个参数的话，传输完成后不会删除源实例中的key。另一个是REPLACE，这个参数的作用是替换目标实例已存在的key。这两个参数在3.0版本以后才可以使用。\nMOVE最早可用版本1.0.0\n不知道大家还记不记得前文中我们提到过的SELECT命令，SELECT用来切换数据库。使用MOVE命令就是将当前数据库的key移动到指定的数据库中，如果指定库中已经存在这个key或者当前库不存在这个key，那么这个命令什么也不做。\n127.0.0.1:6379&gt; KEYS *1) &quot;age&quot;2) &quot;key1hello&quot;3) &quot;key2hello&quot;127.0.0.1:6379&gt; MOVE age 1(integer) 1127.0.0.1:6379&gt; KEYS *1) &quot;key1hello&quot;2) &quot;key2hello&quot;127.0.0.1:6379&gt; SELECT 1OK127.0.0.1:6379[1]&gt; KEYS *1) &quot;age&quot;\nOBJECT最早可用版本2.2.3\nOBJECT用来查看Redis对象内部的相关信息。这一命令在调试时经常被使用。下面我们来介绍OBJECT命令的具体用法：\n\nOBJECT REFCOUNT key：返回指定key的值的引用数量\nOBJECT ENCODING key：返回指定key的内部存储使用的编码格式\nOBJECT IDLETIME key：返回指定key的空闲时间（有多长时间没有被读写），目前最小精度为10秒，这一命令经常在Redis淘汰机制中使用（淘汰策略为LRU或noeviction）\nOBJECT FREQ key：返回指定key访问频率的对数，当淘汰策略为LFU时，这一命令会被用到\nOBJECT HELP：返回帮助信息\n\n对象的编码格式也有很多种：\n\nStrings会被编码为raw或int\nLists会被编码为ziplist或linkedlist\nSets会被编码为intset或hashtable\nHashs会被编码为ziplist或hashtable\nSorted Sets会被编码为ziplist或skiplist\n\n127.0.0.1:6379&gt; OBJECT REFCOUNT key1hello(integer) 1127.0.0.1:6379&gt; OBJECT IDLETIME key2hello(integer) 3637127.0.0.1:6379&gt; OBJECT ENCODING age&quot;int&quot;\nPERSIST最早可用版本2.2.0\n删除指定key的过期时间，使之变成永久的key。\nPEXPIRE最早可用版本2.6.0\nPEXPIRE的作用和EXPIRE一样，只不过参数中的时间单位是毫秒。\nPEXPIREAT最早可用版本2.6.0\n作用和EXPIREAT相同，参数同样是毫秒。\nPTTL最早可用版本2.6.0\n返回指定key的剩余存活时间的毫秒数。2.8以后的版本返回值有些变化，如果key不存在，则返回-2；如果key是永久的，则返回-1。\nRANDOMKEY最早可用版本1.0.0\n此命令用于从当前数据库返回一个随机的key。\nRENAME最早可用版本1.0.0\n重命名一个key。如果key不存在，则会返回错误。而如果新的key已经存在，则此命令会覆盖原来的key（它其实是执行了一个隐式的DEL命令，因此如果原来的key存储的对象很大的话， 删除操作延时会很高）。在3.2版本以前，如果源key和目标key相同的话，会报错。\nRENAMENX如果新的key不存在的话，重命名key，如果存在的话返回0，成功返回1。\nRESTORE最早可用版本2.6.0\n用法：RESTORE key ttl serialized-value [REPLACE]\n此命令是将一组数据反序列化，并存到key。如果ttl是0，则key是永久的。在Redis3.0版本以后，如果不使用REPLACE参数并且key已经存在，则会返回一个错误“Target key name is busy”。\nSCAN最早可用版本2.8.0\n用法：SCAN cursor MATCH pattern COUNT count\n其中cursor为游标，MATCH和COUNT为可选参数。\nSCAN命令和SSCAN、HSCAN、ZSCAN命令都用于增量的迭代元素集，它每次返回小部分数据，不会像KEYS那样阻塞Redis。SCAN命令是基于游标的，每次调用后，都会返回一个游标，用于下一次迭代。当游标返回0时，表示迭代结束。\nSCAN每次返回的数量并不固定，也有可能返回数据为空。另外，SCAN命令和KEYS命令一样支持匹配。\n我们在Redis里存入10000个key用于测试。\n结果如下：\n127.0.0.1:6379&gt; scan 0 match key24* count 10001) &quot;1688&quot;2) 1) &quot;key2411&quot;   2) &quot;key2475&quot;   3) &quot;key2494&quot;   4) &quot;key2406&quot;   5) &quot;key2478&quot;127.0.0.1:6379&gt; scan 1688 match key24* count 10001) &quot;2444&quot;2)  1) &quot;key2458&quot;    2) &quot;key249&quot;    3) &quot;key2407&quot;    4) &quot;key2434&quot;    5) &quot;key241&quot;    6) &quot;key2497&quot;    7) &quot;key2435&quot;    8) &quot;key2413&quot;    9) &quot;key2421&quot;   10) &quot;key248&quot;127.0.0.1:6379&gt; scan 2444 match key24* count 10001) &quot;818&quot;2)  1) &quot;key2459&quot;    2) &quot;key2462&quot;    3) &quot;key2409&quot;    4) &quot;key2454&quot;    5) &quot;key2431&quot;    6) &quot;key2423&quot;    7) &quot;key2476&quot;    8) &quot;key2428&quot;    9) &quot;key2493&quot;   10) &quot;key2420&quot;127.0.0.1:6379&gt; scan 818 match key24* count 10001) &quot;9190&quot;2)  1) &quot;key2402&quot;    2) &quot;key2415&quot;    3) &quot;key2429&quot;    4) &quot;key2424&quot;    5) &quot;key2425&quot;    6) &quot;key2400&quot;    7) &quot;key2472&quot;    8) &quot;key2479&quot;    9) &quot;key2448&quot;   10) &quot;key245&quot;   11) &quot;key2487&quot;   12) &quot;key2430&quot;   13) &quot;key2405&quot;127.0.0.1:6379&gt; scan 9190 match key24* count 10001) &quot;12161&quot;2)  1) &quot;key2488&quot;    2) &quot;key2437&quot;    3) &quot;key2404&quot;    4) &quot;key2440&quot;    5) &quot;key2461&quot;    6) &quot;key2416&quot;    7) &quot;key2436&quot;    8) &quot;key2403&quot;    9) &quot;key2460&quot;   10) &quot;key2452&quot;   11) &quot;key2449&quot;   12) &quot;key2482&quot;127.0.0.1:6379&gt; scan 12161 match key24* count 10001) &quot;11993&quot;2)  1) &quot;key2483&quot;    2) &quot;key2491&quot;    3) &quot;key242&quot;    4) &quot;key2466&quot;    5) &quot;key2446&quot;    6) &quot;key2465&quot;    7) &quot;key243&quot;    8) &quot;key2438&quot;    9) &quot;key2457&quot;   10) &quot;key246&quot;   11) &quot;key2422&quot;   12) &quot;key2418&quot;127.0.0.1:6379&gt; scan 11993 match key24* count 10001) &quot;7853&quot;2) 1) &quot;key2498&quot;   2) &quot;key2451&quot;   3) &quot;key2439&quot;   4) &quot;key2495&quot;   5) &quot;key2408&quot;   6) &quot;key2410&quot;127.0.0.1:6379&gt; scan 7853 match key24* count 10001) &quot;5875&quot;2)  1) &quot;key2486&quot;    2) &quot;key2490&quot;    3) &quot;key244&quot;    4) &quot;key2401&quot;    5) &quot;key2463&quot;    6) &quot;key2481&quot;    7) &quot;key2477&quot;    8) &quot;key2468&quot;    9) &quot;key2433&quot;   10) &quot;key2489&quot;   11) &quot;key2455&quot;   12) &quot;key2426&quot;   13) &quot;key24&quot;   14) &quot;key2450&quot;   15) &quot;key2414&quot;   16) &quot;key2442&quot;   17) &quot;key2473&quot;   18) &quot;key2467&quot;   19) &quot;key2469&quot;   20) &quot;key2456&quot;127.0.0.1:6379&gt; scan 5875 match key24* count 10001) &quot;14311&quot;2)  1) &quot;key2453&quot;    2) &quot;key2492&quot;    3) &quot;key2480&quot;    4) &quot;key2427&quot;    5) &quot;key2443&quot;    6) &quot;key2417&quot;    7) &quot;key2432&quot;    8) &quot;key240&quot;    9) &quot;key2445&quot;   10) &quot;key2484&quot;   11) &quot;key2444&quot;   12) &quot;key247&quot;   13) &quot;key2485&quot;127.0.0.1:6379&gt; scan 14311 match key24* count 10001) &quot;16383&quot;2)  1) &quot;key2441&quot;    2) &quot;key2474&quot;    3) &quot;key2447&quot;    4) &quot;key2471&quot;    5) &quot;key2470&quot;    6) &quot;key2464&quot;    7) &quot;key2412&quot;    8) &quot;key2419&quot;    9) &quot;key2499&quot;   10) &quot;key2496&quot;127.0.0.1:6379&gt; scan 16383 match key24* count 10001) &quot;0&quot;2) (empty list or set)\n可以看到虽然我们设置的count为1000，但Redis每次返回的数值只有10个左右。\nSORT最早可用版本1.0.0\n当有N个元素需要排序，并且要返回M个元素时，SORT命令的时间复杂度为O(N+M*log(M))\n此命令用于返回或保存list，set和sorted set的键，默认将数字或者可排序的key进行排序，Redis会将其视为双精度浮点数。\n如果想要对字符串按字典顺序排序，可以使用ALPHA参数。\n如果想要按照外部字段进行排序，可以使用BY参数。\nTOUCH最早可用版本3.2.1\n修改某一个或多个key的最后访问时间，如果key不存在，则忽略。\nTTL最早可用版本1.0.0\n返回指定key的剩余存活时间，单位为秒。\n在2.6版本及以前，如果key不存在或者是永久key，都会返回-1。从2.8版本开始，如果key不存在，则返回-2，如果key为永久key，则返回-1。\nTYPE最早可用版本1.0.0\n返回key存储的值的类型。类型即为我们在Redis基础数据结构一文中描述的5中数据类型。\nUNLINK最早可用版本4.0.0\n这个命令和DEL类似，会删除指定的key。所不同的是，此命令的时间复杂度为O(1)，它先将key从keyspace中删除，此时指定的key已经删除，但是内存没有释放。所以，这个命令会在另一个线程中做释放内存的操作。这一步的操作时间复杂度为O(N)。\nWAIT最早可用版本3.0.0\n这个命令会阻塞客户端，直到前面所有的写操作都完成并且保存了指定数量的副本。该命令总会返回副本数量或者超时。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Lists","url":"/2018/11/23/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9ALists/","content":"List是Redis的基础数据类型之一，类似于Java中的LinkedList。一个列表最多包含232个元素，常被用作模拟队列操作，接下来我们具体介绍一下List相关的命令。\nBLPOP最早可用版本：2.0.0\n时间复杂度：O(1)\n用法：\nBLPOP key [key ...] timeout\nBLPOP是LPOP的阻塞版本，当列表没有元素可以被弹出时，连接将被阻塞。当给定多个key，会按参数key的顺序检查各个列表，弹出第一个非空列表的的头元素。timeout表示阻塞的最大秒数，timeout为0表示无限阻塞。\n这里有一个问题，当多个元素同时push进一个list时，阻塞的BLPOP命令会有什么操作。在说明之前，我们先思考一下如何操作才会出现这样的情况：\n\n对list执行LPUSH mylist a b c这样的命令\n对同一个list进行多次push操作，这些操作是在事务中执行的\n使用Redis2.6以后的版本执行Lua脚本进行push操作\n\n对于这个问题，Redis2.4版本和Redis2.6以后的版本处理方法有所不同。\n假如客户端A执行命令\nBLPOP mylist 0\n这时mylist为空，客户端A会被阻塞，此时客户端B执行了命令\nLPUSH mylist a b c\n如果在Redis2.6版本之后，客户端A会返回c，因为在客户端Bpush了元素a、b、c后，其从左到右的顺序是c、b、a，但是在Redis2.4版本中，客户端会在push操作的上下文，所以当LPUSH开始往list里push第一个元素时，它就被传送到客户端A，也就是客户端A会接收到a。\n有时，我们会有这样的需求：我们需要为了等待Set的新元素而阻塞队列，这样就需要一个阻塞版的SPOP，可惜目前还没有支持这样的命令。不过我们可以使用BLPOP命令来实现，下面是实现的伪代码：\n消费者：\nLOOP forever    WHILE SPOP(key) returns elements        ... process elements ...    END    BRPOP helper_keyEND\n生产者：\nMULTISADD key elementLPUSH helper_key xEXEC\nBRPOP最早可用版本：2.0.0\n时间复杂度：O(1)\n它与BLPOP基本相同，不同的地方在于它是从尾部弹出元素，而BLPOP是从头部弹出元素。\nBRPOPLPUSH最早可用版本：2.2.0\n时间复杂度：O(1)\n用法：\nBRPOPLPUSH source destination timeout\n它是RPOPLPUSH的阻塞版本，当source包含元素时，它与RPOPLPUSH表现的一样，当source为空时，Redis会被阻塞，直到另一个客户端push元素，或者达到timeout时间限制。\nLINDEX最早可用版本：1.0.0\n时间复杂度：O(N)，N是找到目标元素所跨越元素的个数，当目标元素为第一个或者最后一个时，时间复杂度为O(1)。\n该命令用于返回列表中指定位置的元素，index是从0开始的，-1表示倒数第一个元素，-2表示倒数第二个元素，以此类推。当key不是一个list时，会返回一个错误。当index超出范围时返回nil。\nLINSERT最早可用版本：2.2.0\n时间复杂度：O(N)，N为在找到基准value前所跨越的元素个数。也就是说，如果插入到头部，时间复杂度为O(1)，如果插入到尾部，时间复杂度为O(N)。\n用法：\nLINSERT key BEFORE|AFTER pivot value\n该命令把value插入到基准值pivot的前面或者后面，如果key不存在，list被当做空列表，不会发生任何操作。如果key存储的不是list，则会报错。命令的返回值是，插入操作后，list的长度，如果找不到基准值pivot，则会返回-1。\nLLEN最早可用版本：1.0.0\n时间复杂度：O(1)\n返回指定key的list的长度，如果key不存在，则被看作是空列表，返回0。如果key存储的不是list，则会报错。\nLPOP最早可用版本：1.0.0\n时间复杂度：O(1)\n该命令用于删除并返回list的第一个元素。当key不存在时，返回nil。\nLPUSH最早可用版本：1.0.0\n时间复杂度：O(1)\n将所有指定的value插入列表的头部，如果key不存在，就先创建一个空列表并进行插入操作，如果key存储的不是list，则会返回一个错误。我们可以一次插入多个元素，他们从左到右依次被插入到list中，因此，\nLPUSH mylist a b c\n 命令生成的列表，c是第一个元素，a是第三个元素。该命令的返回值是插入操作后列表的长度。需要注意的一点是：在Redis2.4版本以前（不包括2.4）是不支持一次插入多个元素的。\nLPUSHX最早可用版本：2.2.0\n时间复杂度：O(1)\n当key存在时，在头部插入指定元素，key不存在时，不进行插入操作。\nLRANGE最早可用版本：1.0.0\n时间复杂度：O(S+N)，S是start元素的偏移量，N是指定范围元素的个数\n用法：\nLRANGE key start stop\n返回指定key的指定范围的元素，start和stop都是下标（从0开始），同样，下标可以是负数，-1表示倒数第一个，-2表示倒数第二个。命令返回的结果会包含下标为stop的元素。如果start超出list的长度返回，则会返回一个空的列表，如果stop超出list的长度返回，则会返回到最后一个元素。\nLREM最早可用版本：1.0.0\n时间复杂度：O(1)\n用法：\nLREM key count value\n移除list中前count次出现的value\n\ncount&gt;0时：从头到尾匹配value\ncount=0时：移除全部匹配到value的元素\ncount&lt;0时，从尾部到头部匹配value\n\n当key不存在时，被当做空列表看待，直接返回0。\nLSET最早可用版本：1.0.0\n时间复杂度：O(N)，N为list的长度\n设置指定下标的value，如果下标超出范围，则会返回一个错误。\nLTRIM最早可用版本：1.0.0\n时间复杂度：O(N)，N删除掉的元素的个数\n该命令用来修剪一个已经存在的list，修剪后的list只包含指定范围的元素。start和stop都是从0开始的索引，例如，\nLTRIM foobar 0 2\n就是只保留foobar的前3个元素。start和stop也可以是负数，-1表示倒数第一个元素，-2表示倒数第二个，以此类推。如果下标超出范围，并不会报错，而是进行如下处理：如果start比list的最后一个元素的下标大，或者start&gt;end，结果就是空list，如果end大于最大下标，Redis会将其当成最后一个元素来处理。\nRPOP最早可用版本：1.0.0\n时间复杂度：O(1)\n删除并返回list的最后一个元素。当key不存在时，返回nil。\nRPOPLPUSH最早可用版本：1.2.0\n时间复杂度：O(1)\n原子性的返回并删除source的最后一个元素，并把该元素存储到destination的第一个元素的位置。举个栗子，source保存了元素a、b、c，destination保存了x、y、z，执行了\nRPOPLPUSH source destination\n后，source保存的会是a、b，而destination保存的则是c、x、y、z。该命令的返回值是那个从source被移出和存入destination的元素。\nRPUSH最早可用版本：1.0.0\n时间复杂度：O(1)\n将指定元素插入到指定key的尾部。如果key不存在，就创建一个空的列表。如果key保存的不是list，则会返回一个错误。在2.4版本之后，可以使用一条命令一次插入多个值，插入的顺序是从左到右。\nRPUSHX最早可用版本：2.2.0\n时间复杂度：O(1)\n它和RPUSH唯一不同的一点就是如果key不存在，就不会进行任何操作。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Pub/Sub","url":"/2019/08/28/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9APub-Sub/","content":"Redis发布订阅模式相关命令。\nPSUBSCRIBE最早可用版本：2.0.0\n时间复杂度：O(N)，N是已订阅的客户端数。\n订阅给定规则的客户端，支持的形式包括：\n\nh?llo 订阅hello,hallo和hxllo等\nh*llo 订阅hllo和heeeello等\nh[ae] 订阅hello和hallo，但不订阅hillo\n\n如果要逐字匹配，要使用\\来转义特殊字符。\nPUBLISH最早可用版本：2.0.0\n时间复杂度：O(N+M)，N是已订阅的客户端数，M是订阅总数\n发布消息到指定频道。\nPUBSUB最早可用版本：2.8.0\n时间复杂度：O(N)，N是活跃的频道数\n该命令用于检查Pub/Sub子系统的状态。\nPUBSUB CHANNELS [pattern]\n列出当前活跃的频道（至少有一个订阅者）。不过不指定pattern，则列出全部频道。\nPUBSUB NUMSUB [channel-1 ... channel-N]\n返回指定频道的订阅者。\nPUBSUB NUMPAT\n返回指定模式的订阅数（使用PSUBSCRIBE命令执行）\nPUNSUBSCRIBE最早可用版本：2.0.0\n时间复杂度：O(N+M)，N是匹配规则的客户端已经订阅的数量，M是系统中匹配规则的订阅总数 \n用法：PUNSUBSCRIBE [pattern [pattern …]]\n退订所有匹配规则的频道，如果没有指定规则，则退订所有的频道。\nSUBSCRIBE最早可用版本：2.0.0\n时间复杂度：O(N)，N是订阅频道的数量\n给客户端订阅指定的频道。\nUNSUBSCRIBE最早可用版本：2.0.0\n时间复杂度：O(N)，N是订阅频道的数量\n给客户端退订指定的频道。如果不指定频道，则退订全部。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Server","url":"/2019/07/01/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AServer/","content":"Redis命令学习，服务器篇\nBGREWRITEAOF最早可用版本：1.0.0\n使Redis重写AOF文件，重写后的AOF文件相较于当前版本的AOF文件占用的空间更小。即使重写失败，数据也不会丢失，因为在重写成功前，旧版本的AOF文件不会改动。重写操作只会在后台没有其他持久化工作时进行：\n\n如果Redis子进程正在保存快照，那么重写AOF的操作会到保存工作完成后才开始进行。这种情况下，该命令仍然会返回OK，但是会增加一条额外的返回信息说明。在Redis2.6以后的版本，你可以使用INFO命令查看重写操作是否被预定执行。\n如果已经有一个重写AOF命令正在进行，那么该命令会报错，并且不会预定执行重写操作。\n\nRedis2.4版本以后，重写AOF操作会自动触发。想要了解更多信息可以查看持久化文档。\nBGSAVE最早可用版本：1.0.0\n在后台保存当前数据库到磁盘。命令会马上返回OK，Redis会fork出一个子进程来进行此操作，而父进程继续提供服务。可以使用LASTSAVE命令查看保存操作是否成功。\nCLIENT GETNAME最早可用版本：2.6.9\n时间复杂度：O(1)\n这个命令会返回当前连接使用CLIENT SETNAME设置的连接名称，如果没有设置，则返回空。\nCLIENT ID最早可用版本：5.0.0\n时间复杂度：O(1)\n返回当前连接的ID。每个连接都会保证两点：\n\n不会重复，所以如果返回的ID相同，那么调用方就可以确定底层是没有断开重连的。\nID单调递增，如果一个连接的ID大于另一个连接的ID，那么它一定晚于这个连接创建。\n\njackeyzhe@ubuntu:~/redis-5.0.4/src$ ./redis-cli 127.0.0.1:6379&gt; CLIENT ID(integer) 3127.0.0.1:6379&gt; jackeyzhe@ubuntu:~/redis-5.0.4/src$ ./redis-cli 127.0.0.1:6379&gt; CLIENT ID(integer) 4\nCLIENT KILL最早可用版本：2.4.0\n时间复杂度：O(N)，N是客户端连接数\n用法：CLIENT KILL [ip:port] [ID client-id][TYPE normal|master|slave|pubsub] [ADDR ip:port][SKIPME yes/no]\n这个命令用来关闭一个指定的客户端连接。在Redis2.8.11之前，都可以指定要关闭的连接地址，像下面这种形式：\nCLIENT KILL addr:port\nip:port应该和CLIENT LIST命令中的一行匹配。\n在2.8.12及以后的版本，则可以使用以下形式：\nCLIENT KILL &lt;filter&gt; &lt;value&gt; ... ... &lt;filter&gt; &lt;value&gt;\n这种形式支持多种根据多种属性匹配客户端：\n\nCLIENT KILL ADDR ip:port ：这种和旧的形式相同\nCLIENT KILL ID client-id ：这种形式允许关闭指定ID的连接\nCLIENT KILL TYPE type ：这种形式支持关闭某种类型的客户端，type取值为：normal, master, slave和pubsub（master在Redis3.2之后可以使用）\nCLIENT KILL SKIPME yes/no ： 参数默认是yes，也就是不会关闭发出命令的客户端，而如果指定为no，则连自己也一起关闭\n\n注意：从Redis5开始type不再使用slave，改为replica\n上述的多种过滤器也可以组合使用。使用新的形式时，返回值为关闭的客户端数量。由于Redis是单线程的，所以这个命令不能关闭一个正在执行命令的客户端。\nCLIENT LIST最早可用版本：2.4.0\n时间复杂度：O(N)，N是客户端连接数\n用法：CLIENT LIST [TYPE normal|master|replica|pubsub]\n这个命令用来查看连接的客户端信息，在Redis5之后，可以使用TYPE参数。\n127.0.0.1:6379&gt; CLIENT LISTid=3 addr=127.0.0.1:44994 fd=8 name= age=342 idle=3 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=keysid=4 addr=127.0.0.1:44996 fd=9 name= age=335 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client\n返回值：每行代表一个客户端连接，字段包括：\n\nid：一个64bit唯一ID\nname：使用CLIENT SETNAME设置的客户端名称\naddr：客户端的地址和端口号\nfd：相应的socket文件描述符\nage：连接时长，单位为秒\nidle：空闲时间，单位为秒\nflags：客户端标志\ndb：当前数据库ID\nsub：已订阅频道的数量\npsub：已订阅模式的数量\nmulti：事务中的命令数\nqbuf：查询缓存的长度\nqbuf-free：查询缓存空闲空间（0表示缓存已满）\nobl：输出缓存的长度\noll ：输出列表长度（缓存满时，回复会被放入这个列表中）\nomem：输出缓存的内存占用量\nevents：文件描述符事件\ncmd：最后一次执行的命令\n\n客户端标志包括以下几种：\n\nA：尽可能快的关闭连接\nb：客户端在等待阻塞时间\nc：写完回复之后关闭连接\nd：被监视的key被修改了，事务将失败\ni：客户端正在等待虚拟机I/O（已废弃）\nM：客户端是master节点\nN：没有设置flag\nO：客户端是MONITOR模式\nP：客户端是Pub/Sub的订阅者\nr：客户端是针对集群节点的只读模式\nS：客户端连接到此实例的从节点\nu：客户端未阻塞\nU：客户端通过Unix套接字连接\nx：客户端正在执行事务\n\n文件描述符事件包括：\nr：客户端套接字可读\nw：客户端套接字可写\nCLIENT PAUSE最早可用版本：2.9.50\n时间复杂度：O(1)\n这个命令可以使所有连接暂停一段时间（单位：毫秒）。这个命令通常用来将连接从一个Redis实例迁移到另一个实例，例如当一个实例需要进行系统升级时，我们应该这样做：\n\n使用CLIENT PAUSE暂停所有客户端\n等待几秒钟，以便从节点与主节点数据同步完成\n将一个从节点切换成主节点\n重新使客户端连接到新的主节点\n\n这个命令通常在事务中和INFO replication命令一起使用，这样做可以使从节点和主节点同步完成。\nCLIENT REPLY最早可用版本：3.2\n时间复杂度：O(1)\n这个命令用来禁止服务器对当前客户端回复。它有以下几种使用场景：\n\n客户端发送fire和forget命令时（不关心什么时候完成的命令）\n加载大量数据\n正在创建缓存\n\n在这些情况下，客户端会忽略服务器的回复，因此，服务器回复是一种资源的浪费。\n命令支持3个参数：\n\nON：默认，接收服务器所有回复\nOFF：不接收服务器的所有回复\nSKIP：不接收下一条命令的回复\n\nCLIENT SETNAME最早可用版本：2.6.9\n时间复杂度：O(1)\n这个命令用来给连接设置一个名字。这个命令会在CLIENT LIST的输出列表中显示。名字的长度没有限制，但一般不超过Redis字符串类型的长度（512MB）。名字里不能有空格。可以通过设置空字符串的方式来删除一个连接的名称，每个新的连接是没有名称的。\nCLIENT UNBLOCK最早可用版本：5.0.0\n时间复杂度：O(log N) N是客户端连接数\n用法：CLIENT UNBLOCK client-id [TIMEOUT|ERROR]\n这个命令可以解除被阻塞的客户端（执行了BPOP、XREAD、WAIT等命令）。\n默认情况下，如果阻塞超时，会解除阻塞。这里也可以有其他参数，TIMEOUT或ERROR。如果设置为ERROR，那么，被强制解除阻塞的连接会返回一个-UNBLOCKED错误。\n这个命令主要用于少量连接监控多个key时，如果要监控新的key，又不想使用更多的连接，那么就解除一个连接的阻塞，监控新的key后再重新阻塞。\nCOMMAND最早可用版本：2.8.13\n时间复杂度：O(N)，N是Redis命令总数\n返回所有Redis命令的相关信息。\n返回信息的第一层包含以下内容：\n\n命令的名称\n命令arity（可接受的参数数量）\n命令标志\n第一个key在参数列表中的位置\n最后一个key在参数列表中的位置\n用于定位重复key的step\n\n命令arity如果是整数，表示命令的请求参数（包括命令名称）数量是一个固定的值；如果是负数，表示请求参数的最小数量。\n1) 1) &quot;get&quot;   2) (integer) 2   3) 1) readonly   4) (integer) 1   5) (integer) 1   6) (integer) 1\n1) 1) &quot;mget&quot;   2) (integer) -2   3) 1) readonly   4) (integer) 1   5) (integer) -1   6) (integer) 1\n命令标志包括以下几种：\n\nwrite：命令会改变数据\nreadonly：命令不会改变key的值\ndenyoom：如果发生OOM，则拒绝命令\nadmin：服务器管理员命令\npubsub：和订阅模式有关的命令\nnoscript：脚本中不能执行的命令\nrandom：命令的执行结果随机\nsort_for_scrpt：如果在脚本中执行，结果会被排序\nloading：允许命令在数据库加载时执行\nstale：副本中有过时数据时，仍然可以执行命令\nskip_monitor：不在MONITOR中显示命令\nasking：集群相关，导入时仍可执行命令\nfast：命令操作时间不变或者是log(N)\nmovablekeys：命令没有预先执行的key，必须自己指定\n\nCOMMAND COUNT最早可用版本：2.8.13\n时间复杂度：O(1)\n返回当前Redis服务器支持的命令数量\nCOMMANC GETKEYS最早可用版本：2.8.13\n时间复杂度：O(N)\n输出命令中的key\n&gt; COMMAND GETKEYS mset a b c d e f1) &quot;a&quot;2) &quot;c&quot;3) &quot;e&quot;\nCOMMAND INFO最早可用版本：2.8.13\n时间复杂度：O(N)\n返回指定命令的详细信息，返回结果的内容和COMMAND一样，如果命令不存在，返回nil。\n&gt; COMMAND INFO get1) 1) &quot;get&quot;   2) (integer) 2   3) 1) readonly      2) fast   4) (integer) 1   5) (integer) 1   6) (integer) 1\nCONFIG GET最早可用版本：2.0.0\n这个命令可以读redis服务器的配置参数，在2.6版本以后，才可以读到全部配置。命令支持模糊匹配\nconfig get *max-*-entries*1) &quot;hash-max-zipmap-entries&quot;2) &quot;512&quot;3) &quot;list-max-ziplist-entries&quot;4) &quot;512&quot;5) &quot;set-max-intset-entries&quot;6) &quot;512&quot;\nCONFIG RESETSTAT最早可用版本：2.0.0\n时间复杂度：O(1)\n重置INFO命令中的一些统计信息，包括\n\nKey命中数\nKey未命中数\n命令处理数量\n连接数\n过期Key的数量\n拒绝的连接数\n最近的fork(2)时间\naof_delayed_fsync 计数器\n\nCONFIG REWRITE最早可用版本：2.8.0\n该命令用于重写redis.conf文件，应用最小的改变，使其反映当前服务器的配置。如果原始文件不存在，该命令也可以重头写一个配置文件。\nCONFIG SET最早可用版本：2.0.0\n该命令用于修改服务器的配置。可以使用CONFIG GET *查看可修改的配置。\nDBSIZE最早可用版本：1.0.0\n返回当前数据库key的数量\nDEBUG OBJECT最早可用版本：1.0.0\n这个命令不应该在客户端使用，具体请看OBJECT命令。\nDEBUG SEGFAULT最早可用版本：1.0.0\n这个命令用于执行无效的内存访问，导致Redis崩溃，它用于在开发过程中模拟错误。\nFLUSHALL最早可用版本：1.0.0\n删除所有数据库中的key。\n4.0.0版本以后，可以使用ASYNC参数，这个参数可以在后台进行删除任务。\nFLUSHDB最早可用版本：1.0.0\n删除当前数据库的所有key。\nINFOINFO命令返回服务器的详细信息。可以执行显示的部分：\n\nserver：Redis server通用信息\nclients：客户端连接部分\nmemory：内存相关信息\npersistence：RDB和AOF相关信息\nstats：通用统计信息\nreplication：主从复制信息\ncpu：CPU相关统计\ncommandstats：Redis命令统计\ncluster：Redis集群部分\nkeyspace：数据库相关信息\n\nLASTSAVE最早可用版本：1.0.0\n返回DB最后一次保存成功的时间。\nMEMORY DOCTOR最早可用版本：4.0.0\n该命令报告Redis服务器遇到的与内存相关的问题，并就可能的补救措施提出建议。\nMEMORY HELP最早可用版本：4.0.0\n该命令返回描述不同子命令的帮助文本。\nMEMORY MALLOC-STATS最早可用版本：4.0.0\n该命令提供了内存分配器的内部统计报告。这个命令只有在使用jemalloc作为分配器时可用。\nMEMORY PURGE最早可用版本：4.0.0\n该命令尝试清除脏页面，以便内存分配器回收。\nMEMORY STATS最早可用版本：4.0.0\n返回内存的使用情况，包括以下维度：（没有特别说明，则以字节为单位）\n\npeak.allocated：Redis内存消耗的峰值\ntotal.allocated：Redis使用的内存总数\nstartup.allocated：Redis启动时，初始化所需要的内存\nreplication.backlog：复制log积压的大小\nclients.slaves：所有副本的总开销\nclients.normal：所有客户端的总开销\naof.buffer：当前AOF缓冲区的总开销\ndbXXX：对于每个数据库，主字典和到期字典的开销\noverhead.total：所有的间接开销\nkeys.count：所有数据库中key的总数\nkeys.bytes-per-key：净内存使用和keys.count的比率\ndataset.bytes：数据集的开销\ndataset.percentage：数据集开销所占百分比\npeak.percentage：peak.allocated占total.allocated的百分比\nfragmentation：碎片内存的比率\n\nMEMORY USAGE最早可用版本：4.0.0\n时间复杂度：O(N)\n用法 MEMORY USAGE key [Samples count]\n该命令返回了指定key和它的value存储所占用的内存大小。\n对于嵌套数据类型，可以使用SAMPLES参数，其中count是采样嵌套的数量，默认是5，如果要对所有嵌套值进行采样，需要将SAMPLES设置为0。\nMONITOR最早可用版本：1.0.0\nMONITOR是一个调试命令，它可以回溯Redis服务器处理的每个命令。它可以帮助理解数据库发生了什么。这个命令可以通过redis-cli和telnet使用。\n安全起见，某些命令是不会被MONITOR记录的（如CONFIG）\nREPLICAOF最早可用版本：5.0.0\n这个命令可以改变从服务器的从属关系。\n对于一台从服务器来说，执行REPLICAOF NO ONE命令，结果是当前服务器变成master。而执行REPLICAOF host port命令会改变原从属关系，是从服务器归属于新的master。\nROLE最早可用版本：2.8.12\n返回Redis实例的角色信息：包括：master、slave和sentinel\n对于master节点：\n1) &quot;master&quot;2) (integer) 31296593) 1) 1) &quot;127.0.0.1&quot;      2) &quot;9001&quot;      3) &quot;3129242&quot;   2) 1) &quot;127.0.0.1&quot;      2) &quot;9002&quot;      3) &quot;3129543&quot;\n第一行是master字符串；第二行是主从复制的偏移量；用于标记重新同步时开始的位置，第三行开始是从节点的信息，包括IP、端口号和最后同步的从节点偏移量。\n对于从节点：\n1) &quot;slave&quot;2) &quot;127.0.0.1&quot;3) (integer) 90004) &quot;connected&quot;5) (integer) 3167038\n第一行返回slave字符串；第二行是IP；第三行是端口号；第四行是与主节点连接状态，可以是connect（需要与主节点连接），connecting（正在连接），sync（尝试进行主从同步），connected（从节点在线）；第五行是从节点收到的数据量\n对于sentinel\n1) &quot;sentinel&quot;2) 1) &quot;resque-master&quot;   2) &quot;html-fragments-master&quot;   3) &quot;stats-master&quot;   4) &quot;metadata-master&quot;\n第一行是sentinel；第二行之后是监控master的名字。\nSAVE最早可用版本：1.0.0\n同步的执行保存当前数据集快照，并写入到RDB文件。不要在生产环境使用这个命令！\nSHUTDOWN最早可用版本：1.0.0\n这个命令有以下操作：\n\n停止全部客户端\n如果设置了save point，就会执行SAVE命令\n如果AOF是enabled，刷新AOF文件\n退出服务器\n\n如果启用了持久化，则可以保证数据不丢失。\n如果执行SHUTDOWN SAVE，即便没有save point，仍然会强制执行保存操作。\n如果执行SHUTDOWN NOSAVE，有保存点也不会执行保存操作。\nSLAVEOF最早可用版本：1.0.0\n该命令被REPLICAOF替代\nSLOWLOG最早可用版本：2.2.12\n这个命令用来读取并重置慢查询的日志。通过slowlog-log-slower-than参数设置慢查询的时间，超过这个时间就会被记录\nTIME最早可用版本：2.6.0\n时间复杂度：O(1)\n该命令返回当前服务器时间的秒数，以及当前秒中已经过去的微秒数。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Streams","url":"/2019/07/01/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AStreams/","content":"Redis5.0迎来了一种新的数据结构Streams，没有了解过的同学可以先阅读前文，今天来介绍一下Streams相关的命令。\nXACK最早可用版本：5.0.0\n时间复杂度：O(1)\n用法：XACK key group ID [ID…]\n这个命令用于删除消费者组的pending entries list中的元素。通常情况下，调用XREADGROUP命令或者消费者调用XCLAIM命令时，会使一个消息阻塞，并存到PEL中，阻塞的消息被发送给消费者时，服务器并不知道消息是否被处理。\n当消费者成功消费消息后，会调用XACK命令，服务器就会将消息从PEL中删除，并释放内存。\nXADD最早可用版本：5.0.0\n时间复杂度：O(1)\n向指定的stream添加元素。如果key不存在，就创建一个新的stream。\nentry由一系列field-value对组成，存储顺序由用户添加顺序决定。XADD命令是唯一一个向stream中添加数据的命令。删除数据的命令则有XDEL和XTRIM。\n在stream中，entry ID是唯一标识。XADD命令中ID参数是*时，会自动生成唯一ID。然而在生产环境中并不常用，通常需要我们指定一种格式较好的唯一ID。\n默认的ID生成策略是：“Unix毫秒时间戳-同一毫秒值内的序列号”。\n当用户显式指定ID时，最小值是0-1，且ID必须是递增的。\n用户可以使用MAXLEN指定stream的最大元素数量\nXADD mystream MAXLEN ~ 1000 * ... entry fields here ..\n上面的波浪线表示不是严格的限制1000个，也可以多出几十个。\nXCLAIM最早可用版本：5.0.0\n时间复杂度：O(log N)\n用法：XCLAIM key group consumer min-idle-time ID [ID …][IDLE ms] [TIME ms-unix-time] [RETRYCOUNT count][FORCE] [JUSTID]\n这个命令用于改变pending消息的所有权，新的owner是命令参数中的consumer。\n命令的使用场景是：\n\n一个消费者关联了一个stream\n消费者A通过XREADGROUP读取一条消息\n这个消息被加入到PEL中，并发送给指定的消费者，但是没有调用XACK命令来确认\n这时消费者突然挂掉\n其他的消费者就会使用XPENDING命令检查待处理消息列表，为了继续处理这些命令，它们使用XCLAIM命令改变这些消息的所有者。\n\n接下来解释一下命令的各个选项：\n\nIDLE：设置消息空闲时间，默认是0。消息只有在空闲时间大于IDLE时才会被认领。\nTIME：和IDLE相同，不过它是绝对时间\nRETRYCOUNT ：设置重试次数，通常XCLAIM不会改变这个值，它通常用于XPENDING命令，用来发现一些长时间未被处理的消息。\nFORCE：在PEL中创建待处理消息，即使指定的ID尚未分配给客户端的PEL。\nJUSTID：只返回认领的消息ID数组，不返回实际消息。\n\nXDEL最早可用版本：5.0.0\n时间复杂度：O(1)\n删除stream中的entry并返回删除的数量。\nXGROUP最早可用版本：5.0.0\n时间复杂度：每个子命令是O(1)\n该命令用于管理stream相关的消费者组。使用XGROUP命令你可以：\n\n创建与一个stream相关联的消费者组\n销毁一个消费者组\n从消费者组中删除指定的消费者\n设置消费者组的last delivered ID\n\n创建新的消费者组的命令是：\nXGROUP CREATE mystream consumer-group-name $\n最后一个参数是stream中已传递的最后一个ID，使用$表示这个消费者组只能获取到新的元素。\n销毁消费者组的命令是：\nXGROUP DESTROY mystream some-consumer-group\n即使消费者组存在活跃的消费者和等待消息，它仍然会被删除，所以执行这个命令需要格外谨慎。\n删除指定消费者的命令是：\nXGROUP DELCONSUMER mystream consumer-group-name myconsumer123\n当一个新的consumer的名字被提到时，就会自动创建消费者。当消费者不再使用时，我们可以将它删除，上面的命令返回消费者在被删除之前所拥有的待处理消息。\n设置last delivered ID的命令是：\nXGROUP SETID mystream my-consumer-group 0\n最后，如果不记得语法，可以使用命令：\nXGROUP HELP\nXINFO最早可用版本：5.0.0\n时间复杂度：O(N)，N是CONSUMERS和GROUPS返回的item数量\n用法：XINFO [CONSUMERS key groupname] [GROUPS key][STREAM key] [HELP]\n这个命令用于返回stream和相关消费者组的不同信息。它有三种形式。\n\nXINFO STREAM         这个命令返回stream的通用信息\nXINFO GROUPS        这个命令用于获得stream相关的消费者组的信息\nXINFO CONSUMERS    这个命令返回指定消费者组的消费者列表\n\nXLEN最早可用版本：5.0.0\n时间复杂度：O(1)\n返回stream中的entry数量。如果key不存在，则返回0。对于长度为0的stream，Redis不会删除，因为可能存在关联的消费者组。\nXPENDING最早可用版本：5.0.0\n时间复杂度：O(N)，N是返回的元素数量\n用法：XPENDING key group [start end count] [consumer]\n通过消费者组捕获数据，但不是确认这些数据。\nXPENDING命令是检查待处理消息列表的接口，用于观察和了解消费者组正在发生的事情：哪些客户端是活跃的，哪些消息等待消费，或者查看是否有空闲的消息。这个命令通常与XCLAIM一起使用，用于处理长时间未被处理的消息。\n这个命令的返回值是：\n&gt; XPENDING mystream group55 - + 101) 1) 1526984818136-0   2) &quot;consumer-123&quot;   3) (integer) 196415   4) (integer) 1\n其中包括：\n\n消息ID\n获取并要确认消息的消费者名称\n自上次消息传递给消费者以来经过的毫秒数\n该消息被传递的次数\n\nXRANGE最早可用版本：5.0.0\n时间复杂度：O(N)，N是返回的元素数量\n用法：XRANGE key start end [COUNT count]\n该命令用于返回stream中指定ID范围的数据，可以使用-和+表示最小和最大ID。ID也可以指定为不完全ID，即只指定Unix时间戳，就可以获取指定时间范围内的数据。\nXREAD最早可用版本：5.0.0\n时间复杂度：O(N)，N是返回的元素数量\n用法：XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key …] ID [ID …]\n从一个或多个stream中读取数据，仅返回ID大于调用者报告的最后接收ID的条目。\nBLOCK项用于指定阻塞时长。STREAMS项必须在最后，用于指定stream和ID。\nXREADGROUP最早可用版本：5.0.0\n时间复杂度：O(log(N)+M) ，N是返回的元素数量，M是一个常量。\n用法：XREADGROUPGROUP group consumer [COUNT count] [BLOCK milliseconds] STREAMS key [key …] ID [ID …]\nXREADGROUP是XREAD的特殊版本，支持消费者组。\nXREVRANGE最早可用版本：5.0.0\n时间复杂度：O(log(N)+M) ，N是返回的元素数量，M是一个常量。\n此命令与XRANGE唯一的区别是顺序相反。\nXTRIM最早可用版本：5.0.0\n时间复杂度：O(log(N)+M) ，N是返回的元素数量，M是一个常量。\n用法：XTRIM key MAXLEN [~] count\n该命令用于裁剪流为指定数量的项目。这个命令被设计为接受多种策略，但目前只实现了MAXLEN一种。\n如果要裁剪到stream中最新的1000个项目：\nXTRIM mystream MAXLEN 1000\n可以使用以下形式提高效率：\nXTRIM mystream MAXLEN ~ 1000\n~表示用户不需要精确的1000个项目，可以多出几十个，但是不能少于1000.\n","tags":["Redis命令"]},{"title":"Redis命令详解：Sets","url":"/2018/12/19/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9ASets/","content":"Redis的Set结构相当于Java中的HashSet，是无序的元素集合，并且元素都是唯一的。由于Set是通过hash表实现的，所以它的增加、删除、查找操作的时间复杂度都是O(1)。最大成员个数为232-1。\nSADD最早可用版本：1.0.0\n时间复杂度：每个元素的添加的时间复杂度为O(1)，如果要添加N个，时间复杂度就为O(N)\n用法：SADD key member [member…]\n将指定的成员保存到key，如果成员已经存在，则直接忽略。如果key不存在，则先新建一个空set，再将成员添加进去。如果key存储不是一个set，则会报错。该命令执行成功后会返回实际添加成功的元素的个数。\n在2.4版本之后可以支持多个参数，即一个命令添加多个成员。\nSCARD最早可用版本：1.0.0\n时间复杂度：O(1)\n返回key存储的set的元素个数。\nSDIFF最早可用版本：1.0.0\n时间复杂度：O(N)，N是所给出的元素个数的总和\n返回第一个set与后面元素的差集。不存在的key都被当做空set处理。\n栗子时间：\n127.0.0.1:6379&gt; SADD key1 a(integer) 1127.0.0.1:6379&gt; SADD key1 b(integer) 1127.0.0.1:6379&gt; SADD key1 c(integer) 1127.0.0.1:6379&gt; SADD key2 b(integer) 1127.0.0.1:6379&gt; SADD key3 c(integer) 1127.0.0.1:6379&gt; SADD key3 d(integer) 1127.0.0.1:6379&gt; SDIFF key1 key2 key31) &quot;a&quot;\nSDIFFSTORE最早可用版本：1.0.0\n时间复杂度：O(N)，N是所给出的元素个数的总和\n用法：SDIFFSTORE destination key [key…]\n这个命令和SDIFF命令的作用相同，但是不同的是，该命令不返回差集，而是将差集存储到destination，如果destination已经存在，就将覆盖旧值。该命令的返回值是差集中元素的个数。\nSINTER最早可用版本：1.0.0\n时间复杂度：O(N*M)，N是最小set的元素个数，M是set的个数\n返回给出的所有set的交集。我们沿用刚刚SDIFF命令中使用的三个key，给key2增加一个元素c，此时三个key存储的元素情况为\nkey1=&#123;a,b,c&#125;key2=&#123;b,c&#125;key3=&#123;c,d&#125;\n127.0.0.1:6379&gt; SADD key2 c(integer) 1127.0.0.1:6379&gt; SINTER key1 key2 key31) &quot;c&quot;\nSINTERSTORE最早可用版本：1.0.0\n时间复杂度：O(N*M)，N是最小set的元素个数，M是set的个数\n该命令与SINTER的关系就像SDIFF与SDIFFSTORE的关系一样，因此我们不过多介绍了。\nSISMEMBER最早可用版本：1.0.0\n时间复杂度：O(1)\n该命令用于判断某个元素是否属于指定的key，如果属于，返回1；如果不属于或者key不存在，返回0。\n127.0.0.1:6379&gt; SADD myset &quot;jackeyzhe&quot;(integer) 1127.0.0.1:6379&gt; SISMEMBER myset &quot;jackeyzhe&quot;(integer) 1127.0.0.1:6379&gt; SISMEMBER myset &quot;2018&quot;(integer) 0\nSMEMBERS最早可用版本：1.0.0\n时间复杂度：O(N)，N是set的元素个数\n返回指定set的全部成员，当SINTER只有一个参数时，作用与该命令相同。\nSMOVE最早可用版本：1.0.0\n时间复杂度：O(1)\n将成员从一个set转移到另一个set中，这个操作是原子操作。如果源set不存在，或者不包含要转移的成员，那么就不会有任何操作，直接返回0。如果转移的成员在目标set中已经存在，那么只需要将该成员从源set中删除即可。如果源set或者目标set中的一个不是set结构，那么该命令就会报错。\n如果成员被成功转移，就会返回1，如果没有进行转移操作，就会返回0。\n127.0.0.1:6379&gt; SADD from_set &quot;a&quot;(integer) 1127.0.0.1:6379&gt; SADD from_set &quot;b&quot;(integer) 1127.0.0.1:6379&gt; SMOVE from_set to_set &quot;a&quot;(integer) 1127.0.0.1:6379&gt; SMEMBERS from_set1) &quot;b&quot;127.0.0.1:6379&gt; SMEMBERS to_set1) &quot;a&quot;\nSPOP最早可用版本：1.0.0\n时间复杂度：O(1)\n用法：SPOP key [count]\n从指定set中删除并返回一个或多个随机元素。3.2版本以后支持count参数，即可以一次返回多个元素。如果key不存在，则返回nil。\n如果count大于set中元素的个数，那么该命令就会返回set中现有的所有元素。\nSRANDMEMBER最早可用版本：1.0.0\n时间复杂度：当没有count参数时是O(1)，否则为O(N)，N为count的绝对值\n该命令用于随机返回set中的元素。从2.6版本开始支持count参数，如果count是正数，则返回count个不同元素的数组；如果count是负数，则允许同一个元素多次返回。\nSREM最早可用版本：1.0.0\n时间复杂度：O(N)，N为指定member的个数\n该命令用于从set中删除指定元素，如果不包含该元素，那么直接忽略。如果key不存在，则会当做空set处理，直接返回0。从2.4版本开始，该命令支持一次删除多个成员。\nSSCAN此命令是SCAN命令的同类，可以通过我的另一篇文章深入理解Redis的scan命令来进行更深入的了解\nSUNION最早可用版本：1.0.0\n时间复杂度：O(N)，N为给出的所有set的元素个数之和\n返回给定set的并集。\n127.0.0.1:6379&gt; SMEMBERS key21) &quot;b&quot;2) &quot;c&quot;127.0.0.1:6379&gt; SMEMBERS key31) &quot;d&quot;2) &quot;c&quot;127.0.0.1:6379&gt; SUNION key2 key31) &quot;d&quot;2) &quot;b&quot;3) &quot;c&quot;\nSUNIONSTORE最早可用版本：1.0.0\n时间复杂度：O(N)，N为给出的所有set的元素个数之和\n该命令与SUNION的关系就像SDIFF与SDIFFSTORE的关系一样。\n127.0.0.1:6379&gt; SUNIONSTORE mykey key2 key3(integer) 3127.0.0.1:6379&gt; SMEMBERS mykey1) &quot;d&quot;2) &quot;b&quot;3) &quot;c&quot;\n","tags":["Redis命令"]},{"title":"Redis命令详解：Sorted Sets","url":"/2019/01/06/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9ASorted-Sets/","content":"Sorted Set（也称ZSET）和Set一样也是string类型的集合，你可以将它理解为Java中SortedSet和HashMap的集合体，一方面它是一个set，保证了元素的唯一性，另一方面它给每个value赋予了一个权重score，用来进行排序。集合中成员的最大个数为232-1个。\nBZPOPMAX最早可用版本：5.0.0\n时间复杂度：O(log(N))，N是元素个数\n用法：BZPOPMAX key [key …] timeout\nBZPOPMAX是ZPOPMAX的原始阻塞版。如果没有存在sorted set不能pop出元素，则连接会被阻塞。该命令会返回第一个非空的有序set的最高分的元素。\ntimeout参数是用来指定最大的阻塞时间，如果是0，则无限阻塞。\n当没有元素被pop出，并且阻塞时间达到timeout时，返回nil。\n如果有元素被pop出，则返回三个值：第一个是该元素来自哪个zset，第二个是pop元素的score，第三个是pop元素的value。\nBZPOPMIN最早可用版本：5.0.0\n时间复杂度：O(log(N))，N是元素个数\n用法：BZPOPMIN key [key …] timeout\nBZPOPMIN是ZPOPMIN的阻塞版本。它与BZPOPMAX相似，唯一不同的是它返回的是第一个非空有序set的最低分的元素。\nZADD最早可用版本：1.2.0\n时间复杂度：O(log(N))，N是元素个数\n用法：ZADD key [NX|XX][CH][INCR]score member [score member …]\n将所有指定的成员和它的score加入zset，如果要插入的成员已经存在，则会更新该成员的分数，并将它排到正确的位置。如果key不存在，则创建一个新的zset并且插入成员。如果key存在，但不是zset类型，就会报错。score是双精度的浮点数，+inf和-inf同样有效。\n在Redis3.2版本之后，ZADD命令支持了以下参数：\n\nXX：只更新已有的成员，不新增\nNX：只新增成员，不更新\nCH：将返回值从新增成员数修改为发生变化的成员总数\nINCR：当指定这个参数时，ZADD命令和ZINCRBY相似，但是只能接受一个成员的参数\n\n分数的范围Redis的Sorted Set的分数范围从-(2^53)到+(2^53)。或者说是-9007199254740992 到 9007199254740992。更大的整数在内部用指数表示。\n相同分数的成员由于所有的成员都是唯一的，当分数相同时，成员将按照字典序进行排序。它比较的是成员的字节数组，当所有成员的分数都相同时，范围查询可以用ZRANGEBYLEX命令（分数范围查询用ZRANGEBYSCORE命令）。\n该命令返回值是新增成员的数量，如果是INCR参数模式，就返回新增成员的分数。\nRedis2.4版本以后该命令才支持指定多个成员/分数对。\nZCARD最早可用版本：1.2.0\n时间复杂度：O(1)\n当key存在时，返回zset的成员数量；否则返回0。\nZCOUNT最早可用版本：2.0.0\n时间复杂度：O(log(N))，N是zset的成员个数\n用法：ZCOUNT key min max\n返回分数在min到max（默认包括min和max）之间的成员个数。\nZCOUNT命令的时间复杂度为O(log(N))，因为它使用了ZRANK进行排序，然后获取范围的成员个数。\nZINCRBY最早可用版本：1.2.0\n时间复杂度：O(log(N))，N是zset的成员个数\n用法：ZINCRBY key increment member\n给指定zset中的指定的成员加上increment分数。如果成员不存在，则新增成员，将分数置为increment。如果key不存在，则先创建一个zset，然后加入新的成员。命令的返回值是成员的新分数。\nZINTERSTORE最早可用版本：2.0.0\n时间复杂度：O(N  K)+O(M  log(M))，N是输入的zset中的最小的成员数量，K为输入的zset的数量。M是结果中zset的成员数量\n用法：ZINTERSTORE destination numkeys key [key …][WEIGHTS weight [weight …]][AGGREGATE SUM|MIN|MAX]\nZINTERSTORE命令用于计算给出的numkeys个zset的交集，并将结果保存到destination中。在给出要计算的key和其他参数之前，必须先给出numkeys。默认情况下，输出的zset成员的分数，会是输入的zset的成员的分数之和。\n127.0.0.1:6379&gt; ZADD myzset1 1 &quot;jackey&quot;(integer) 1127.0.0.1:6379&gt; ZADD myzset1 2 &quot;zhe&quot;(integer) 1127.0.0.1:6379&gt; ZADD myzset2 1 &quot;jackey&quot;(integer) 1127.0.0.1:6379&gt; ZADD myzset2 2 &quot;zhe&quot;(integer) 1127.0.0.1:6379&gt; ZADD myzset2 3 &quot;2018&quot;(integer) 1127.0.0.1:6379&gt; ZINTERSTORE deszset 2 myzset1 myzset2(integer) 2127.0.0.1:6379&gt; ZRANGE deszset 0 -1 WITHSCORES1) &quot;jackey&quot;2) &quot;2&quot;3) &quot;zhe&quot;4) &quot;4&quot;\nWEIGHTS用来对每一个zset设置一个乘数因子，在计算分数时乘以指定的数值，默认是1。\nAGGREGATE参数用来指定分数的聚合策略，默认是SUM，也就是相加。还可以选择取最大或最小的分数。\n如果destination已经存在，则覆盖原来的值。命令的返回值是结果的成员个数。\nZLEXCOUNT最早可用版本：2.8.9\n时间复杂度：O(log(N))，N是zset的成员个数\n用法：ZLEXCOUNT key min max\n当所有成员的分数都相同时，使用这个命令计算min和max之间的成员个数。\n关于min和max：\n\n成员名称前需要加上[，[符号和成员名称之间不能有空格\n可以使用-和+表示最大值和最小值\n计算数量时，包括min和max\n\n127.0.0.1:6379&gt; ZADD myzset 0 a 0 b 0 e 0 d 0 i 0 f 0 k(integer) 7127.0.0.1:6379&gt; ZLEXCOUNT myzset - +(integer) 7127.0.0.1:6379&gt; ZLEXCOUNT myzset b e(error) ERR min or max not valid string range item127.0.0.1:6379&gt; ZLEXCOUNT myzset [b [e(integer) 3127.0.0.1:6379&gt; ZRANGE myzset 0 -11) &quot;a&quot;2) &quot;b&quot;3) &quot;d&quot;4) &quot;e&quot;5) &quot;f&quot;6) &quot;i&quot;7) &quot;k&quot;\nZPOPMAX最早可用版本：5.0.0\n时间复杂度：O(log(N)*M)，N是zset的成员数量，M是弹出的成员数量\n用法：ZPOPMAX key [count]\n该命令用于移除并返回一定数量的分数最高的成员。count默认是1，count大于zset成员，当返回多个元素时，分数最高的最先被返回。\nZPOPMIN最早可用版本：5.0.0\n时间复杂度：O(log(N)*M)，N是zset的成员数量，M是弹出的成员数量\n该命令和ZPOPMAX相反，返回的是分数最低的元素。只有这点不同，其他都相同。\nZRANGE最早可用版本：1.2.0\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是返回的成员数量\n用法：ZRANGE key start stop [WITHSCORES]\n该命令返回指定范围的成员，按照分数从低到高的顺序排。start和stop都是从0开始，也可以是负数，-1表示倒数第一个。返回的时候包括start和stop位置的成员。\n如果start大于zset成员数量或者start大于stop，则返回空集合；如果stop大于最后一位，则返回start到最后一位的成员。\nWITHSCORES参数表示返回的结果中是否要带分数。\nZRANGEBYLEX最早可用版本：2.8.9\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是返回的成员数量\n用法：ZRANGEBYLEX key min max [LIMIT offset count]\n前面我们提到过，当所有的成员的分数相同时，它们会按照字典顺序排列。对于中情况，ZRANGEBYLEX命令就是用来返回指定区间成员的。指定成员时可以使用(或者[，(表示不包含指定的成员，[表示包含。\n成员字符串作为二进制数组来排序，默认是ASCII字符集的顺序。\nLIMIT参数用于分页，类似于SQL中的LIMIT关键字。\nZRANGEBYSCORE最早可用版本：1.0.5\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是返回的成员数量\n用法：ZRANGEBYSCORE key min max [WITHSCORES][LIMIT offset count]\n这个命令用来返回指定分数范围的成员，包括min和max。如果分数相同，则按字典顺序排列。\nLIMIT参数用来分页。\n在Redis2.0以后，可用使用WITHSCORES参数，使返回值中带有分数。\n我们可以使用(表示不包括指定的分数，举个栗子：\nZRANGEBYSCORE zset (1 5\n取的分数范围是1&lt;score&lt;=5\nZRANK最早可用版本：2.0.0\n时间复杂度：O(log(N))\n该命令用于返回指定的成员从低到高的排名。返回值从0开始，第一个元素的rank是0，第二个是1……\n如果成员存在，返回它的rank值；如果不存在，返回nil。\nZREM最早可用版本：1.2.0\n时间复杂度：O(M*log(N))，N是zset的成员数量，M是要删除的成员数量\n从zset中删除指定的成员。返回值为实际删除的成员数量。\nRedis2.4版本以后支持一次指定多个成员。\nZREMRANGEBYLEX最早可用版本：2.8.9\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是要删除的成员数量\n用法：ZREMRANGEBYLEX key min max\n该命令用于删除指定返回的成员，最好用于所有分数都相同的集合，否则结果会不准确。\n关于min和max的描述可以查看ZRANGEBYLEX命令。\nZREMRANGEBYRANK最早可用版本：2.0.0\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是要删除的成员数量\n用法：ZREMRANGEBYRANK key start stop\n用于删除指定rank范围的成员。start和stop的介绍可以查看ZRANGE命令。\nZREMRANGEBYSCORE最早可用版本：1.2.0\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是要删除的成员数量\n用法：ZREMRANGEBYSCORE key min max\n删除指定分数范围的成员，默认包括min和max的分数，在2.1.6版本以后可以不包括min和max，具体可以查看ZRANGEBYSCORE命令。\nZREVRANGE最早可用版本：1.2.0\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是要返回的成员数量\n用法：ZREVRANGE key start stop [WITHSCORES]\n返回分数从高到低的成员，也就是说，顺序与ZRANGE相反。其他条件都相同。\nZREVRANGEBYLEX最早可用版本：1.2.0\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是要返回的成员数量\n该命令是ZRANGEBYLEX命令的倒序版本。\n####ZREVRANGEBYSCORE\n最早可用版本：2.2.0\n时间复杂度：O(log(N)+M)，N是zset的成员数量，M是要返回的成员数量\n是ZRANGEBYSCORE命令的倒序。\nZREVRANK最早可用版本：2.0.0\n时间复杂度：O(log(N))\n是ZRANK的倒序。\nZSCAN最早可用版本：2.8.0\n时间复杂度：每次调用为O(1)\n用法：ZSCAN key cursor [MATCH pattern][COUNT count]\n这是一个SCAN类的命令，可以看这里进行更深入的了解。\nZSCORE最早可用版本：1.2.0\n时间复杂度：O(1)\n该命令用于返回指定成员的分数。如果指定成员不存在或者key不存在，则返回nil。\nZUNIONSTORE最早可用版本：2.0.0\n时间复杂度：O(N)+O(M log(M))，N是输入的zset的大小之和，M是结果的zset的大小\n用法：ZUNIONSTORE destination numkeys key [key …][WEIGHTS weight [weight …]][AGGREGATE SUM|MIN|MAX]\n计算给出的zset的并集，并把结果存到destination，在给定要计算的key和其他参数之前，要给出numkeys，也就是key的数量。默认情况下，结果中的成员的分数，是输入的zset的该成员分数的和。\n关于WEIGHTS和AGGREGATE参数，可以查看ZINTERSTORE命令中的介绍。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Strings","url":"/2018/10/07/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9AStrings/","content":"String类型是Redis中比较常用的类型，因此，和String相关的命令也比较多\nAPPEND最早可用版本2.0.0\n当指定的key存在，并且value是字符串时，APPEND命令会在字符串末尾追加指定的字符串，如果指定的key不存在，则会创建一个空的字符串，并且追加上指定的value，效果类似于SET命令。\n该命令的返回值是执行后字符串的长度。\n127.0.0.1:6379&gt; EXISTS mykey(integer) 0127.0.0.1:6379&gt; APPEND mykey Jackeyzhe(integer) 9127.0.0.1:6379&gt; APPEND mykey 2018(integer) 13127.0.0.1:6379&gt; GET mykey&quot;Jackeyzhe2018&quot;\nAPPEND常被用作为定长的数据提供紧凑的存储。可以通过GETRANGE命令来获取指定长度范围的字符串，这里推荐使用Unix的时间戳作为key，既不会因为单个key过大而影响效率，又节省了大量命名空间。\nBITCOUNT最早可用版本2.6.0\n该命令的时间复杂度是O(N)，用来统计字符串中被设置为1的比特数。默认检查整个字符串，当然也可以指定起始和结束位置。起始和结束位置可以是负数，例如-1表示最后一个字节，-2表示倒数第二字节，以此类推。\n127.0.0.1:6379&gt; SETBIT bitkey 0 1   #0001(integer) 0127.0.0.1:6379&gt; BITCOUNT bitkey(integer) 1127.0.0.1:6379&gt; SETBIT bitkey 2 1   #0101(integer) 0127.0.0.1:6379&gt; BITCOUNT bitkey(integer) 2\n这个命令可以用来统计实时的数据。例如，统计用户上线历史，我们可以使用用户名作为key，如果第n天上线，将对应的第n位置为1。这样，即使统计10年的数据，每个用户所使用的内存空间仅仅是456字节。对于这样的数据量来讲，BITCOUNT处理的速度和其他时间复杂度为O(1)的命令是一个数量级的。\nBITFIELD最早可用版本3.2.0\n用法：BITFIELD key [GET type offset] [SET type offset value] ][INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL]\n这个命令把Redis的字符串看作是一个bit数组。可以把指定偏移位置的bit当做指定的类型处理。例如，以下命令是对偏移量100的8位有符号整数增1，获取偏移量为0的4位无符号整数。\n&gt; BITFIELD mykey INCRBY i8 100 1 GET u4 01) (integer) 12) (integer) 0\n该命令支持的子命令有：\n\nGET\nSET\nINCRBY\n\n另外还有一个OVERFLOW命令用来进行INCRBY后的益处控制。下面是OVERFLOW的三种控制方法，默认为WRAP算法。\n\nWRAP：回环算法，适用于有符号和无符号两种类型。对于无符号整型，回环计数将对整型最大值进行取模操作；对有符号整数，上溢从最小负数开始，下溢从最大正数开始。例如，i8最大为127，加1后变成-128。\nSAT：饱和算法，上溢后保持最大整数，下溢后保持最小整数。\nFAIL：失败算法，这种模式下发生上溢或者下溢，不会做任何操作，返回值为NULL。\n\n该命令的偏移量有两种指定方式，如果是不带前缀的数字，则以字符串位计算，如果数字前有#前缀，则计算偏移量时应该指定数字乘以整型宽度。\nBITOP最早可用版本：2.6.0\n时间复杂度：O(N)\n用法：BITOP operation destkey key [key …]\n对一个或者字符串进行位操作，支持与(AND)、或(OR)、非(NOT)、异或(XOR)操作。除了非操作，其他的都支持多个key作为输入。对于长度不同的字符串，较短的字符串缺少的部分会以0补齐，空key也会被看作全部为0的字符串序列。\n该命令返回保存到destkey的字符串长度，也就是输入字符串的最大长度。\n127.0.0.1:6379&gt; set key1 &quot;abcde&quot;OK127.0.0.1:6379&gt; set key2 &quot;abcd&quot;OK127.0.0.1:6379&gt; BITOP and dest key1 key2(integer) 5127.0.0.1:6379&gt; get dest&quot;abcd\\x00&quot;\nBITPOS最早可用版本：2.8.7\n时间复杂度：O(N)\n用法：BITPOS key bit [start][end]\n该命令用于返回第一个被设置为0或1的位置。可以使用start和end参数指定查询范围，需要注意的是，这个范围指的是字节范围而不是位范围，也就是说start=0，end=2表示在前三个字节中查找。start和end都可以为负值，-1表示最后一位，-2表示倒数第二位，以此类推。\n我们通过一些例子来看一下某些特殊情况下的返回值。\n127.0.0.1:6379&gt; set mykey &quot;&quot;OK127.0.0.1:6379&gt; get mykey&quot;&quot;127.0.0.1:6379&gt; bitpos mykey 0(integer) -1127.0.0.1:6379&gt; bitpos mykey 1(integer) -1127.0.0.1:6379&gt; set key1 &quot;\\xff&quot;OK127.0.0.1:6379&gt; bitpos key1 1(integer) 0127.0.0.1:6379&gt; bitpos key1 0(integer) 8\n如果是空字符串，那么查找0和1都会返回-1。如果是类似”\\xff”这样的字符串，它的0-7位都是1，如果查询0时，会返回再往右数一位也就是第8位。\nDECR最早可用版本：1.0.0\n时间复杂度：O(1)\n对指定的key进行减1操作，操作数最大为64位有符号整数。如果key不存在，则会先将其设置为0，如果类型不符合，则会抛出错误。\n127.0.0.1:6379&gt; GET unexist(nil)127.0.0.1:6379&gt; DECR unexist(integer) -1127.0.0.1:6379&gt; SET mykey &quot;fdsfe&quot;OK127.0.0.1:6379&gt; DECR mykey(error) ERR value is not an integer or out of range\nDECRBY最早可用版本：1.0.0\n时间复杂度：O(1)\n这个命令与DECR的参数要求和使用方法相同，唯一不同的是它用来减去指定的数值。\nGET最早可用版本：1.0.0\n时间复杂度：O(1)\n这个不做过多介绍，是最常用的命令之一。返回指定key的值，如果不是字符串，就返回错误。\nGETBIT最早可用版本：2.2.0\n时间复杂度：O(1)\n返回指定偏移量位的bit值，当key不存在时，返回0。\nGETRANGE最早可用版本：2.4.0\n时间复杂度：O(N)\n用法：GETRANGE key start end\n这个命令在Redis2.0之前叫做SUBSTR，返回指定的key的指定范围（包含start和end）的子串。start和end同样也可以是负数，这点可以参考BITPOS命令。\nGETSET最早可用版本：1.0.0\n时间复杂度：O(1)\n自动把新的value保存到指定key中，并且返回旧的value。如果key存在，但是保存的数据不是字符串则会报错。\nINCR最早可用版本：1.0.0\n时间复杂度：O(1)\n该命令用于对指定key进行加1操作，与DECR命令正好相反。执行此操作时，字符串被解析为10进制的64位有符号整数。由于Redis内部有整数形式（integer representation）来保存整数，因此不会有整数存储为字符串的额外开销。\nINCRBY最早可用版本：1.0.0\n时间复杂度：O(1)\n它与INCR命令的关系就像DECR命令和DECRBY命令的关系一样，只是指定了要加的数值。\nINCRBYFLOAT最早可用版本：2.6.0\n时间复杂度：O(1)\n该命令会把字符串解析为浮点数，然后加上指定的浮点数。如果value不是字符串类型或者不能解析为浮点数，则会报错。返回值的精度为小数点后17位。其内部以科学计数法的形式存储。\nMGET最早可用版本：1.0.0\n时间复杂度：O(N)，N为取回key的个数\n该命令返回多个key的值，对于不是string类型或者不存在的key，都返回nil。\nMSET最早可用版本：1.0.1\n时间复杂度：O(N)，N为需要设置的key的个数\n设置所有的key，如果已经存在，则覆盖旧值。MSET命令是原子操作，并且不会失败。\nMSETNX最早可用版本：1.0.1\n时间复杂度：O(N)，N为需要设置的key的个数\n设置所有的key，如果有一个key已经存在，则所有的key都会设置不成功。返回1表示所有的key都已经设置成功，返回0表示所有的key都没有设置成功。\nPSETEX最早可用版本：2.6.0\n时间复杂度：O(1)\n用法：PSETEX key milliseconds value\n该命令类似于SETEX（在后面介绍），唯一不同的时，该命令设置过期时间以毫秒为单位。\nSET最早可用版本：1.0.0\n时间复杂度：O(1)\n用法：SET key value [EX seconds][PX milliseconds][NX|XX]\n也是最常用的命令之一。在2.6.12版本，SET命令加上了一些参数：\n\nEX seconds – 设置键key的过期时间，单位时秒\nPX milliseconds – 设置键key的过期时间，单位时毫秒\nNX – 只有键key不存在的时候才会设置key的值\nXX – 只有键key存在的时候才会设置key的值\n\n加上这参数之后，SET命令已经取代了SETNX、SETEX、PSETEX这三个命令，因此，Redis不再推荐使用这些命令，并且有可能在未来版本中抛弃这些命令。\nSETBIT最早可用版本：2.2.0\n时间复杂度：O(1)\n用法：SETBIT key offset value\n设置指定位置的bit值。当key不存在时，会先生成一个字符串，这个字符串必须保证offset处有值。offset必须大于0，小于232（因为bitmap的大小限制为512M）。\nSETEX最早可用版本：2.0.0\n时间复杂度：O(1)\n为指定key设置value，并且给定超时时间（单位是秒），SETEX是原子操作。\nSETNX最早可用版本：1.0.0\n时间复杂度：O(1)\nSETNX是”SET if Not Exist”的缩写，也就是说，当key不存在时，才会SET成功，成功返回1，失败返回0。\nSETRANGE最早可用版本：2.2.0\n时间复杂度：O(1)\n这个命令用来覆盖key的一部分内容，如果offset超出value的长度，则会为string补0。offset最大值是229 -1 (536870911)。\n127.0.0.1:6379&gt; SET follow jackeyOK127.0.0.1:6379&gt; SETRANGE follow 8 lol(integer) 11127.0.0.1:6379&gt; GET follow&quot;jackey\\x00\\x00lol&quot;\nSTRLEN最早可用版本：2.2.0\n时间复杂度：O(1)\n返回指定key存储的value的长度，如果value不是字符串，则会报错。如果key不存在，则返回0。\n","tags":["Redis命令"]},{"title":"Redis命令详解：Transactions","url":"/2019/03/04/Redis%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%9ATransactions/","content":"Redis的事务和我们常见的数据库的事务最大的区别就是，Redis的事务中如果有一个命令执行失败，其他命令仍然可以执行成功。Redis的事务以MULTI开始，由EXEC触发。在EXEC前的操作都将被放入缓存队列中。在事务执行过程中其他客户端的命令不会插到事务中执行。下面就来介绍一下Redis事务相关的命令。\nDISCARD最早可用版本：2.0.0\n放弃所有队列中的命令，将连接状态置为正常状态。如果事务被WATCH，则取消所有的WATCH。\nEXEC最早可用版本：1.2.0\n执行队列中的全部命令，将连接状态置为正常状态。如果某些key处于被监视状态，并且队列中有和这些key相关的命令。那么EXEC命令只有在这些key的值没有变化的情况下事务才会执行，否则事务被打断。\nMULTI最早可用版本：1.2.0\n标记事务块的开始，之后的命令被顺序插入缓存队列中，可以用EXEC命令执行这些命令。\nUNWATCH最早可用版本：2.2.0\n时间复杂度：O(1)\n清除掉所有被WATCH的key，如果调用了EXEC或者DISCARD命令，则不用手动调用UNWATCH命令。\nWATCH最早可用版本：2.2.0\n时间复杂度：对每个都是O(1)\n将指定的key标记为被监视状态，如果事务执行前被改动，则事务会被打断。\n最后举一个事务被打断的栗子\n127.0.0.1:6379&gt; SET lock_time 1OK127.0.0.1:6379&gt; WATCH lock_timeOK127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; SET transcation_key z #这时另一个客户端执行了命令 SET lock_time 2QUEUED127.0.0.1:6379&gt; INCR lock_timeQUEUED127.0.0.1:6379&gt; EXEC(nil)127.0.0.1:6379&gt; GET transcation_key(nil)","tags":["Redis命令"]},{"title":"Redis总结篇","url":"/2019/09/07/Redis%E6%80%BB%E7%BB%93%E7%AF%87/","content":"Redis的文章已经写了很长时间了，在这期间，也依靠对Redis的熟悉在面试过程中获得了一些加分。在新的工作中也面临了新的挑战，因此决定对Redis的文章暂时告一段落，这里也对之前的学习进行一下总结。\n第一步首先，我们要了解什么是Redis，并尝试安装Redis，以方便后面进行一些试验。然后就是掌握最基础的数据结构，这在《Redis基础数据结构》一文中都有介绍。\n命令在有了基础之后，就可以开始尝试进行一些实际操作。对Redis命令的了解是少不了的。各个命令按照功能可以分为以下类别：\n\nConnection\nKeys\nStrings\nHashs\nLists\nSets\nSorted Sets\nHyperLogLog\nTransactions\nServer\nStreams\nPub/Sub\nCluster\nGeo\nScripting\n\n在有了这些基础后，我们知道了生产环境中是禁止使用keys命令的，通常使用scan命令来查询/遍历key。所以我们在《深入理解Redis的scan命令》一文中对SCAN命令有了更详细的介绍。\n集群当然，只知道这些还不够，在实际工作中，Redis通常以集群的方式部署，所以我们又介绍了部署Redis集群的三种方式。其中包括：\n\n哨兵模式：《玩转Redis集群之Sentinel》\nCodis代理：《玩转Redis集群之Codis》\n官方集群Cluster：《玩转Redis集群之Cluster》\n\n开发Codis的团队现在还做了分布式MySQL——TiDB，感兴趣的同学可以了解一下。\n源码学到这里，你已经学会了“怎么用”，但是作为一名优秀的程序员，一定不能就此止步，还应该知道你用的东西究竟是怎么做出来的。因此，我们一起走近了源码，对Redis命令执行过程，以及一些底层存储方式做了更加深入的了解。\n走近源码：Redis的启动过程\n走近源码：Redis如何执行命令\n走近源码：Redis命令执行过程（客户端）\n走近源码：神奇的HyperLogLog\n走近源码：压缩列表是怎样炼成的\n走近源码：Redis跳跃列表究竟怎么跳\n其他最后，我们还了解了一些其他的技术，包括管道、Lua以及Redis的通信协议。\n速度不够，管道来凑——Redis管道技术\nRedis Lua脚本小学教程\nRedis Lua脚本中学教程（上）\nRedis Lua脚本中学教程（下）\nRedis Lua脚本大学教程\n浅谈Redis通信协议\n未来Redis的相关知识远远不止这些，所以我还要和大家一起继续学习。这里推荐一些学习资料：\n\nRedis官网\n作者antirez的博客\nRedis设计与实现\n老钱的Redis小册\n\n\n","tags":["Redis"]},{"title":"Redis基础数据结构","url":"/2018/09/17/Redis%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","content":"Redis是一款完全免费的，高性能的key-value数据库，目前被大多数公司用来做缓存。Redis作为一个内存数据库，它的读写速度非常快：读速度可以达到110000次/s，写的速度是81000次/s 。相比于其他key-value数据库，Redis的另一大特性就是支持多种数据类型。今天我们来一起聊一聊Redis的5种基础数据类型。\n安装Redis在学习之前，我们要先自己安装一个Redis环境用来自己动手操作，感受一下。\nWindows下载地址：https://github.com/MSOpenTech/redis/releases\n\nWindows用户可以在这个地址下载相应版本的压缩包，在C盘进行解压，解压后，将目录重命名为redis。在cmd中进入该目录，然后运行redis-server.exe redis.windows.conf。另外，也可以把目录加到环境变量中，这样就不需要再cd进入这个目录了。\nRedis的server安装好后，再打开一个新的cmd， 运行redis-cli.exe -h 127.0.0.1 -p 6379，就可以开始进行操作了。其中-h参数表示host，-p参数表示port，可以省略，默认是6379。\nLinux$ wget http://download.redis.io/releases/redis-4.0.11.tar.gz$ tar xzf redis-4.0.11.tar.gz$ cd redis-4.0.11$ make\n执行以上命令下载并安装Redis，接着进入src目录，运行redis-server。再执行 \n$ ./redis-cli \n命令，就可以开始操作了。\nUbuntuUbuntu可以直接使用apt-get安装\n$sudo apt-get update$sudo apt-get install redis-server\n启动方法这里不再赘述。\nMacMac用户可以使用homebrew安装Redis\nbrew install redis\n其他方法除了上述方法以外，我们还可以从GitHub下载源码，对源码进行编译。URL是git@github.com:antirez/redis.git。也可以从官网下载Docker，通过运行Docker来操作。\n基础数据类型Redis支持5种基础数据类型，下面我们来一一介绍，由于我本身是Java程序员，因此会将这些数据类型与Java中的数据类型进行类比。当然，你也可以拿自己熟悉的语言来理解。\nStringString是最基本的，也是最常用的类型。它是二进制安全的，也就是说，我们可以将对象序列化成json字符串作为value值存入Redis。在分配内存时，Redis会为一个字符串分配一些冗余的空间，以避免因字符串的值改变而出现频繁的内存分配操作。当字符串长度小于1M时，每次扩容都会加倍现有空间，当长度大于1M时，每次扩容，增加1M，Redis字符串的最大长度是512M。\nHashHash是键值对集合，相当于Java中的HashMap，实际结构也和HashMap一样，是数组+链表的结构。所不同的是扩容的方式不同，HashMap是进行一次rehash，而Redis为了不阻塞服务，会创建一个新的数组，在查询时会同时查询两个Hash，然后在逐渐将旧的Hash内容转移到新的中去。一个Hash最大可以存储232-1个键值对。\nListList相当于Java中的LinkedList，它的插入和删除操作的时间复杂度为O(1)，而查询操作的时间复杂度为O(n)。我们可以利用List的rpush、rpop、lpush和lpop命令来构建队列或者栈。列表最多可以存储232-1个元素。\nSetSet是String类型的无序集合，并且元素唯一，相当于Java中的HashSet，它的插入、删除、查询操作的时间复杂度都是O(1)。其最大元素数也是232-1个。\nzsetzset可以看做是Java中SortedSet和HashMap的结合，一方面它不允许元素重复，另一方面，它通过score为每个元素进行排序。\n两个规则对于以上5种数据结构，有两个通用的规则：\n\n如果不存在，就先创建，再进行操作\n如果元素为空，就会释放内存\n\n过期时间我们可以对上面所有的类型设置过期时间，如果时间到了，Redis 会自动删除相应的对象。\n小结本文简单介绍了Redis的安装方法和Redis的5中基本数据结构。主要目的是帮助没有基础的同学快速入门，对于已经了解Redis的同学也是知识的巩固，想要了解更多关于Redis的知识，可以持续关注我，后面还有更精彩的内容分享给大家。\n","tags":["Redis"]},{"title":"Rust入坑指南：万物初始","url":"/2020/04/08/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E4%B8%87%E7%89%A9%E5%88%9D%E5%A7%8B/","content":"有没有同学记得我们一起挖了多少个坑？嗯…其实我自己也不记得了，今天我们再来挖一个特殊的坑，这个坑可以说是挖到根源了——元编程。\n元编程是编程领域的一个重要概念，它允许程序将代码作为数据，在运行时对代码进行修改或替换。如果你熟悉Java，此时是不是想到了Java的反射机制？没错，它就是属于元编程的一种。\n反射Rust也同样支持反射，Rust的反射是由标准库中的std::any::Any包支持的。\n这个包中提供了以下几个方法\n\nTypeId是Rust中的一种类型，它被用来表示某个类型的唯一标识。type_id(&amp;self)这个方法返回变量的TypeId。\nis()方法则用来判断某个函数的类型。\n可以看一下它的源码实现\npub fn is&lt;T: Any&gt;(&amp;self) -&gt; bool &#123;  let t = TypeId::of::&lt;T&gt;();  let concrete = self.type_id();  t == concrete&#125;\n可以看到它的实现非常简单，就是对比TypeId。\ndowncast_ref()和downcast_mut()是一对用于将泛型T转换为具体类型的方法。其返回的类型是Option&lt;&amp;T&gt;和Option&lt;&amp;mut T&gt;，也就是说downcast_ref()将类型T转换为不可变引用，而downcast_mut()将T转换为可变引用。\n最后我们通过一个例子来看一下这几个函数的具体使用方法。\nuse std::any::&#123;Any, TypeId&#125;;fn main() &#123;    let v1 = &quot;Jackey&quot;;    let mut a: &amp;Any;    a = &amp;v1;    println!(&quot;&#123;:?&#125;&quot;, a.type_id());    assert!(a.is::&lt;&amp;str&gt;());    print_any(&amp;v1);    let v2: u32 = 33;    print_any(&amp;v2);&#125;fn print_any(any: &amp;Any) &#123;    if let Some(v) = any.downcast_ref::&lt;u32&gt;() &#123;        println!(&quot;u32 &#123;:x&#125;&quot;, v);    &#125; else if let Some(v) = any.downcast_ref::&lt;&amp;str&gt;() &#123;        println!(&quot;str &#123;:?&#125;&quot;, v);    &#125; else &#123;        println!(&quot;else&quot;);    &#125;&#125;\n宏Rust的反射机制提供的功能比较有限，但是Rust还提供了宏来支持元编程。\n到目前为止，宏对我们来说是一个既熟悉又陌生的概念，熟悉是因为我们一直在使用println!宏，陌生则是因为我们从没有详细介绍过它。\n对于println!宏，我们直观上的使用感受是它和函数差不多。但两者之间还是有一定的区别的。\n我们知道对于函数，它接收参数的个数是固定的，并且在函数定义时就已经固定了。而宏接收的参数个数则是不固定的。\n这里我们说的宏都是类似函数的宏，此外，Rust还有一种宏是类似于属性的宏。它有点类似于Java中的注解，通常作为一种标记写在函数名上方。\n#[route(GET, &quot;/&quot;)]fn index() &#123;\nroute在这里是用来指定接口方法的，对于这个服务来讲，根路径的GET请求都被路由到这个index函数上。这样的宏是通过属于过程宏，它的定义使用了#[proc_macro_attribute]注解。而函数类似的过程宏在定义时使用的注解是#[proc_macro]。\n除了过程宏以外，宏的另一大分类叫做声明宏。声明宏是通过macro_rules!来声明定义的宏，它比过程宏的应用要更加广泛。我们曾经接触过的vec!就是声明宏的一种。它的定义如下：\n#[macro_export]macro_rules! vec &#123;    ( $( $x:expr ),* ) =&gt; &#123;        &#123;            let mut temp_vec = Vec::new();            $(                temp_vec.push($x);            )*            temp_vec        &#125;    &#125;;&#125;\n下面我们来定义一个属于自己的宏。\n自定义宏需要使用derive注解。（例子来自the book）\n我们先来创建一个叫做hello_macro的lib库，只定义一个trait。\npub trait HelloMacro &#123;    fn hello_macro();&#125;\n接着再创建一个子目录hello_macro_derive，在hello_macro_derive/Cargo.toml文件中添加依赖\n[lib]proc-macro = true[dependencies]syn = &quot;0.14.4&quot;quote = &quot;0.6.3&quot;\n然后就可以在hello_macro_derive/lib.rs文件中定义我们自定义宏的功能实现了。\nextern crate proc_macro;use crate::proc_macro::TokenStream;use quote::quote;use syn;#[proc_macro_derive(HelloMacro)]pub fn hello_macro_derive(input: TokenStream) -&gt; TokenStream &#123;    // Construct a representation of Rust code as a syntax tree    // that we can manipulate    let ast = syn::parse(input).unwrap();    // Build the trait implementation    impl_hello_macro(&amp;ast)&#125;fn impl_hello_macro(ast: &amp;syn::DeriveInput) -&gt; TokenStream &#123;    let name = &amp;ast.ident;    let gen = quote! &#123;        impl HelloMacro for #name &#123;            fn hello_macro() &#123;                println!(&quot;Hello, Macro! My name is &#123;&#125;&quot;, stringify!(#name));            &#125;        &#125;    &#125;;    gen.into()&#125;\n这里使用了两个crate：syn和quote，其中syn是把Rust代码转换成一种特殊的可操作的数据结构，而quote的作用则与它刚好相反。\n可以看到，我们自定义宏使用的注解是#[proc_macro_derive(HelloMacro)]，其中HelloMacro是宏的名称，在使用时，我们只需要使用注解#[derive(HelloMacro)]即可。\n在使用时我们应该先引入这两个依赖\nhello_macro = &#123; path = &quot;../hello_macro&quot; &#125;hello_macro_derive = &#123; path = &quot;../hello_macro/hello_macro_derive&quot; &#125;\n然后再来使用\nuse hello_macro::HelloMacro;use hello_macro_derive::HelloMacro;#[derive(HelloMacro)]struct Pancakes;fn main() &#123;    Pancakes::hello_macro();&#125;\n运行结果显示，我们能够成功在实现中捕获到结构体的名字。\n\n总结我们在本文中先后介绍了Rust的两种元编程：反射和宏。其中反射提供的功能能力较弱，但是宏提供的功能非常强大。我们所介绍的宏的相关知识其实只是皮毛，要想真正理解宏，还需要花更多的时间学习。\n","tags":["Rust"]},{"title":"Rust入坑指南：亡羊补牢","url":"/2019/12/30/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E4%BA%A1%E7%BE%8A%E8%A1%A5%E7%89%A2/","content":"如果你已经开始学习Rust，相信你已经体会过Rust编译器的强大。它可以帮助你避免程序中的大部分错误，但是编译器也不是万能的，如果程序写的不恰当，还是会发生错误，让程序崩溃。所以今天我们就来聊一聊Rust中如何处理程序错误，也就是所谓的“亡羊补牢”。\n基础概念在编程中遇到的非正常情况通常可以分为三类：失败、错误、异常。\nRust中用两种方式来消除失败：强大的类型系统和断言。\n对于类型系统，熟悉Java的同学应该比较清楚。例如我们给一个接收参数为int的函数传入了字符串类型的变量。这是由编译器帮我们处理的。\n\n关于断言，Rust支持6种断言。分别是：\n\nassert!\nassert_eq!\nassert_ne!\ndebug_assert!\ndebug_assert_eq!\ndebug_assert_ne!\n\n从名称我们就可以看出来这6种断言，可以分为两大类，带debug的和不带debug的，它们的区别就是assert开头的在调试模式和发布模式下都可以使用，而debug开头的只可以在调试模式下使用。再来解释每个大类下的三种断言，assert!是用于断言布尔表达式是否为true，assert_eq!用于断言两个表达式是否相等，assert_ne!用于断言两个表达式是否不相等。当不符合条件时，断言会引发线程恐慌（panic!）。\nRust处理异常的方法有4种：Option、Result&lt;T, E&gt;、线程恐慌（Panic）、程序终止（Abort）。接下来我们对这些方法进行详细介绍。\nOptionOption我们在Rust入坑指南：千人千构一文中我们进行过一些介绍，它是一种枚举类型，主要包括两种值：Some(T)和None，Rust也是靠它来避免空指针异常的。\n在前文中，我们并没有详细介绍如何从Option中提取出T，其实最基本的，可以用match来提取。而我也在前文中给你提供了官方文档的链接，不知道你有没有看。如果还没来得及看也没有关系，我把我看到的一些方法分享给你。\n这里介绍两种方法，一种是expect，另一种是unwrap系列的方法。我们通过一个例子来感受一下。\nfn main() &#123;    let a = Some(&quot;a&quot;);    let b: Option&lt;&amp;str&gt; = None;    assert_eq!(a.expect(&quot;a is none&quot;), &quot;a&quot;);    assert_eq!(b.expect(&quot;b is none&quot;), &quot;b is none&quot;);  //匹配到None会引起线程恐慌，打印的错误是expect的参数信息    assert_eq!(a.unwrap(), &quot;a&quot;);   //如果a是None，则会引起线程恐慌    assert_eq!(b.unwrap_or(&quot;b&quot;), &quot;b&quot;); //匹配到None时返回指定值    let k = 10;    assert_eq!(Some(4).unwrap_or_else(|| 2 * k), 4);// 与unwrap_or类似，只不过参数是FnOnce() -&gt; T    assert_eq!(None.unwrap_or_else(|| 2 * k), 20);&#125;\n这是从Option中提取值的方法，有时我们会觉得每次处理Option都需要先提取，然后再做相应计算这样的操作比较麻烦，那么有没有更加高效的操作呢？答案是肯定的，我从文档中找到了map和and_then这两种方法。\n其中map方法和unwrap一样，也是一系列方法，包括map、map_or和map_or_else。map会执行参数中闭包的规则，然后将结果再封为Option并返回。\nfn main() &#123;    let some_str = Some(&quot;Hello!&quot;);    let some_str_len = some_str.map(|s| s.len());    assert_eq!(some_str_len, Some(6));&#125;\n但是，如果参数本身返回的结果就是Option的话，处理起来就比较麻烦，因为每执行一次map都会多封装一层，最后的结果有可能是Some(Some(Some(…)))这样N多层Some的嵌套。这时，我们就可以用and_then来处理了。\n利用and_then方法，我们就可以有如下的链式调用：\nfn main() &#123;    assert_eq!(Some(2).and_then(sq).and_then(sq), Some(16));&#125;fn sq(x: u32) -&gt; Option&lt;u32&gt; &#123;     Some(x * x) &#125;\n关于Option我们就先聊到这里，大家只需要记住，它可以用来处理空值，然后能够使用它的一些处理方法就可以了，实在记不住这些方法，也可以在用的时候再去文档中查询。\nResult&lt;T, E&gt;聊完了Option，我们再来看另一种错误处理方法，它也是一个枚举类型，叫做Result&lt;T, E&gt;，定义如下：\n#[must_use = &quot;this `Result` may be an `Err` variant, which should be handled&quot;]pub enum Result&lt;T, E&gt; &#123;    Ok(T),    Err(E),&#125;\n实际上，Option可以被看作Result&lt;T, ()&gt;。从定义中我们可以看到Result&lt;T, E&gt;有两个变体：Ok(T)和Err(E)。\nResult&lt;T, E&gt;用于处理真正意义上的错误，例如，当我们想要打开一个不存在的文件时，或者我们想要将一个非数字的字符串转换为数字时，都会得到一个Err(E)结果。\nResult&lt;T, E&gt;的处理方法和Option类似，都可以使用unwrap和expect方法，也可以使用map和and_then方法，并且用法也都类似，这里就不再赘述了。具体的方法使用细节可以自行查看官方文档。\n这里我们来看一下如何处理不同类型的错误。\nRust在std::io模块定义了统一的错误类型Error，因此我们在处理时可以分别匹配不同的错误类型。\nuse std::fs::File;use std::io::ErrorKind;fn main() &#123;    let f = File::open(&quot;hello.txt&quot;);    let f = match f &#123;        Ok(file) =&gt; file,        Err(error) =&gt; match error.kind() &#123;            ErrorKind::NotFound =&gt; match File::create(&quot;hello.txt&quot;) &#123;                Ok(fc) =&gt; fc,                Err(e) =&gt; panic!(&quot;Problem creating the file: &#123;:?&#125;&quot;, e),            &#125;,            ErrorKind::PermissionDenied =&gt; panic!(&quot;Permission Denied!&quot;),            other_error =&gt; panic!(&quot;Problem opening the file: &#123;:?&#125;&quot;, other_error),        &#125;,    &#125;;&#125;\n在处理Result&lt;T, E&gt;时，我们还有一种处理方法，就是try!宏。它会使代码变得非常精简，但是在发生错误时，会将错误返回，传播到外部调用函数中，所以我们在使用之前要考虑清楚是否需要传播错误。\n对于上面的代码，使用try!宏就会非常精简。\nuse std::fs::File;fn main() &#123;    let f = try!(File::open(&quot;hello.txt&quot;));&#125;\ntry!使用起来虽然简单，但也有一定的问题。像我们刚才提到的传播错误，再就是有可能出现多层嵌套的情况。因此Rust引入了另一个语法糖来代替try!。它就是问号操作符“?”。\nuse std::fs::File;use std::io;use std::io::Read;fn main() &#123;    read_username_from_file();&#125;fn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; &#123;    let mut f = File::open(&quot;hello.txt&quot;)?;    let mut s = String::new();    f.read_to_string(&amp;mut s)?;    Ok(s)&#125;\n问号操作符必须在处理错误的代码后面，这样的代码看起来更加优雅。\n恐慌（Panic）我们从最开始就聊到线程恐慌，那道理什么是恐慌呢？\n在Rust中，无法处理的错误就会造成线程恐慌，手动执行panic!宏时也会造成恐慌。当程序执行panic!宏时，会打印相应的错误信息，同时清理堆栈并退出。但是栈回退和清理会花费大量的时间，如果你想要立即终止程序，可以在Cargo.toml文件中[profile]区域中增加panic = &#39;abort&#39;，这样当发生恐慌时，程序会直接退出而不清理堆栈，内存空间都由操作系统来进行回收。\n程序报错时，如果你想要查看完整的错误栈信息，可以通过设置环境变量RUST_BACKTRACE=1的方式来实现。\n如果程序发生恐慌，我们前面所说的Result&lt;T, E&gt;就不能使用了，Rust为我们提供了catch_unwind方法来捕获恐慌。\nuse std::panic;fn main() &#123;    let result = panic::catch_unwind(|| &#123;panic!(&quot;crash and burn&quot;)&#125;);    assert!(result.is_err());    println!(&quot;&#123;&#125;&quot;, 1 + 2);&#125;\n在上面这段代码中，我们手动执行一个panic宏，正常情况下，程序会在第一行退出，并不会执行后面的代码。而这里我们用了catch_unwind方法对panic进行了捕获，结果如图所示。\n\nRust虽然打印了恐慌信息，但是并没有影响程序的执行，我们的代码println!(&quot;&#123;&#125;&quot;, 1 + 2);可以正常执行。\n总结至此，Rust处理错误的方法我们已经基本介绍完了，为什么说是基本介绍完了呢？因为还有一些大佬开发了一些第三方库来帮助我们更加方便的处理错误，其中比较有名的有error-chain和failure，这里就不做过多介绍了。\n通过本节的学习，相信你的Rust程序一定会变得更加健壮。\n","tags":["Rust"]},{"title":"Rust入坑指南：千人千构","url":"/2019/10/27/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8D%83%E4%BA%BA%E5%8D%83%E6%9E%84/","content":"坑越来越深了，在坑里的同学让我看到你们的双手！\n前面我们聊过了Rust最基本的几种数据类型。不知道你还记不记得，如果不记得可以先复习一下。上一个坑挖好以后，有同学私信我说坑太深了，下来的时候差点崴了脚。我只能对他说抱歉，下次还有可能更深。不过这篇文章不会那么深了，本文我将带大家探索Structs和Enums这两个坑，没错，是双坑。是不是很惊喜？好了，言归正传。我们先来介绍Structs。\nStructsStructs在许多语言里都有，是一种自定义的类型，可以类比到Java中的类。Rust中使用Structs使用的是struct关键字。例如我们定义一个用户类型。\nstruct User &#123;    username: String,    email: String,    sign_in_count: u64,    active: bool,&#125;\n初始化时可以直接将上面对应的数据类型替换为正确的值。\nfn build_user(email: String, username: String) -&gt; User &#123;    User &#123;        email: email,        username: username,        active: true,        sign_in_count: 1,    &#125;&#125;\n下面仔细观察这email: email和username: username这两行代码，有没有觉得有点麻烦？，如果User的所有属性值都是从函数参数传进来，那么我们每个参数名都要重复一遍。还好Rust为我们提供了语法糖，可以省去一些代码。\n初始化Struct时省去变量名对于上面的初始化代码，我们可以做一些简化。\nfn build_user(email: String, username: String) -&gt; User &#123;    User &#123;        email,        username,        active: true,        sign_in_count: 1,    &#125;&#125;\n你可以认为这是Rust的一个语法糖，当变量名和字段名相同时，初始化Struct的时候就可以省略变量名。让开发者不必做过多无意义的重复工作（写两遍email）。\n在其他实例的基础上创建Struct除了上面的语法糖以外，在创建Struct时，Rust还提供了另一个语法糖，例如我们新建一个user2，它只有邮箱和用户名与user1不同， 其他属性都相同，那么我们可以使用如下代码：\n#![allow(unused_variables)]fn main() &#123;struct User &#123;    username: String,    email: String,    sign_in_count: u64,    active: bool,&#125;let user1 = User &#123;    email: String::from(&quot;someone@example.com&quot;),    username: String::from(&quot;someusername123&quot;),    active: true,    sign_in_count: 1,&#125;;let user2 = User &#123;    email: String::from(&quot;another@example.com&quot;),    username: String::from(&quot;anotherusername567&quot;),    ..user1&#125;;&#125;\n这里的..user1表示剩下的字段的值都和user1相同。\nTuple Struct接下来再来介绍两个特殊形式的Struct，一种是Tuple Struct，定义时与Tuple相似\nstruct Color(i32, i32, i32);struct Point(i32, i32, i32);\n它与Tuple的不同在于，你可以赋予Tuple Struct一个有意义的名字，而不只是无意义的一堆值。需要注意的是，这里我们定义的Color和Point是两种不同的类型，它们之间不能互相赋值。另外，如果你想要取得Tuple Struct中某个字段的值，和Tuple一样，使用.即可。\n空字段Struct这里还有一种特殊的Struct，即没有字段的Struct。它叫做类单元结构（unit-like structs）。这种结构体一般用于实现某些特征，但又没有需要存储的数据。\nStruct 方法方法和函数非常相似，不同之处在于，定义方法时，必须有与之关联的Struct，并且方法的第一个参数必须是self。我们先来看一下如何定义一个方法：\nstruct Rectangle &#123;    width: u32,    height: u32,&#125;impl Rectangle &#123;    fn area(&amp;self) -&gt; u32 &#123;        self.width * self.height    &#125;&#125;\n我们提到，方法必须与Struct关联，这里使用impl关键字定义一段指定Struct的实现代码，然后在这个代码块中定义Struct相关的方法，注意我们的area方法符合规则，第一个参数是self。调用时只需要用.就可以。\nfn main() &#123;    let rect1 = Rectangle &#123; width: 30, height: 50 &#125;;\trect1.area();&#125;\n这里的&amp;self其实是代替了rectangle: &amp;Rectangle，至于这里为什么要使用&amp;符号，我们在前文 )已经做了介绍。当然，这里self也不是必须要加&amp;符号，你可以认为它是一个正常的参数，根据需要来使用。\n有些同学可能会有些困惑，我们已经有了函数了，为什么还要使用方法？这其实主要是为了代码的结构。我们需要将Struct实例可以做的操作都放到impl实现代码块中，方便修改和查找。而使用函数则可能存在开发人员随便找个位置来定义的尴尬情况，这对于后期维护代码的开发人员来讲将是一种灾难。\n现在我们已经知道，方法必须定义在impl代码块中，且第一个参数必须是self，但有时你会在Impl代码块中看到第一个参数不是self的，而且Rust也允许这种行为。\nimpl Rectangle &#123;    fn square(size: u32) -&gt; Rectangle &#123;        Rectangle &#123; width: size, height: size &#125;    &#125;&#125;\n这是什么情况？刚才说的不对？其实不然，这种函数叫做相关函数（associated functions）。它仍然是函数，而不是方法并且直接和Struct相关，类似于Java中的静态方法。调用时直接使用双冒号（::），我们之前见过很多次的String::from(&quot;Hi&quot;)就是String的相关函数。\n最后提一点，Rust支持为一个Struct定义多个实现代码块。但是我们并不推荐这样使用。\n至此，第一个坑Struct就挖好了，接下来就是第二个坑Enum。\nEnum很多编程语言都支持枚举类型，Rust也不例外。因此枚举对于大部分开发人员来说并不陌生，这里我们简单介绍一些使用方法及特性。\n先来看一下Rust中如何定义枚举和获取枚举值。\nenum IpAddrKind &#123;    V4,    V6,&#125;let six = IpAddrKind::V6;let four = IpAddrKind::V4;\n这里的例子只是最简单的定义枚举的方法，每个枚举的值也可以关联其他类型的的值。例如\nenum Message &#123;    Quit,    Move &#123; x: i32, y: i32 &#125;,    Write(String),    ChangeColor(i32, i32, i32),&#125;\n此外，Enum也可以像Struct拥有impl代码块，你也可以在里面定义方法。\nOption枚举Option是Rust标准库中定义的一个枚举。如果你用过Java8的话，一定知道一个Optional类，专门用来处理null值。Rust中是不存在null值的，因为它太容易引起bug了。但如果确实需要的时候怎么办呢，这就需要Option枚举登场了。我们先来看一看它的定义：\nenum Option&lt;T&gt; &#123;    Some(T),    None,&#125;\n很简单对不对。它是一个枚举，只有两个值，一个是Some，一个是None，其中Some还关联了一个类型T的值，这个T类似于Java中的泛型，即它可以是任意类型。\n在使用时，可以直接使用Some或None，前面不用加Option::。当你使用None时，必须要指定T的具体类型。\nlet some_number = Some(5);let some_string = Some(&quot;a string&quot;);let absent_number: Option&lt;i32&gt; = None;\n需要注意的是Option\\&lt;T>与T并不是相同的类型。你可以在官方文档中查看从Option\\&lt;T>中提取出T的方法。\nmatch流程控制Rust有一个很强大的流程控制操作叫做match，它有些类似于Java中的switch。首先匹配一系列的模式，然后执行相应的代码。与Java中switch不同的是，switch只能支持数值/枚举类型（现在也可以支持字符串），match可以支持任意类型。\nenum Coin &#123;    Penny,    Nickel,    Dime,    Quarter,&#125;fn value_in_cents(coin: Coin) -&gt; u8 &#123;    match coin &#123;        Coin::Penny =&gt; 1,        Coin::Nickel =&gt; 5,        Coin::Dime =&gt; 10,        Coin::Quarter =&gt; 25,    &#125;&#125;\n此外，match还可以支持模式中绑定值。\nenum UsState &#123;    Alabama,    Alaska,    // --snip--&#125;enum Coin &#123;    Penny,    Nickel,    Dime,    Quarter(UsState),&#125;fn value_in_cents(coin: Coin) -&gt; u8 &#123;    match coin &#123;        Coin::Penny =&gt; 1,        Coin::Nickel =&gt; 5,        Coin::Dime =&gt; 10,        Coin::Quarter(state) =&gt; &#123;            println!(&quot;State quarter from &#123;:?&#125;!&quot;, state);            25        &#125;,    &#125;&#125;\nmatch与Option\\&lt;T>前面我们聊到了从Option\\&lt;T>中提取T的值，我们来介绍一种通过match提取的方法。\nfn plus_one(x: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; &#123;    match x &#123;        None =&gt; None,        Some(i) =&gt; Some(i + 1),    &#125;&#125;let five = Some(5);let six = plus_one(five);let none = plus_one(None);\n这种方法在参数中必须声明T的具体类型，这里再思考一个问题，如果我们确定x一定不会是None，那么可不可以去掉None的那个条件？\n_占位符答案是不可以，Rust要求match必须列举出所有可能的条件。例如，如果一个u8类型的，就需要列举0到255这些条件。这样做的话，可能一天也写不了几个match语句吧。所以Rust又给我们准备了一个语法糖。\n针对上述情况，就可以写成下面这样：\nlet some_u8_value = 0u8;match some_u8_value &#123;    1 =&gt; println!(&quot;one&quot;),    3 =&gt; println!(&quot;three&quot;),    5 =&gt; println!(&quot;five&quot;),    7 =&gt; println!(&quot;seven&quot;),    _ =&gt; (),&#125;\n我们只需要列举我们关心的几种情况，然后用占位符_表示剩余所有情况。看到这我只想感叹一句，这糖真甜啊。\nif let对于我们只关心一个条件的match来讲，还有一种更加简洁的语法，那就是if let。\n举个栗子，我们只想要Option\\&lt;u8>中值为3时打印相关信息，利用我们已经掌握的知识，可以这样写。\nlet some_u8_value = Some(0u8);match some_u8_value &#123;    Some(3) =&gt; println!(&quot;three&quot;),    _ =&gt; (),&#125;\n如果用if let呢，就会更加简洁一些。\nif let Some(3) = some_u8_value &#123;    println!(&quot;three&quot;);&#125;\n这里要注意，当match只有一个条件时，才可以使用if let替代。\n有同学可能会问，既然叫if let，那么有没有else条件呢？答案是有的。对于下面这种情况\nlet mut count = 0;match coin &#123;    Coin::Quarter(state) =&gt; println!(&quot;State quarter from &#123;:?&#125;!&quot;, state),    _ =&gt; count += 1,&#125;\n如果替换成if let语句，应该是\nlet mut count = 0;if let Coin::Quarter(state) = coin &#123;    println!(&quot;State quarter from &#123;:?&#125;!&quot;, state);&#125; else &#123;    count += 1;&#125;\n总结第二个坑也挖好了，来总结一下吧。本文我们首先介绍了Struct，它类似于Java中的类，可以供开发人员自定义类型。然后介绍了两种初始化Struct时的简化代码的方法。接着是定义Struct相关的方法。在介绍完Struct以后，紧接着又介绍了大家都很熟悉的Enum枚举类型。重点说了Rust中特殊的枚举Option，然后介绍了match和if let这两种流程控制语法。\n最后，按照国际惯例，我还是要诚挚的邀请你早日入坑。坑里真的是冬暖夏凉~\n","tags":["Rust"]},{"title":"Rust入坑指南：坑主驾到","url":"/2019/09/21/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E5%9D%91%E4%B8%BB%E9%A9%BE%E5%88%B0/","content":"欢迎大家和我一起入坑Rust，以后我就是坑主，我主要负责在前面挖坑，各位可以在上面看，有手痒的也可以和我一起挖。这个坑到底有多深？我也不知道，我是抱着有多深就挖多深的心态来的，下面我先跳了，各位请随意。\nRust简介众所周知，在编程语言中，更易读的高级语言和控制底层资源的低级语言是一对矛盾体。Rust想要挑战这一现状，它尝试为开发者提供更好的体验的同时给予开发者控制底层细节的权限（比如内存使用）。\n低级语言在开发过程中很容易出现各种细微的错误，它们难以发现但是可能影响巨大。其他大部分低级语言只能靠覆盖面更广的测试用例和经验丰富的开发者来解决这些问题。而Rust则依靠严格的编译器来杜绝这些问题。\nPs：以后会见识到Rust编译器的「厉害」\nRust的一些工具：\n\nCargo，依赖包的管理和构建工具，可以帮你减轻添加、编译和管理依赖包的痛苦\nRustfmt，用于保证开发者代码风格的一致性\nRust语言服务器支持集成IDE（我用的是IDEA）\n\n安装Rust如果你的操作系统是Linux或macOS，在终端执行命令\n$ curl https://sh.rustup.rs -sSf | sh\n安装过程中的选项使用默认就好（一路回车），直到出现以下信息时，表示安装成功。\nRust is installed now. Great!\n安装脚本会自动把Rust添加到环境变量PATH中，可以重启终端或者手动执行命令使添加生效。\n$ source $HOME/.cargo/env\n当然也可以添加到你的.bash_profile文件中：\n$ export PATH=&quot;$HOME/.cargo/bin:$PATH&quot;\n最后，执行以下命令来检查Rust是否安装成功\n$ rustc --version\n另外，当你尝试编译Rust代码，但报了linker不可执行的错误时，你需要手动安装一个linker，C编译器通常会包含正确的linker。Rust的一些公共包也会依赖C语言代码和编译器。所以最好现在安装一个。\nIDEA集成RustIDEA中集成Rust也很简单，只需要在Preference-&gt;Plugins中搜索Rust，安装Rust插件后重启IDEA就可以了。\nHello World又到了经典的Hello World时间，这次我不想直接一个简单的print就结束了，我们一开始提到了Cargo是Rust依赖包的管理工具，所以我想体验一下Cargo的用法。\n首先新建一个项目，可以直接用在IDEA中new project，也可以使用Cargo命令\ncargo new hello-worldcd hello-world\n新建好项目以后，它的结构长这样子\n\n其中\n\nmain.rs是我们代码的入口文件\nCargo.toml是记录Rust元数据的文件，包括依赖。\nCargo.lock是记录增加依赖log的文件，不能手动修改。\n\n接着我们在Cargo.toml文件中添加我们需要的依赖\n[dependencies]ferris-says = &quot;0.1&quot;\n这时IDEA会自动安装依赖包，如果没有安装，也可以手动执行命令来安装\ncargo build\n依赖安装好以后，就可以开始写代码了：\nuse ferris_says::say;use std::io::&#123;stdout, BufWriter&#125;;fn main() &#123;    let stdout = stdout();    let out = b&quot;Hello World!&quot;;    let width = 12;    let mut writer = BufWriter::new(stdout.lock());    say(out, width, &amp;mut writer).unwrap();&#125;\n执行结果\n----------------| Hello World! |----------------              \\               \\                  _~^~^~_              \\) /  o o  \\ (/                &#x27;_   -   _&#x27;                / &#x27;-----&#x27; \\\n没错，这是一个小螃蟹，至于它是谁，来看看官方解释\n\nFerris is the unofficial mascot of the Rust Community. Many Rust programmers call themselves “Rustaceans,” a play on the word “crustacean.” We refer to Ferris with the pronouns “they,” “them,” etc., rather than with gendered pronouns.\nFerris is a name playing off of the adjective, “ferrous,” meaning of or pertaining to iron. Since Rust often forms on iron, it seemed like a fun origin for our mascot’s name!\nYou can find more images of Ferris on http://rustacean.net/.\n\n关于toml文件可能有些读者不太熟悉（其实我自己也不太熟），这里简单介绍一下吧，它的全称是「Tom’s Obvious, Minimal Language」，是一种配置文件格式。它的语义是比较明显的，因此易于阅读。同时格式可以明确的映射到hash表，所以也可以被多种语言轻松解析。\nGitHub地址是：https://github.com/toml-lang/toml\n有兴趣的同学可以做更深入的了解。\n后记至此，我确信自己已经跳进来了，有想跟进的朋友记得关注我哦。\n","tags":["Rust"]},{"title":"Rust入坑指南：居安思危","url":"/2020/03/31/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E5%B1%85%E5%AE%89%E6%80%9D%E5%8D%B1/","content":"任何事情都是相对的，就像Rust给我们的印象一直是安全、快速，但实际上，完全的安全是不可能实现的。因此，Rust中也是会有不安全的代码的。\n严格来讲，Rust语言可以分为Safe Rust和Unsafe Rust。Unsafe Rust是Safe Rust的超集。在Unsafe Rust中并不会禁用任何的安全检查，Unsafe Rust出现的原因是为了让开发者可以做一些更加底层的操作。这些事情本身也是不安全的，如果仍然要进行Rust的安全检查，那么就无法进行这些操作。\n在进行下面这5种操作时，Unsafe Rust不会进行安全检查。\n\n解引用原生指针\n调用unsafe的函数或方法\n访问或修改可变的静态变量\n实现unsafe的trait\n读写联合体中的字段\n\n基础语法Unsafe Rust的关键字是unsafe，它可以用来修饰函数、方法和trait，也可以用来标记代码块。\n标准库中也有不少函数是unsafe的。例如String中的from_utf8_unchecked()函数。它的定义如下：\npub unsafe fn from_utf8_unchecked(bytes: Vec&lt;u8&gt;) -&gt; String &#123;  String &#123; vec: bytes &#125;&#125;\n这个函数被标记为unsafe的原因是函数并没有检查传入参数是否是合法的UTF-8序列。也就是提醒使用者注意，使用这个函数要自己保证参数的合法性。\n用unsafe标记的trait也比较常见，在前面我们见过的Send和Sync都是unsafe的trait。它们被用来保证线程安全， 将其标记为unsafe是告诉开发者，如果自己实现这两个trait，那么代码就会有安全风险。\n我们在调用unsafe函数或方法时，需要使用unsafe代码块。\nfn main() &#123;    let sparkle_heart = vec![240, 159, 146, 150];        let sparkle_heart = unsafe &#123;        String::from_utf8_unchecked(sparkle_heart)    &#125;;    assert_eq!(&quot;💖&quot;, sparkle_heart);&#125;\n在了解了unsafe的基础语法之后，我们再来具体看看前面提到的5种操作。\n解引用原生指针Rust的原生指针分为两种：可变类型*mut T和不可变类型*const T。\n与引用和智能指针不同，原生指针具有以下特性：\n\n可以不遵循借用规则，在同一代码块中可以同时出现可变和不可变指针，也可以同时有多个可变指针\n不保证指向有效内存\n允许是null\n不会自动清理内存\n\n由这些特性可以看出，原生指针并不受Rust那一套安全规则的限制，因此，解引用原生指针是一种不安全的操作。换句话说，我们应该把这种操作放在unsafe代码块中。下面这段代码就展示了原生指针的第一条特性，以及如何解引用原生指针。\nfn main() &#123;    let mut num = 5;    let r1 = &amp;num as *const i32;    let r2 = &amp;mut num as *mut i32;    unsafe &#123;        println!(&quot;r1 is: &#123;&#125;&quot;, *r1);        println!(&quot;r2 is: &#123;&#125;&quot;, *r2);    &#125;&#125;\n在Rust编程中，原生指针常被用作和C语言打交道，原生指针有一些特有的方法，例如可以用is_null()来判断原生指针是否是空指针，用offset()来获取指定偏移量的内存地址的内容，使用read()/write()方法来读写内存等。\n调用unsafe的函数或方法调用unsafe的函数或方法必须放到unsafe代码块中，这点我们在基础知识中已经介绍过。因为函数本身被标记为unsafe，也就意味着调用它可能存在风险。这点无需赘述。\n访问或修改可变的静态变量对于不可变的静态变量，我们访问它不会存在任何安全问题，但是对于可变的静态变量而言，如果我们在多线程中都访问同一个变量，那么就会造成数据竞争。这当然也是一种不安全的操作。所以要放到unsafe代码块中，此时线程安全应由开发者自己来保证。\nstatic mut COUNTER: u32 = 0;fn add_to_count(inc: u32) &#123;    unsafe &#123;        COUNTER += inc;    &#125;&#125;fn main() &#123;    add_to_count(3);    unsafe &#123;        println!(&quot;COUNTER: &#123;&#125;&quot;, COUNTER);    &#125;&#125;\n在这个例子中我们没有使用多线程，这里只是想展示一下如何访问和修改可变静态变量。\n实现unsafe的trait当trait中包含一个或多个编译器无法验证其安全性的方法时，这个trait就必须被标记为unsafe。而想要实现unsafe的trait，首先在实现代码块的关键字impl前也要加上unsafe标记。其次，无法被编译器验证安全性的方法，其安全性必须由开发者自己来保证。\n前面我们也提到了，常见的unsafe的trait有Send和Sync这两个。\n读写联合体中的字段Rust中的Union联合体和Enum相似。我们可以使用union关键字来定义一个联合体。\nunion MyUnion &#123;    i: i32,    f: f32,&#125;fn main() &#123;    let my_union = MyUnion&#123;i: 3&#125;;    unsafe &#123;        println!(&quot;&#123;&#125;&quot;, my_union.i);    &#125;&#125;\n在初始化时，我们每次只能指定一个字段的值。这就造成我们在访问联合体中的字段时，有可能会访问到未定义的字段。因此，Rust让我们把访问操作放到unsafe代码块中，以此来警示我们必须自己保证程序的安全性。\n总结本文我们聊了Unsafe Rust的一些使用场景和使用方法。你只需要记住Unsafe的5种操作就好，在遇到这些操作时，一定要使用unsafe代码块。unsafe代码块不光是为了“骗”过编译器，要时刻提醒自己，unsafe代码块中的程序要由开发者自己保证其正确性。\n\n解引用原生指针\n调用unsafe的函数或方法\n访问或修改可变的静态变量\n实现unsafe的trait\n读写联合体中的字段\n\n","tags":["Rust"]},{"title":"Rust入坑指南：常规套路","url":"/2019/10/08/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E5%B8%B8%E8%A7%84%E5%A5%97%E8%B7%AF/","content":"搭建好了开发环境之后，就算是正式跳进Rust的坑了，今天我就要开始继续向下挖了。\n由于我们初来乍到 ，对Rust还不熟悉，所以我决定先走一遍常规套路。\n变不变的变量学习一门语言第一个要了解的当然就是变量啦。Rust使用关键字let来定义变量。我们写一个简单的demo\n\nso easy！等等，这个小红线是怎么回事？编译错误？？？别着急，哪里不对点哪里。\nIDEA告诉我，这个错误是\nCannot assign twice to immutable variable [E0384]\n不可变的变量不能赋值两次。我定义的变量是不可变的？这能叫变量？\n官方文档对此的解释是，对于一个变量，你在一部分代码中不希望它改变，而在另一部分代码中修改了它。那么第一部分代码的运行结果也许就不符合预期了。所以Rust的编译器为了杜绝这种情况，也就是说Rust的变量默认是不可修改的。也就是说你只能对它赋值一次，之后它就是只读的了。有点像Java中的final变量。嗯…很严格。\n但我们编程中还是需要变量的，怎么办？下面是跟着IDEA学习写代码环节。直接使用Alt + Enter，IDEA会在定义x时加上mut关键字。\n\n果然不会报错了。感谢IDEA。接下来运行试试\nThe value of x is: 5The value of x is: 6\n打印结果符合我们的预期。\nmut关键字在官方文档也有解释，定义变量时加上mut，表明这个变量是一个可变变量。Rust之所以设计了可变变量，还有一个比较重要的原因就是，对于比较复杂的数据类型，每次修改都要复制并且分配新的内存。这种性能损耗有时候是无法接受的。因此可以选择使用可变变量来提高性能。\n变量和常量Rust本身也支持常量，可能大多数同学和我有一样的疑问，常量和不可变变量有什么区别呢？\n事实上它们还是有区别的。最明显的就是名字不一样。（这是一句废话）\n主要的区别有以下几种：\n\n定义常量时不能使用mut关键字\n常量定义使用的关键字是const，并且需要指定数据类型。定义变量使用的是let\n常量可以在任何范围内定义，并且可以在多个代码块中使用\n给常量赋值时，不能使用函数的返回值或者是计算式。只能使用一个「常量」\n\n变量的覆盖Rust是一门静态编程语言，对于大多数静态编程语言中，在同一范围内，变量名是不允许重复的。但是Rust允许这样定义。类似于这样：\nfn main() &#123;    let x = 5;    let x = x + 1;    let x = x + 2;    println!(&quot;The value of x is: &#123;&#125;&quot;, x);&#125;\n这让人看起来有些疑惑，作为一个Java程序员，在我看来第二行就应该报编译错误了。但我们刚提到了Rust是允许这样定义的。对于上述代码来讲，每次定义x都会覆盖前一次定义的x。\n对于Java来讲，将一个int类型的变量转换成String类型的变量可能需要这样做：\nint codeInt = 1;String codeStr = String.valueOf(codeInt);\n我们需要定义两个变量来分别接收不同类型的变量，为了变量名更有意义，可能要在变量名中加上变量类型。而在Rust中就不用考虑这个问题。\nlet s = &quot;123&quot;;let s: u32 = s.parse().expect(&quot;Not a number!&quot;);\n这样定义之后，再使用变量s时，它都是u32类型的变量了。\n上面这个例子就是覆盖变量和可变变量的区别：可变变量不可以修改变量类型，覆盖变量可以做类型转换。\n数据类型可能有些同学不太理解Rust为什么是静态语言。这是因为在编译阶段，Rust编译器必须要明确每个变量的类型。编译器通常会根据变量的值或者使用方法来为变量指定一个数据类型。如果某个变量可能的数据类型有多个，那么就需要开发者手动指定。\n像上一节的例子中，第二次定义s如果不指定类型u32，编译就会报错。Rust支持的数据类型都有哪些呢？\n和多数编程语言一样，Rust支持的数据类型可以分为基本数据类型和复合数据类型两大类。先说基本数据类型，基本数据类型分为整数型、浮点型、布尔型和字符型。我们逐个介绍一下。\n整数型Rust支持的整数类型分为有符号整数和无符号整数\n\n\n\nLength\nSigned\nUnsigned\n\n\n\n\n8-bit\ni8\nu8\n\n\n16-bit\ni16\nu16\n\n\n32-bit\ni32\nu32\n\n\n64-bit\ni64\nu64\n\n\n128-bit\ni128\nu128\n\n\narch\nisize\nusize\n\n\n\n如果没有指定数据类型，Rust默认使用i32，这个类型通常是性能最好的。\n再顺便聊一下整数的几种表示。\n\n\n\nNumber literals\nExample\n\n\n\n\nDecimal\n98_222\n\n\nHex\n0xff\n\n\nOctal\n0o77\n\n\nBinary\n0b1111_0000\n\n\nByte(u8)\nb’A’\n\n\n\n十进制中_一般被当作千分符。\n浮点型Rust的浮点类型不像整型那么多，它只支持两种：f32和f64分别表示32位和64位浮点数，默认的浮点类型是f64。\n布尔类型布尔类型没有什么特别的，Rust支持隐式和显式声明两种\nlet t = true;let f: bool = false;\n字符型需要注意的是字符类型char使用单引号，字符串使用双引号。字符类型的值可以是Unicode标准值。范围是从U+0000到U+D7FF和U+E000到U+10FFFF。这意味着它可以是中文韩文 emoji等等，而并不局限于直觉上的「字符」。\n聊完了基本数据类型，再来聊一聊复合类型，Rust包含两种复合类型：Tuple和Array。\nTuple类型Tuple是一种可以存储不同类型的数字的集合。它的长度固定。声明方法是：\nlet tup: (i32, f64, u8) = (500, 6.4, 1);\n如果想要取得tuple中的某一个值，通常有两种方法，一种是将tuple分别赋值给多个变量\nfn main() &#123;    let tup = (500, 6.4, 1);    let (x, y, z) = tup;    println!(&quot;The value of y is: &#123;&#125;&quot;, y);&#125;\n另一种方法是用直接用「.」来取值。\nfn main() &#123;    let tup = (500, 6.4, 1);    let x = tup.0;        let y = tup.1;    let z = tup.2;    println!(&quot;x: &#123;&#125;, y: &#123;&#125;, z: &#123;&#125;&quot;, x, y, z);&#125;\nArray类型Array也是多个值的集合，但与Tuple不同的是，Array中的各个元素的数据类型必须相同。Array的长度也是固定的，这点上Rust的Array和其他语言的有所不同。Array的定义方法是：\nfn main() &#123;    let a = [1, 2, 3, 4, 5];&#125;\nRust的数组存储在栈中，而不是堆。如果你不能在定义时确定数组的长度，那么需要使用vector类型，这个我们在后面讨论。Array还有一些其他的定义方法。\nlet a: [i32; 5] = [1, 2, 3, 4, 5];\ni32表示数组中元素的类型，5表示元素数量。\n如果初始化时所有元素的值都相同，还可以这样定义：\nlet a = [3; 5];\n这表示定义一个长度为5的数组，每个元素都是3。\n代码写在哪——函数函数在每个编程语言中都是基本的概念，因此我们不做过多赘述。Rust定义函数的方法是：\nfn main() &#123;    let a = 1;    let b = 2;        let sum = add(a,b);    println!(&quot;The value of sum is: &#123;&#125;&quot;, sum);&#125;fn add(x: i32, y: i32) -&gt; i32 &#123;    x + y&#125;\nRust在定义函数时，需要指定参数的名称和类型和返回值的类型。而返回值只能是表达式。作为函数返回的表达式是不能以分号结尾的。\n该往哪走——流程控制Rust的流程控制语句包括条件控制语句和循环语句。条件控制语句有if，循环语句包括loop、while和for。\nifRust中if的条件必须是bool类型，它不像js中，会自动将变量转换成bool类型。此外，if还可以用于let语句中。例如：\nlet number = if condition &#123;  5&#125; else &#123;   6&#125;;\n这种方式需要注意的是，每个表达式中返回的值必须是同一类型的。\nlooploop循环中，如果没有break或者是手动停止，那么它会一直循环下去。写法很简单。\nloop &#123;  println!(&quot;again!&quot;);&#125;\nloop的用处是它可以有返回值\nlet result = loop &#123;  counter += 1;  if counter == 10 &#123;    break counter * 2;  &#125;&#125;;\nwhilewhile循环是当条件成立时进入循环。\nwhile number != 0 &#123;  // do something&#125;\nfor当我们需要遍历数组时，可以使用for循环。\nfor element in a.iter() &#123;  println!(&quot;the value is: &#123;&#125;&quot;, element);&#125;\n总结以上，是Rust的一些基本概念。和其他的编程语言大同小异，记得一些特殊的地方就好，例如变量的不可变性。我们还有一些数据类型没有涉及，比如vector，String等，这些会在后面详细讲解。\n至此，我已经又向下挖了一层了。不知道你入坑了没有？已经入坑的同学还请麻烦帮忙往外刨（分）土（享）。\n","tags":["Rust"]},{"title":"Rust入坑指南：智能指针","url":"/2020/03/09/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/","content":"在了解了Rust中的所有权、所有权借用、生命周期这些概念后，相信各位坑友对Rust已经有了比较深刻的认识了，今天又是一个连环坑，我们一起来把智能指针刨出来，一探究竟。\n智能指针是Rust中一种特殊的数据结构。它与普通指针的本质区别在于普通指针是对值的借用，而智能指针通常拥有对数据的所有权。在Rust中，如果你想要在堆内存中定义一个对象，并不是像Java中那样直接new一个，也不是像C语言中那样需要手动malloc函数来分配内存空间。Rust中使用的是Box::new来对数据进行封箱，而Box&lt;T&gt;就是我们今天要介绍的智能指针之一。除了Box&lt;T&gt;之外，Rust标准库中提供的智能指针还有Rc&lt;T&gt;、Ref&lt;T&gt;、RefCell&lt;T&gt;等等。在详细介绍之前，我们还是先了解一下智能指针的基本概念。\n基本概念我们说Rust的智能指针是一种特殊的数据结构，那么它特殊在哪呢？它与普通数据结构的区别在于智能指针实现了Deref和Drop这两个traits。实现Deref可以使智能指针能够解引用，而实现Drop则使智能指针具有自动析构的能力。\nDerefDeref有一个特性是强制隐式转换：如果一个类型T实现了Deref&lt;Target=U&gt;，则该类型T的引用在应用的时候会被自动转换为类型U。\nuse std::rc::Rc;fn main() &#123;    let x = Rc::new(&quot;hello&quot;);    println!(&quot;&#123;:?&#125;&quot;, x.chars());&#125;\n如果你查看Rc的源码，会发现它并没有实现chars()方法，但我们上面这段代码却可以直接调用，这是因为Rc实现了Deref。\n#[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]impl&lt;T: ?Sized&gt; Deref for Rc&lt;T&gt; &#123;    type Target = T;    #[inline(always)]    fn deref(&amp;self) -&gt; &amp;T &#123;        &amp;self.inner().value    &#125;&#125;\n这就使得智能指针在使用时被自动解引用，像是不存在一样。\nDeref的内部实现是这样的：\n#[lang = &quot;deref&quot;]#[doc(alias = &quot;*&quot;)]#[doc(alias = &quot;&amp;*&quot;)]#[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]pub trait Deref &#123;    /// The resulting type after dereferencing.    #[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]    type Target: ?Sized;    /// Dereferences the value.    #[must_use]    #[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]    fn deref(&amp;self) -&gt; &amp;Self::Target;&#125;#[lang = &quot;deref_mut&quot;]#[doc(alias = &quot;*&quot;)]#[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]pub trait DerefMut: Deref &#123;    /// Mutably dereferences the value.    #[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]    fn deref_mut(&amp;mut self) -&gt; &amp;mut Self::Target;&#125;\nDerefMut和Deref类似，只不过它是返回可变引用的。\nDropDrop对于智能指针非常重要，它是在智能指针被丢弃时自动执行一些清理工作，这里所说的清理工作并不仅限于释放堆内存，还包括一些释放文件和网络连接等工作。之前我总是把Drop理解成Java中的GC，随着对它的深入了解后，我发现它比GC要强大许多。\nDrop的内部实现是这样的：\n#[lang = &quot;drop&quot;]#[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]pub trait Drop &#123;    #[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]    fn drop(&amp;mut self);&#125;\n这里只有一个drop方法，实现了Drop的结构体，在消亡之前，都会调用drop方法。\nuse std::ops::Drop;#[derive(Debug)]struct S(i32);impl Drop for S &#123;    fn drop(&amp;mut self) &#123;        println!(&quot;drop &#123;&#125;&quot;, self.0);    &#125;&#125;fn main() &#123;    let x = S(1);    println!(&quot;create x: &#123;:?&#125;&quot;, x);    &#123;        let y = S(2);        println!(&quot;create y: &#123;:?&#125;&quot;, y);    &#125;&#125;\n上面代码的执行结果为\n\n可以看到x和y在生命周期结束时都去执行了drop方法。\n对智能指针的基本概念就先介绍到这里，下面我们进入正题，具体来看看每个智能指针都有什么特点吧。\nBox前面我们已经提到了Box在Rust中是用来在堆内存中保存数据使用的。它的使用方法非常简单：\nfn main() &#123;    let x = Box::new(&quot;hello&quot;);    println!(&quot;&#123;:?&#125;&quot;, x.chars())&#125;\n我们可以看一下Box::new的源码\n#[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]#[inline(always)]pub fn new(x: T) -&gt; Box&lt;T&gt; &#123;  box x&#125;\n可以看到这里只有一个box关键字，这个关键字是用来进行堆内存分配的，它只能在Rust源码内部使用。box关键字会调用Rust内部的exchange_malloc和box_free方法来管理内存。\n#[cfg(not(test))]#[lang = &quot;exchange_malloc&quot;]#[inline]unsafe fn exchange_malloc(size: usize, align: usize) -&gt; *mut u8 &#123;    if size == 0 &#123;        align as *mut u8    &#125; else &#123;        let layout = Layout::from_size_align_unchecked(size, align);        let ptr = alloc(layout);        if !ptr.is_null() &#123;            ptr        &#125; else &#123;            handle_alloc_error(layout)        &#125;    &#125;&#125;#[cfg_attr(not(test), lang = &quot;box_free&quot;)]#[inline]pub(crate) unsafe fn box_free&lt;T: ?Sized&gt;(ptr: Unique&lt;T&gt;) &#123;    let ptr = ptr.as_ptr();    let size = size_of_val(&amp;*ptr);    let align = min_align_of_val(&amp;*ptr);    // We do not allocate for Box&lt;T&gt; when T is ZST, so deallocation is also not necessary.    if size != 0 &#123;        let layout = Layout::from_size_align_unchecked(size, align);        dealloc(ptr as *mut u8, layout);    &#125;&#125;\nRc在前面的学习中，我们知道Rust中一个值在同一时间只能有一个变量拥有其所有权，但有时我们可能会需要多个变量拥有所有权，例如在图结构中，两个图可能对同一条边拥有所有权。\n对于这样的情况，Rust为我们提供了智能指针Rc（reference counting）来解决共享所有权的问题。每当我们通过Rc共享一个所有权时，引用计数就会加一。当引用计数为0时，该值才会被析构。\nRc是单线程引用计数指针，不是线程安全类型。\n我们还是通过一个简单的例子来看一下Rc的应用吧。（示例来自the book）\n如果我们想要造一个“双头”的链表，如下图所示，3和4都指向5。我们先来尝试使用Box实现。\n\nenum List &#123;    Cons(i32, Box&lt;List&gt;),    Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;fn main() &#123;    let a = Cons(5,                 Box::new(Cons(10,                               Box::new(Nil))));    let b = Cons(3, Box::new(a));    let c = Cons(4, Box::new(a));&#125;\n上述代码在编译时就会报错，因为a绑定给了b以后就无法再绑定给c了。\n\nenum List &#123;    Cons(i32, Rc&lt;List&gt;),    Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;use std::rc::Rc;fn main() &#123;    let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil)))));    let b = Cons(3, Rc::clone(&amp;a));    let c = Cons(4, Rc::clone(&amp;a));    println!(&quot;count a &#123;&#125;&quot;, Rc::strong_count(&amp;a));&#125;\n这时我们可以看到a的引用计数是3，这是因为这里计算的是节点5的引用计数，而a本身也是对5的一次绑定。这种通过clone方法共享所有权的引用称作强引用。\nRust还为我们提供了另一种智能指针Weak，你可以把它当作是Rc的另一个版本。它提供的引用属于弱引用。它共享的指针没有所有权。但他可以帮助我们有效的避免循环引用。\nRefCell前文中我们聊过变量的可变性和不可变性，主要是针对变量的。按照前面所讲的，对于结构体来说，我们也只能控制它的整个实例是否可变。实例的具体某个成员是否可变我们是控制不了的。但在实际开发中，这样的场景也是比较常见的。比如我们有一个User结构体：\nstruct User &#123;    id: i32,    name: str,    age: u8,&#125;\n通常情况下，我们只能修改一个人的名称或者年龄，而不能修改用户的id。如果我们把User的实例设置成了可变状态，那就不能保证别人不会去修改id。\n为了应对这种情况，Rust为我们提供了Cell&lt;T&gt;和RefCell&lt;T&gt;。它们本质上不属于智能指针，而是可以提供内部可变性的容器。内部可变性实际上是一种设计模式，它的内部是通过一些unsafe代码来实现的。\n我们先来看一下Cell&lt;T&gt;的使用方法吧。\nuse std::cell::Cell;struct Foo &#123;    x: u32,    y: Cell&lt;u32&gt;,&#125;fn main() &#123;    let foo = Foo &#123; x: 1, y: Cell::new(3)&#125;;    assert_eq!(1, foo.x);    assert_eq!(3, foo.y.get());    foo.y.set(5);    assert_eq!(5, foo.y.get());&#125;\n我们可以使用Cell的set/get方法来设置/获取起内部的值。这有点像我们在Java实体类中的setter/getter方法。这里有一点需要注意：Cell&lt;T&gt;中包裹的T必须要实现Copy才能够使用get方法，如果没有实现Copy，则需要使用Cell提供的get_mut方法来返回可变借用，而set方法在任何情况下都可以使用。由此可见Cell并没有违反借用规则。\n对于没有实现Copy的类型，使用Cell&lt;T&gt;还是比较不方便的，还好Rust还提供了RefCell&lt;T&gt;。话不多说，我们直接来看代码。\nuse std::cell::RefCell;fn main() &#123;    let x = RefCell::new(vec![1, 2, 3]);    println!(&quot;&#123;:?&#125;&quot;, x.borrow());    x.borrow_mut().push(5);    println!(&quot;&#123;:?&#125;&quot;, x.borrow());&#125;\n从上面这段代码中我们可以观察到RefCell&lt;T&gt;的borrow_mut和borrow方法对应了Cell&lt;T&gt;中的set和get方法。\nRefCell&lt;T&gt;和Cell&lt;T&gt;还有一点区别是：Cell&lt;T&gt;没有运行时开销（不过也不要用它包裹大的数据结构），而RefCell&lt;T&gt;是有运行时开销的，这是因为使用RefCell&lt;T&gt;时需要维护一个借用检查器，如果违反借用规则，则会引起线程恐慌。\n总结关于智能指针我们就先介绍这么多，现在我们简单总结一下。Rust的智能指针为我们提供了很多有用的功能，智能指针的一个特点就是实现了Drop和Deref这两个trait。其中Droptrait中提供了drop方法，在析构时会去调用。Dereftrait提供了自动解引用的能力，让我们在使用智能指针的时候不需要再手动解引用了。\n接着我们分别介绍了几种常见的智能指针。Box&lt;T&gt;可以帮助我们在堆内存中分配值，Rc&lt;T&gt;为我们提供了多次借用的能力。RefCell&lt;T&gt;使内部可变性成为现实。\n最后再多说一点，其实我们以前见到过的String和Vec也属于智能指针。\n至于它们为什么属于智能指针，Rust又提供了哪些其他的智能指针呢？这里就留个坑吧，感兴趣的同学可以自己踩一下。\n","tags":["Rust"]},{"title":"Rust入坑指南：最后一舞","url":"/2020/04/19/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E6%9C%80%E5%90%8E%E4%B8%80%E8%88%9E/","content":"Rust入坑指南系列我觉得应该告一段落了，最后来做一个总结吧。\n在我看来，Rust语言本身设计得算是非常好了。Ownership和borrow帮助我们保证了程序了安全性。同时也提供了Unsafe，给开发者更多玩一些骚操作的空间。唯一的缺点就是入门比较困难了吧，我现在的水平感觉自己也就是刚刚入门。而《Rust入坑指南》也是希望帮助更多想要学习Rust的同学快速入门。\n这里简单回顾一下学习过程吧。\n最开始接触一门语言一定绕不开Hello，World\nRust也是一样，所以我们的第一篇文章就是关于Rust的安装、Hello，World程序的。\nRust入坑指南：坑主驾到\n接着就是介绍一些基础的语法、Rust的所有权、数据结构这些概念。关于这部分知识我用了四篇文章来做介绍。其中最重要的应该是Rust所有权了，这也是Rust语言的亮点之一。\nRust入坑指南：常规套路\nRust入坑指南：核心概念\nRust入坑指南：千人千构\nRust入坑指南：鳞次栉比\n接着呢，我们介绍了Package和Crate，用来帮助我们组织代码的。同时Crate也是为了让我们可以直接使用别人的代码，避免重复造轮子。\nRust入坑指南：有条不紊\n之后又是两个比较通用的概念，大多数编程语言时都要涉及到的：异常处理和泛型\nRust入坑指南：亡羊补牢\nRust入坑指南：海纳百川\n如果你对代码的正确性不放心，那么一定要写下完备的单元测试，这是对自己的代码负责。\nRust入坑指南：步步为营\n除了OwnerShip和borrow之外，Rust的另外两个比较核心的概念也需要了解，分别是生命周期和智能指针。这两篇文章可以帮你快速了解这两个概念。\nRust入坑指南：朝生暮死\nRust入坑指南：智能指针\n接着是并发编程，Rust声称的安全并发，究竟是怎么保证的？\nRust入坑指南：齐头并进（上）\nRust入坑指南：齐头并进（下）\nSafe Rust有这样那样的限制，有的开发者可能会觉得束手束脚，难以发挥实力。这时就可以考虑看看Unsafe Rust了。\nRust入坑指南：居安思危\n最后是Rust的元编程，我们从最开始就在使用的println!宏，它是如何定义的呢？我们又怎么定义自己的宏？希望这篇文章对你有帮助。\nRust入坑指南：万物初始\n经过这几个月的学习，我对Rust也有了一个初步的了解，在这里要感谢对我的分享提出意见的同学。也希望我的分享能对大家有所帮助。\n虽然标题叫最后一舞，但是后面我还是会继续保持学习，也会不定期分享一些入门的代码案例给大家。\nRust编程，我们后会有期。\n","tags":["Rust"]},{"title":"Rust入坑指南：有条不紊","url":"/2019/11/03/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E6%9C%89%E6%9D%A1%E4%B8%8D%E7%B4%8A/","content":"随着我们的坑越来越多，越来越大，我们必须要对各种坑进行管理了。Rust为我们提供了一套坑务管理系统，方便大家有条不紊的寻找、管理、填埋自己的各种坑。\nRust提供给我们一些管理代码的特性：\n\nPackages：Cargo的一个特性，帮助你进行构建、测试和共享crates\nCrates：生成库或可执行文件的模块树\nModules和use：用于控制代码组织、范围和隐私路径\nPaths：struct、function和module的命名方法\n\n下面我们来具体看一下这些特性是如何帮助我们组织代码的。\nPackages和Cratespackage可以理解为一个项目，而crate可以理解为一个代码库。crate可以供多个项目使用。那我们的项目中package和crate是怎么定义的呢？\n之前我们总是通过IDEA来新建项目，今天我们换个方法，在命令行中使用cargo命令来创建。\n$ cargo new hello-world     Created binary (application) `hello-world` package$ ls hello-worldCargo.tomlsrc$ ls hello-world/srcmain.rs\n可以看到，我们使用cargo创建项目后，只有两个文件，Cargo.toml和src目录下的main.rs。\nCargo.toml是管理项目依赖的文件，每个Cargo.toml定义一个package。main.rs文件的存在表示package中包含一个二进制crate，它是二进制crate的入口文件，crate的名称和package相同。如果src目录下存在lib.rs文件，说明package中包含一个和package名称相同的库crate。\n一个package可以包含多个二进制crate，它们由src/lib目录下的文件定义。如果你的项目想引用他人的crate，可以在Cargo.toml文件中增加依赖。每个crate都有自己的命名空间，因此如果你引入了一个crate里面定义了一个名为hello的函数，你仍然可以在自己的crate中再定义一个名为hello的函数。\nModuleModule帮助我们在crate中组织代码，同时Module也是封装代码的重要工具。接下来还是通过一个栗子来详细了解Module。\n前面我们说过，库crate定义在src/lib.rs文件中。这里首先创建一个包含了库crate的package:\ncargo new --lib restaurant\n然后在src中定义一些module和函数。\nmod front_of_house &#123;    mod hosting &#123;        fn add_to_waitlist() &#123;&#125;        fn seat_at_table() &#123;&#125;    &#125;    mod serving &#123;        fn take_order() &#123;&#125;        fn serve_order() &#123;&#125;        fn take_payment() &#123;&#125;    &#125;&#125;\n可以看到我们使用关键字mod来定义Module，Module中可以继续定义Module或函数。这样我们就可以比较方便的把相关的函数放到一个Module中，并为Module命名，提高代码的可读性。另外Module中还可以定义struct和枚举。由于Module中可以嵌套定义子Module，最终我们定义出来的代码类似一个树形。\n那么如何访问Module中的函数呢？这就要提到Path了。这部分比较好理解，Module树相当于系统文件目录，而Path则是目录的路径。\nPath这里的路径和系统文件路径一样，都分为相对路径和绝对路径两种。其中绝对路径必须以crate开头，因为它代码整个Module树的根节点。路径之间使用的是双冒号来表示引用。\n现在我来尝试在一个函数中调用add_to_waitlist函数：\n\n可以看到这里不管用绝对路径还是相对路径都报错了，错误信息是模块hosting和函数add_to_waitlist是私有（private）的。我们先暂时放下这个错误，根据这里的错误提示，我们知道了当我们定义一个module时，默认情况下是私有的，我们可以通过这种方法来封装一些代码的实现细节。\nOK，回到刚才的问题，那我们怎么才能解决这个错误呢？地球人都知道应该把对应的模块与函数公开出来。Rust中标识模块或函数为公有的关键字是pub。\n我们用pub关键字来把对应的模块和函数公开\n\n这样我们就可以在module外来调用module内的函数了。\nRust中的私有规则现在我们再回过头来看Rust中的一些私有规则，如果你试验了上面的例子，也许会有一些发现。\nRust中私有规则适用于所有项（函数、方法、结构体、枚举、模块和常量），它们默认都是私有的。父模块中的项不能访问子模块中的私有项，而子模块中的项可以访问其祖辈（父模块及以上）中的项。\nStruct和Enum的私有性Struct和Enum的私有性略有不同，对于Struct来讲，我可以只将其中的某些字段设置为公有的，其他字段可以仍然保持私有。\nmod back_of_house &#123;    pub struct Breakfast &#123;        pub toast: String,        seasonal_fruit: String,    &#125;    impl Breakfast &#123;        pub fn summer(toast: &amp;str) -&gt; Breakfast &#123;            Breakfast &#123;                toast: String::from(toast),                seasonal_fruit: String::from(&quot;peaches&quot;),            &#125;        &#125;    &#125;&#125;pub fn eat_at_restaurant() &#123;    // Order a breakfast in the summer with Rye toast    let mut meal = back_of_house::Breakfast::summer(&quot;Rye&quot;);    // Change our mind about what bread we&#x27;d like    meal.toast = String::from(&quot;Wheat&quot;);    println!(&quot;I&#x27;d like &#123;&#125; toast please&quot;, meal.toast);&#125;\n而对于Enum，如果一个Enum是公有的，那么它的所有值都是公有的，因为私有的值没有意义。\n相对路径和绝对路径的选择这种选择不存在正确与否，只有是否合适。因此这里我们只是举例说明一些合适的情况。\n我们仍以上述代码为例，如果我们可以预见到以后需要把front_of_house模块和eat_at_restaurant函数移动到一个新的名为customer_experience的模块中，就应该使用相对路径，这样我们就对其进行调整。\n类似的，如果我们需要把eat_at_restaurant函数移动到dining模块中，那么我们选择绝对路径的话就不需要做调整。\n综上，我们需要对代码的优化方向有一些前瞻性，并以此来判断需要使用相对路径还是绝对路径。\n相对路径除了以当前模块开头外，还可以以super开头。它表示的是父级模块，类似于文件系统中的两个点(..)。\nuse关键字绝对路径和相对路径可以帮助我们找到指定的函数，但用起来也非常的麻烦，每次都要写一大长串路径。还好Rust为我们提供了use关键字。在很多语言中都有import关键字，这里的use就有些类似于import。不过Rust会提供更加丰富的用法。\nuse最基本的用法就是引入一个路径。我们就可以更加方便的使用这个路径下的一些方法：\nmod front_of_house &#123;    pub mod hosting &#123;        pub fn add_to_waitlist() &#123;&#125;    &#125;&#125;use crate::front_of_house::hosting;pub fn eat_at_restaurant() &#123;    hosting::add_to_waitlist();&#125;\n这个路径可以是绝对路径，也可以是相对路径，但如果是相对路径，就必须要以self开头。上面的例子可以写成：\nuse self::front_of_house::hosting;\n这与我们前面讲的相对路径似乎有些矛盾，Rust官方说会在之后的版本处理这个问题。\nuse还可以更进一步，直接指向具体的函数或Struct或Enum。但习惯上我们使用函数时，use后面使用的是路径，这样可以在调用函数时知道它属于哪个模块；而在使用Struct/Enum时，则具体指向它们。当然，这只是官方建议的编程习惯，你也可以有自己的习惯，不过最好还是按照官方推荐或者是项目约定的规范比较好。\n对于同一路径下的某些子模块，在引入时可以合并为一行，例如：\nuse std::io;use std::cmp::Ordering;// 等价于use std::&#123;cmp::Ordering, io&#125;;\n有时我们还会遇到引用不同包下相同名称Struct的情况，这时有两种解决办法，一是不指定到具体的Struct，在使用时加上不同的路径；二是使用as关键字，为Struct起一个别名。\n方法一：\nuse std::fmt;use std::io;fn function1() -&gt; fmt::Result &#123;    // --snip--&#125;fn function2() -&gt; io::Result&lt;()&gt; &#123;    // --snip--&#125;\n方法二：\nuse std::fmt::Result;use std::io::Result as IoResult;fn function1() -&gt; Result &#123;    // --snip--&#125;fn function2() -&gt; IoResult&lt;()&gt; &#123;    // --snip--&#125;\n如果要导入某个路径下的全部模块或函数，可以使用*来表示。当然我是非常不建议使用这种方法的，因为导入全部的话，如果出现名称冲突就会很难排查问题。\n对于外部的依赖包，我们需要先在Cargo.toml文件中添加依赖，然后就可以在代码中使用use来引入依赖库中的路径。Rust提供了一些标准库，即std下的库。在使用这些标准库时是不需要添加依赖的。\n有些同学看到这里可能要开始抱怨了，说好了介绍怎么拆分文件，到现在还是在一个文件里玩，这不是欺骗读者嘛。\n别急，这就开始拆分。\n开始拆分我们拿刚才的一段代码为例\nmod front_of_house &#123;    mod hosting &#123;        fn add_to_waitlist() &#123;&#125;        fn seat_at_table() &#123;&#125;    &#125;    mod serving &#123;        fn take_order() &#123;&#125;        fn serve_order() &#123;&#125;        fn take_payment() &#123;&#125;    &#125;&#125;\n首先我们可以把front_of_house模块下的内容拆分出去，需要在src目录下新建一个front_of_house.rs文件，然后把front_of_house模块下的内容写到文件中。lib.rs文件中，只需要声明front_of_house模块即可，不需要具体的定义。声明模块时，将花括号即内容改为分号就可以了。\nmod front_of_house;\n然后我们可以继续拆分front_of_house模块下的hosting模块和serving模块，这时需要新建一个名为front_of_house的文件件，在该文件夹下放置要拆分的模块的同名文件，把模块定义的内容写在文件中，front_of_house.rs文件同样只保留声明即可。\n拆分后的文件目录如图\n\n本文主要讲了Rust中Package、Crate、Module、Path的概念和用法，有了这些基础，我们后面才有可能开发一些比较大的项目。\nps：本文的代码示例均来自the book。\n","tags":["Rust"]},{"title":"Rust入坑指南：朝生暮死","url":"/2020/03/02/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E6%9C%9D%E7%94%9F%E6%9A%AE%E6%AD%BB/","content":"今天想和大家一起把我们之前挖的坑再刨深一些。在Java中，一个对象能存活多久全靠JVM来决定，程序员并不需要去关心对象的生命周期，但是在Rust中就大不相同，一个对象从生到死我们都需要掌握的很清楚。\n在Rust入坑指南：核心概念一文中我们介绍了Rust的几个核心概念：所有权（Ownership）、所有权转移和所有权借用。今天就来介绍Rust中的另外一个核心概念：生命周期。\n为什么生命周期要单独介绍呢？因为我在这之前一直没搞清楚Rust中的生命周期参数究竟是怎么一回事。\n现在我终于弄明白了，于是迫不及待要和大家分享，当然如果我有什么说的不对的地方请帮忙指正。\n在Rust中，值的生命周期与作用域有关，这里你可以结合所有权一起理解。在一个函数内，Rust中值的所有权的范围即为其生命周期。Rust通过借用检查器对值的生命周期进行检查，其目的是为了避免出现悬垂指针。这点很容易理解，我们通过一段简单的代码来看一下。\nfn main() &#123;    let a;  // &#x27;a ---------------+    &#123;                   //       |        let b = 1; // &#x27;b ----+   |        a = &amp;b;           // |   |    &#125;// ---------------------+   |    println!(&quot;a: &#123;&#125;&quot;, a); //     |&#125; // ----------------------------+\n在上面这段代码中，我已经标注了a和b的生命周期。在代码的第5行，b将所有权出借给了a，而在第7行我们想使用a时，b的生命周期已经结束，也就是说，从第7行开始，a成为了一个悬垂指针。因此这段代码会报一个编译错误。\n\n而当所有权在函数之间传递时，Rust的借用检查器就没有办法来确定值的生命周期了。这个时候我们就需要借助生命周期参数来帮助Rust的借用检查器来进行生命周期的检查。生命周期参数分为显式的和隐式的两种。\n显式生命周期参数显式生命周期的标注方式通常是&#39;a这样的。它应该写在&amp;之后，mut之前（如果有）。\n函数签名中的生命周期参数在正式开始学习之前，我们还要先明确一些概念。下面是一个代有生命周期参数的函数签名。\nfn foo &lt;&#x27;a&gt;(s: &amp;&#x27;a str, t: &amp;&#x27;a str) -&gt;&amp;&#x27;a str;\n其中第一个&#39;a，是生命周期参数的声明。参数的生命周期叫做输入声明周期，返回值的生命周期叫做输出生命周期。需要记住的一点是：输出的生命周期长度不能长于输入的生命周期。\n另外还要注意：禁止在没有任何输入参数的情况下返回引用。因为这样明显会造成悬垂指针。试想当你没有任何输入参数时返回了引用，那么引用本身的值在函数返回时必然会被析构，返回的引用也就成了悬垂指针。\n同样的道理我们可以得出另一个结论：从函数中返回一个引用，其生命周期参数必须与函数的参数相匹配，否则，标注生命周期参数也毫无意义。\n说了这么多“不允许”之后，我们来看一个正常使用生命周期参数的例子吧。\nfn the_longest&lt;&#x27;a&gt; (s1: &amp;&#x27;a str, s2: &amp;&#x27;a str) -&gt; &amp;&#x27;a str &#123;    if s1.len() &gt; s2.len() &#123;        s1    &#125; else &#123;        s2    &#125;&#125;fn main() &#123;    let s1 = String::from(&quot;Rust&quot;);    let s1_r = &amp;s1;    &#123;        let s2 = String::from(&quot;C&quot;);        let res = the_longest(s1_r, &amp;s2);        println!(&quot;&#123;&#125; is the longest&quot;, res);    &#125;&#125;\n我们来看看这段代码的各个值的生命周期是否符合我们前面说的那一点原则。在调用th_longest函数时，两个参数的生命周期已经确定，s1的生命周期贯穿了main函数，s2的生命周期在内部的代码块中。函数返回时，将返回值绑定给了res，也就是说返回的生命周期为res的生命周期，由于后定义先析构的原则，res的生命周期是短于s2的生命周期的，当然也短于s1的生命周期。因此这个例子符合了我们说的输出的生命周期长度不能长于输入的生命周期的原则。\n对于像示例当中有多个参数的函数，我们也可以为其标注不同的生命周期参数，但是编译器无法确定两个生命周期参数的大小，因此需要我们显式的指定。\nfn the_longest&lt;&#x27;a, &#x27;b: &#x27;a&gt; (s1: &amp;&#x27;a str, s2: &amp;&#x27;b str) -&gt; &amp;&#x27;a str &#123;    if s1.len() &gt; s2.len() &#123;        s1    &#125; else &#123;        s2    &#125;&#125;\n这里&#39;b: &#39;a的意思是&#39;b的存活周期长于&#39;a。这点有些令人疑惑，&#39;a明明是长于&#39;b的，为什么会这样标注呢？还记得我们说过生命周期参数的意义吗？它是用来帮助Rust借用检查器来检查非法借用的，输出生命周期必须短于输入生命周期。因此这里的&#39;a实际上是返回值的生命周期，而不是第一个输入参数的生命周期。\n函数中的生命周期参数的使用我们暂时先介绍到这里。生命周期在其他使用场景中的使用方法也比较类似，不过还是有一些值得注意的地方的。\n结构体中的生命周期参数如果一个结构体包含引用类型的成员，那么结构体应该声明生命周期参数&lt;&#39;a&gt;。这是为了保证结构体实例的生命周期应该短于或等于任意一个成员的生命周期。\nstruct ImportantExcept&lt;&#x27;a&gt; &#123;    part: &amp;&#x27;a str,&#125;fn main() &#123;    let novel = String::from(&quot;call me Ishmael. Some year ago...&quot;);    let first_sentence = novel.split(&#x27;.&#x27;)        .next()        .expect(&quot;Could not find a &#x27;.&#x27;&quot;);    let i = ImportantExcept &#123; part: first_sentence&#125;;    assert_eq!(i.part, &quot;call me Ishmael&quot;);&#125;\n在这段代码中first_sentence先于结构体实例i被定义，因此i的生命周期是短于first_sentence的，如果反过来，i的生命周期长于first_sentence即长于part，那么在part被析构以后，i.part就会成为悬垂指针。\n方法中的生命周期参数现在我们为刚才的结构体增加一个实现方法\nimpl&lt;&#x27;a&gt; ImportantExcept&lt;&#x27;a&gt; &#123;    fn get_first_sentence(s: &amp;&#x27;a str) -&gt; &amp;&#x27;a str &#123;        let first_sentence = s.split(&#x27;.&#x27;)            .next()            .expect(&quot;Could not find a &#x27;.&#x27;&quot;);        first_sentence    &#125;&#125;\n因为ImportantExcept包含引用成员，因此需要标注生命周期参数。在impl后面声明生命周期参数&lt;&#39;a&gt;在结构体名称后面使用。在get_first_sentence方法中使用的生命周期参数也是刚刚定义好的那个。这样就可以约束输入引用的生命周期长度长于结构体实例的生命周期长度。\n静态生命周期参数前面聊的都是我们自己定义的生命周期参数，现在来聊聊Rust中内置的生命周期参数&#39;static。&#39;static生命周期存活于整个程序运行期间。所有的字符串字面量都有&#39;static生命周期，类型为&amp;&#39;static str。\n隐式生命周期参数在某些情况下，我们可以省略生命周期参数，对于省略的生命周期参数通常有三条规则：\n\n每个输入位置上省略的生命周期都将成为一个不同的生命周期参数\n如果只有一个输入生命周期的位置，则该生命周期将分配给输出生命周期\n如果存在多个输入生命周期的位置，但是其中包含&amp;self或&amp;mut self，则self的生命周期将分配给输出生命周期\n\n生命周期限定生命周期参数也可以像trait那样作为范型的限定\n\nT: ‘a：表示T类型中的任何引用都要“活得”和’a一样长\nT：Trait + ‘a：表示T类型必须实现Trait这个trait，并且T类型中的任何引用都要“活得”和’a一样长\n\n总结现在我把我对Rust生命周期的了解都分享完了。其实只要记住一个原则就可以了，那就是：生命周期参数的目的是帮助借用检查器验证引用的合法性，避免出现悬垂指针。\nRust还有几个深坑，我们下次继续。\n","tags":["Rust"]},{"title":"Rust入坑指南：核心概念","url":"/2019/10/13/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","content":"如果说前面的坑我们一直在用小铲子挖的话，那么今天的坑就是用挖掘机挖的。\n今天要介绍的是Rust的一个核心概念：Ownership。全文将分为什么是Ownership以及Ownership的传递类型两部分。\n什么是Ownership每种编程语言都有自己的一套内存管理的方法。有些需要显式的分配和回收内存（如C），有些语言则依赖于垃圾回收器来回收不使用的内存（如Java）。而Rust不属于以上任何一种，它有一套自己的内存管理规则，叫做Ownership。\n在具体介绍Ownership之前，我想要先声明一点。Rust入坑指南：常规套路一文中介绍的数据类型，其数据都是存储在栈中。而像String或一些自定义的复杂数据结构（我们以后会对它们进行详细介绍），其数据则存储在堆内存中。明确了这一点后，我们来看下Ownership的规则有哪些。\nOwnership的规则\n在Rust中，每一个值都有对应的变量，这个变量称为值的owner\n一个值在某一时刻只能有一个owner\n当owner超出作用域后，值会被销毁\n\n这三条规则非常重要，记住他们会帮助你更好的理解本文。\n变量作用域Ownership的规则中，有一条是owner超过范围后，值会被销毁。那么owner的范围又是如何定义的呢？在Rust中，花括号通常是变量范围作用域的标志。最常见的在一个函数中，变量s的范围从定义开始生效，直到函数结束，变量失效。\nfn main() &#123;                      // s is not valid here, it’s not yet declared    let s = &quot;hello&quot;;   // s is valid from this point forward    // do stuff with s&#125;                      // this scope is now over, and s is no longer valid\n这个这和其他大多数编程语言很像，对于大多数编程语言，都是从变量定义开始，为变量分配内存。而回收内存则是八仙过海各显神通。对于有依赖GC的语言来说，并不需要关心内存的回收。而有些语言则需要显式回收内存。显式回收就会存在一定的问题，比如忘记回收或者重复回收。为了对开发者更加友好，Rust使用自动回收内存的方法，即在变量超出作用域时，回收为该变量分配的内存。\nOwnership的移动前面我们提到，花括号通常是变量作用域隔离的标志（即Ownership失效）。除了花括号以外，还有其他的一些情况会使Ownership发生变化，先来看两段代码。\nlet x = 5;let y = x;println!(&quot;x: &#123;&#125;&quot;, x);\nlet s1 = String::from(&quot;hello&quot;);let s2 = s1;println!(&quot;s1: &#123;&#125;&quot;, s1);\n作者注：双冒号是Rust中函数引用的标志，上面的意思是引用String中的from函数，这个函数通常用来构建一个字符串对象。\n这两段代码看起来唯一的区别就是变量的类型，第一段使用的是整数型，第二段使用的是字符串型。而执行结果却是第一段可以正常打印x的值，第二段却报错了。这是什么原因呢？\n我们来分析一下代码。对于第一段代码，首先有个整数值5，赋给了变量x，然后把x的值copy了一份，又赋值给了y。最后我们成功打印x。看起来比较符合逻辑。实际上Rust也是这么操作的。\n对于第二段代码我们想象中，也可以是这样的过程，但实际上Rust并不是这样做的。先来说原因：对于较大的对象来说，这样的复制是非常浪费空间和时间的。那么Rust中实际情况是怎么样呢？\n首先，我们需要了解Rust中String类型的结构：\n\n上图中左侧是String对象的结构，包括指向内容的指针、长度和容量。这里长度和容量相同，我们暂时先不关注。后面详细介绍String类型时会提到两者的区别。这部分内容都存储在栈内存中。右侧部分是字符串的内容，这部分存储在堆内存中。\n有的朋友可能想到了，既然复制内容会造成资源浪费，那我只复制结构这部分好了，内容再多，我复制的内容长度也是可控的，而且也是在栈中复制，和整数类型类似。这个方法听起啦不错，我们来分析一下。按照上面这种说法，内存结构大概是这个样子。\n\n这种会有什么问题呢？还记得Ownership的规则吗？owner超出作用域时，回收其数据所占用的内存。在这个例子中，当函数执行结束时，s1和s2同时超出作用域，那么上图中右侧这块内存就会被释放两次。这也会产生不可预知的bug。\nRust为了解决这一问题，在执行let s2 = s1;这句代码时，认为s1已经超出了作用域，即右侧的内容的owner已经变成了s2，也可以说s1的ownership转移给了s2。也就是下图所示的情况。\n\n另一种实现：clone如果你确实需要深度拷贝，即复制堆内存中的数据。Rust也可以做到，它提供了一个公共方法叫做clone。\nlet s1 = String::from(&quot;hello&quot;);let s2 = s1.clone();println!(&quot;s1 = &#123;&#125;, s2 = &#123;&#125;&quot;, s1, s2);\nclone的方法执行后，内存结构如下图：\n\n函数间转移前面我们聊到的是Ownership在String之间转移，在函数间也是一样的。\nfn main() &#123;    let s = String::from(&quot;hello&quot;);  // s 作用域开始    takes_ownership(s);             // s&#x27;s 的值进入函数                                    // ... s在这里已经无效&#125; // s在这之前已经失效fn takes_ownership(some_string: String) &#123; // some_string 作用域开始    println!(&quot;&#123;&#125;&quot;, some_string);&#125; // some_string 超出作用域并调用了drop函数  // 内存被释放\n那有没有办法在执行takes_ownership函数后使s继续生效呢？一般我们会想到在函数中将ownership还回来。然后很自然的就想到我们之前介绍的函数的返回值。既然传参可以转移ownership，那么返回值应该也可以。于是我们可以这样操作：\nfn main() &#123;    let s1 = String::from(&quot;hello&quot;);     // s2 comes into scope    let s2 = takes_and_gives_back(s1);  // s1 被转移到函数中                                        // takes_and_gives_back，    \t\t\t\t\t\t\t\t\t// 将ownership还给s2&#125; // s2超出作用域，内存被回收，s1在之前已经失效// takes_and_gives_back 接收一个字符串然后返回一个fn takes_and_gives_back(a_string: String) -&gt; String &#123; // a_string 开始作用域    a_string  // a_string 被返回，ownership转移到函数外&#125;\n这样做是可以实现我们的需求，但是有点太麻烦了，幸好Rust也觉得这样很麻烦。它为我们提供了另一种方法：引用（references）。\n引用和借用引用的方法很简单，只需要加一个&amp;符。\nfn main() &#123;    let s1 = String::from(&quot;hello&quot;);    let len = calculate_length(&amp;s1);    println!(&quot;The length of &#x27;&#123;&#125;&#x27; is &#123;&#125;.&quot;, s1, len);&#125;fn calculate_length(s: &amp;String) -&gt; usize &#123;    s.len()&#125;\n这种形式可以在没有ownership的情况下访问某个值。其原理如下图：\n\n这个例子和我们在前面写的例子很相似。仔细观察会发现一些端倪。主要有两点不同：\n\n在传入参数的时候，s1前面加了&amp;符。这意味着我们创建了一个s1的引用，它并不是数据的owner，因此在它超出作用域时也不会销毁数据。\n函数在接收参数时，变量类型String前也加了&amp;符。这表示参数要接收的是一个字符串的引用对象。\n\n我们把函数中接收引用的参数称为借用。就像实际生活中我写完了作业，可以借给你抄一下，但它不属于你，抄完你还要还给我。（友情提示：非紧急情况不要抄作业）\n另外还需要注意，我的作业可以借给你抄，但是你不能改我写的作业，我本来写对了你给我改错了，以后我还怎么借给你？所以，在calculate_length中，s是不可以修改的。\n可修改引用如果我发现我写错了，让你帮我改一下怎么办？我授权给你，让你帮忙修改，你也需要表示能帮我修改就可以了。Rust也有办法。还记得我们前面介绍的可变变量和不可变变量吗？引用也是类似，我们可以使用mut关键字使引用可修改。\nfn main() &#123;    let mut s = String::from(&quot;hello&quot;);    change(&amp;mut s);&#125;fn change(some_string: &amp;mut String) &#123;    some_string.push_str(&quot;, world&quot;);&#125;\n这样，我们就能在函数中对引用的值进行修改了。不过这里还要注意一点，在同一作用域内，对于同一个值，只能有一个可修改的引用。这也是因为Rust不想有并发修改数据的情况出现。\n如果需要使用多个可修改引用，我们可以自己创建新的作用域：\nlet mut s = String::from(&quot;hello&quot;);&#123;    let r1 = &amp;mut s;&#125; // r1 超出作用域let r2 = &amp;mut s;\n另一个冲突就是“读写冲突”，即不可变引用和可变引用之间的限制。\nlet mut s = String::from(&quot;hello&quot;);let r1 = &amp;s; // no problemlet r2 = &amp;s; // no problemlet r3 = &amp;mut s; // BIG PROBLEMprintln!(&quot;&#123;&#125;, &#123;&#125;, and &#123;&#125;&quot;, r1, r2, r3);\n这样的代码在编译时也会报错。这是因为不可变引用不希望在被使用之前，其指向的值被修改。这里只要稍微处理一下就可以了：\nlet mut s = String::from(&quot;hello&quot;);let r1 = &amp;s; // no problemlet r2 = &amp;s; // no problemprintln!(&quot;&#123;&#125; and &#123;&#125;&quot;, r1, r2);// r1 和 r2 不再使用let r3 = &amp;mut s; // no problemprintln!(&quot;&#123;&#125;&quot;, r3);\nRust编译器会在第一个print语句之后判断出r1和r2不会再被使用，此时r3还没有创建，它们的作用域不会有交集。所以这段代码是合法的。\n空指针对于可操作指针的编程语言来讲，最令人头疼的问题也许就是空指针了。通常情况是，在回收内存以后，又使用了指向这块内存的指针。而Rust的编译器帮助我们避免了这个问题（再次感谢Rust编译器）。\nfn main() &#123;    let reference_to_nothing = dangle();&#125;fn dangle() -&gt; &amp;String &#123;    let s = String::from(&quot;hello&quot;);    &amp;s&#125;\n来看一下上面这个例子。在dangle函数中，返回值是字符串s的引用。但是在函数结束时，s的内存已经被回收了。所以s的引用就成了空指针。此时就会报expected lifetime parameter的编译错误。\n另一种引用：Slice除了引用之外，还有另一种没有ownership的数据类型叫做Slice。Slice是一种使用集合中一段序列的引用。\n这里通过一个简单的例子来说明Slice的使用方法。假设我们需要得到给你字符串中的第一个单词。你会怎么做？其实很简单，遍历每个字符，如果遇到空格，就返回之前遍历过的字符的集合。\n对字符串的遍历方法我来剧透一下，as_bytes函数可以把字符串分解成字节数组，iter是返回集合中每个元素的方法，enumerate是提取这些元素，并且返回(元素位置,元素值)这样的二元组的方法。这样是不是可以写出来了。\nfn first_word(s: &amp;String) -&gt; usize &#123;    let bytes = s.as_bytes();    for (i, &amp;item) in bytes.iter().enumerate() &#123;        if item == b&#x27; &#x27; &#123;            return i;        &#125;    &#125;    s.len()&#125;\n来，感受下这个例子，虽然它返回的是第一个空格的位置，但是只要会字符串截取，还是可以达到目的的。不过不能剧透字符串截取了，不然暴露不出问题。\n这么写的问题在哪呢？来看一下main函数。\nfn main() &#123;    let mut s = String::from(&quot;hello world&quot;);    let word = first_word(&amp;s);    s.clear();&#125;\n这里在获取空格位置后，对字符串s做了一个clear操作，也就是把s清空了。但word仍然是5，此时我们再去对截取s的前5个字符就会出问题。可能有人认为自己不会这么蠢，但是你愿意相信你的好（zhu）伙（dui）伴（you）也不会这么做吗？我是不相信的。那怎么办呢？这时候slice就要登场了。\n使用slice可以获取字符串的一段字符序列。例如&amp;s[0..5]可以获取字符串s的前5个字符。其中0为起始字符的位置下标，5是结束字符位置的下标加1。也就是说slice的区间是一个左闭右开区间。\nslice还有一些规则：\n\n如果起始位置是0，则可以省略。也就是说&amp;s[0..2]和&amp;s[..2]等价\n如果起始位置是集合序列末尾位置，也可以省略。即&amp;s[3..len]和&amp;s[3..]等价\n根据以上两条，我们还可以得出&amp;s[0..len]和&amp;s[..]等价\n\n这里需要注意的是，我们截取字符串时，其边界必须是UTF-8字符。\n有了slice，就可以解决我们的问题了\nfn first_word(s: &amp;String) -&gt; &amp;str &#123;    let bytes = s.as_bytes();    for (i, &amp;item) in bytes.iter().enumerate() &#123;        if item == b&#x27; &#x27; &#123;            return &amp;s[0..i];        &#125;    &#125;    &amp;s[..]&#125;\n现在我们在main函数中对s执行clear操作时，编译器就不同意了。没错，又是万能的编译器。\n除了slice除了可以作用于字符串以外，还可以作用于其他集合，例如：\nlet a = [1, 2, 3, 4, 5];let slice = &amp;a[1..3];\n关于集合，我们以后会有更加详细的介绍。\n总结本文介绍的Ownership特性对于理解Rust来讲非常重要。我们介绍了什么是Ownership，Ownership的转移，以及不占用Ownership的数据类型Reference和Slice。\n怎么样？是不是感觉今天的坑非常给力？如果之前在地下一层的话，那现在已经到地下三层了。所以请各位注意安全，有序降落。\n","tags":["Rust"]},{"title":"Rust入坑指南：步步为营","url":"/2020/02/21/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E6%AD%A5%E6%AD%A5%E4%B8%BA%E8%90%A5/","content":"俗话说：“测试写得好，奖金少不了。”\n有经验的开发人员通常会通过单元测试来保证代码基本逻辑的正确性。如果你是一名新手开发者，并且还没体会到单元测试的好处，那么建议你先读一下我之前的一篇文章代码洁癖系列（七）：单元测试的地位。\n写单元测试一般需要三个步骤：\n\n准备测试用例，测试用例要能覆盖尽可能多的代码\n执行需要测试的代码\n判断结果，是否是你希望得到的结果\n\n了解了这些以后，我们就来看看在Rust中应该怎么写单元测试。\n首先我们建立一个library项目\n$ cargo new adder --lib     Created library `adder` project\n然后在src/lib.rs文件中开始写测试代码\n#[cfg(test)]mod tests &#123;    #[test]    fn it_works() &#123;        assert_eq!(2 + 2, 4);    &#125;&#125;\n此时在命令行运行cargo test就会得到测试结果\n\n可以看到，结果显示，Rust运行了一项测试并且测试通过。后面的Doc-tests我们先放下，以后再聊。\n当然，这并不是我们常见的测试，在日常开发中，我们通常是先写我们的业务代码然后再对各个函数进行单元测试，最后还会对某个模块进行集成测试。那么我们就来模拟一下日常开发过程中应该如何来写测试。\n单元测试我们仍然是用上面的项目，先来在src/lib.rs中写一段“业务代码”\npub fn add_two(a: i32) -&gt; i32 &#123;    internal_adder(a, 2)&#125;fn internal_adder(a: i32, b: i32) -&gt; i32 &#123;    a + b&#125;\n这是一段非常简单的代码，对外暴露的函数只是一个加2的功能，内部调用了一个两数相加的函数。现在我们就对这个内部函数做一个单元测试。\n#[cfg(test)]mod tests &#123;    use super::*;    #[test]    fn internal() &#123;        assert_eq!(4, internal_adder(2, 2));    &#125;&#125;\n在测试模块中，如果想要使用我们业务代码中的函数，就需要通过use super::*;将其引入可用范围。接着，还是执行cargo test，测试结果与刚才类似。\n\n测了半天全是通过的没什么意思，单元测试真正的作用是要发现代码中的问题，所以我们来尝试一个错误的试一下。假设我们希望2+2等于5。\n\n这里我们的assert_eq!左右不相等，引起了线程恐慌，因此导致测试失败。结果中给出了失败的原因，引起失败的位置，并且有一句提示：note: run with RUST_BACKTRACE=1 environment variable to display a backtrace. 我们按照这个提示，设置变量RUST_BACKTRACE=1，此时再执行cargo test。\n\nRust就会将错误栈打印出来，根据结果提示，这并不是完整的错误栈，我们还可以将RUST_BACKTRACE设置为full来查看更加详细的信息。这里我就不做演示了。\n集成测试接下来我们再演示一下集成测试。我们通常将集成测试单独放到一个目录中，在lib.rs文件中，rust识别测试mod的名称是tests，同样的，我们在src下创建tests目录。tests目录下就是我们的所有集成测试代码。\n\n如图，integration_test是我们测试代码的文件，common目录下的mod.rs文件中是一些集成测试必要的配置。这里我们只是放了一个空的setup函数。\n在集成测试中，我们就要像正常他人使用我们的代码时那样来进行测试，首先需要将我们的mod引入到可用范围，当然还需要加上common的mod。\nuse adder;mod common;#[tests]fn it_adds_two() &#123;    common::setup();    assert_eq!(4, adder::add_two(2));&#125;\n接着就可以测试我们对外暴露的函数了。\n\nok，集成测试的方法我们也掌握了。现在来看看一直被我们忽略的Doc-tests吧。\n文档测试我们已经知道，Rust中的注释是双斜线//，像我们刚刚写的library代码，如果想要把它发布到crate.io上让别人使用，那么我们就需要增加相应的文档，这里文档的每行都应该是三斜线///开头，而文档中也应该放一些例子供他人参考。（注意：下面注释中的代码需要包含在markdown的代码块格式中，这里写上三个`的话文档格式会乱掉。。。运行测试代码时请自行补充）\n/// Adds two to the number given.////// # Examples////// /// let arg = 5;/// let answer = adder::add_two(arg);////// assert_eq!(7, answer);/// pub fn add_two(a: i32) -&gt; i32 &#123;    internal_adder(a, 2)&#125;\n现在我给add_two函数加上了文档，我们再次执行cargo test命令。\n\n现在我们就明白了，Doc-tests测试就是运行我们文档中的例子。\n常用特性到目前为止，我们已经知道了在Rust中如何写测试代码了。接下来我们再来了解几个比较常用的特性。\n运行指定的测试代码我们在开发过程中肯定不会每次都去跑全量的单元测试，那样太浪费时间了。通常是我们开发完一个功能之后，编写对应的单元测试，然后单独跑这个测试。那么Rust中能不能单独跑一个单元测试呢？答案是肯定的。\n相信细心的同学已经发现了，Rust测试结果中，是针对每个测试单独统计结果，并且每个测试都有自己的名字，像我们前面写的it_works和internal。假设我们的代码中同时存在这两个函数，如果你想要单独跑internal这一个测试，就可以使用cargo test internal命令。\n你也可以使用这种方法来执行多个名称类似的测试，假如我们有名称为internal_a的测试，那么执行cargo test internal命令时它也会被执行。\n忽略某个测试当我们有一个测试执行时间非常长的时候，我们一般不会轻易去执行，这时如果你想要执行多个测试，除了用我们上面提到的方法，去指定不同的名称列表以外。还可以把这个测试忽略掉。\n现在我不想执行internal测试了，只需要对代码进行如下改动：\n#[test]#[ignore]fn internal() &#123;  assert_eq!(4, internal_adder(2, 2));&#125;\n这时再来运行测试，结果如图所示。\n\n我们发现此时internal测试已经被忽略了。\n测试异常情况除了测试代码逻辑正常的情况，我们有时还需要测试一些异常情况，比如接收到非法参数时程序能否返回我们希望看到的异常。\n我们首先来看一下如何测试程序返回异常信息。\nRust为我们提供了一个叫做should_panic的注解。我们可以使用它来测试程序是否返回异常：\npub fn add_two(a: i32) -&gt; i32 &#123;    internal_adder(a, 2)&#125;fn internal_adder(a: i32, b: i32) -&gt; i32 &#123;    if a &lt; 0 &#123;        panic!(&quot;a should bigger than 0&quot;);    &#125;    a + b&#125;#[cfg(test)]mod tests &#123;    use super::*;    #[test]    #[should_panic]    fn internal() &#123;        assert_eq!(4, internal_adder(-2, 2));    &#125;&#125;\n此时我们运行测试时就会发现internal测试通过，因为它发生了线程恐慌，这是我们希望看到的结果。\n\n另外，我们还可以再指定我们具体期望的异常，那么就可以在should_panic后面加上expected参数。\n#[test]#[should_panic(expected = &quot;a should be positive&quot;)]fn internal() &#123;  assert_eq!(4, internal_adder(-2, 2));&#125;\n大家可以自行运行一下这段测试代码看看效果。\n总结文中我向大家介绍了在Rust中如何进行单元测试、集成测试，还有比较特殊的文档测试。最后还介绍了3种常见的测试特性。\n最后想友情提醒大家一下，在开发过程中，不要写完一堆功能后再开始写单元测试，这时你很有可能会因为测试代码过于繁琐而放弃。建议大家每写一个功能，随即开始进行单元测试，这样也能立即看到自己的代码的执行效果，提高成就感。这就是所谓的“步步为营”。\n","tags":["Rust"]},{"title":"Rust入坑指南：海纳百川","url":"/2020/01/14/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E6%B5%B7%E7%BA%B3%E7%99%BE%E5%B7%9D/","content":"今天来聊Rust中两个重要的概念：泛型和trait。很多编程语言都支持泛型，Rust也不例外，相信大家对泛型也都比较熟悉，它可以表示任意一种数据类型。trait同样不是Rust所特有的特性，它借鉴于Haskell中的Typeclass。简单来讲，Rust中的trait就是对类型行为的抽象，你可以把它理解为Java中的接口。\n泛型在前面的文章中，我们其实已经提及了一些泛型类型。例如Option、Vec和Result&lt;T, E&gt;。泛型可以在函数、数据结构、Enum和方法中进行定义。在Rust中，我们习惯使用T作为通用的类型名称，当然也可以是其他名称，只不过习惯上优先使用T（Type）来表示。它可以帮我们消除一些重复代码，例如实现逻辑相同但参数类型不同的两个函数，我们就可以通过泛型技术将其进行合并。下面我们分别演示泛型的几种定义。\n在函数中定义泛型在函数的定义中，可以是参数，也可以是返回值。前提是必须要在函数名的后面加上。\nfn largest&lt;T&gt;(list: &amp;[T]) -&gt; T &#123;\n在数据结构中定义如果数据结构中某个字段可以接收任意数据类型，那么我们可以把这个字段的类型定义为T，同样的，为了让编译器认识这个T，我们需要在结构体名称后边标识一下。\nstruct Point&lt;T&gt; &#123;    x: T,    y: T,&#125;\n上面的例子中，x和y都是可以接受任意类型，但是，它们两个的类型必须相同，如果传入的类型不同，编译器仍然会报错。那如果想要让x和y能够接受不同的类型应该怎么办呢？其实也很简单，我们定义两种不同的泛型就好了。\nstruct Point&lt;T, U&gt; &#123;    x: T,    y: U,&#125;\n在Enum中定义在Enum中定义泛型我们已经接触过比较多了，最常见的例子就是Option和Result&lt;T, E&gt;。其定义方法也和在数据结构中的定义方法类似\nenum Result&lt;T, E&gt; &#123;    Ok(T),    Err(E),&#125;\n在方法中定义我们在实现定义了泛型的数据结构或Enum时，方法中也可以定义泛型。例如我们对刚刚定义的Point进行实现。\nimpl&lt;T&gt; Point&lt;T&gt; &#123;    fn x(&amp;self) -&gt; &amp;T &#123;        &amp;self.x    &#125;&#125;\n可以看到，我们的方法返回值的类型是T的引用，为了让编译器识别T，我们必须要在impl后面加上&lt;T&gt;。\n另外，我们在对结构体进行实现时，也可以实现指定的类型，这样就不需要在impl后面加标识了。\nimpl Point&lt;f32&gt; &#123;    fn distance_from_origin(&amp;self) -&gt; f32 &#123;        (self.x.powi(2) + self.y.powi(2)).sqrt()    &#125;&#125;\n了解了泛型的几种定义之后，你有没有想过一个问题：Rust中使用泛型会对程序运行时的性能造成不良影响吗？答案是不会，因为Rust对于泛型的处理都是在编译阶段进行的，对于我们定义的泛型，Rust编译器会对其进行单一化处理，也就是说，我们定义一个具有泛型的函数（或者其他什么的），Rust会根据需要将其编译为具有具体类型的函数。\nlet integer = Some(5);let float = Some(5.0);\n例如我们的代码使用了这两种类型的Option，那么Rust编译器就会在编译阶段生成两个指定具体类型的Option。\nenum Option_i32 &#123;    Some(i32),    None,&#125;enum Option_f64 &#123;    Some(f64),    None,&#125;\n这样我们在运行阶段直接使用对应的Option就可以了，而不需要再进行额外复杂的操作。所以，如果我们泛型定义并使用的范围很大，也不会对运行时性能造成影响，受影响的只有编译后程序包的大小。\nTraitTrait可以说是Rust的灵魂，Rust中所有的抽象都是依靠Trait来实现的。\n我们先来看看如何定义一个Trait。\npub trait Summary &#123;    fn summarize(&amp;self) -&gt; String;&#125;\n定义trait使用了关键字trait，后面跟着trait的名称。其内容是trait的「行为」，也就是一个个函数。但是这里的函数没有实现，而是直接以;结尾。不过这这并不是必须的，Rust也支持下面这种写法：\npub trait Summary &#123;    fn summarize(&amp;self) -&gt; String &#123;        String::from(&quot;(Read more...)&quot;)    &#125;&#125;\n对于这样的写法，它表示summarize函数的默认实现。\nTrait的实现上面是一种默认实现，接下来我们介绍一下在Rust中，对一个Trait的常规实现。Trait的实现是需要针对结构体的，即我们要写明是哪个结构体的哪种行为。\npub struct NewsArticle &#123;    pub headline: String,    pub location: String,    pub author: String,    pub content: String,&#125;impl Summary for NewsArticle &#123;    fn summarize(&amp;self) -&gt; String &#123;        format!(&quot;&#123;&#125;, by &#123;&#125; (&#123;&#125;)&quot;, self.headline, self.author, self.location)    &#125;&#125;pub struct Tweet &#123;    pub username: String,    pub content: String,    pub reply: bool,    pub retweet: bool,&#125;impl Summary for Tweet &#123;    fn summarize(&amp;self) -&gt; String &#123;        format!(&quot;&#123;&#125;: &#123;&#125;&quot;, self.username, self.content)    &#125;&#125;\n上述代码中，我们分别定义了结构体NewArticle和Tweet，然后为它们实现了trait，定义了summarize函数对应的逻辑。\n作为参数的Trait此外，trait还可以作为函数的参数，也就是需要传入一个实现了对应trait的结构体的实例。\npub fn notify(item: impl Summary) &#123;    println!(&quot;Breaking news! &#123;&#125;&quot;, item.summarize());&#125;\n作参数时，我们需要使用impl关键字来定义参数类型。\nRust还提供了另一种语法糖来，即Trait限定，我们可以使用泛型约束的语法来限定Trait参数。\npub fn notify&lt;T: Summary&gt;(item: T) &#123;    println!(&quot;Breaking news! &#123;&#125;&quot;, item.summarize());&#125;\n如上述代码，我们可以通过Trait来限定泛型T的范围。这样的语法糖可以在多个参数的函数中帮助我们简化代码。下面两行代码就有比较明显的对比\npub fn notify(item1: impl Summary, item2: impl Summary) &#123;pub fn notify&lt;T: Summary&gt;(item1: T, item2: T) &#123;\n如果某个参数有多个trait限定，就可以使用+来表示\npub fn notify&lt;T: Summary + Display&gt;(item: T) &#123;\n如果我们有更多的参数，并且有每个参数都有多个trait限定，及时我们使用了上面这种语法糖，代码仍然有些繁杂，会降低可读性。所以Rust又为我们提供了where关键字。\nfn some_function&lt;T, U&gt;(t: T, u: U) -&gt; i32    where T: Display + Clone,          U: Clone + Debug&#123;\n它帮助我们在函数定义的最后写一个trait限定列表，这样可以使代码的可读性更高。\nTrait作为返回值fn returns_summarizable() -&gt; impl Summary &#123;    Tweet &#123;        username: String::from(&quot;horse_ebooks&quot;),        content: String::from(&quot;of course, as you probably already know, people&quot;),        reply: false,        retweet: false,    &#125;&#125;\nTrait作为返回值类型，和作为参数类似，只需要在定义返回类型时使用impl Trait。\n总结本文我们简单介绍了泛型和Trait，包括它们的定义和使用方法。泛型主要是针对数据类型的一种抽象，而Trait则是对数据类型行为的一种抽象，Rust中并没有严格意义上的继承，多是用组合的形式。这也体现了「多组合，少继承」的设计思想。\n最后留个预告，这个坑还没完，我们下次继续往深处挖。\n","tags":["Rust"]},{"title":"Rust入坑指南：鳞次栉比","url":"/2019/11/27/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E9%B3%9E%E6%AC%A1%E6%A0%89%E6%AF%94/","content":"很久没有挖Rust的坑啦，今天来挖一些排列整齐的坑。没错，就是要介绍一些集合类型的数据类型。“鳞次栉比”这个标题是不是显得很有文化？\n在Rust入坑指南：常规套路一文中我们已经介绍了一些基本数据类型了，它们都存储在栈中，今天我们重点介绍3种数据类型：string，vector和hash map。\nStringString类型我们在之前的学习中已经有了较多的接触，但是没有进行过详细的介绍。有些有编程基础的同学可能不屑于学习String类型，毕竟它在所有编程语言中可以说是最常用的类型了，大家也都很熟悉了。对于有这种心理的同学，我想对他们说：我刚开始也是这样想的，直到后来我被编译器揍的满头包，才下定决心回来认真学习一下String类型。\nRust的字符串分为以下几种类型：\n\nstr：表示固定长度的字符串\nString：表示可增长的字符串\nCStr：表示由C分配，被Rust借用的字符串，一般用于和C语言交互\nCString：表示由Rust分配并且可以传递给C语言的字符串\nOsStr：表示和操作系统相关的字符串，主要为了兼容Windows\nOsString：OsStr的可变版本\nPath：表示路径\nPathBuf：是Path的可变版本\n\n本文我们重点讨论前两种，因为它们是开发过程中最常用的，也是比较容易混淆的。对于str，我们常见的是它的引用类型，&amp;str。如果你看过了Rust入坑指南：核心概念 )一文后，相信你已经了解了引用类型和Ownership的概念。也就是说String类型具有Ownership而&amp;str没有。\n在Rust中，String本质上是Vec\\，Vec是向量集合的关键字，我们在后面会介绍。String类型由三个部分组成，分别是：指向堆中字节序列的指针，记录堆中字节序列的长度和堆分配的容量。通过一段代码也许你很有更深的理解。\nfn main() &#123;    let mut a = String::from(&quot;foo&quot;);    println!(&quot;&#123;:p&#125;&quot;, a.as_ptr());    println!(&quot;&#123;:p&#125;&quot;, &amp;a);    assert_eq!(a.len(), 3);    a.reserve(10);    assert_eq!(a.capacity(), 13);&#125;\n在这段代码中我们可以看到，a.as_ptr()获取指针和&amp;a获取的指针是不一样的。\n\n这里我们解释一下，as_ptr获取到的指针是堆中字节序列的指针地址，而&amp;a的地址是字符串变量在栈上的指针地址。另外，len()和capacity()方法得到的长度都是字节数量，而非字符数量。这里你可以自己动手试试中文字符的长度。\n聊完了字符串的基本概念以后，相信你已经对Rust的字符串有了一个大概的认识。接下来我们就一起来看一看字符串的CRUD的方法吧。\n创建字符串话不多说，先来展示一下创建字符串的各种方法。\nfn main() &#123;    let string: String = String::new();    let string: String = String::from(&quot;hello rust&quot;);    let string: String = String::with_capacity(10);    let str: &amp;&#x27;static str = &quot;Jackey&quot;;    let string: String = str.to_owned();    let string: String = str.to_string();&#125;\n我们比较常用的是前两种，下面介绍一下后面几个方法。with_capacity()是创建一个空字符串，参数表示在堆中分配的字节数。to_owned和to_string是演示了如何把&amp;str类型转换成String类型。\n修改字符串Rust修改字符串的常用方法也有很多，例如在字符串后追加，连接两个字符串，更新字符串等。下面这段代码就展示了一些修改字符串的方法。\nfn main() &#123;    let mut hello = String::from(&quot;Hello, &quot;);    hello.push(&#x27;J&#x27;);    // 追加单个字符    hello.push_str(&quot;ackey! &quot;);    //追加字符串    println!(&quot;push: &#123;&#125;&quot;, hello);    hello.extend([&#x27;M&#x27;, &#x27;y&#x27;, &#x27; &#x27;].iter());   //追加多个字符，参数为迭代器    hello.extend(&quot;name&quot;.chars());    println!(&quot;extend: &#123;&#125;&quot;, hello);    hello.insert(0, &#x27;h&#x27;);   //类似于push，可以指定插入的位置    hello.insert(1, &#x27;a&#x27;);    hello.insert(2, &#x27;!&#x27;);    hello.insert_str(0, &quot;Haha&quot;);    println!(&quot;insert: &#123;&#125;&quot;, hello);    let left = &quot;Hello, &quot;.to_string();    let right = &quot;World&quot;.to_string();    let result = left + &amp;right;    println!(&quot;+: &#123;&#125;&quot;, result);   //使用+连接字符串时，第二个必须为引用    let mut message = &quot;rust&quot;.to_string();   //使用+=连接字符串时，字符串必须定义为可变    message += &quot;!&quot;;    println!(&quot;+=: &#123;&#125;&quot;, message);    let s = String::from(&quot;foobar&quot;);    let s: String = s        .chars()        .enumerate()        .map(|(_i, c)| &#123;c.to_uppercase().to_string()&#125;)        .collect();    println!(&quot;update chars: &#123;&#125;&quot;, s);      let s1 = String::from(&quot;hello&quot;);    let s2 = String::from(&quot;rust&quot;);    let s3 = format!(&quot;&#123;&#125;-&#123;&#125;&quot;, s1, s2);    println!(&quot;format: &#123;&#125;&quot;, s3);&#125;\n我们对上面的代码做一些补充的解释。\npush和insert类似，带有_str的方法接收的参数是字符串，否则只能接收单个字符。insert可以指定插入的位置，而push只能在字符串末尾插入。\n使用「+」连接字符串时，第一个参数是String类型，第二个则需要是引用类型&amp;str。这类似于我们调用一个add方法，它的定义是这样的：\nfn add(self, s: &amp;str) -&gt; String &#123;\n所以，第一个参数的ownership转移到了函数中，又通过返回结果传递出来。也就是说，在使用了+操作符之后，left已经没有ownership了。\n字符串查找在Rust中，字符串是不能根据位置来获取到指定字符的。也就是下面这段代码是编译不过的。\nlet s1 = String::from(&quot;hello&quot;);let h = s1[0];\n因为，Rust会认为这个0是指第一个字节，而Rust字符串中的字符可能占有多个字节（还记得前面我让你用中文字符实验代码吗？）所以，如果你单纯的想要获取一个字节，编译器不知道你是真的想要获取字节对应的数值，还是要获取那个字符。\n我们在处理字符串时通常有以下方法：\nfn main() &#123;    let hello = &quot;Здравствуйте&quot;;    let s = &amp;hello[0..4];    println!(&quot;&#123;&#125;&quot;, s);    let chars = hello.chars();    for c in chars &#123;        println!(&quot;&#123;&#125;&quot;, c);    &#125;    let bytes = hello.bytes();    for byte in bytes &#123;        println!(&quot;&#123;&#125;&quot;, byte);    &#125;    let get = hello.get(0..1);    let mut s = String::from(&quot;hello&quot;);    let get_mut = s.get_mut(3..5);    let message = String::from(&quot;hello-world&quot;);    let (left, right) = message.split_at(6);    println!(&quot;left: &#123;&#125;, right: &#123;&#125;&quot;, left, right);&#125;\n通常是使用字符切片，也可以使用chars方法获取到Chars迭代器，然后可以对每个字符进行单独处理。此外，使用get或get_mut方法也可以接收索引范围，返回指定的字符串切片。返回结果是Option类型，这是因为如果指定的索引返回不能返回完整字符，那么Rust就会返回None。这里也可以使用is_char_boundary方法来判断一个位置是否是非法边界。\n最后，也可以使用split_at或split_at_mut方法来分割字符串。这要求分割的位置正好是字符边界位置，如果不是，程序就会崩溃。\n删除字符串Rust的标准库提供了一些删除字符串的方法，我们来演示一些：\nfn main() &#123;    let mut hello = String::from(&quot;hello&quot;);    hello.remove(3);    println!(&quot;remove: &#123;&#125;&quot;, hello);    hello.pop();    println!(&quot;pop: &#123;&#125;&quot;, hello);    hello.truncate(1);    println!(&quot;truncate: &#123;&#125;&quot;, hello);    hello.clear();    println!(&quot;clear: &#123;&#125;&quot;, hello);&#125;\n结果如图：\n\nremove方法用来删除字符串中的某个字符，其接收的参数是字符的起始位置，如果是不是某个字符的起始位置，会导致程序崩溃。\npop方法会弹出字符串末尾的字符，truncate方法是截取指定长度字符串，而clear方法则是用来清空字符串。\n至此，关于Rust中的字符串的基本概念和CRUD我们都已经介绍完了，接下来我们再来看另一种集合类型Vector。\nVectorVector是用来存储相同数据类型的多个数据一种数据类型。它的关键字是Vec&lt;T&gt;。下面我们一起来看看向量的CRUD吧。\n创建向量fn main() &#123;    let v1: Vec&lt;i32&gt; = Vec::new();    let v2 = vec![1, 2, 3];&#125;\n上面这段代码演示了创建一个向量的两种方式，第一种是使用new函数来创建一个空的向量，由于没有添加元素，所以要显式的指定存储元素的类型。第二种是创建一个有初始值的向量集合，我们直接使用vec！宏，然后指定初始值即可，不需要指定向量中元素的数据类型，因为编译器可以自己推断出来。\n更新向量fn main() &#123;    let mut v = Vec::new();    v.push(1);    v.push(2);&#125;\n创建一个空的向量之后，如果我们想要增加元素，就可以直接使用push方法，向末尾追加元素。\n删除向量fn main() &#123;    let mut v = Vec::new();    v.push(1);    v.push(2);    v.push(3);    v.pop();    for i in &amp;v &#123;        println!(&quot;&#123;&#125;&quot;, i);    &#125;&#125;\n删除单个元素可以使用pop方法，而要删除整个向量，只能像其他结构体一样，到其ownership失效。\n读取向量元素fn main() &#123;    let v = vec![1, 2, 3, 4, 5];    let third: &amp;i32 = &amp;v[2];    println!(&quot;The third element is &#123;&#125;&quot;, third);    match v.get(2) &#123;        Some(third) =&gt; println!(&quot;The third element is &#123;&#125;&quot;, third),        None =&gt; println!(&quot;There is no third element.&quot;),    &#125;    let v = vec![100, 32, 57];    for i in &amp;v &#123;        println!(&quot;&#123;&#125;&quot;, i);    &#125;&#125;\n当你需要读取单个指定元素时，有两种方法可以用，一种是使用[]，另一种是使用get方法。两种方法的区别是：第一种返回的是元素的类型，而get返回的是Option类型。如果你指定的位置越界了，那么使用第一种方法程序会直接崩溃，而使用第二种方法则会返回None。\n此外，还可以通过遍历向量的形式来读取元素。如果想要存储不同类型的数据，我们可以借助枚举类型。\nfn main() &#123;    enum SpreadsheetCell &#123;        Int(i32),        Float(f64),        Text(String),    &#125;    let row = vec![        SpreadsheetCell::Int(3),        SpreadsheetCell::Text(String::from(&quot;blue&quot;)),        SpreadsheetCell::Float(10.12),    ];&#125;\nHashMapHashMap存储了KV结构的数据，各个Key必须是同一种类型，各个Value必须是同一种类型。由于HashMap是三种集合类型中使用最少的，所以在使用之前，需要手动引入进来\nuse std::collections::HashMap;\n创建HashMap首先我们来了解一下如何创建一个新的Hash Map并增加元素。\nuse std::collections::HashMap;fn main() &#123;    let field_name = String::from(&quot;Favorite color&quot;);    let field_value = String::from(&quot;Blue&quot;);    let mut map = HashMap::new();    map.insert(field_name, field_value);&#125;\n注意，在使用insert方法时，field_name和field_value都会失去所有权。那如何再使用它们呢？我们只能从Hash Map中再拿出来。\n访问Hash Map的数据use std::collections::HashMap;fn main() &#123;    let field_name = String::from(&quot;Favorite color&quot;);    let field_value = String::from(&quot;Blue&quot;);    let mut map = HashMap::new();    map.insert(field_name, field_value);    let favorite = String::from(&quot;Favorite color&quot;);    let color = map.get(&amp;favorite);    match color &#123;        Some(x) =&gt; println!(&quot;&#123;&#125;&quot;, x),        None =&gt; println!(&quot;None&quot;),    &#125;&#125;\n可以看到，我们使用get可以获取到指定Key的值，get方法返回的是Option类型，如果没有指定的Value，则会返回None。此外，也可以使用for循环来遍历Hash Map。\nuse std::collections::HashMap;fn main() &#123;    let mut scores = HashMap::new();    scores.insert(String::from(&quot;Blue&quot;), 10);    scores.insert(String::from(&quot;Yellow&quot;), 50);    for (key, value) in &amp;scores &#123;        println!(&quot;&#123;&#125;: &#123;&#125;&quot;, key, value);    &#125;&#125;\n更新Hash Map当我们向同一个Key insert值时，旧的值就会被覆盖。如果只想要在Key不存在时插入，则可以使用entry。\nuse std::collections::HashMap;fn main() &#123;    let mut scores = HashMap::new();    scores.insert(String::from(&quot;Blue&quot;), 10);    scores.entry(String::from(&quot;Yellow&quot;)).or_insert(50);    scores.entry(String::from(&quot;Blue&quot;)).or_insert(50);    println!(&quot;&#123;:?&#125;&quot;, scores);&#125;\n总结今天带大家一起挖了三个坑，string，vector和hash map，分别介绍了每种数据类型的CRUD。对string的介绍占了比较大的篇幅，因为它是最常用的数据类型之一。当然这部分的相关知识还有很多，欢迎大家和我一起学习交流。\n","tags":["Rust"]},{"title":"Rust入坑指南：齐头并进（上）","url":"/2020/03/15/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E9%BD%90%E5%A4%B4%E5%B9%B6%E8%BF%9B%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"我们知道，如今CPU的计算能力已经非常强大，其速度比内存要高出许多个数量级。为了充分利用CPU资源，多数编程语言都提供了并发编程的能力，Rust也不例外。\n聊到并发，就离不开多进程和多线程这两个概念。其中，进程是资源分配的最小单位，而线程是程序运行的最小单位。线程必须依托于进程，多个线程之间是共享进程的内存空间的。进程间的切换复杂，CPU利用率低等缺点让我们在做并发编程时更加倾向于使用多线程的方式。\n当然，多线程也有缺点。其一是程序运行顺序不能确定，因为这是由内核来控制的，其二就是多线程编程对开发者要求比较高，如果不充分了解多线程机制的话，写出的程序就非常容易出Bug。\n多线程编程的主要难点在于如何保证线程安全。什么是线程安全呢？因为多个线程之间是共享内存空间的，因此就会存在同时对相同的内存进行写操作，那就会出现写入数据互相覆盖的问题。如果多个线程对内存只有读操作，没有任何写操作，那么也就不会存在安全问题，我们可以称之为线程安全。\n常见的并发安全问题有竞态条件和数据竞争两种，竞态条件是指多个线程对相同的内存区域（我们称之为临界区）进行了“读取-修改-写入”这样的操作。而数据竞争则是指一个线程写一个变量，而另一个线程需要读这个变量，此时两者就是数据竞争的关系。这么说可能不太容易理解，不过不要紧，待会儿我会举两个具体的例子帮助大家理解。不过在此之前，我想先介绍一下Rust中是如何进行并发编程的。\n管理线程在Rust标准库中，提供了两个包来进行多线程编程：\n\nstd::thread，定义一些管理线程的函数和一些底层同步原语\nstd::sync，定义了锁、Channel、条件变量和屏障\n\n我们使用std::thread中的spawn函数来创建线程，它的使用非常简单，其参数是一个闭包，传入创建的线程需要执行的程序。\nuse std::thread;use std::time::Duration;fn main() &#123;    thread::spawn(|| &#123;        for i in 1..10 &#123;            println!(&quot;hi number &#123;&#125; from the spawned thread!&quot;, i);            thread::sleep(Duration::from_millis(1));        &#125;    &#125;);    for i in 1..5 &#123;        println!(&quot;hi number &#123;&#125; from the main thread!&quot;, i);        thread::sleep(Duration::from_millis(1));    &#125;&#125;\n这段代码中，我们有两个线程，一个主线程，一个是用spawn创建出来的线程，两个线程都执行了一个循环。循环中打印了一句话，然后让线程休眠1毫秒。它的执行结果是这样的：\n\n从结果中我们能看出两件事：第一，两个线程是交替执行的，但是并没有严格的顺序，第二，当主线程结束时，它并没有等子线程运行完。\n那我们有没有办法让主线程等子线程执行结束呢？答案当然是有的。Rust中提供了join函数来解决这个问题。\nuse std::thread;use std::time::Duration;fn main() &#123;    let handle = thread::spawn(|| &#123;        for i in 1..10 &#123;            println!(&quot;hi number &#123;&#125; from the spawned thread!&quot;, i);            thread::sleep(Duration::from_millis(1));        &#125;    &#125;);    for i in 1..5 &#123;        println!(&quot;hi number &#123;&#125; from the main thread!&quot;, i);        thread::sleep(Duration::from_millis(1));    &#125;    handle.join().unwrap();&#125;\n这样主线程就必须要等待子线程执行完毕。\n在某些情况下，我们需要将一些变量在线程间进行传递，正常来讲，闭包需要捕获变量的引用，这里就涉及到了生命周期问题，而子线程的闭包的存活周期有可能长于当前的函数，这样就会造成悬垂指针，这在Rust中是绝对不允许的。因此我们需要使用move关键字将所有权转移到闭包中。\nuse std::thread;fn main() &#123;    let v = vec![1, 2, 3];    let handle = thread::spawn(move || &#123;        println!(&quot;Here&#x27;s a vector: &#123;:?&#125;&quot;, v);    &#125;);    handle.join().unwrap();&#125;\n使用thread::spawn创建线程是不是非常简单。但是也是因为它的简单，所以可能无法满足我们一些定制化的需求。例如制定线程的栈大小，线程名称等。这时我们可以使用thread::Builder来创建线程。\nuse std::thread::&#123;Builder, current&#125;;fn main() &#123;    let mut v = vec![];    for id in 0..5 &#123;        let thread_name = format!(&quot;child-&#123;&#125;&quot;, id);        let size: usize = 3 * 1024;        let builder = Builder::new().name(thread_name).stack_size(size);        let child = builder.spawn(move || &#123;            println!(&quot;in child:&#123;&#125;&quot;, current().name().unwrap());        &#125;).unwrap();        v.push(child);    &#125;    for child in v &#123;        child.join().unwrap_or_default();    &#125;&#125;\n我们使用thread::spawn创建的线程返回的类型是JoinHandle&lt;T&gt;，而使用builder.spawn返回的是Result&lt;JoinHandle&lt;T&gt;&gt;，因此这里需要加上unwrap方法。\n除了刚才提到了这些函数和结构体，std::thread还提供了一些底层同步原语，包括park、unpark和yield_now函数。其中park提供了阻塞线程的能力，unpark用来恢复被阻塞的线程。yield_now函数则可以让线程放弃时间片，让给其他线程执行。\nSend和Sync聊完了线程管理，我们再回到线程安全的话题，Rust提供的这些线程管理工具看起来和其他没有什么区别，那Rust又是如何保证线程安全的呢？\n秘密就在Send和Sync这两个trait中。它们的作用是：\n\nSend：实现Send的类型可以安全的在线程间传递所有权。\nSync：实现Sync的类型可以安全的在线程间传递不可变借用。\n\n现在我们可以看一下spawn函数的源码\n#[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]pub fn spawn&lt;F, T&gt;(f: F) -&gt; JoinHandle&lt;T&gt; where    F: FnOnce() -&gt; T, F: Send + &#x27;static, T: Send + &#x27;static&#123;    Builder::new().spawn(f).expect(&quot;failed to spawn thread&quot;)&#125;\n其参数F和返回值类型T都加上了Send + &#39;static限定，Send表示闭包必须实现Send，这样才可以在线程间传递。而&#39;static表示T只能是非引用类型，因为使用引用类型则无法保证生命周期。\n在Rust入坑指南：智能指针一文中，我们介绍了共享所有权的指针Rc&lt;T&gt;，但在多线程之间共享变量时，就不能使用Rc&lt;T&gt;，因为它的内部不是原子操作。不过不要紧，Rust为我们提供了线程安全版本：Arc&lt;T&gt;。\n下面我们一起来验证一下。\nuse std::thread;use std::rc::Rc;fn main() &#123;    let mut s = Rc::new(&quot;Hello&quot;.to_string());    for _ in 0..3 &#123;        let mut s_clone = s.clone();        thread::spawn(move || &#123;            s_clone.push_str(&quot; world!&quot;);        &#125;);    &#125;&#125;\n这个程序会报如下错误\n\n那我们把Rc替换为Arc试一下。\nuse std::sync::Arc;...let mut s = Arc::new(&quot;Hello&quot;.to_string());\n很遗憾，程序还是报错。\n\n这是因为，Arc默认是不可变的，我们还需要提供内部可变性。这时你可能想到来RefCell，但是它也是线程不安全的。所以这里我们需要使用Mutex&lt;T&gt;类型。它是Rust实现的互斥锁。\n互斥锁Rust中使用Mutex&lt;T&gt;实现互斥锁，从而保证线程安全。如果类型T实现了Send，那么Mutex&lt;T&gt;会自动实现Send和Sync。它的使用方法也比较简单，在使用之前需要通过lock或try_lock方法来获取锁，然后再进行操作。那么现在我们就可以对前面的代码进行修复了。\nuse std::thread;use std::sync::&#123;Arc, Mutex&#125;;fn main() &#123;    let mut s = Arc::new(Mutex::new(&quot;Hello&quot;.to_string()));    let mut v = vec![];    for _ in 0..3 &#123;        let s_clone = s.clone();        let child = thread::spawn(move || &#123;            let mut s_clone = s_clone.lock().unwrap();            s_clone.push_str(&quot; world!&quot;);        &#125;);        v.push(child);    &#125;    for child in v &#123;        child.join().unwrap();    &#125;&#125;\n读写锁介绍完了互斥锁之后，我们再来了解一下Rust中提供的另外一种锁——读写锁RwLock&lt;T&gt;。互斥锁用来独占线程，而读写锁则可以支持多个读线程和一个写线程。\n在使用读写锁时要注意，读锁和写锁是不能同时存在的，在使用时必须要使用显式作用域把读锁和写锁隔离开。\n总结本文我们先是介绍了Rust管理线程的两个函数：spawn、join。并且知道了可以使用Builder结构体定制化创建线程。然后又学习了Rust提供线程安全的两个trait，Send和Sync。最后我们一起学习了Rust提供的两种锁的实现：互斥锁和读写锁。\n关于Rust并发编程坑还没有到底，接下来还有条件变量、原子类型这些坑等着我们来挖。今天就暂时歇业了。\n","tags":["Rust"]},{"title":"Rust入坑指南：齐头并进（下）","url":"/2020/03/23/Rust%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%9A%E9%BD%90%E5%A4%B4%E5%B9%B6%E8%BF%9B%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"前文中我们聊了Rust如何管理线程以及如何利用Rust中的锁进行编程。今天我们继续学习并发编程。\n原子类型许多编程语言都会提供原子类型，Rust也不例外，在前文中我们聊了Rust中锁的使用，有了锁，就要小心死锁的问题，Rust虽然声称是安全并发，但是仍然无法帮助我们解决死锁的问题。原子类型就是编程语言为我们提供的无锁并发编程的最佳手段。熟悉Java的同学应该知道，Java的编译器并不能保证代码的执行顺序，编译器会对我们的代码的执行顺序进行优化，这一操作成为指令重排。而Rust的多线程内存模型不会进行指令重排，它可以保证指令的执行顺序。\n通常来讲原子类型会提供以下操作：\n\nLoad：从原子类型读取值\nStore：为一个原子类型写入值\nCAS（Compare-And-Swap）：比较并交换\nSwap：交换\nFetch-add（sub/and/or）：表示一系列的原子的加减或逻辑运算\n\nOk，这些基础的概念聊完以后，我们就来看看Rust为我们提供了哪些原子类型。Rust的原子类型定义在标准库std::sync::atomic中，目前它提供了12种原子类型。\n\n下面这段代码是Rust演示了如何用原子类型实现一个自旋锁。\nuse std::sync::Arc;use std::sync::atomic::&#123;AtomicUsize, Ordering&#125;;use std::thread;fn main() &#123;    let spinlock = Arc::new(AtomicUsize::new(1));    let spinlock_clone = spinlock.clone();    let thread = thread::spawn(move|| &#123;        spinlock_clone.store(0, Ordering::SeqCst);    &#125;);    while spinlock.load(Ordering::SeqCst) != 0 &#123;&#125;    if let Err(panic) = thread.join() &#123;        println!(&quot;Thread had an error: &#123;:?&#125;&quot;, panic);    &#125;&#125;\n我们利用AtomicUsize的store方法将它的值设置为0，然后用load方法获取到它的值，如果不是0，则程序一直空转。在store和load方法中，我们都用到了一个参数：Ordering::SeqCst，在声明中能看出来它也是属于atomic包。\n我们在文档中发现它是一个枚举。其定义为\npub enum Ordering &#123;    Relaxed,    Release,    Acquire,    AcqRel,    SeqCst,&#125;\n它的作用是将内存顺序的控制权交给开发者，我们可以自己定义底层的内存排序。下面我们一起来看一下这5种排序分别代表什么意思\n\nRelaxed：表示「没有顺序」，也就是开发者不会干预线程顺序，线程只进行原子操作\nRelease：对于使用Release的store操作，在它之前所有使用Acquire的load操作都是可见的\nAcquire：对于使用Acquire的load操作，在它之前的所有使用Release的store操作也都是可见的\nAcqRel：它代表读时使用Acquire顺序的load操作，写时使用Release顺序的store操作\nSeqCst：使用了SeqCst的原子操作都必须先存储，再加载。\n\n一般情况下建议使用SeqCst，而不推荐使用Relaxed。\n线程间通信Go语言文档中有这样一句话：不要使用共享内存来通信，应该使用通信实现共享内存。\nRust标准库选择了CSP并发模型，也就是依赖channel来进行线程间的通信。它的定义是在标准库std::sync::mpsc中，里面定义了三种类型的CSP进程：\n\nSender：发送异步消息\nSyncSender：发送同步消息\nReceiver：用于接收消息\n\n我们通过一个栗子来看一下channel是如何创建并收发消息的。\nuse std::thread;use std::sync::mpsc;fn main() &#123;    let (tx, rx) = mpsc::channel();    thread::spawn(move || &#123;        let val = String::from(&quot;hi&quot;);        tx.send(val).unwrap();    &#125;);    let received = rx.recv().unwrap();    println!(&quot;Got: &#123;&#125;&quot;, received);&#125;\n首先，我们先是使用了channel()函数来创建一个channel，它会返回一个（Sender, Receiver）元组。它的缓冲区是无界的。此外，我们还可以使用sync_channel()来创建channel，它返回的则是（SyncSender, Receiver）元组，这样的channel发送消息是同步的，并且可以设置缓冲区大小。\n接着，在子线程中，我们定义了一个字符串变量，并使用send()函数向channel中发送消息。这里send返回的是一个Result类型，所以使用unwrap来传播错误。\n在main函数最后，我们又用recv()函数来接收消息。\n这里需要注意的是，send()函数会转移所有权，所以，如果你在发送消息之后再使用val变量时，程序就会报错。\n现在我们已经掌握了使用Channel进行线程间通信的方法了，这里还有一段代码，感兴趣的同学可以自己执行一下这段代码看是否能够顺利执行。如果不能，应该怎么修改这段代码呢？\nuse std::thread;use std::sync::mpsc;fn main() &#123;    let (tx, rx) = mpsc::channel();    for i in 0..5 &#123;        let tx = tx.clone();        thread::spawn(move || &#123;            tx.send(i).unwrap();        &#125;);    &#125;    for rx in rx.iter() &#123;        println!(&quot;&#123;:?&#125;&quot;, j);    &#125;&#125;\n线程池在实际工作中，如果每次都要创建新的线程，每次创建、销毁线程的开销就会变得非常可观，甚至会成为系统性能的瓶颈。对于这种问题，我们通常使用线程池来解决。\nRust的标准库中没有现成的线程池给我们使用，不过还是有一些第三方库来支持的。这里我使用的是threadpool。\n首先需要在Cargo.toml中增加依赖threadpool = &quot;1.7.1&quot;。然后就可以使用use threadpool::ThreadPool;将ThreadPool引入我们的程序中了。\nuse threadpool::ThreadPool;use std::sync::mpsc::channel;fn main() &#123;    let n_workers = 4;    let n_jobs = 8;    let pool = ThreadPool::new(n_workers);    let (tx, rx) = channel();    for _ in 0..n_jobs &#123;        let tx = tx.clone();        pool.execute(move|| &#123;            tx.send(1).expect(&quot;channel will be there waiting for the pool&quot;);        &#125;);    &#125;    assert_eq!(rx.iter().take(n_jobs).fold(0, |a, b| a + b), 8);&#125;\n这里我们使用ThreadPool::new()来创建一个线程池，初始化4个工作线程。使用时用execute()方法就可以拿出一个线程来进行具体的工作。\n总结今天我们介绍了Rust并发编程的三种特性：原子类型、线程间通信和线程池的使用。\n原子类型是我们进行无锁并发的重要手段，线程间通信和线程池也都是工作中所必须使用的。当然并发编程的知识远不止于此，大家有兴趣的可以自行学习也可以与我交流讨论。\n","tags":["Rust"]},{"title":"antirez:Redis6真的来了","url":"/2019/12/23/antirez-Redis6%E7%9C%9F%E7%9A%84%E6%9D%A5%E4%BA%86/","content":"12月20号，Redis发布了Redis6-rc1版本，作者antirez也在自己的博客中宣布了这一消息，并对Redis6版本做了一些介绍，以下是译文。\n没错，新版本的Redis已经到了候选发布状态（RC版），几个月后，你就可以在各大应用市场看到新版本的Redis了。我想这大概是迄今为止，Redis最接近“企业级”应用的一个版本了，有趣的是，在这之前我花了很长时间来理解什么是企业级应用。我认为我并不喜欢这个词，但是它还是有一些意义的。现在Redis可以说是无处不在，并且仍然能够“缩小规模”：你可以直接下载它，在30秒内完成编译，然后在不需要进行任何配置的情况下启动它。但是无处不在意味着加密和ACL之类的环境也是必须要支持的，因此Redis必须要感谢我，尽管我极力追求简单性。\n有趣的是，Redis ACL虽然是以一种武断的方式进行添加，但它几乎与你在其他系统中看到的都不一样，对SSL的支持也是经过了数次迭代，以达到最终使用最有意义的想法的目的。从让核心功能尽可能简洁的角度来看，我对这些改动还是很满意的。\nRedis6为我们带来的不仅仅是ACLs和SSL，据我所知，这是Redis发行的功能最多，也是参与人数最多的一个版本了。都有谁为Redis6做出了贡献呢？我生成了一个贡献者列表（我知道这个列表很烂，我只是简单生成了一下），他们至少有两个commit，并且不包括合并提交。另外，由于我不断在这里或那里修复许多小东西，因此我的提交次数可能会远超他人。\n685  antirez81  zhaozhao.zz76  Oran Agra51  artix28  Madelyn Olson27  Yossi Gottlieb15  David Carlier14  Guy Benoish14  Guy Korland13  Itamar Haber9  Angus Pearson8  WuYunlong8  yongman7  vattezhang7  Chris Lamb5  Dvir Volk5  meir@redislabs.com5  chendianqiang5  John Sully4  dejun.xdj4  Daniel Dai4  Johannes Truschnigg4  swilly223  Bruce Merry3  filipecosta903  youjiali19952  James Rouzier2  Andrey Bugaevskiy2  Brad Solomon2  Hamid Alaei2  Michael Chaten2  Steve Webster2  Wander Hillen2  Weiliang Li2  Yuan Zhou2  charsyam2  hujie2  jem2  shenlongxing2  valentino2  zhudacai 002284902  喜欢兰花山丘\n感谢上面各位的贡献，这是一次很棒的团队合作。\n下面的列表是新功能列表：\n\n很多新的模块API\n更好的过期周期\nSSL\nACLs\nRESP3\n客户端缓存\n线程I/O\n副本上的无盘复制\nRedis-benchmark支持集群+Redis-cli的改进\n系统支持重写\nRedis Cluster代理与Redis6一起发布（不同仓库）\nRedis 6发布了Disque模块（不同仓库）\n\n如你所见，有很多比较大的改动，接下来我会选择其中几个进行介绍。\nRESP310年之后，我们需要一个新的协议，我在这篇博客中进行了广泛的讨论：http://antirez.com/news/125，但我随后改变了主意，所以RESP3在Redis6中是“选择加入”。连接最开始是RESP2模式，只有当你使用新的HELLO命令握手时，你才会进入新的协议模式。\n我们为什么需要一个新的协议呢？因为旧的语义不够清晰。RESP3中还有一些其他功能，但主要思想是能够直接从Redis返回复杂数据类型，而客户端不必知道要转换为哪种类型的数组，或者返回的数字是否能够转换成布尔值等等。\n由于RESP3不是仅有的协议，所以它的使用速度比预期要慢一些，但这也许不是一件坏事，因为这样我们就有足够的时间来进行适应和调整。\nACLs对Redis ACLs最好对介绍就是ACL文档本身(https://redis.io/topics/acl)，即使它可能需要进行一些更新以匹配最近对修改。我在这里更想讨论一下使用它的动机。Redis需要ACLs是因为人们需要在更广泛的使用ACLs，以达到更好的控制客户端可以做的某些操作。另一个增加ACLs的主要原因是进行隔离，以保护数据免受应用程序错误的侵害。如果你的工作节点只能做BRPOPLPUSH操作，那么新的开发人员使用FLUSHALL的机会就比较少，也能够降低生产环境执行FLUSHALL的可能性。\n在Redis中ACLs的操作都是免费的，因为如果你不用它们，性能上就不会受影响，毕竟这部分开销无法衡量。我想这是最好的处理方法了，值得一提的是，我们现在为ACL提供了Redis模块接口，因此你可以编写自定义身份验证方法。\nSSL现在是2019年，马上要到2020年了，因此有一些新的规则。唯一的问题就是如何正确执行，正确执行的前提是错误执行并理解其局限性，然后对Redis连接进行抽象以正确执行。这项工作是在完全没有我的帮助下进行的，这也体现了Redis开发过程的改变。\n客户端缓存关于客户端缓存，我写了一篇博客（http://antirez.com/news/130），然而我认为这是Redis6最不成熟的功能。没错，服务器可以协助你在客户端缓存，这看起来很酷，但我想要在Redis6 GA版本出来之前对这个功能进行进一步优化。尤其是增加一种新的模式，在这个模式下，服务器不维护客户端的状态或者尽量少的维护客户端的状态，更多使用消息进行交互。而且，现在无法将某些“cache slots”的过期消息合并成一个。这是一个不错的想法，我们将在一月份着重做这部分功能。\nDisque成为一个模块最终，我做到了（https://github.com/antirez/disque-module），我对这个结果非常满意。\nDisque作为一个模块确实显示的Redis模块系统的强大。集群消息总线API，能够阻止和恢复客户端，计时器，模块私有数据的AOF和RDB控制。如果你还不知道什么是Disque，可以去看一下这个仓库的README。\n集群代理我的同事Fabio在这个工作中花费了几个月的时间：https://github.com/artix75/redis-cluster-proxy\n我想看到这个功能已经很多年了，当主题是Redis集群支持时，客户端的需要进行的操作总是很分散，现在我们有了代理（还在进行中），就可以做很多有趣的事情了。最主要的功能是为客户端抽象Redis集群，就像它们正在与单个实例通信一样。另一个功能就是至少在简单且客户端仅使用简单命令和功能时执行多路复用。当要阻止或执行事务时，代理为客户端分配一组不同的连接。代理也是完全线程级的，所以让大部分CPU时间花在I/O上，这是一种最大化CPU使用率的好方法。你可以查看README文件，然后试一试这个功能。\nModulesRedis6的模块API完全是一个新的等级了。这是Redis发展最快的部分之一，因为我们从最开始就使用模块系统来开发非常复杂的东西，而不仅仅是琐碎的示例。前端时间，我启动了Disque端口，这也促使我为模块系统带来新功能。现在，你可以把Redis看成一个框架，可以讲系统作为模块进行编写，避免从头造轮子，同时也可以获得BSD许可，Redis实际上是一个可以用来编写系统的开放平台。\n内部Redis内部有非常多的优化：复制命令的方式发生了很大变化，过期使用了另一种算法，该算法更快且缓存更明显。\nStatus和ETA现在我们已经有了RC1，我希望在3月末，最晚5月，你就可以看到GA版本准备就绪。\n现在，Redis6绝对是可测试的，并且遇到错误的机会很小。但它包含了大量的代码更改，并且新功能由新的代码组成，也从没有人在生产环境中运行过这些代码。所以如果你找到了bug，请以最好的方式描述一下发生的情况并报告给我们。\n感谢所有人为该版本做出贡献的人和在接下来几个月中帮助我们维护它处于稳定状态的人。\n啊，我差点忘了，这是第6版的LOLWUT命令交互图：\n\n每次运行都会随机生成不同的景观。\n博客原文地址 http://antirez.com/news/131 \n译者注正如作者所说，Redis6带来了很多新的功能。刚发布的版本肯定不能拿到线上去玩，但是自己折腾一下，提前感受一下也不错。特别是RESP3和客户端缓存都是我比较期待的。另外还有新的过期算法，这个之前没怎么听说，也值得研究一下。不说了，我先下为敬。\n","tags":["Redis"]},{"title":"王小锤学Java：retainAll函数那点儿事","url":"/2019/03/13/retainAll%E5%87%BD%E6%95%B0%E9%82%A3%E7%82%B9%E5%84%BF%E4%BA%8B/","content":"王小锤是一家电商网站的Java程序员，下午刚打开电脑，公司的运营妹子小美就过来找他：“小锤，你能帮我导一份数据吗？我需要昨天成为SVIP用户，并且之前给过差评的这些账号，不过要把一个叫大宝的账号去掉，他老板亲戚。”对于小美的需求，小锤从来没有拒绝过，这次也不例外。\n答应小美之后，小锤心想，这么简单的事情，一个SQL就搞定了。小美又要夸我效率高了，想想还有点小激动呢。小锤打开电脑的一瞬间整个人都不好了。原来今天早上下班前，他才刚刚完成微服务的拆分。现在订单和用户已经分成了两个服务，评价表和用户等级表也已经在两个数据库里了。这就尴尬了，本来一个SQL能解决的事情，现在还需要跨服务调接口了。\n不过小锤马上就有思路了：\n\n有了思路之后，小锤立刻就开始写代码了。\n两分钟后……新鲜的代码出炉了（这里给个类似的Demo）。\npublic class NoBadReviewSVIP &#123;    public static void main(String[] args) &#123;        String[] svipArr = new String[]&#123;&quot;大宝&quot;,&quot;凉凉&quot;,&quot;Tom&quot;&#125;;        String[] badReviewUserArr = new String[]&#123;&quot;凉凉&quot;,&quot;Tom&quot;,&quot;大宝&quot;,&quot;小锤&quot;&#125;;        List&lt;String&gt; svips = new ArrayList&lt;String&gt;(Arrays.asList(svipArr));//从用户服务查        svips.remove(&quot;大宝&quot;);        List&lt;String&gt; badReviewUsers = new ArrayList&lt;String&gt;(Arrays.asList(badReviewUserArr));//从订单服务查        if (svips.retainAll(badReviewUsers)) &#123;            System.out.println(svips);            System.out.println(&quot;有交集&quot;);        &#125; else &#123;            System.out.println(&quot;没有交集，告诉小美&quot;);        &#125;    &#125;&#125;\n小锤先运行了一下，发现结果是没有交集，于是小锤告诉小美：“没有你要的用户，昨天新的SVIP都没有给过差评。”小美说：“不可能，老板说有一个叫Tom的用户就给了差评，你再好好查查。”\n小锤一查发现Tom真的给了差评，而且昨天刚刚成为SVIP。到底是哪里出了问题呢？小锤对着代码看了一个多小时也没发现问题，于是向身边的大锤请教。大锤只看了一眼，告诉他if的条件不对，让他看retainAll的具体实现。\nretainAll的实现于是小锤就开始看retainAll的源码了，发现它调用了一个batchRemove的私有方法。\nprivate boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123;    final Object[] elementData = this.elementData;    //w表示两个list的公共元素个数    int r = 0, w = 0;    boolean modified = false;    try &#123;        for (; r &lt; size; r++)            //如果有公共元素，存入elementData            if (c.contains(elementData[r]) == complement)                elementData[w++] = elementData[r];    &#125; finally &#123;        // c.contains()抛异常，就把剩余元素复制到elementData        if (r != size) &#123;            System.arraycopy(elementData, r,                             elementData, w,                             size - r);            //此时w为调用方list的长度            w += size - r;        &#125;        //如果元素数量改变        if (w != size) &#123;            // 多余空间教给GC            for (int i = w; i &lt; size; i++)                elementData[i] = null;            //记录list修改的次数            modCount += size - w;            //重新设置list大小            size = w;            //返回true            modified = true;        &#125;    &#125;    return modified;&#125;\n这么一看小锤就明白了，对于\nlistA.retainAll(listB)\nlistA中保存了listA和listB的交集，如果listA中元素数量改变，那么就返回true；如果listA的元素不变，则返回false。\n也就是说，判断两个list是否有交集，只需要执行上述代码，然后判断listA的size是否大于0就可以了。\n想通了这一点，小锤马上修改了代码，重新运行，得到正确结果，并且把结果告诉了小美。\n万万没想到，小锤最后仍然没有得到小美的夸奖，因为所有的SVIP都给了差评，小美要负责挨个询问原因并且帮助他们解决问题，所以根本没有时间搭理小锤。\n日常自黑\n","tags":["技术杂谈"]},{"title":"volatile vs synchronized","url":"/2018/08/20/volatile-vs-synchronized/","content":"今天来聊一聊Java并发编程中两个常用的关键字：volatile和synchronized。\n在介绍这两个关键字之前，首先要搞明白并发编程中的两个问题：\n\n线程之间是如何通信的\n线程之间如何同步\n\nJava内存模型Java线程的通信由Java内存模型（JMM）控制，Java内存模型的抽象如图：\n\nJava线程之间的通信总是隐式进行，通信过程对程序员完全透明。多个线程通过读-写共享内存来实现通信。\n图中线程A与线程B通信的具体步骤是：\n\n线程A把更新过的共享变量刷新到主内存中\n线程B从主内存读取共享变量\n\n例如，共享变量x的初始值为0，线程A将x修改为1（x=x+1），线程B读取到的x就是1，对于程序员来讲，就是线程A给线程B发消息说它把x的值更新为1。\n第一个问题搞明白了，再思考一下第二个问题。线程之间如何同步？在并发编程中，有三个重要的概念：原子性、可见性、一致性。\n原子性在Java中，对基本数据类型的读取和赋值操作都属于原子操作。\nx = 10;x = x + 1;\n上面两条语句中，第一句是原子操作，而第二句不是，为什么呢？实际上，第二句代码被编译为3条指令：\n\n从内存中取x的值\nx+1操作\n计算结果存入内存\n\n可见性当多个线程访问同一变量时，如果有一个线程修改了这个变量，那么其他线程立刻可以看到修改后的值。\n有序性CPU执行指令是按照先后顺序执行的，但是指令的顺序并不一定等同于代码的顺序，编译器编译过程中，为了提高性能，常常进行指令重排序。这种重排序不会改变单线程的语义，也就是说，你写的一段代码如果是单线程执行，编译器可能对执行进行重排序，但不论如何排序，最后得到的结果都是相同的。\n另外，如果存在数据依赖性，编译器不会改变依赖关系的执行顺序。数据依赖性是指两个操作访问同一个变量，其中一个是写操作，那么这两个操作就有数据依赖性。\n重排序对应多线程有哪些影响呢，我们通过一段代码来看一下：\nclass ReorderExample &#123;    int a = 0;    boolean flag = false;    public void writer() &#123;        a = 1; // 1        flag = true; // 2    &#125;    Public void reader() &#123;        if (flag) &#123; // 3        int i = a * a; // 4    \t……    \t&#125;    &#125;&#125;\n上述代码中，flag是变量a被初始化的标识，如果此时有两个线程A和B，A执行writer()方法，B执行reader()方法。由于1和2、3和4不存在数据依赖性，那么就有可能出现这种情况：\n\nA先执行语句2\nB执行了语句3和4\nA执行语句1\n\n最终的结果并不是我们想要的，此时，重排序破坏了语义。\n线程同步对于上面所说的线程同步问题如何避免呢？可以使用Java中的volatile和synchronized这两个关键字。\nvolatilevolatile关键字比较轻量级，只可以修饰变量。volatile修饰的变量，如果值被更新，会立即刷新主内存，而读volatile修饰的变量时，JMM会把线程对应的本地内存置为无效，从主内存中读取。这样volatile就可以保证线程的可见性。\nvolatile关键字在一定程度上可以保证有序性：\n\n当第二个操作是volatile写时，不能进行重排序\n当第一个操作是volatile读时，不能进行重排序\n当第一个操作是volatile写，第二个操作是volatile读时，不能重排序\n\n为了实现这些语义，JMM采用屏障插入策略：\n\n在volatile写操作前插入StoreStore屏障，后面插入StoreLoad屏障\n在volatile读操作后面插入LoadLoad屏障和LoadStore屏障\n\n也就是说，volatile写操作前的所有写操作都必须执行完，且需要等到volatile写操作执行后才能执行读操作。volatile读操作执行完之后才可以进行其他操作。也就是说volatile相当于一个屏障，其前面的操作不能放到volatile操作后面，后面的操作也不能放到volatile操作前面。\nsynchronizedsynchronized比较重量级，可以用来修饰方法。synchronized关键字是给修饰对象加锁，只有获得锁的线程才可以执行，执行完后释放锁。因此synchronized保证了原子性和可见性。\n\n文中图片来源于网络\n","tags":["Java"]},{"title":"【译】Googler如何解决编程问题","url":"/2019/04/02/%E3%80%90%E8%AF%91%E3%80%91Googler%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%BC%96%E7%A8%8B%E9%97%AE%E9%A2%98/","content":"本文是Google工程师Steve Merritt的一篇博客，向大家介绍他自己和身边的同事解决编程问题的方法。\n原文地址：https://blog.usejournal.com/how-a-googler-solves-coding-problems-ec5d59e73ec5 \n在本文中，我将完整的向你介绍一种解决编程问题的策略，这个策略是我在日常工作中一直使用的，并且用它来帮助各个等级的程序员（包括新手、大学生和实习生）学习和成长。应用这个结构化流程可以最大幅度的减少那令人沮丧的调试时间，并且能够在尽可能短的时间内编写出更加整洁、错误率更低的代码。\n一步步接下来我将用一个栗子来说明。\n问题：有两个字符串sourceString和searchString，返回searchString在sourceString中第一次出现的位置，如果searchString不是sourceString的字串，就返回-1。\n第一步：画下来当你拿到一个需求，马上就开始着手写代码是一个非常愚蠢的主意。在写一篇文章之前，首先要弄清楚论证和论据，并且你要保证论证是有意义的。如果你没有这么做的话，当你意识到你所写的东西前言不搭后语时，你可能会因为浪费了大把的时间而想请自己吃一顿大嘴巴子。编程也是一样的道理，甚至比这还严重，严重到像洗澡的时候把洗发水弄进眼睛里。\n问题的解决方法通常很重要，即使它看上去很简单。在写代码之前，首先要做的就是把这个方法在纸面上呈现出来，并且保证在不同的情况下适用。\n所以不找急着写代码，甚至都不要思考如何写。后面你会有充足的时间去敲代码，在这之前，你要把自己当成一台计算机，弄清楚你这台计算机会怎么解决这个问题。\n你可以使用流程图，或者使用其他能帮你具象化的方法，总之我们的目标是解决问题。你可以用纸和笔随意发挥，不需要收到键盘的限制。\n我们从一些简单的情况开始，如果一个方法是“输入一个字符串”，那么“abc”可以作为第一个例子。首先要搞清楚正确的结果是什么，然后去想怎么样得到正确的结果，并且一步一步的进行。\n我们假设输入的字符串是这样：\nsourceString: &quot;abcdyesefgh&quot;searchString: &quot;yes&quot;\n我的想法是这样的：\n我看到了searchString在sourceString里，但是我要怎么做呢？我从sourceString的第一个字符开始，逐字去读，一直到最后，判断每一个三个字符是不是yes。比如abc，bcd，cdy等等。当index值为4时，我找到了字符串yes，所以我知道结果是index为4。\n当我们写下算法时，必须要保证我们考虑了所有的情况，并且处理了所有可能的场景。当我们找到匹配的字符串时，返回结果。如果找不到匹配的字符串，同样要返回结果。\n我们来尝试另一组字符串：\nsourceString: &quot;abcdyefg&quot;searchString: &quot;yes&quot;\n我们重复刚才的操作，当我们读到下标为4的字符时，找到的字符串是yef，这是最接近的结果了，但是第三个字符却不同，所以我们继续往后读，一直到最后，没有找到匹配的字符串，所以我们决定返回-1。\n我们确定这一系列步骤（程序设计中，我们称之为算法）解决了我们的问题，并且处理了两种不同的场景，每次都得到了正确的结果。这时，我们就对我们的算法比较有信心了，并且可以将它形成条目。我们一起来进行下一步：\n第二步：用英语写下来这里我们考虑将第一步形成的算法用英语写下来。这可以使每一步变得更加具体，以便我们后面写代码的时候有所参考。\n\n从字符串的第一个字符开始\n查看每一组3个的字符（其实是searchString的长度）\n如果匹配上searchString，就返回当前的index\n如果一直到末尾都没有找到匹配的字符串，就返回-1\n\n看起来很棒，不是吗\n第三步：写伪代码伪代码并不是真正的代码，只是一种模拟形式，这里我写下上面的算法的伪代码：\nfor each index in sourceString,    there are N characters in searchString    let N chars from index onward be called POSSIBLE_MATCH    if POSSIBLE_MATCH is equal to searchString, return indexat the end, if we haven&#x27;t found a match yet, return -1.\n我也可以用一种更接近代码的形式来写：\nfor each index in sourceString,    N = searchString.length    POSSIBLE_MATCH = sourceString[index to index+N]    if POSSIBLE_MATCH === searchString:        return indexreturn -1\n你的伪代码写得有多像真正的代码，你就会发现它有多么好用。\n第四步：翻译可以编码的内容注意：对于简单的问题，这一步可以和上一步合并\n到这时我们才第一次需要考虑语法、方法参数和语言规则的问题。可能你不是全部代码都会写，但是没关系，先把会写的写出来。\nfunction findFirstMatch(searchString, sourceString) &#123;    let length = searchString.length;    for (let index = 0; index &lt; sourceString.length; index++) &#123;        let possibleMatch = &lt;the LENGTH chars starting at index i&gt;        if (possibleMatch === searchString) &#123;            return index;        &#125;    &#125;    return -1;&#125;\n注意我留了一部分没有写，这是故意的！我不确定JavaScript切分字符串的语法要怎么写，所以我会在下一步查找它。\n第五步：不要猜测我发现所有新手程序员都会犯一个共同的错误，就是从网上找到一个方法，觉得“可能有用”，然后不经过测试就写进代码里。你不理解的代码越多，就越不可能找到正确的方法。\n你的程序的错误可能是你不了解代码的两倍还多。有一处不理解，如果程序出错，那么罪魁祸首只有一处。如果你有两处不理解，那就有三种可能出错（A出错，B出错，或者A和B都出错）。如果有三处不理解，就会有七种情况……很快它就失控了。\n边注：程序出错的情况种类遵循Mersenne公式 a(n) = (2^n) — 1\n首先要测试你的新代码。从互联网上找答案是好的，但是在写进你的代码之前，你要先对它进行单独的测试，确保它能按照你想要的方式执行。\n在上一步中，我不确定JavaScript怎么切分字符串，所以我选择面向Google编程\nhttps://www.google.com/search?q=how+to+select+part+of+a+string+in+javascript\n第一条结果来自w3schools，有点小过时，但比较可靠\nhttps://www.w3schools.com/jsref/jsref_substr.asp\n基于此，我觉得我应该使用substr(index, searchString.length)来提取sourceString ，但这只是个假设，所以我要先来测试一下：\n&gt;&gt; let testStr = &quot;abcdefghi&quot;&gt;&gt; let subStr = testStr.substr(3, 4);  // simple, easy usage&gt;&gt; console.log(subStr);&quot;defg&quot;&gt;&gt; subStr = testStr.substr(8, 5);   // ask for more chars than exist&quot;i&quot;\n现在我确定这个函数是可以用的，如果程序出错，就不是这个函数不可用导致的。最后我补充上最后的代码。\nfunction findFirstMatch(searchString, sourceString) &#123;    let length = searchString.length;    for (let index = 0; index &lt; sourceString.length; index++) &#123;        let possibleMatch = (            sourceString.substr(index, length));        if (possibleMatch === searchString) &#123;            return index;        &#125;    &#125;    return -1;&#125;\n结论如果你读到这里，我要说的只有：”干就完了！“\n再尝试处理一下上周遇到的困难，用上我教你的方法，我保证你很快就会有提高。\n祝你好运，编码愉快！\n译者注：个人认为作者还是强调要先想清楚，再动手写代码。而且要学会面向Google编程\n","tags":["技术杂谈"]},{"title":"【译】MySQL挑战：建立10万连接","url":"/2019/03/07/%E3%80%90%E8%AF%91%E3%80%91MySQL%E6%8C%91%E6%88%98%EF%BC%9A%E5%BB%BA%E7%AB%8B10%E4%B8%87%E8%BF%9E%E6%8E%A5/","content":"原文地址：https://www.percona.com/blog/2019/02/25/mysql-challenge-100k-connections/\n本文的目的是探索一种在一台MySQL服务器上建立10w个连接的方法。我们要建立的是可以执行查询的连接，而不是10w个空闲连接。\n你可能会问，我的MySQL服务器真的需要10w连接吗？我见过很多不同的部署方案，例如使用连接池，每个应用的连接池里放1000个连接，部署100个这样的应用服务器。还有一些非常糟糕的实践，使用“查询慢则重连并重试”的技术。这会造成雪球效应，有可能导致在几秒内需要建立上千个连接的情况。\n所以我决定设置一个“小目标”，看能否实现。\n准备阶段先看一下硬件，服务器由packet.net（一个云服务商）提供，配置如下：\ninstance size: c2.medium.x86Physical Cores @ 2.2 GHz(1 X AMD EPYC 7401P)Memory: 64 GB of ECC RAMStorage : INTEL® SSD DC S4500, 480GB\n我们需要5台这样的服务器，1台用来作MySQL服务器，其余4台作为客户端。MySQL服务器使用的是Percona  Server的带有线程池插件的MySQL 8.0.13-4，这个插件需要支持上千个连接。\n初始化服务器配置网络设置：\n- &#123; name: &#x27;net.core.somaxconn&#x27;, value: 32768 &#125;- &#123; name: &#x27;net.core.rmem_max&#x27;, value: 134217728 &#125;- &#123; name: &#x27;net.core.wmem_max&#x27;, value: 134217728 &#125;- &#123; name: &#x27;net.ipv4.tcp_rmem&#x27;, value: &#x27;4096 87380 134217728&#x27; &#125;- &#123; name: &#x27;net.ipv4.tcp_wmem&#x27;, value: &#x27;4096 87380 134217728&#x27; &#125;- &#123; name: &#x27;net.core.netdev_max_backlog&#x27;, value: 300000 &#125;- &#123; name: &#x27;net.ipv4.tcp_moderate_rcvbuf&#x27;, value: 1 &#125;- &#123; name: &#x27;net.ipv4.tcp_no_metrics_save&#x27;, value: 1 &#125;- &#123; name: &#x27;net.ipv4.tcp_congestion_control&#x27;, value: &#x27;htcp&#x27; &#125;- &#123; name: &#x27;net.ipv4.tcp_mtu_probing&#x27;, value: 1 &#125;- &#123; name: &#x27;net.ipv4.tcp_timestamps&#x27;, value: 0 &#125;- &#123; name: &#x27;net.ipv4.tcp_sack&#x27;, value: 0 &#125;- &#123; name: &#x27;net.ipv4.tcp_syncookies&#x27;, value: 1 &#125;- &#123; name: &#x27;net.ipv4.tcp_max_syn_backlog&#x27;, value: 4096 &#125;- &#123; name: &#x27;net.ipv4.tcp_mem&#x27;, value: &#x27;50576   64768 98152&#x27; &#125;- &#123; name: &#x27;net.ipv4.ip_local_port_range&#x27;, value: &#x27;4000 65000&#x27; &#125;- &#123; name: &#x27;net.ipv4.netdev_max_backlog&#x27;, value: 2500 &#125;- &#123; name: &#x27;net.ipv4.tcp_tw_reuse&#x27;, value: 1 &#125;- &#123; name: &#x27;net.ipv4.tcp_fin_timeout&#x27;, value: 5 &#125;\n系统限制设置：\n[Service]LimitNOFILE=1000000LimitNPROC=500000\n相应的MySQL配置（my.cnf文件）：\nback_log=3500max_connections=110000\n客户端使用的是sysbench0.5版本，而不是1.0.x。具体原因我们在后面做解释。\n执行命令：sysbench –test=sysbench/tests/db/select.lua –mysql-host=139.178.82.47 –mysql-user=sbtest–mysql-password=sbtest –oltp-tables-count=10 –report-interval=1 –num-threads=10000 –max-time=300 –max-requests=0 –oltp-table-size=10000000 –rand-type=uniform –rand-init=on run\n第一步，10,000个连接这一步非常简单，我们不需要做过多调整就可以实现。这一步只需要一台机器做客户端，不过客户端有可能会有如下错误：\nFATAL: error 2004: Can&#39;t create TCP/IP socket (24)\n这是由于打开文件数限制，这个限制限制了TCP/IP的sockets数量，可以在客户端上进行调整：\nulimit -n100000\n此时我们来观察一下性能：\n[  26s] threads: 10000, tps: 0.00, reads: 33367.48, writes: 0.00, response time: 3681.42ms (95%), errors: 0.00, reconnects:  0.00[  27s] threads: 10000, tps: 0.00, reads: 33289.74, writes: 0.00, response time: 3690.25ms (95%), errors: 0.00, reconnects:  0.00\n第二步，25,000个连接这一步会在MySQL服务端发生错误：\nCan&#39;t create a new thread (errno 11); if you are not out of available memory, you can consult the manualfor a possible OS-dependent bug\n关于这个问题的解决办法可以看这个链接：\nhttps://www.percona.com/blog/2013/02/04/cant_create_thread_errno_11/\n不过这个办法不适用于我们现在的情况，因为我们已经把所有限制调到最高：\ncat /proc/`pidof mysqld`/limitsLimit                     Soft Limit Hard Limit           UnitsMax cpu time              unlimited  unlimited            secondsMax file size             unlimited  unlimited            bytesMax data size             unlimited  unlimited            bytesMax stack size            8388608    unlimited            bytesMax core file size        0          unlimited            bytesMax resident set          unlimited  unlimited            bytesMax processes             500000     500000               processesMax open files            1000000    1000000              filesMax locked memory         16777216   16777216             bytesMax address space         unlimited  unlimited            bytesMax file locks            unlimited  unlimited            locksMax pending signals       255051     255051               signalsMax msgqueue size         819200     819200               bytesMax nice priority         0          0Max realtime priority     0          0Max realtime timeout      unlimited unlimited            us\n这也是为什么我们最开始要选择有线程池的服务：https://www.percona.com/doc/percona-server/8.0/performance/threadpool.html\n在my.cnf文件中加上下面这行设置，然后重启服务\nthread_handling=pool-of-threads\n查看一下结果\n[   7s] threads: 25000, tps: 0.00, reads: 33332.57, writes: 0.00, response time: 974.56ms (95%), errors: 0.00, reconnects:  0.00[   8s] threads: 25000, tps: 0.00, reads: 33187.01, writes: 0.00, response time: 979.24ms (95%), errors: 0.00, reconnects:  0.00\n吞吐量相同，但是又95%的响应从3690 ms降到了979 ms。\n第三步，50,000个连接到这里，我们遇到了最大的挑战。首先尝试建立5w连接的时候，sysbench报错：\nFATAL: error 2003: Can&#39;t connect to MySQL server on &#39;139.178.82.47&#39; (99)\nError (99)错误比较神秘，它意味着不能分配指定的地址。这个问题是由一个应用可以打开的端口数限制引起的，我们的系统默认配置是：\ncat /proc/sys/net/ipv4/ip_local_port_range : 32768   60999\n这表示我们只有28,231可用端口（60999减32768），或者是你最多能建立的到指定IP地址的TCP连接数。你可以在服务器和客户端扩宽这个范围：\necho 4000 65000 &gt; /proc/sys/net/ipv4/ip_local_port_range\n这样我们就能建立61,000个连接了，这已经接近一个IP可用端口的最大限制了（65535）。这里的关键点是，如果我们想要达到10w连接，就需要为MySQL服务器分配更多的IP地址，所以我为MySQL服务器分配了两个IP地址。\n解决了端口个数问题后，我们又遇到了新的问题：\nsysbench 0.5:  multi-threaded system evaluation benchmarkRunning the test with following options:Number of threads: 50000FATAL: pthread_create() for thread #32352 failed. errno = 12 (Cannot allocate memory)\n这个问题是由sysbench的内存分配问题引起的。sysbench只能分配的内存只能创建32,351个连接，这个问题在1.0.x版本中更为严重。\nSysbench 1.0.x的限制Sysbench 1.0.x版本使用了不同的Lua编译器，导致我们不可能创建超过4000个连接。所以看起来Sysbench比 Percona Server更早到达了极限，所以我们需要使用更多的客户端。每个客户端最多32,351个连接的话，我们最少要使用4个客户端才能达到10w连接的目标。\n为了达到5w连接，我们使用了两台机器做客户端，每台机器开启25,000个线程。结果如下：\n[  29s] threads: 25000, tps: 0.00, reads: 16794.09, writes: 0.00, response time: 1799.63ms (95%), errors: 0.00, reconnects:  0.00[  30s] threads: 25000, tps: 0.00, reads: 16491.03, writes: 0.00, response time: 1800.70ms (95%), errors: 0.00, reconnects:  0.00\n吞吐量和上一步差不多（总的tps是16794*2 = 33588），但是性能降低了，有95%的响应时间长了一倍。这是意料之中的事情，因为与上一步相比，我们的连接数扩大了一倍。\n第四步，75,000个连接这一步我们再增加一台服务器做客户端，每台客户端上同样是跑25,000个线程。结果如下：\n[ 157s] threads: 25000, tps: 0.00, reads: 11633.87, writes: 0.00, response time: 2651.76ms (95%), errors: 0.00, reconnects:  0.00[ 158s] threads: 25000, tps: 0.00, reads: 10783.09, writes: 0.00, response time: 2601.44ms (95%), errors: 0.00, reconnects:  0.00\n第五步，100,000个连接终于到站了，这一步同样没什么困难，只需要再开一个客户端，同样跑25,000个线程。结果如下：\n[ 101s] threads: 25000, tps: 0.00, reads: 8033.83, writes: 0.00, response time: 3320.21ms (95%), errors: 0.00, reconnects:  0.00[ 102s] threads: 25000, tps: 0.00, reads: 8065.02, writes: 0.00, response time: 3405.77ms (95%), errors: 0.00, reconnects:  0.00\n吞吐量仍然保持在32260的水平（8065*4），95%的响应时间是3405ms。\n这里有个非常重要的事情，想必大家已经发现了：在有线程的情况下10w连接数的响应速度甚至要优于没有线程池的情况下的1w连接数的响应速度。线程池使得Percona Server可以更加有效的管理资源，然后提供更好的响应速度。\n结论10w连接数是可以实现的，并且可以更多，实现这个目标有三个重要的组件：\n\nPercona Server的线程池\n正确的网络设置\n为MySQL服务器配置多个IP地址（每个IP限制65535个连接）\n\n附录最后贴上完整的my.cnf文件\n[mysqld]datadir &#123;&#123; mysqldir &#125;&#125;ssl=0skip-log-binlog-error=error.log# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0character_set_server=latin1collation_server=latin1_swedish_ciskip-character-set-client-handshakeinnodb_undo_log_truncate=off# generaltable_open_cache = 200000table_open_cache_instances=64back_log=3500max_connections=110000# filesinnodb_file_per_tableinnodb_log_file_size=15Ginnodb_log_files_in_group=2innodb_open_files=4000# buffersinnodb_buffer_pool_size= 40Ginnodb_buffer_pool_instances=8innodb_log_buffer_size=64M# tuneinnodb_doublewrite= 1innodb_thread_concurrency=0innodb_flush_log_at_trx_commit= 0innodb_flush_method=O_DIRECT_NO_FSYNCinnodb_max_dirty_pages_pct=90innodb_max_dirty_pages_pct_lwm=10innodb_lru_scan_depth=2048innodb_page_cleaners=4join_buffer_size=256Ksort_buffer_size=256Kinnodb_use_native_aio=1innodb_stats_persistent = 1#innodb_spin_wait_delay=96innodb_adaptive_flushing = 1innodb_flush_neighbors = 0innodb_read_io_threads = 16innodb_write_io_threads = 16innodb_io_capacity=1500innodb_io_capacity_max=2500innodb_purge_threads=4innodb_adaptive_hash_index=0max_prepared_stmt_count=1000000innodb_monitor_enable = &#x27;%&#x27;performance_schema = ON\n","tags":["MySQL"]},{"title":"【译】Redis喜提新数据结构：Redis Streams","url":"/2019/03/28/%E3%80%90%E8%AF%91%E3%80%91Redis%E5%96%9C%E6%8F%90%E6%96%B0%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%EF%BC%9ARedis-streams/","content":"本文是Redis作者antirez的一篇博客\n原文地址：http://antirez.com/news/128\n我们在Redis5版本迎来了一个新的数据结构，它的名字叫做”Streams”。（撒花）Streams一经推出，就引起了社区中各位大佬的关注。所以我决定过一段时间做一个社区调查，讨论一下它的使用场景，并会在博客中将结果记录下来（是Redis作者的博客）。今天我想聊的是另一个问题：我怀疑有很多用户认为Streams的使用场景是和Kafka一样的。实际上，这个数据结构的设计背景也是消息的生产和消费，但你应该认为Redis Streams只是更擅长做这样的事情。流是一种很好的模型和”心理模型”，它能帮助我们更好的设计系统，但是Redis Streams像其他Redis数据结构一样，它更加通用，可以用来处理更多不同的问题。所以这篇博客我们会重点关注Redis Streams作为一种数据结构有哪些特性，而完全忽略它的阻塞操作、消费群和所有消息相关的内容。\nStreams是steroids上的CSV文件如果你想记录一系列的结构化数据，并且确定数据库是足够大的，你可能会说：我们以追加写入的方式打开一个文件，每一行记录是一个CSV数据项：\ntime=1553096724033,cpu_temp=23.4,load=2.3time=1553096725029,cpu_temp=23.2,load=2.1\n这看起来很简单，然后人们一直这样做了好多年，并且一直持续着：如果你知道你在做什么，那么这将成为一种固定的模式。如果同样的事情发生在内存中会怎样呢？内存的顺序写入能力更强，并且会自动移除掉CSV文件的一些限制：\n\n很难批量查询\n太多的冗余信息：每个条目的时间几乎相同，字段也相同。但是移除字段会降低灵活性，就不能再增加别的字段了\n每个条目的偏移量都是它在文件中的字节偏移量，而如果我们修改了文件结构，那么这些偏移量就会失效。所以这里缺少一个唯一标识的ID。\n不能删除条目，只能标记无效。如果不重写日志的话，又没有垃圾回收，重写日志经常会因为各种原因出错，所以最好不要重写。\n\n不过使用这样的CSV条目也有一些好处：没有固定格式，字段可以改变，生成比较容易，而且存储格式比较紧凑。我们保留了其优点，去掉了限制，于是设计出了像Redis Sorted Set这样的混合数据结构——Redis Streams。他们看起来像基本数据结构一样，但是为了得到这样的效果，内部是有多种表现形式的。\nStreams 101（就是Streams基础部分）Redis Streams是一种通过基数树连接的增量压缩的宏节点。（好难理解的概念，把原话贴出来：Redis Streams are represented as delta-compressed macro nodes that are linked together by a radix tree）。它的作用是，快速查找一个随机项，获取范围值，删除旧值来创建一个有大小上限的流。对程序员来说，我们的接口和CSV文件很相似：\n&gt; XADD mystream * cpu-temp 23.4 load 2.3&quot;1553097561402-0&quot;&gt; XADD mystream * cpu-temp 23.2 load 2.1&quot;1553097568315-0&quot;\n通过这个例子可以看到，XADD命令自动生成并返回了一个entry ID。它是单调递增的，并且有两部分组成，&lt;时间&gt;-&lt;数量&gt;，时间是毫秒级，而数量则是同一毫秒生成的entry数量递增。\n所以第一个从上面所说的”追加写入CSV”文件抽象出来概念就是，如果用星号作为XADD命令的ID参数，就从服务器获取了一个entry ID。这个ID不仅仅是entry的唯一标识，也和entry加入流的时间有关。XRANGE命令可以批量获取或获取单个数据项。\n&gt; XRANGE mystream 1553097561402-0 1553097561402-01) 1) &quot;1553097561402-0&quot;   2) 1) &quot;cpu-temp&quot;      2) &quot;23.4&quot;      3) &quot;load&quot;      4) &quot;2.3&quot;\n在这个例子中，为了得到单个数据项，我用了相同的ID作为起始和结束值。然而我可以获取任意范围的数据项，并且用COUNT参数限制结果的数量。我也可以将起止参数都设置为时间戳，获取一段时间内的数据项。\n&gt; XRANGE mystream 1553097560000 15530975700001) 1) &quot;1553097561402-0&quot;   2) 1) &quot;cpu-temp&quot;      2) &quot;23.4&quot;      3) &quot;load&quot;      4) &quot;2.3&quot;2) 1) &quot;1553097568315-0&quot;   2) 1) &quot;cpu-temp&quot;      2) &quot;23.2&quot;      3) &quot;load&quot;      4) &quot;2.1&quot;\n篇幅原因，我们不再展示更多的Streams API了。我们有相关的文档，感兴趣的同学可以去阅读。目前为止，我们只需要关注基本使用方法：XADD用来增加数据，XRANGE（或XREAD）用来读取数据。我们来看一下我为什么说Streams是一个强大的数据结构。\n网球运动员前几天我和一个最近在学习Redis的朋友一起建模一个应用程序：这是一个用来追踪当地网球场、球员和比赛的app。很明显，球员是一个小的模型，在Redis中只需要用一个hash就足够了，key的形式可以是player:\\&lt;id>。当你进一步使用Redis建模时，就会意识到你需要去追踪指定网球俱乐部的一场比赛。如果球员1和球员2打了一场比赛，球员1获胜。那么我们可以这样来记录：\n&gt; XADD club:1234.matches * player-a 1 player-b 2 winner 1&quot;1553254144387-0&quot;\n通过这样简单的操作，我们就可以获得如下的信息：\n\n一场比赛的唯一标识：流里的ID\n不需要创建一个表示比赛的对象\n分页查询比赛情况，或者查看某场比赛是否在指定时间就进行\n\n在Streams出现之前，我们需要创建一个Sorted Set，分数是时间。Sorted Set的元素是比赛的ID（存在一个Hash里）。这不仅是增加了工作量，而且还造成了更多的内存浪费，比你想象的要多得多。\n现在看起来Streams像是一个追加模式的，以时间为分数，元素是小型Hash的Sorted Set。简而言之，这是Rediscover建模环境中的一次革命。\n内存使用情况上面的例子不仅仅是固化模式的问题，相比旧有的Sorted Set+ Hash的模式，Streams对内存的节省做了很好的优化，然而这一点是不容易被发现的。\n假设我们要记录100万场比赛，\nSorted Set + Hash的内存使用量是220MB（242RSS）\nStream的内存使用量是16.8MB（18.11RSS）\n这不仅仅是一个数量级的差异（实际上是13倍的差异），这意味着我们旧有的模式实在是太浪费了，而新的模式是完美可行的。Redis Streams还有其他神奇的地方：宏节点可以包括多个元素，它们使用叫做listpack的数据进行编码。listpacks会对二进制形式的整数进行编码，即时它是语义字符串。最重要的是，我们使用了增量压缩和相同字段压缩。我们可以通过ID或时间进行查询，因为宏节点是用基数树连接的。基数树叶被设计为使用很少的内存。所有的事情都使用极少的内存，但有趣的是，用户并不能从语义上看到使Streams更加高效的实现细节。\n现在我们来做一个简单的计算，如果我保存了100万个entry，使用了18MB内存，那么1000万个就是180MB，1亿个使用1.8GB，保存10亿数据也只使用18GB内存。\n时间序列有一个比较重要的事情需要注意，在我看来，上面我们用来记录网球比赛的例子与把Redis Streams作为一个时间序列来使用非常不同。没错，逻辑上我们仍然是记录一类事件，但本质上的区别是记录日志和创建一个entry并存入对象的不同。在使用时间序列时，我们只是记录一个外部事件，而不需要真的展示一个对象。你可能认为这个区别不重要，但事实不是这样。对Redis用户来说很重要的是，如果需要保存一系列有序的对象，并且给每个对象赋一个ID，那么就需要使用Redis Streams。\n然而即时是一个简单的时间序列，也是一个很大的用例，因为在Streams出现之前，Redis在面对这种用例时令人有些绝望。一个节省内存，并且灵活的流，对开发者来说是一个重要的工具。\n结论Streams非常灵活并且有很多使用场景，我想尽量用简短的语言，以确保上面的例子和内存分析更加通俗易懂。也许大多数读者已经搞懂了。不过在上个月我和别人交流时感觉到Streams和流式处理还是有着很强的关联。就像这个数据结构只能用来处理流一样，事实并非如此。\n","tags":["Redis"]},{"title":"【译】给小白准备的Web架构基础知识","url":"/2019/04/21/%E3%80%90%E8%AF%91%E3%80%91Web%E6%9E%B6%E6%9E%84101/","content":"警告：本文内容是入门级的，大佬请按秩序有序撤离。\n原文地址：Web Architecture 101\n\n上图很好的展示了我们在Storyblocks的架构。如果你是一个新手工程师，可能会觉得这个架构非常复杂。在我们深入研究每个组件的细节之前，首先应该对它们有个大概的了解。\n\n当一个用户在Google搜索“Strong Beautiful Fog And Sunbeams In The Forest”时，第一条结果来自Storyblocks，我们主要的照片网站。用户点击结果就会在浏览器中跳转到图片详情页。在引擎下，用户的浏览器想DNS服务器发送一个请求，查询如何连接Storyblocks，然后向Storyblocks发送请求。\n请求会先到达我们的负载均衡器，负载均衡器会随机选择一个正在运行的服务器来处理请求。服务器先从缓存中查找一部分关于图片的信息，并从数据库查找剩余信息。我们注意到此时还没有对图片的颜色进行配置，因此我们发送“color profile”任务到我们的任务队列，处理任务的服务器会异步执行队列中的任务，并且将结果适时更新到数据库中。\n接下来，我们试图从使用照片标题在全文检索服务中找到与输入的照片相似的照片。如果登录用户是Storyblocks的会员，我们会去账号服务中查找用户的相关信息。最后，我们会把页面访问数据发送到数据“firehose”，以便存储到我们的云存储系统上，并最终落地到数据仓库中。数据分析师会使用数据仓库中的数据来解决商业问题。\n到这里，服务器已经呈现了一个HTML页面，并通过负载均衡器将它返回给用户。页面包含的JavaScript和CSS会放到连接了CDN的云存储系统中，所以用户的浏览器连接CDN取回数据。最后，由浏览器给用户呈现完整的页面。\n\n接下来，我会对每个组件挨个进行简单的介绍，以求给你建立一个良好的关于学习架构的思维模型。我会在另外一个系列的文章中分享我在Storyblocks这段时间的实践经验，给你提供良好的建议。\n1. DNSDNS是“Domain Name System”的缩写，它是使万维网成为可能的核心技术。最基础的DNS提供了域名（例如google.com）和IP地址的（例如85.129.83.120）的键值对以供查找，这是计算机路由请求到指定服务器所必需的。类别电话号码，域名和IP地址的区别就像是“打给哲少”和“拨打201-867–5309”。就像过去你需要一个电话本来查找哲少的电话号码，如今你需要DNS服务器来查找域名对应的IP地址。所以你可以认为DNS就是互联网上的电话本。\n关于DNS的细节我们还可以展开讲很多，但这里我们略过，因为这不是入门级介绍所关心的。\n2. Load Balancer在介绍负载均衡器之前，我们先来讨论一下应用的水平和垂直扩展。它们有什么不同呢？这篇帖子介绍的很明白，水平扩展是通过向资源池中增加更多的机器，垂直扩展是在已有的机器中增加更高的配置（CPU、内存等）。\n在Web开发中，为了应对服务器宕机，网络波动，数据中心不可用等突发情况，你一定经常使用横向扩展，因为它既简单又快捷。拥有一台以上的服务器使你的应用程序在部分服务器掉电时仍然可以正常运行。换句话说，你的程序具有较好的容错性。其次，横向扩展允许你通过让每个部分运行在不同的服务器上来解耦后端的依赖（Web服务器、数据库、服务 X等）。最后，当你的服务器达到一定规模时可能无法再进行垂直扩展。因为这个世界上没有任何一台计算机的性能好到可以支撑你所有应用的计算。举一个典型的栗子——Google的搜索平台。当然一原则对于多数规模较小的公司也适用，例如Storyblocks就部署了150到400个AWS EC2实例。对于这样的情况，要想通过垂直扩展来提供全部计算是一项艰难的挑战。\n我们再说回负载均衡器，它们使水平扩展成为可能。它们将传入进来的请求路由到众多服务器中的一个，并将响应结果返回给客户端。这些服务器通常是彼此的克隆或镜像，它们中的任何一个都应该以相同的方式处理，这样就通过分发请求的方式解决避免某台机器出现过载问题。\n负载均衡的概念非常简单，但是实现起来非常复杂。我们暂且不介绍。\n3. Web Application Servers在上层的Web应用服务描述起来非常简单。它们用来执行主要的业务逻辑，处理用户请求，并将HTML返回到用户的浏览器。为了完成任务，它们通常要与各种后端基础组件交互，比如数据库、缓存、任务队列、检索服务、其他微服务、数据/日志队列等等。如上所述，为了处理用户请求，你至少有两个，通常更多的负载均衡器。\n你应该知道应用服务的实现需要选择一种语言（Node.js、Ruby、PHP、 Scala、 Java、 C# 、.NET等）和对应MVC框架（Node.js的Express，Ruby的Rails，Scala的Play，PHP的Laravel等）。然而深挖这些语言和框架的细节也超出了本文的讨论范围。\n4. Database Servers每个Web应用项目都利用一个或多个数据库来存储信息。数据库提供了定义数据结构、对数据的增删改查、跨数据计算的方法。多数情况下，Web应用服务器和任务队列直接通信。另外每个后端服务可能都拥有独立的数据库。\n虽然我一直强调本文不会介绍某个组件的细节，但是如果不提SQL和NOSQL也是一种不负责任的行为。\nSQL的全称是“结构化查询语言”，它在18世纪70年代被发明。它给大家提供了查询关系型数据集的标准方法。SQL数据库将数据存储在通过公共ID（通常是整数）连接在一起的表中。让我们来看一个存储用户历史地址信息的例子。你可能需要两张表，用户表和用户地址表，它们通过用户ID连接在一起。下图展示了一个简化版本。两个表通过外键连接。\n\n如果你不是很了解SQL，我强烈推荐你学习一下Khan Academy的一门课程。SQL现在已经非常普及了，因此你至少要了解一些基础知识才能构建你的应用程序。\nNoSQL代表“非SQL”，是一种新的数据库技术集，用于处理大规模Web应用产生的大量数据（大多数SQL不支持水平扩展，并且垂直扩展也只能扩展到某个点）。如果你不了解NoSQL，可以看下面这些介绍：\n\nhttps://www.w3resource.com/mongodb/nosql.php\nhttp://www.kdnuggets.com/2016/07/seven-steps-understanding-nosql-databases.html\nhttps://resources.mongodb.com/getting-started-with-mongodb/back-to-basics-1-introduction-to-nosql\n\n但是总的来说，业界还是要将SQL作为数据库的统一接口，即使是对菲关系型数据库，所以如果你还不了解SQL的话，就真的要赶快去学习一下了。\n5. Caching Service缓存服务提供了简单的kv存储数据，尽可能使保存和查找数据的时间复杂度接近O(1)。应用程序一般把计算比较复杂的结果保存到缓存服务中，以便再次取值时直接从缓存中读取而不用重新进行复杂的计算。应用可能缓存的信息包括，数据库查询的结果，调用外部服务的返回值，一个URL返回的HTML等等。下面是一些实际的例子：\n\nGoogle会将搜索结果缓存\nFacebook在你登录后会缓存你看到的大部分信息，比如帖子、好友等。关于Facebook的缓存技术缓存可以看这篇文章\nStoryblocks缓存来自服务器端React渲染，搜索结果和预输入结果等的HTML输出。\n\n目前应用最广泛的两种缓存服务是Redis和Memcache。我会在另一篇文章中对它们进行更深入的介绍。\n6. Job Queue &amp; Servers很多应用程序需要在后台异步处理一些和返回结果无关的逻辑。比如，Google为了提供搜索服务，需要爬取网页并进行索引。它并不是在你每次搜索的时候都去做这件事，而是异步爬取，并更新索引。\n虽然现在有很多不同的架构都支持异步操作，但最普及的是我所说的“任务队列”架构。它包含两个组件：一个任务队列和至少一个任务服务器来执行队列中的任务。\n任务队列通常保存一系列需要异步执行的任务。最简单的规则是先进先出（FIFO），大多数应用按照优先级给任务排序。当应用需要执行一个任务时，无论是定时任务还是用户操作，都会把任务放到队列中去。\n还拿Storyblocks举例，我们使用一个后台的任务队列为我们的市场提供支持。我们会跑一些视频图片解码，处理CSV元数据标记，汇总用户统计信息，发送重置密码邮件等任务。我们一开始采用FIFO的原则，后来改为优先级队列，以保证有些具有时效性的任务能尽快完成，比如发送重置密码邮件。\n任务服务器用来处理任务。它们轮询任务队列以确定是否有任务要执行以及是否有任务，如果有，就从任务队列中弹出一个任务来执行。底层语言和框架的选择非常多，但它们不在本文讨论范围。\n7. Full-text Search Service许多web应用支持某种搜索功能——用户输入文本，应用返回“相关”的结果。支撑这种功能的技术一般称为全文检索，它利用反向索引快速找到包含关键字的文档。\n\n现在某些数据库也支持检索功能（比如MySQL已经支持全文检索），通常是运行独立的搜索服务来计算和存储反向索引，并提供查询接口。目前最受欢迎的全文检索平台是Elasticsearch，另外还有一些其他比较好的平台 例如Sphinx和Apache Solr。\n8. Services一旦一个APP达到一定的规模，就会有某些服务被独立出来运行。它们不会对外暴露，但是可以和应用内部的服务之间交互。Storyblocks有几个运营和计划的服务：\n\nAccount service存储我们所有网站上的用户数据，这使我们可以更轻松的提供交叉销售机会并创建更统一的用户体验\nContent service存储我们所有的视频、音频和图片的元数据。也提供了下载接口和查看历史下载记录的接口。\nPayment service提供对用户信用卡进行计费的接口\nHTML → PDF service提供了一个简单的HTML转PDF的接口\n\n9. Data当下，一家公司的生死由他们驾驭数据的能力决定。如今几乎每个APP一旦达到一定规模，就需要通过数据管道来收集、存储和分析数据。典型的管道有三个步骤：\n\nAPP发送数据，典型的关于用户交互的事件，数据发送到“firehose”——提供获取和处理数据的接口。原始数据通常需要进行转换、增强并发送到另一个firehose。AWS Kinesis和Kafka是两个公共工具。\n原始数据和转换/增强后的数据都被保存到云端。AWS Kinesis提供了一个名为firehose的设置，可以将原始数据保存到其云存储（S3），配置起来非常容易。\n转换/增强后的数据通常会被加载进数据仓库用作数据分析。我们使用的是AWS Redshift，大部分创业公司和增长的部分也是如此，尽管大公司会使用Oracle或其他专有的仓库技术。\n\n另外一个没有在架构图中画出来的一个步骤：将数据从应用程序和服务的操作数据库加载到数据仓库中。例如在Storyblocks，我们每晚将VideoBlocks, AudioBlocks, Storyblocks, account service和贡献值门户网站的数据加载到Redshift。通过将核心业务数据与我们的用户交互事件数据放在一起，为我们的分析师提供了一整个数据集。\n10. Cloud storage“云存储是一种简单、可靠且可扩展的存储、检索和共享数据的方法”——来自AWS。你可以使用它存储或多或少的存储和访问本地文件系统的任何内容，并且可以通过HTTP上的RESTful API与其进行交互。Amazon的S3是目前最流行的云存储产品，也是我们在Storyblocks广泛依赖的产品，用于存储我们的视频、照片和音频资产，我们的CSS和JavaScript，我们的用户数据等等。\n11. CDNCDN的全称是“Content Delivery Network”，该技术提供了通过Web获取静态HTML，CSS，JavaScript和图像资源的方式，比直接从单个源服务器提供服务要快得多。它的工作原理是在世界各地的许多“边缘”服务器上分发内容，以便用户从“边缘”服务器而不是源服务器下载资源。例如下图中，一个用户从西班牙请求源服务器在纽约的网站，但是静态资源会从在英国的CDN边缘服务器加载，防止许多缓慢的跨大西洋HTTP请求。\n\n这篇文章进行了更详尽的介绍。通常web应用应该始终使用CDN来提供CSS，JavaScript，图片，视频和其他资源。某些应用也可能利用CDN来提供静态HTML页面。\n总结这是一篇入门级的Web架构总结。希望能够对你有帮助。我希望发布一系列的进阶文章，在接下来一两年内我会对这些组件进行深入研究。\n","tags":["技术杂谈"]},{"title":"【译】antirez：Redis6将支持客户端缓存","url":"/2019/09/07/%E3%80%90%E8%AF%91%E3%80%91antirez%EF%BC%9ARedis6%E5%B0%86%E6%94%AF%E6%8C%81%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BC%93%E5%AD%98/","content":"本文翻译自Redis作者antirez的一篇博客，原文地址是：http://antirez.com/news/130\n纽约Redis日已经结束了，我仍然与意大利时区同步，早上5点30起床，并立即走上了曼哈顿的街道，我很喜欢这里的风景，并且享受着成为这里的一部分。当时我正在考虑发布Redis 6的release版本，这是在未来一段时间最重要的事了。新版本的Redis协议（RESP3）推进得还很慢，如果没有一个好的理由，明智的人是不会更换工具的。但我为什么要坚持提升协议呢？有两个主要的原因，一是需要给客户端提供更加具有语义的回复，二是提供一个旧版本不能实现的新功能：客户端缓存。\n时间倒回一年前，我到达圣安东尼奥的Redis Conf 2018。当时公司就有一个共识是客户端缓存是Redis在未来非常重要的事情。如果我们需要更快的存储和更快的缓存，我们就需要在客户端存储一部分信息。这是提供低延迟和大规模数据服务的很自然的想法。事实上，基本上每个大公司也都是这样做的，因为这是唯一的办法。然而Redis没有办法在这一过程中协助客户。一个巧合让Ben Malec想要在Redis Conf上做一些关于客户端缓存的演讲，他只使用Redis提供的工具和一些非常聪明的想法。\n作者注：演讲地址是https://www.youtube.com/watch?v=kliQLwSikO4\nBen的演讲启发了我，为了实现Ben的设计，其中有两个关键点。第一个是使用Redis Cluster的“hash slot”的概念，把key分成了16k个组。采用这种方式使得客户端不需要追踪每一个key的位置，可以使用一个简单的元数据来定位key所在的group。Ben使用Pub/Sub模式来通知key的改变，所以他需要应用程序的一些帮助，然而这种模式是很固定的。要修改一个key？还需要发布失效消息。在客户端是否缓存了key呢？要记住缓存每个key和收到失效消息时的时间戳，记住每个slot的失效时间。当使用一个缓存的key时，先做一个懒清除，通过检查缓存key的时间戳是否早于slot收到失效信息的时间戳。这种情况下，这个key就是过时的数据，你可以再次访问服务器。\n在看完演讲之后，我意识到这是一个在服务器内使用的好主意，为了让Redis能够为客户端做一部分工作，是客户端缓存更加简单高效，所以我回到家写下了我的设计文档：https://groups.google.com/d/msg/redis-db/xfcnYkbutDw/kTwCozpBBwAJ\n但为了实现我的设计，我必须专注于修改Redis协议使它变得更加完善，所以我开始编写RESP3和Redis 6的其他特性（比如ACL）的规范和代码，客户端缓存是Redis许多迭代想法中的一种，有些想法因为时间不够放弃了。\n当时我在纽约街头思考这个想法。后来和一些朋友去吃午饭喝咖啡。当我返回酒店房间后，距离第二天起飞还有一整晚的时间，所以我开始按照一年前写的提案来写Redis 6的客户端缓存的实现。\nRedis服务器助理客户端缓存，最终叫做“tracking”（我也可能改主意），是一个由几个关键想法组成的非常简单功能。\nkey空间被分割到”caching slots“，但他们比Ben使用的hash slots要多得多。我们使用CRC64的24位输出，所以有超过1600万个不同的slot。为什么这么多呢？因为我认为你想要有一个1亿key的服务器。然而一个失效信息影响的key不应该多于客户端缓存中的key。Redis中失效表占用130M的内存：8字节的指针指向16M的条目。这对我来说是可以接受的，如果你想要使用新功能，你将充分利用你在客户端的所有内存，所以使用130MB在服务器端是好的，你可以获得更细粒度的失效。\n客户端使用“opt in”方法开启这个功能，只需要一个简单的命令：\nCLIENT TRACKING on\n服务器总是返回+OK，从这时起，每个命令都在命令表中被标记为“只读”，不再给调用者返回keys，并记住客户端请求的所有的key。保存这种信息时非常简单的，每个Redis客户端都有自己的唯一ID，所以如果ID是123的客户端发送了MGET命令，需要从slot 1，2和5获取key，那么失效表中我们就需要记录如下信息：\n1 -&gt; [123]2 -&gt; [123]5 -&gt; [123]\n接着，ID为444的客户端也需要到slot5请求key了，那么表信息将变成：\n5 -&gt; [123,444]\n现在其他客户端修改了slot 5中的某个key，Redis将会检查失效表，发现客户端123和444都缓存了这个slot上的key。我们将会给这些客户端发送失效信息，然后会记录下slot最后的失效时间戳，并在以后懒检查缓存对象的时间戳，并对照后判断是否失效。此外，客户端可以回收表中缓存的指定slot的对象。这种具有24位hash函数的方法不是问题，因为我们即使缓存几千万的key，也不会有很长的列表。发送了失效信息后，我们就可以删除失效表中的项，这样直到这些客户端不再读这些slot的key，我们就不再向他们发送失效消息。\n需要注意的是，客户端不必强制使用24位hash函数。也可能使用20位，然后移动Redis发送的失效消息的slot。不确定是否有很多很好的理由这样做，但是内存受限时，这可能是一种想法。\n如果你密切关注我说的话，你会开始考虑同一连接既会接收到正常的客户端回复，又会接收失效消息。这可以通过RESP3实现，因为失效作为“推送”消息类型发送。如果客户端是一个阻塞类型的，并且不是事件驱动类型的客户端，就会变得比较复杂：\n应用程序需要一些方法来不时读取新数据，这看起来既复杂又脆弱。在这种情况下，为了接收失效消息，使用另一个应用程序线程和不同的客户端可能会更好。所以你可以使用以下命令来允许这样的操作：\nCLIENT TRACKING on REDIRECT 1234\n基本上我们可以说我们使用当前连接获得的所有key，并希望失效消息发送到客户端1234。在连接池的情况下多个客户端可能会要求将失效消息重定向到单个客户端。你需要做的就是创建特殊连接以接收失效消息，调用CLIENT ID以了解此客户端连接哪个ID，然后启用跟踪。\n现在只剩下一个问题了：如果我们失去了失效连接怎么办？我们可能因为不能接收到失效消息而陷入麻烦。通常，应用会检测连接，尝试重连，并清除缓存。为了确保失效连接处于连接状态，不时地向服务器发送ping请求可能是一个更好的主意。然而，为了降低过期数据的风险，Redis也将开始通知客户端将失效消息重定向到其他客户端，只要使用特殊的推送消息：下一个请求就会使客户端知道连接已经断开。\n我刚才描述的已经合并到Redis的unstable分支。可能不是最终的处理方法，但是在第一个Redis 6发布版本之前还有几个月的时间，我们还有时间修改所有的事情：可以告诉我你的反馈。我也会再寻找其他RESP2可行的方法。这只有在重定向开启时才有效，并且客户端要进入Pub/Sub模式监听消息。通过这种方式，完全可以复用旧客户端。\n我希望这足以刺激你的胃口：如果我们在Redis中运行的很好，然后记录下来，让客户端作者知道该如何支持，数据可能比以往更接近应用程序，甚至在小型团队运行的应用程序中，到目前为止还没有尝试客户端缓存。对于正在准备做的大型团队和非常大的应用程序，降低实现成本和复杂性。\n","tags":["Redis"]},{"title":"【译】什么才是优秀的代码","url":"/2019/11/27/%E3%80%90%E8%AF%91%E3%80%91%E4%BB%80%E4%B9%88%E6%89%8D%E6%98%AF%E4%BC%98%E7%A7%80%E7%9A%84%E4%BB%A3%E7%A0%81/","content":"究竟什么是优秀的代码？Robert Martin的一句话可以完美诠释。\n\n代码质量的唯一衡量标准是每分钟说多少次WTF\n\n我来解释一下这句话。当我在做code review时，通常会有三种不同的感受：\n\nWhat-the-F**k (厌恶脸) — 这段代码并不是必要的\nWhat-the-F**k (一脸钦佩) — 这家伙真聪明啊\nWhat-the-F**k (愤怒值爆表) — 这什么垃圾玩意\n\n所以当我们看代码时影响我们的第一印象的因素是什么呢？\n\n这是一段整洁又漂亮的代码。\n能够写出整洁又漂亮的代码是一个优秀工程师的标志。\nIt is Clean and Beautifully written code.\nAnd writing clean and beautiful code is the mark of a GREAT softwarecraftsman.\n\n要学会这项伟大的事业有两个关键点：知识和工作。\n知识会教给你如何在变得更加专业的模式、原则、实践和启发式的方法。但这些知识需要你通过大量的读写、不断的实践和努力才能获得。\n简而言之，学会如何写整洁的代码并非一件简单的事。你必须要为之付出努力，必须一次又一次的练习、发现问题、经历失败，直到正确处理为止。这一过程没有任何捷径。\n下面我将向你介绍一些如何写出整洁且漂亮的代码的技巧。\n“What is in a NAME”Kendrick Lamar说过：\n\n如果我要讲一个真实的故事，那么一定是从故事的命名开始。\nIf I’m gonna tell a real story, I’m gonna start with my name\n\n命名会遍布代码的每一个角落，我们会为函数、类、参数、包等命名，还会为源文件、目录命名。总之，在写代码的过程中，好的命名可能是使代码变整洁的最重要的因素。\n你的命名应该显示出你的意图。选择一个好的名称会耗费你的一些时间，但是如果随意命名，以后你会浪费更多的时间来理解它。所以请尽可能使你的命名更加合理，读你的代码的人也会因此感谢你。\n你要谨记的是，变量、函数和类的命名应该能回答三个问题：它为什么存在、它是做什么的、它用在哪里。\n这不仅需要良好的描述能力，还需要了解跨国界的文化背景。要教会你这些，没有人能比你自己更合适了。\n“Functions should DO ONLY ONE THING.”我们所说的「单一责任」原则。\nLouis Sullivan对此有过完美的描述\n\n形式跟随函数\nForm follows function.\n\n每个系统都是由特定的编程语言构建而成。函数通常是动词，而类通常是名词。函数任何编程语言中通常都是出现在第一行，写出整洁代码的本质其实就是写出整洁的函数。\n要想写出整洁的函数，首先应该遵循两条黄金法则：\n\n函数应该尽可能短小\n函数应该只做一件事\n\n这意味着你的函数不应该出现嵌套的结构。所以函数的缩进级别不应该大于一个或两个，这会让你的代码更容易阅读、理解和消化。除此之外，我们还需要确保函数中的语句都处于同一抽象级别。\n在一个函数中如果混有不同的抽象级别通常会使代码变得十分混乱，并且难以管理。优秀的工程师会把函数当作给别人讲的一段故事，而不只是编码。\n他们通常使用所选编程语言的功能来构建更加丰富、更具表现力和更加简洁的代码块。这使他们成为更好的“故事讲述者”。\n“Comments do not make up for bad code”Venus Williams为我们敲响警钟：\n\n每个人都有自己的注释，这是谣言的开始。\nEveryone makes their own comments. That’s how rumors get started.\n\n注释是一把双刃剑，在正确的位置添加注释可以为他人提供最有用的帮助。另一方面，浪费空间来添加无用的注释会使代码更加的混乱。如果注释提供了错误的信息，那对代码来说可以说是一种灾难了。\n总之，注释是一种不可缺少的恶魔。为什么呢？通常，代码的注释越旧，维护起来就越困难。很多程序员因为修改代码时不维护注释而臭名昭著。\n随着代码的发展，代码做出了许多改动，但注释并没有随之修改，这是一个很大的问题。\n请铭记，带有少量注释的简洁代码远胜于带有大量注释的混乱代码。不要浪费时间来解释你制造的混乱，而是要花些时间来清理这些混乱。\n“Formatting Code is always a priority”Robert C. Martin说过\n\n代码格式与沟通有关，而沟通是专业开发人员的首要任务。\nCode formatting is about communication, and communication is the professional developer’s first order of business.\n\n上面的说法可能不被大家认可，但这是一个优秀的开发人员的最重要的品质。格式化的代码是你心灵的窗户，我们希望人们对我们的秩序、对细节的关注和思想的清晰印象深刻。当他人看代码时，如果看到的是混乱的、开头结尾不清晰的代码块，这会直接损害我们的声誉，这一点毋庸置疑！\n如果你认为代码“可以使用”是专业程序员的第一要务，那么你不会有很好的发展的。你今天的功能很有可能在下个版本进行更改，但代码的可读性是不会改变的。\n当原始代码已经改的面目全非时，代码的风格和可读性将影响代码的可维护性。\n以后，你会因为自己的代码风格和纪律被人们记住，而不会因为某段代码。因此，你需要注意你的代码格式，使它受到简单的规则约束，这样的规则必须是所有团队成员都能理解的。\nWrite your “try-catch-finally” statement firstGeorges Canguilhem明确提到：\n\n人类都会犯错误，但是坚持错误的却是恶魔。\nTo err is human, to persist in error is diabolical.\n\n错误处理是每个程序员都必须要做的事情，输入可能异常，设备也可能故障。作为开发人员，我们希望程序按照我们的预期来执行。然而，问题不是处理错误，而是清晰易读的错误处理方式。\n很多代码以处理错误为主，导致主代码逻辑被淹没在其中。这种做法是完全错误的。代码应该整洁、健壮，并且以一种优雅的方式处理错误，这也是优秀工程师的一种标志。\n其中一个错误处理方法是通过适当的try-catch代码块来捕获所有错误。在执行try-catch-finally中try部分的代码时，任何时间发生异常，都会进入catch部分执行。\n因此，在代码中使用try-catch-finally可能是一种比较好的选择。它能帮助你在try部分定义你希望执行的代码，而不用去担心代码出错时怎么办。\n你抛出的每一个异常都应该具有完整的上下文以确定错误的来源和位置。具有创造性的错误信息会在代码写出来后很久，甚至是作者已经离开很长时间后仍然被人们记住。\nBringing it all together那么如何综合一下上面提到的技巧呢？\n\n答案是代码意识，是软件工程中的一种常识。\nThe answer is code-sense; the software equivalent of common sense.\n\n据Robert Martin所说，”编写整洁的代码需要通过一段痛苦的过程来获得一些小技巧，这些小技巧被称为代码意识“。有些人天生就有这种感觉，而有些人则需要通过实践、坚持不懈和毅力来获取它。这种意识不仅仅是帮助我们区分好代码和坏代码，更能够给我们代码将坏代码转换成好代码的能力。\n代码意识可以帮助开发者们选择最好的工具来指导他们创造出更有价值的整洁又漂亮的代码。\n简而言之，具有代码意识到程序员就像是画家，他可以将一块空白的屏幕变成精美的艺术品，并被人们记住很长时间。\n就像Harold Abelson所说的一样：\n\n代码写出来首先是给人读的，然后才是指导机器要怎么做。\n\nReferences“A handbook of Agile Software Craftsmanship” — Robert Martin.\n“A handbook of Agile estimation” — Mike Cohn\n原文地址https://medium.com/swlh/excellent-code-clean-and-beautiful-code-b541ca4b5a39\n译者点评这位作者以一种比较激进的语言告诉大家要注意代码的可读性，不过大家的观点都很类似，无非就是命名、代码格式、注释和错误处理这些点。希望这篇有些激进的文章可以帮助到一些同学，减少别人在review你的代码时，使用WTF的次数。\n","tags":["技术杂谈"]},{"title":"【译】做好这几件事，代码质量可以提升一个档次","url":"/2019/10/30/%E3%80%90%E8%AF%91%E3%80%91%E5%81%9A%E5%A5%BD%E8%BF%99%E5%87%A0%E4%BB%B6%E4%BA%8B%EF%BC%8C%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F%E5%8F%AF%E4%BB%A5%E6%8F%90%E5%8D%87%E4%B8%80%E4%B8%AA%E6%A1%A3%E6%AC%A1/","content":"这篇文章又是关于代码质量的，有些同学可能觉得我比较啰嗦。不过我就是想用这种方式让大家重视起来。其实说来说去就那么几种方法，但是实际执行起来真是难于登天。\n\n低质量的代码真的是一种灾难。当你的代码变得越来越混乱，维护起来就会花费大量的时间。在最坏的情况下，代码将变得不可维护，并且项目会慢慢终止。\n为了避免这种情况，你需要注意你的代码质量。尝试在代码质量上花费一些时间，长久来看，这将对你有很大的好处。\n无论你是管理者，测试人员或者是开发者都应该去自觉维护代码质量，因为在整个开发流程中，大家的目标都是交付可用的、高质量的代码。\n要想提高代码质量，需要做到以下六件事，其中一些是一个人可以完成的，而有些则必须要团队配合。\n\n1.  四眼原则四眼原则是易于理解和执行的。它的意思是必须要有至少两个人（包括作者）检查过代码，目前最流行的方法是Pull request。\n\nPull request是让你告诉别人你已经在GitHub上向分支push了一些代码改动。在开启Pull request之后，你就可以和协作者讨论潜在的问题，并且可以在你的代码被merge之前继续对它进行修改。\n——Github.com\n\n在代码审查期间，有几件事需要考虑。其中之一是检查代码是否违反了约定的代码规则。这一过程可以通过在管道中使用linter来实现自动化，但有时也需要手动执行。\n另外一个需要检查的是代码的可维护性和错误处理。这件事还没办法自动化。最后，需要检查的是代码的完整性。这一修改是否完成了需要完成的全部功能？\n2. 持续集成“开发环境是好的。”这是某些开发人员常说的，还有就是：“在我电脑上没问题”。\n如果希望避免这种问题的争论。持续集成可以给你提供很大的帮助。\n\n持续集成是一种软件开发实践，团队的开发人员经常集成他们的工作，通常每人至少每天集成一次——这使得每天需要集成很多次。每次集成都应该由自动构建（包括测试）尽快确认是否存在集成错误。\n—— Martin Fowler\n\n持续集成的意义在于，它可以快速的向开发者提供结果反馈。\n持续集成的两个基本作用是：\n\n保持快速构建，没什么比一次耗时一小时的构建更让人沮丧的了。\n快速修复损坏的构建。持续集成会让你始终在一个稳定的版本的基础上进行开发。\n\n持续集成通过快速向开发者提供反馈来帮助提高代码质量。如果测试不通过，那么构建就会失败，此时开发者就会注意到。此外，最好在构建脚本中添加linter来检查是否符合编码规范。毫无疑问，这也是用于提高代码质量的。\n3. 编码规范拥有一系列的代码规约是非常重要的。但是在你开始制定代码规约之前，团队的每个人都应该参与。因为这期间可能存在大量的关于最优约定的讨论。\n编码规范中应该包括怎样声明和命名一个变量等等。规则的数量是没有限制的，并且以后可以继续调整，前提是这些规则对你和你的团队有帮助。\n当编码规范制定好以后，请务必遵守。就像我前面提到的，最好的检查办法是在管道中增加linter，这样就不需要人工干预了。如果不这样做，也可以选择在本地安装linter。但要保证在每次提交之前规范使用linter。这样你的团队的代码风格将非常统一，有利于提升代码的可读性和可维护性。\n高质量的代码可以加快软件开发的速度，因为它可以被复用，并且开发人员不必花费大量时间修改bug和完善代码。同时新人加入项目也会更快适应。\n4. 测试，测试，测试代码质量越高，bug就越少。我们通常通过测试过滤出严重的bug，确保代码按照预期执行。\n制定清晰的测试策略对于提高代码质量至关重要。至少要保证你的代码可以通过单元测试。如果你以其他方式进行测试就更好了，例如集成测试或回归测试。\n根据测试金字塔，项目中数量最多的测试应该是单元测试，因为它们既简单又快速。有很多工具可以帮助你创建单元测试并生成代码覆盖率报告。\n\n跑单元测试和生成代码覆盖率报告可以通过持续集成自动进行。当代码覆盖率达不到要求时，持续集成也会构建失败。\n5. 分析bug代码中有bug是必然的事情，如何处理这些bug才是关键。如果你想要提升自己，学会从错误中学习至关重要。这也是为什么你要分析bug。\n发现bug后，先分析bug的影响。是一个低优先级的还是高优先级的？如果是高优先级的，就需要尽快解决。\n分析bug时，你需要问自己一些问题。是什么导致了错误？为什么没有测出来？其他地方也有可能发生吗？以及我们应该怎样避免类似的bug产生？\n当然，我们也要学会使用工具追踪bug。目前市面上有许多可用的bug追踪工具，你可以根据需要选择适合自己的工具。\n6. 开始量化在开始量化时，可以用几个指标来衡量代码的质量。\n缺陷指标缺陷的数量和缺陷的严重程度是衡量代码质量的重要指标。如果你想追踪bug，可以使用bug燃尽图。bug燃尽图和软件敏捷开发中的正常燃尽图一样。唯一不同的是bug燃尽图包含未修复的bug，而不是事故点。\n复杂度指标复杂度通常由圈复杂度衡量，它是程序的源代码线性独立路径数量的一个衡量。\n圈复杂度数和缺陷频率之间存在一定的相关性：\n\n许多研究调查了函数或方法中圈复杂度数和缺陷频率数之间的相关性。有些研究发现了圈复杂度和缺陷数的正相关性：函数和方法越复杂，缺陷也就会越多。然而，圈复杂度和程序大小之间的相关性已被多次证明。\n\n从理论上来讲，降低代码的复杂度会使缺陷更少。\n原文地址 https://medium.com/better-programming/things-that-you-can-do-to-improve-code-quality-c746c30e7521 \n","tags":["瞎扯"]},{"title":"【译】利用Lombok消除重复代码","url":"/2019/11/20/%E3%80%90%E8%AF%91%E3%80%91%E5%88%A9%E7%94%A8Lombok%E6%B6%88%E9%99%A4%E9%87%8D%E5%A4%8D%E4%BB%A3%E7%A0%81/","content":"当你在写Getter和Setter时，一定无数次的想过，为什么会有POJO这么烂的东西。你不是一个人！（不是骂人…）无数的开发人员花费了大量的时间来写这种样板代码，而他们本来可以利用这些时间做出更有价值的输出。\n从我开始写Java以来，已经写了几千行代码了，其中大概50%都是样板代码，在转型之前，我就这么一直毫无怨言的写着。而最近两年，我不再Java了，转而开始写一些Python，Go和JavaScript的代码。这时我才感觉到Java中的重复的样板代码是多么令人沮丧。\n值得庆幸的是，现在的IDE为我们提供了自动生成这些代码的功能。但是我仍然需要按快捷键或者点鼠标来操作，这是非常影响我的编码思路的。\nLombok简介\nProject Lombok is a java library that automatically plugs into your editor and build tools, spicing up your java. Never write another getter or equals method again\n\n上面这段话摘自Lombok的首页。这是一个每个人都需要使用的库，简直是一种仙丹！开个玩笑。Java是一门很棒的语言，但是它的冗长经常会令人感到苦恼。\nLombok到底有多香呢？我总结了以下几点：\n\nGetter和Setter注解会自动生成getter、setter方法\nNoArgsConstructor和AllArgConstructor可以帮助你快速生成构造函数\nToString会使POJO打印更加友好的日志\nData会让你的POJO成为一个完全符合规范的POJO\nSneakyThrows可以偷偷抛出检查异常，而不需要再写throws子句\n\n想了解更多Lombok特性的话，可以自行前往https://projectlombok.org/features/all查看。\nLombok是如何工作的？Lombok是在Java注解处理器和几个编译时注解的帮助下工作的，它将注入额外的Java字节码来帮助我们处理重复的代码。你可以查看它生成的Java代码，这一过程被幽默的称为“Delombokisation”。\n我应该如何开始使用？Lombok引入了一个额外的编译时依赖。\n如果你使用vanilla javac进行编译，你需要指定lombok.jar作为注解处理器：javac -cp lombok.jar MyCode.java\n如果你使用的是maven，那么需要在pom.xml中插入以下代码来保证你的代码可以使用Lombok。\n&lt;dependencies&gt;\t&lt;dependency&gt;\t\t&lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\t\t&lt;artifactId&gt;lombok&lt;/artifactId&gt;\t\t&lt;version&gt;1.18.4&lt;/version&gt;\t\t&lt;scope&gt;provided&lt;/scope&gt;\t&lt;/dependency&gt;&lt;/dependencies&gt;\n如果你使用的是Gradle，那么你需要使用Gradle Lombok插件\nplugins &#123;    id &#x27;io.franzbecker.gradle-lombok&#x27; version &#x27;1.14&#x27;    id &#x27;java&#x27;&#125;\n设置你的IDE从你开始使用Java起，你应该就开始使用一个智能的IDE来自动编译或给你的代码提供一些建议。为了将Lombok集成进IDE，你需要告诉Lombok io来安装合适的钩子。\n获取Lombok的jar包后，执行java -jar lombok.jar来完成所有的设置。\nIntelliJ IDEA和Visual Studio用户需要一个单独的Lombok插件，你可以选择从插件库中安装。\n代码拿来！talk is cheap, show me your code.程序员就应该拿代码说话。下面我们就来看一个完整的例子。\nGetters和Setters为被注解的自动生成getXXX和setXXX方法。\nimport lombok.Getter;import lombok.Setter;class UptimeResponse &#123;    // GetXXX and SetXXX are automatically generated    @Getter @Setter private long uptime;    @Getter @Setter private long currentTime;    @Getter @Setter private String status;    UptimeResponse() &#123;        this.uptime = ManagementFactory                          .getRuntimeMXBean().getUptime();        this.currentTime = System.currentTimeMillis();        this.status = &quot;OK&quot;;    &#125;&#125;// So this works automagicallyUptimeResponse res = new UptimeResponse();res.setStatus(&quot;FAIL&quot;);System.out.println(res.getUptime());\nConstructors可以自动创建默认的POJO构造函数，它将字段初始化为默认值。\n\nNoArgConstructor创建一个无参构造函数，所有的字段都会初始化为默认值\nAllArgsConstructor创建一个全参数构造函数，每个字段都会初始化为指定值\nRequiredArgsConstructor创建一个构造函数，参数包括所有final字段和标记为NotNull的字段\n\nimport lombok.*@AllArgsConstructorclass Document &#123;    @Getter @Setter private String title;    @Getter @Setter private String content;    // ...&#125;// This works automagicallyDocument d = new Document(&quot;Hello World&quot;, &quot;Message Body&quot;);d.getTitle();   // Hello Worldd.getContent(); // Message Body\nEquals and hash codesLombok可以生成的样板代码是包含局部变量的equals方法和hashcode方法。你可以手动排除一些字段。\nimport lombok.*;@RequiredArgsConstructor@EqualsAndHashCodeclass User &#123;    @Getter    private final String username;    @EqualsAndHashCode.Exclude    @Getter    @Setter    private String lastAction;  // not required for equality checks&#125;// This works automagicallyUser u1 = new User(&quot;amitosh&quot;);u1.setLastAction(&quot;Hello&quot;);User u2 = new User(&quot;amitosh&quot;);u2.setLastAction(&quot;Compile&quot;);u1.equals(u2) // Gives true\nTo StringLombok的ToString注解自动生成toString方法，其中包含类封装的全部字段。这是用于生成调试表示的快速方法。\nimport lombok.ToString;import lombok.Getter;import lombok.Setter;@ToStringclass Entry &#123;    @Getter @Setter private String id;    @Getter @Setter private String target;&#125;// This works automagicallyEntry e = new Entry();// ...System.out.println(e);  // Nice output with values of id and target\nData classes这个注解用于生成符合规范的完整POJO。它是ToString、EqualsAndHashCode以及所有非final字段的Getter和Setter的集合体。\nimport lombok.Data;@Dataclass Message &#123;    private String sender;    private String content;&#125;// This works automagicallyMessage m = new Message(&quot;amitosh&quot;, &quot;Hello World&quot;);m.setSender(&quot;agathver&quot;);m.getContent();  // Hello Worldm.toString();    // ...\nSneakyThrowsJava是一门静态检查语言，但有时检查会比较多余。例如有时我们不关心异常，或者确定代码中不会出现异常，所以就不想去写捕获和处理异常的代码。这时SneakyThrows注解可以帮助我们一起骗过编译器。\n但要注意不能滥用这个注解。\nimport lombok.SneakyThrows; public class SneakyThrowsExample &#123;   @SneakyThrows(UnsupportedEncodingException.class)   public String utf8ToString(byte[] bytes) &#123;       // This exception is never generated as UTF-8 is guaranteed       // to be supported by the JVM       return new String(bytes, &quot;UTF-8&quot;);   &#125;&#125;\nDelomboking不是所有的工具都支持Lombok的，最著名的是JavaDoc工具。你需要有一个中间态的代码来使文档正确表示。此外，有时候你可能会想看看Lombok生成的代码到底是什么样的。幸好Lombok提供了“delomboking”，用来将Lombok转换成Java源代码。\n要转换一个文件夹下的全部代码，可以使用以下命令：\njava -jar lombok.jar delombok src -d src-delomboked\nmaven和gradle插件也包含了delomboking任务，在你需要的时候可以使用。\nLombok是一个提高你的Java生产力的工具。我无法想象没有它时应该怎么写Java程序。真心希望你在读完本文以后能够认识到它的强大！\n原文地址https://medium.com/@agathver/banish-repetitive-java-code-with-lombok-f9b97d0d4137\n译者点评Lombok是一款非常好用的工具，它可以帮助我们快速构建POJO类。但是如果直接使用@Data注解时，会破坏类的封装特性。这点不符合面向对象编程的思想，但工作中会使用一些序列化工具，这些工具要求所有字段都要有setter方法。为了编码的方便，可能使用@Data方法是一个好的选择。\n","tags":["Java"]},{"title":"【译】别让你的团队掉入Code Review的坑","url":"/2019/09/12/%E3%80%90%E8%AF%91%E3%80%91%E5%88%AB%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%9B%A2%E9%98%9F%E6%8E%89%E5%85%A5Code-Review%E7%9A%84%E5%9D%91/","content":"代码审查是许多高效团队的工程实践。即使你的软件已经有很多优点了，但团队在做代码审查时仍然会遇到一些陷阱。\n这篇文章我讲向你介绍一些需要特别注意的陷阱，以防代码审查工作拖累你的团队。知道可能遇到的问题或陷阱，将会帮助你进行更加高效、有效的的代码审查工作。这是我们调查了900名微软员工后得到的结论。\n一个典型的代码审查过程一个典型的使用工具进行的代码审查过程大致是这样的：开发者完成一段代码，她提交代码准备开始让别人review。然后她选择需要审查她的代码的人。审查代码的人开始查看代码并给出一些评论。作者按照这些评论完善代码。当所有人都觉得没问题了以后，代码就可以合并进代码库了。在另一篇中我详细介绍了微软的代码审查的过程。\n我为我的邮件订阅读者们准备了一份代码审查电子书，书中包含了全部的代码审查最佳实践的清单。我还加了一些额外的见解，大家可以自行申请。\n\n代码审查并不总是一个平稳的过程这些步骤听起来像是 一个很平稳的过程，其实并不是，像其他所有事情一样，实际过程往往不如预期。在代码审查过程中，经常会遇到一些陷阱，这会降低整个审查代码的积极性。如果不能正确处理，代码审查会对整个团队的工作效率产生影响。所以让我们来看一下代码审查过程中究竟存在哪些坑。\n关键问题主要有两种：代码审查花费的时间和代码审查所能提供的价值。\n等待回复是一种煎熬作者需要面对的一个最主要的陷阱就是及时收到回复。等待评论的过程中不能在代码中做其他工作是一个巨大的问题。即使开发者可以完成其他任务，如果代码审查工作耗时过长，也会对开发者对工作效率和满意度造成不好的影响。\n开发者必须兼顾多项职责代码审查并不是代码审查人员唯一的任务。相反，它只是开发者日常工作之一（即使需要每天花费大量时间来做）。所以代码审查人员很可能在做其他工作，并且必须在开始代码审查之前停下或完成这些工作。\n如果时间不理想，或者代码审查人员在之前没有预料到这种变化，那么她可能在代码审查之前需要一点时间。远程团队还需要注意时差，这意味着代码审查可能花费更多的时间。\n开发者会面对代码审查不被当作实际工作的问题对于作者和代码审查人员来说，时间都是有限的，如果团队想要开发者做代码审查工作，但又不把它计入工作时间，这是有很大问题的。\n\n代码审查工作和效果没有奖励如果只是声称对代码审查工作进行评估而没有任何奖励，这将没有意义。大多数公司只会对开发者开发的功能或者写出的代码进行奖励，这减弱了开发人员在工作中帮助他人的动力和能力。代码审查应该成为绩效评估和晋升决策的基石。\n社会因素和团队动力等待代码审查并不总是和缺乏时间以及缺少奖励系统有关。由于社会性质，延迟审查可能是由于不安全感或团队动力。尤其是如果代码审查工作是压倒性的或者审查者是新手，那么代码审查会让人不知所措。\n大量代码很难审查大量的代码是审查工作的另一个巨大的陷阱。想象一下如果你是代码审查人员，当你需要审查一段代码时，你通常会想：我很快就能看完，但是当你打开的时候，发现这是一段巨大的改动，修改遍布整个代码库，你的第一反应是什么？\n大概是：天呐！（译者注：也可能是我*！）\n没错，这正是我们在分析了数千次代码评审中看到的。随着代码数量的增加，不仅是审核时间需要增加，评论的质量也会随之下降。这也是可以理解的。\n\n大量的代码更改非常难以审核，另外，如果审核人员对这部分代码并不熟悉，那这将是一场噩梦。\n理解代码修改需要一些指导大多数审核人员会面临无法理解代码修改的动机的困境。如果没有任何对修改的描述和解释，代码审查工作将会非常难以进行。研究表明如果审核人员不理解代码修改的目的，或者由于量大而不堪重负，那么她很难给出良好的建议。\n\nIt’s just this big incomprehensible mess… then you can’t add any value because they are just going to explain it to you and you’re going to parrot back what they say.\n\n没有得到有价值的反馈会降低开发人员审核代码的动力毫无疑问，花费时间去等待代码审查，却没有得到有用的反馈是一个常见的问题。虽然团队可能从知识分享中受益，但这样仍会降低开发者的动力和收益。\n审核人员不能给出有效建议的原因通常有这样几种\n\n审核人员不具备这方面专业知识\n审核人员没有时间去完整的审查代码\n审核人员没有理解修改的目的\n\n一旦主要讨论代码格式，你就需要采取行动另一个在代码审查过程中很重要的问题叫做bikeshedding。它的意思是开发者专注于小问题的讨论（例如代码格式）而忽略了更严重的问题。导致这种行为的原因有很多，通常是由于审核人员并不理解改动的目的或者没有时间做代码审查。有时bikeshedding可能代表团队的动力出了问题。\n达成共识可能需要面对面讨论有时候会发生代码作者和审核人员或者是审核人员之间很难达成共识的情况。由于团队动力和这件事紧密相关，因此必须小心处理这种情况通过工具和书面形式只会加剧这种问题。如果要讨论有争议的问题，那么面对面或者视频也许是更好的方案。\n代码审核的好处大于工作量我希望这些陷阱不会改变你对代码审核的想法，因为如果你了解了这些陷阱并妥善处理它们，代码审核工作对于整个技术团队是非常有好处的，而且会有更有效的方法来做代码审核。\n原文地址https://www.michaelagreiler.com/code-review-pitfalls-slow-down/\n最后，如果可以的话请帮忙填一下调查问卷。\n\n","tags":["Code Review"]},{"title":"【译】大O的友好指南","url":"/2019/03/10/%E3%80%90%E8%AF%91%E3%80%91%E5%A4%A7O%E7%9A%84%E5%8F%8B%E5%A5%BD%E6%8C%87%E5%8D%97/","content":"原文链接：https://medium.com/@daily_javascript/a-friendly-guide-to-big-o-ea781c5f68f0\n\n并不是每个公司在面试的时候都会问关于算法复杂度大O的问题，但是如果你想要到Facebook、Google或Amazon这样的公司工作的话，这是你必须要了解的知识。如果你没有很好的数学功底，那么你去看课本上关于大O的概念的话将会是一场灾难。\nLet T(n) = the number of operations performed in an algorithm as a function of n. T(n) = O(f(n)) if and only if there exists two constants, n0 &gt; 0 and c &gt; 0, and a function f(n) such that for all n &gt; n0, cf(n) ≥ T(n).\n那么我们就一起来学习一下它的数学证明吧。开个玩笑，实际上证明要比概念复杂得多（译者：不会就直说嘛）。\n回想一下，你是否曾经接受过这样的任务，为了完成它，你需要按照指定的步骤，一步一步来执行。在计算机科学中，这一系列指定的步骤被称为算法。\n在现实生活中，我们为了完成一项任务，往往会寻找更好的办法：更快、更便宜、或者更明确的方法。算法也是一样，我们常常需要更好的算法来实现。但是我们怎么知道哪种算法对计算机而言是更好的呢？\n一个比较直观的方法就是，选择不同算法之中，完成同一项任务用时最短的那个，也就是我们常说的运行时间最短的。不幸的是，我们没有办法精确的比较出哪个算法的运行时间更短，因为它受很多因素的影响。\n例如：\n\n写算法所用的语言\n相同语言的版本差异\n计算机硬件差异，每次读取数据的大小\n\n我们能做的是通过计算算法从开始到完成一共做了多少步工作来近似的比较两个算法的运行时间。所以我们应该做出一些假设，而不管每个人使用的硬件和语言的差异，找到一个公认的方法来比较不同算法解决问题的能力。\n假设1：计算机每次从上到下读取一个步骤\n假设2：定义变量、调用函数、逻辑对比以及所有的算术运算都被当成一个步骤\n假设3：内存是无限大的，而且访问任何位置的数据所消耗的时间是一样的\n做出了上面的假设之后，我们来看一个简单的例子：\nfunction m(a, b) &#123;    ans = 1        while(b &gt; 0) &#123;        ans = ans * a        b = b - 1    &#125;    return ans&#125;\n这个算法先定义了一个变量，这是一个步骤；然后开始了循环，这是三步（比较、乘法、减法）。最后返回变量，这也是一个步骤。所以这个算法的总步骤就是\n2 + (3 * b)\n如果b=100，这个算法就要进行302步，\n如果b=1000，这个算法就要进行3002步，\n如果b=10,000，这个算法就要进行30,002步。\n可以看到，由于我们不需要精确的比较，所以数字2对结果的影响微乎其微。这就是为什么当我们计算大O的时候，你只需要关心影响最大的因素，而可以忽略常数以及影响较小的因素。我们再来看一个例子：\nx + x^2 + x^3\n你可以放心的忽略掉x和x2，因为它们没有x3对结果的影响大。\n大O只是用来判断运行时间增加的速率，也叫作渐近分析。\n所以我们已经知道了如何计算大O，但是我们怎么知道要选择哪些影响因素呢？我们需要尽可能大的输入，来忽略常数和低阶因素。大O表示的是最坏情况，这才是最有意义的比较结果。\n","tags":["算法"]},{"title":"【译】如何成为一名优秀的初级工程师","url":"/2019/10/22/%E3%80%90%E8%AF%91%E3%80%91%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E4%B8%80%E5%90%8D%E4%BC%98%E7%A7%80%E7%9A%84%E5%88%9D%E7%BA%A7%E5%B7%A5%E7%A8%8B%E5%B8%88/","content":"很多人都在想着如何成为一名高级工程师，而我想要的是先成为优秀的初级工程师。\n明年将是我正式受雇佣写代码的第15年了。（译者：老外的写代码职业生涯真的挺长的）\n回首往昔，我第一天工作的日子仍然历历在目。那时的我每天都在格子间中写着SAP、算法、数据结构、SQL和C++，还涉及了更广泛的主题，包括知识管理和项目管理。我了解所有的这些知识，但我缺乏的是在需要的地方使用这些知识的信心。\n我花了很多年才获得这种信心，在那时，我也意识到实际上我只需要做到两件事就可以成功：\n\n学习某些新东西的态度\n无论学到什么都有能力付诸行动\n\n这两者都是养成的技能，不仅需要努力，而且还要谦虚的承认自己的无知，并与适合的人交流以消除这种无知。这就像三个正在建立隔离围墙的工人的故事。\n你问第一个人他在干什么。他说他在搭砖头，他并不担心自己做的对或错。第二个工人说他正在建造一堵墙，但并不知道为什么要建这堵墙。第三个眨着眼睛充满热情的工人说他正在建造一座大教堂。他为自己在建造大教堂时能发挥作用而自豪。\n优秀的初级开发者应该像第三个工人一样。他们与大方向保持一致，并且按照自己的方法不断进步。他们知道自己正在解决什么问题，并且为自己在建造大教堂（软件开发）中发挥的作用感到自豪。最后你需要保持谦卑，埋头工作，你会得到你想要的。\n还有一些我作为初级开发者时学到的比较好的东西想要分享给你。\n每天暴漏自己的无知Elbert Hubbard说过：\n\n永远无知的秘诀是：对你自己的意见和知识感到满意。\nThe recipe for perpetual ignorance is: Be satisfied with your opinions and content with your knowledge.\n\n你是一名初级开发者，你并没有了解所有事情，这没关系。即使在行业内打拼多年的资深工程师，也不是能了解所有的事情。无知并不是错误，不暴漏出这种无知才是更严重的问题。\n在开会、讨论、进行代码演练时，你会听到几件事，这些事会在你脑中转瞬即逝。不要假装自己理解并点头，不懂就问。如果你不说出来，就错失了学习的机会，这最终会危及你的职业生涯。\n在多年后的今天，我每天仍然会问许多问题。记住，没什么是愚蠢的问题，问一个愚蠢的问题并弄明白比整天坐在屏幕前要好得多。\n加快获得缺失知识的速度最好的方法是在每一个机会中暴露自己的无知。\n写代码之前先读代码Rasheed Ogunlaru说\n\n读到的代码有多好，写出来的就能有多好。\nHow you look at the code is pretty much how you’ll see it.\n\n我还记得我的第一个研发任务，需要我为现有功能编写出口，而我在这个任务上花了50个小时。周一我向领导汇报时，她说：“我们有现成的实现这个功能的代码，你应该直接用，这样能更快做出来。“\n我错在哪了？我没有去读已有代码。现实生活中，开发人员往往在读代码上花费的时间要比写代码多。即使是添加新功能或纠正缺陷，也需要了解已有代码。这没有捷径，读代码，读代码还是读代码。\n读代码可以让你了解别人是怎么写代码的，以及有哪些你可以复用的库。需要注意的是：\n\n编码标准\n命名约定\n设计模式\n注释\n用到的测试脚本和测试用例等\n\n记住，聪明的开发者不会重复造轮子。他们会尽可能尝试服用并构建已有功能。这不仅节省了时间，而且在共享代码的开发人员之间建立 了友情。\n现在已经有了解决问题的办法了。所以当你尝试完成一个功能时，先看一下其他人是否已经解决了这个问题。这不是偷工减料，而是在努力完成。\n寻求建设性批评Elbert Hubbard提到了一个避免批评的最佳技术：\n\n想要避免批评就要什么也不说，什么也不做，什么也不成为。\nTo avoid criticism say nothing, do nothing, be nothing.\n\n我们所有人都喜欢接收赞美，在别人称赞我们的工作时会感到很开心，这没问题。然而作为一名初级开发者，相比赞美，我认为你更应该接受建设性的批评。良药苦口利于病。\n我记得我第一次接受一名资深工程师的代码审查。在40分钟时间内，他细致的审查了我的代码，结束时，他的评论比我的代码还要多。经过这么多努力，我真的很难过。但这次代码审查确实帮助我发现了我的短板，并向我详细展示了我可以改善的地方。这是我前进道路上的启明灯。\n也就是说，想要得到建设性批评，就要主动寻求。我合作过很多资深工程师，他们从没拒绝过我的请求，即使他们很忙。当然，你需要根据他们的时间制定可行的日程，以进行有意义的会话。\n如果资深工程师花时间帮你审查代码并提出一些改进意见，表示他们对你的工作很有兴趣。不要浪费机会，主动上去寻求建设性批评。\n正如Andy Marks所说：\n\n如果你对你的代码感到自豪，就把它展示出来。如果你没有展示出来，意识到你的自豪感的人都是想与你合作的人。\nIf you take pride in your code, it will show up in the code. If you don’t it also shows. The people who recognize your sense of pride are people you want to work with.\n\n寻求大局观Murat Ildan说过：\n\n想要看到更多风景，就走出黑暗的山谷，爬上光明的山顶！\nTo see the big picture, get out of the dark valleys, climb to the sunny summits!\n\n还记得我们讲的三个工人的故事中那个知道自己在盖教堂的工人吗？关注大局往往就是这样。作为一名初级开发者，在大多数时间中，你只会接触一小段代码或者解决已有代码中的bug。你在完成分配给你的工作，这没有错。但如果你想成为整个交易的一部分；你需要花点时间找出交易的全部内容。\n你调整视角，并询问有关代码如何适应整个系统的问题。\n为什么使用特定的设计模式？\n为什么使用特定的语言？\n缺点是什么？它可以与你的代码一起使用吗？\n这些代码是否易于维护？\n等等……\n最好也是最简单的方法是获得导师的指导。技术导师可以帮助你提高技能水平并通过大的项目帮你巩固。但是没有明确的方法去寻找技术导师。也许是一杯咖啡就可以打破僵局。\n大多数初级开发者因为不理解功能或者对项目目标做出假设而犯错。花时间了解系统运作的实际情况将会对你成为优秀的开发者有很大帮助。\n最后，一名优秀的高级开发者不仅仅了解编程很长一段时间中，我都认为一名优秀的高级开发者就是拥有多年的开发经验（5年Java，7年Python等等……）。经验越丰富就越优秀。\n但是我错了。一名优秀的高级开发者不仅仅是只了解编程。他们充满好奇。他们是优秀的导师。最重要的是他们具有不可思议的代码意识，知道什么时候不做某事。例如，他们知道从头开始重写一个库只是为了使其更具有可读性，或者在团队选择旧框架时切换到新的框架并不总是明智的选择。他们不是在规则风险。他们是更加谨慎的做正确的事情。\n不是每个人在他（她）的职业生涯中都能成为“高级”开发者。一个好的高级开发者不仅需要好的经验，还应该有正确的态度和能力，以便将来应用这些经验。资历与能力有关，与年龄无关。\n就像Kevin de Leon说的：\n\n如果你什么都不做，资历就没有任何意义。\nSeniority means nothing if you do not do anything with it.\n\n原文链接https://medium.com/swlh/how-to-be-a-good-junior-developer-cd86b77086fc\n","tags":["瞎扯"]},{"title":"【译】如何提出好的Code Review反馈","url":"/2019/09/22/%E3%80%90%E8%AF%91%E3%80%91%E5%A6%82%E4%BD%95%E6%8F%90%E5%87%BA%E5%A5%BD%E7%9A%84Code-Review%E5%8F%8D%E9%A6%88/","content":"没错，Code Review系列还在继续，今天我们一起来聊一聊如何提出好的Code Review反馈。\nCode Review是保证代码的质量和可维护性，以及向团队成员分享知识的重要手段。但是，随着团队产出代码质量的提升，Code Review所带来的价值反而会下降。本文我将向你说明如何提出好的Code Review反馈。这一调研结果是来自于对微软数百人的高效工程师的访问。\n\n代码审查检查的是关于代码的问题和质量在一次代码审查过程中，由一名工程师做出的修改将由其他工程师来进行检查和讨论。代码审查的主要目标是查出代码的问题，保证代码的质量。即使Code Review还带来了一些其他的像传播和学习知识这样的好处，但我们仍要谨记这两个最重要的目标。\n有些团队担心的，有些团队已经遇到了代码审查的主要缺点：降低编码速度。这意味着团队的生产效率被代码审查拖慢了。\n为什么会这样呢？\n前文我们已经有过介绍，降低团队效率的原因可能有很多，但通常是反馈的等待时间长和响应慢有关。如果再加上毫无意义的反馈交流，那么代码审查对于所有开发者都将是噩梦般的存在。但团队可以轻松规避这些问题。\n本文我主要向你介绍的是如何提出有价值的反馈。\n微软的代码审查研究在微软，我进行了一项研究来了解代码审查。在这其中一项，我们分析了超过200万条代码审阅批注，以了解哪些反馈是有价值的，哪些是在浪费时间。但我们要先从代码审查应该看什么来介绍。\n代码审查时应该看些什么我们假设你被要求来审查一些代码。代码的作者发给你了几个代码文件，并对代码修改的目的做了简单的描述。那么现在你要开始审查代码了，你应该关注什么？\n\n功能缺陷\n逻辑问题\n缺少验证（例如边界问题）\nAPI的用法\n设计模式\n架构问题\n可测性\n可读性\n安全问题\n命名约定\n团队编码规范\n文档\n使用最佳做法\n特定语言的问题\n使用过期方法的问题\n性能（比如复杂度）\n替代解决方案\n\n真多啊！为了系统的查找这些问题，最好使用代码审查清单，它可以帮助你快速检查这些问题，并确保不会遗漏。我会写一份完整的，更加详细的代码审查清单，记得订阅我，方便第一时间获取。\n现在，你看了所有的问题，你一定会问自己：哪些是最有价值的问题？\n哪个反馈是最有价值的让我们来再次想象一下实际工作中你是如何开始一次代码审查的。\n也许你打开审查之后，会先浏览所有文件，然后调整自己。哪里发生了变化？代码的哪部分受到影响？修改的中心在哪？\n当你已经熟悉了这些修改以后，你就会注意一些问题了：注释和变量中的错别字，缺少注释，缩进等代码风格相关的问题，即使这些是要寻找的问题，也不要陷入这些问题中。实际上，这些问题是有价值的，但并不是我们最主要的目标。\n那么，还要看哪些问题呢？\n有关缺陷、验证缺失和最佳实践的反馈是最有价值的最有价值的代码审查反馈都是关于代码中实际问题的。所有开发人员都将这种反馈视为最有用的类型。但我们发现，在研究中这种问题只占全部反馈的很小一部分。下图中，你可以看到代码审查过程中开发人员都在讨论哪些问题，以及对于作者而言他们认为哪些是有价值的。\n\n最有价值的代码审查批注解决了以下问题：\n\n功能缺陷。这看起来是轻而易举的，评分最高的代码审查反馈是发现系统中的功能缺陷。但代码审查并不是发现功能缺陷的主要手段，事实上只有一小部分批注是关于功能缺陷的。但是，如果找到一个，那么代码审查带来的收益就是巨大的。\n验证缺失和极端案例。开发人员认为显示了被遗忘的方案、未涵盖的逻辑问题或极端情况的代码审查反馈是非常有价值的。这种反馈围绕寻找当前方案可能失败的情况和备用方案来展开。\n最佳实践和代码约定。代码审查对于维护一致的、可维护和可理解的代码库非常有用。所以，那些指出代码中包含不符合代码规范和最佳实践的反馈是很有价值的。\nAPI使用和设计模式。其他的有价值的反馈主要是关注API或第三方库使用是否正确，或者是缺少或错误的使用了设计模式。\n\n代码审查反馈是一把双刃剑我们讨论的一些问题并不像功能缺陷那样更容易显示价值。这些问题可能很有价值，但也有可能是在浪费所有人的时间。可能你已经猜到了，我们在讨论的是代码风格、代码规范和注释。这类问题通常被称为“挑剔问题”。\n文档、编码风格和编码规范。处理丢失或过时的文档，突出注释中的错别字，或指出不好的命名是你经常收到的代码审查反馈。但它们真的有价值吗？\n有时代码审查者并不能马上看到反馈的价值。但是找出错别字也不是大的问题不是吗？这些批准真正的价值是随着时间流逝而带来的复合效应。快速解决此类问题可以保证代码库一直保持可维护性和可理解性。\n尽管如此，它们还是会被看作是“挑剔”的行为，并且这个词已经具有负面含义了。所以团队必须保证所有人都能理解这类反馈的价值所在。\n另一方面，避免对代码缩进和代码风格进行冗长而重复的讨论是非常重要的。这无疑会拖慢团队的生产效率。为了让团队保持生产力，让我们先制定一种代码风格，然后继续前进！\n超出代码审查范围的反馈是无用的多数被认为是有价值的反馈都是关注当前范围代码审查。但是，代码审查的范围并不是代码更改文件中可见的代码，也不会超出代码修改的目的。因此，提出实施范围之外的问题对于大多数开发人员来说是无用的。\n\n替代解决方案。即使替代解决方案被认为是有价值的，但是对当前代码没有明显价值的实现并不能帮助你的团队提升生产效率。\n现有技术债务和重构。类似的，开始突显的旧的技术债务和潜在的重构机会超出了常规的代码审查范围。这些问题应该单独讨论。\n计划和未来的工作。另一个没有用的反馈类型就是批注过于关注未来的工作或者不在当前开发周期的工作。如果你看到这些问题，用backlogs或issue tracker这样的工具记录下来，这样做对你的团队是有价值的。之后，在合适的时间可以拿出来讨论。\n提出问题只是为了了解实现。即使代码审查是一种学习和分享知识的工具，但提出了解实现的问题并不是代码审查的目的。别忘了，代码审查是作者为了获得同意以便继续工作。\n\n如果你不理解代码时应该做什么？当你不理解代码时应该做什么呢？你怎么才能提出有价值的反馈？\n这是一个好问题，实际上，研究和经验告诉我们，如果你不理解代码，你无法提供有价值的反馈，至少能提供的不多。\n如果是这种情况，你最好先了解一些潜在问题。你为什么不能理解代码？因为你是团队的新成员？因为你缺乏经验？你以前没有使用过代码库？新编写的代码一团糟？\n如果是最后一个原因，那么你所有的问题都是有效的，应该作为代码审查的一部分。但是可能你添加的不仅仅是问题，也许会加一些关于如何改进代码，为什么难以理解代码的反馈等等。\n如果你对代码不熟悉怎么办？如果你之前在工作中没有接触过代码库，你可能很难理解代码审查中的内容。一个好的方法是求助同事，请他/她为你解释一遍代码。这里重要的是不要在代码审查过程中随机询问有关代码库的问题。\n没错，学习和传播知识是代码审查的两个重要的好处。不过，这些都是代码审查的附加价值，真正的关注点应该在于检查代码是否正确以及是否高质量。\n除非明确告诉你通过这种方式来教你。否则你应该正常做代码审查，这比你只观察别人怎么做要好。慢慢的，你就能更好的理解代码，了解团队惯例和最佳实践，以及向代码审查添加有用的反馈。\n缺乏经验的开发人员提出有价值的反馈较少不只是你，也不是初级开发者的错。这只是一个事实。我们的研究表明经验丰富的开发人员更能提出有价值的反馈。刚开始在组织内部工作的经验较少的开发人员，在前三个月很少能提出有价值的反馈。之后我们可以看到他们提出反馈的价值在一年内如何增加和稳定。\n为了提出有价值的反馈，你必须熟悉代码多项代码审查研究表明，有价值的反馈多数来自于曾经参与开发或审查对应代码的开发人员。好消息是，只要之前改过一次就足够了。也就是说在我们的研究中，一次修改代码的开发人员和修改了上百次代码的开发人员在审核时没有显著的区别。如下图所示。\n\n领域专家可以提高你的代码审查价值来自其他团队或者是跨团队的领域专家作为审阅者会对你的代码审查更有价值。你可以选择增加安全专家、大数据专家或可用性专家，即使他们并不像你的团队那样熟悉你们的代码。\n这样做的好处是给代码审查带来了特别的经验和外部的视角。他们进行代码审查的目的也不同。他们可能不会去寻找最佳实践和团队规范，而是检查代码中你需要他们检查的真正存在的问题。\n像对待自己一样对待别人代码审查是很社会化的活动，在致力于积极打造反馈文化的团队中，它被高度赞赏和高价值的工程实践。不幸的是，并不是任何地方都是这样。在一些团队中，代码审查被滥用为权利展示甚至权利争斗的工具。这样的代码审查没有任何帮助。\n如果你想要从代码审查中受益，了解如何提出建设性反馈是非常明智的选择。指出一些代码的质量不够高。如果你批判同事的代码，请务必始终以尊重的方式进行，并一直提出改进建议。\n另一方面，也不需要在代码审查过程中加入过多的赞美之词。在微软的代码审查研究中我们发现，作者不太在意对他们代码的称赞。\n为什么会这样？我们要再次提到代码审查的目标。通常每个批注都是一个小的工作项。即使是赞美，有太多也不会增加价值。它只会加剧处理批注的工作量。\n指出良好的工作对于团队合作精神是必不可少的，并且是一个很好的团队合作的动力。但是这最好是在其他场合提出，比如会议上或者是咖啡时间。\n外部情况影响反馈的价值还有几件事会影响你在代码审查过程中获得的价值。在研究中我们发现开发人员很难查看非代码的文件，比如配置文件或者编译文件。换句话说，开发人员会针对源码提供更有价值的反馈。\n影响代码反馈质量的另一个因素是审查文件的数量。需要审查的文件数越多，你收到反馈的质量就越低。保持审核的小巧有很多好处，并且是最有价值的代码审查最佳实践之一。\n总而言之，有价值的反馈是针对代码审查目标的反馈：检查当前代码更改是否正确以及是否高质量。不利于实现此目标的讨论应该在代码审查过程之外讨论。因为好的代码审查反馈也是及时提出的反馈，它会帮助作者更快通过审核。\n原文链接https://www.michaelagreiler.com/great-code-review-feedback/\n","tags":["Code Review"]},{"title":"【译】工程师如何打怪升级","url":"/2019/11/05/%E3%80%90%E8%AF%91%E3%80%91%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%A6%82%E4%BD%95%E6%89%93%E6%80%AA%E5%8D%87%E7%BA%A7/","content":"初级、中级和高级工程师并不是通过你从事编码工作的年限来判断的。初级工程师有时可能比高级工程师工作时间还要长。能否成为更高等级的工程师完全取决于你对专业技能的掌握程度，这也不是说高级工程师必须在每一方面都是专家，但是可以肯定的是，高级工程师一定比初级和中级工程师更加专业。\n不仅仅是编程能力更加专业，高级工程师和初中级工程师相比还有其他区别。这区别到底是什么呢？\n知识很明显，高级工程师需要掌握更多的知识。学习设计模式、架构、自动化测试、性能、安全等知识是初级工程师迈向中高级工程师的必经之路。\n知道软件开发过程中需要做什么是一件非常重要的事情。但是当你掌握了上面所说的全部知识时，你并不是已经成为高级工程师了，这些知识只是不同等级工程师之间的区别之一。\n编码很多人认为编码是人和计算机之间的一种交流，实际上是人和人的交流然后用于指导计算机。代码最终还需要编译成二进制码。\n你的代码需要保证可读性，这样以后其他开发者才能在这基础上做其他工作。最好能做到让一个从来没有看过你的代码的团队一打开就能立即进行新功能开发或者修复bug。这也是初级和高级工程师的区别之一。\n这里我们忽略了中级工程师，因为中级工程师在编程能力的比较中处于灰色地带。因为中级工程师介于初级与高级之间，并且更倾向于高级。编码能力更更加依赖于经验，而中级工程师通常至少参与过一次完整的软件开发流程。他们已经从一些简单的错误中吸取了教训。\n如何辨别初级工程师初级工程师通常没有开发经验。有些是刚毕业，第一次全职做开发。初级开发者的心态通常就是「能用就行」。能用的软件和好用的软件在他们看来都一样。\n写出简洁的代码是一件困难的事情。这也是初级工程师不具备的能力，他们写的代码往往比较繁琐。你可以通过诡异的单行代码以及庞杂的抽象逻辑来识别初级工程师。这是初级工程师炫耀的方法，让其他开发人员知道他们的能力，但这是错误的做法。\n初级工程师编码时专注于计算机端，而忽略了人力端。\n高级工程师是怎样的当查看高级工程师的代码时，你也许会想：这是全部代码吗？剩下的在哪？高级工程师写的代码总是简单、直接的。这时编程时可以修炼的最强技能之一。高级工程师遵循KISS原则：Keep it simple, stupid。\n高级工程师在编码方式和初级工程师有所不同，他们会优先考虑代码的可维护性和可扩展性。两种级别的工程师编码时的心态也完全不同，高级工程师更多考虑的时后续维护代码的人，而初级工程师考虑的是使代码可以被计算机执行。\n不止于编程能力除了编程能力之外，还有一些其他因素可以告诉你一名工程师的级别。\n初级工程师通常只是做一些简单的任务。他们不负责任何架构设计。中级工程师也不设计解决方案，他们同样是执行任务。与初级工程师不同的是，他们并不需要太多的监督和指导，只要分配给他们较为常规的任务，他们就能独立完成。高级工程师则可以独立的开发一款应用。\n这并不意味着高级工程师在开发过程中就不会碰到问题，每个开发人员每天都会遇到大量的问题，对于高级工程师来说也是如此。\n不同之处是高级工程师知道如何正确提问以及怎么解决问题。中级工程师在对于常规问题也知道如何正确提问，但遇到更加复杂的问题时则需要寻求帮助。\n高级工程师从不迷失方向，他们知道如何以正确的方式跟踪问题。不是说高级工程师不需要向其他人寻求帮助。有时最正确的选择就是向相关领域的专家寻求帮助。\n中级工程师也需要具备正确提出问题的能力，除非分配给他们的是需要很深入的专业知识的复杂任务。\n你不能指望初级工程师总是准确的提出问题。因为他们缺乏经验，他们需要更有经验的工程师来指导。我们要为初级工程师提供必要的资源，以及不断推动他们朝着正确的方向前进。\n如何晋级我们都希望提升自己的等级，成为更好的工程师，但是要怎么做呢？\n初级到中级由于初级工程师缺乏经验，因此他们至少要经历几次完整的软件开发流程。这一过程中他们不遇到很多坑，然后需要在下次避免再踩这样的坑。\n编码时，初级工程师要学会使自己的代码变得更加简洁。多为下一位接手这段代码的人考虑。你需要学会怎么调试代码，这会帮助你更好的理解它的运行过程。\n此外，你需要对一些最佳实践更加熟悉，同时你需要学习架构、性能、安全等知识，从而跨越到达到中级工程师的知识鸿沟。\n中级到高级从中级工程师到高级工程师可能会比较困难。有些开发人员整个职业生涯都停留在中级工程师。\n高级工程师知道什么可以做什么不可以做。这是最难学的一门课程，你只能从过去犯的错误中学习。\n如果你想要达到高级水平，你必须要准备好承担没有人能处理的任务。你应该考虑的是如何把工作做得更好，而不仅仅是怎么完成。\n作为高级工程师，你还有一项工作就是要帮助缺乏经验的工程师，当他们不知道怎么做时，你就是他们坚强的后盾。\n高级工程师的技术栈可能不会使你感到惊讶，除了编程能力，他们对公司所用的工具和应用的使用都是非常熟悉的。\n结论初级、中级、高级工程师的区别不只是工作年限。当然，你可以肯定的说，高级工程师要比初级和中级工程师更加专业，知识更丰富。但这不是决定工程师等级的唯一因素。\n高级工程师写代码更加简洁，并且编码时的心态与初级开发者不同。但是除了编码，知识如何提出并跟进问题也是一项必要的能力。只有高级工程师才能凭借丰富的经验从容的应对各种问题。\n初级工程师想要成长就要专注于编写更加简洁的代码，并且多经历完整的开发流程。从中级进阶到高级则需要学会处理更多困难的问题。你应该愿意承担更有挑战的任务，并成为技术栈的主人。高级工程师的另一项工作就是作为其他开发人员的后盾。\n最后送给你一句Martin Fowler的话：“任何傻瓜都能编写计算机可以理解的程序，好的工程师编写人类可以理解的代码。”\n原文链接https://medium.com/better-programming/the-differences-between-a-junior-mid-level-and-senior-developer-bb2cb2eb000d\n译者点评本文作者从编码能力和处理问题的能力阐述了不同级别工程师之间的区别。个人认为作者对于中级工程师的能力和国内的互联网公司的要求还是有些区别的。至少我接触的中级工程师都要具备一定的系统设计能力了。而高级工程师则需要具备更加全面的问题处理的能力。不过对于大多数人来说，可能头衔已经是中级了，但是并没有掌握文中提到的那些知识，这反而成为了中级到高级的鸿沟。相信大家也都意识到了学习的重要性，既然入了这行，就一起活到老学到老吧。\n","tags":["技术杂谈"]},{"title":"【译】感谢你的Code Review","url":"/2020/02/17/%E3%80%90%E8%AF%91%E3%80%91%E6%84%9F%E8%B0%A2%E4%BD%A0%E7%9A%84Code-Review/","content":"作为一名初级工程师，当我看到一些问题时，通常会主动去解决它们，因此我总会进行一些大范围的代码修改。\n这意味着我需要发出大量的代码审查。在一次修改中通常会涉及到从UI到数据库的所有部分。\n我对于自己能够维护整个系统而骄傲，也为自己的快速处理问题的能力而骄傲。同时也为自己的勇敢和解决重大问题的能力而自豪。\n直到有一天，一位资深工程师把我拉到一边，给我提出了迄今为止我收到过最好的代码审查返回。它告诉我应该将巨型的代码审查拆分为更小的增量修改。\n我第一反应是感到恼怒。我不理解他为什么要我这么做。我对自己解决重大问题的能力非常有信心！为什么他说我的工作做的不好？！一点一点的进行修改只会拖慢我的脚步！\n虽然当时我还不知道小规模、增量修改的种种好处，但是我很庆幸当时听了这位高级工程师的意见，很高兴我开始学着进行小规模、增量的修改。\n这种方法给我后来的职业生涯带来了巨大的好处。\n增量修改的好处进行增量修改有诸多好处，下面我来列举一些。\n\n更少的合并冲突。你改的文件越多，和其他人的修改发生冲突的可能性就越大，小规模的修改可以有效的避免冲突，即使有冲突时也能更快的解决。\n更快的代码审查。对代码审查人员来说，审查5个文件无疑要比审查50多个文件轻松许多。与那些需要面对面交谈十分钟才能开始看的代码相比，小规模的修改能够更快速的开始审查并且更容易解释。当审查人员面对大量的代码审查工作时，他们有可能会犯懒，非常希望能找个人替他们完成这项工作。因此你可能需要花费很长时间才能找到一个愿意审查你的代码的人。\n更早的修正。你的代码审查者可能与你的思路相左。他们可能会要求你重做所有的事情。如果你之前只花了几个小时进行修改，那么这对你来说可能不是什么大问题。但是如果你在这个问题上已经花费了两天时间，那么重做可能是一件非常痛苦的事情。\n更快速的测试。如果你的代码修改涉及到了从UI到数据库的所有层级，你可能需要对整个产品进行重新测试。而如果你只进行小规模修改，那就只需要测试你所修改的那部分。如果你需要解决很多代码审查反馈或者是合并很多代码时，这种好处就非常明显了。重新测试所有东西会花费大量的时间，特别是手动测试。\n更少的bug。小规模修改意味着你不需要同时将所有东西都装进脑子里。你可以专注于你进行优化的这一部分代码，保证你可以把它做到最好。（我曾经见过一个工程师，它对自己的大规模改动感到不知所措，后来他养成了检查和修复都追求完美的习惯。希望你不要成为那样的人，即使没人抱怨，但是你的同事将会慢慢变得不信任你的代码。）\n更容易排除故障。如果你需要改动一些代码，那么小规模的改动可以帮助你更加容易的定位问题。\n增量部署。如果你想要不停机更新，那么更小的、增量的改动会帮助你解决这个问题。（但这并不是全部解决方法）\n还原更加简单。当你写了bug时，你的改动越小，还原就更加简单。如果你合入了大量代码，并且其他人又在后来进行了改动，那么还原你的代码就会是一件非常痛苦的事情。也许你可以进行快速修复，但这并不是一定奏效，生产环境出现事故时，剔除有问题的代码会使团队的其他人更加放心。\n部署回滚更加简单。如果单次部署更新了web服务和即时生效的UI功能，那么如果你想要回滚后端服务就必须先要回滚UI的改动。由于这样部署方式，想要做到不停机更新可能并不容易。最好的办法就是把它们分别合入代码仓库并部署。\n更低的风险。这实际上是上述所有情况的结果。\n\n为你的未来交学费那天我从那位高级工程师那里收到的代码审查反馈，已经被证明是职业生涯迄今为止收到的最好的代码审查反馈了。\n多年后，我遇到了另一名工程师，他一直在进行大规模、彻底的变更。我把相同的反馈分享给了他，他看起来很生我的气，但是我完全可以理解他。在我看到他有进步之前，我离开了那家公司，希望他最终能体验到小规模修改带来的好处。\n相信他以后会是一名优秀的工程师。\n译者点评小范围的修改确实是很有必要的。我自己在做code review的时候看到那种几百行的代码修改也是很头疼的。作者对于小规模修改的好处总结的还是比较全面的，希望大家能有收获。\n原文地址https://medium.com/better-programming/the-best-code-review-feedback-i-ever-received-43313a503517\n","tags":["Code Review"]},{"title":"【译】微软如何进行代码审查","url":"/2019/06/23/%E3%80%90%E8%AF%91%E3%80%91%E5%BE%AE%E8%BD%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/","content":"带你了解全球最大的软件公司的code review\n原文链接：https://www.freecodecamp.org/news/how-code-reviews-work-at-microsoft-4ebdea0cd0c0/\n你是否想知道全球最大的软件公司之一是如何通过代码审查来保证代码质量的？我也一样，这就是我和微软的同事一起研究在我们公司怎么进行代码审查的原因。它是一种常规的做法吗？每个开发人员都需要进行代码审查吗？他们使用什么工具呢？\n这些问题在本文中会找到答案。\n首先，我想先介绍一些关于微软的关键信息。微软有14万员工，其中大约44%（超过6万）是工程师。Office、Visual Studio和Windows等一些产品同时有上千人在共同的代码库上开发的。\n我说这些是为了告诉你协调和管理软件开发的过程意味着什么。可以想象到的是，协调不同团队共同开发是一件非常重要的事。代码审查在这一过程中扮演着重要的角色。\n在微软，代码审查是开发过程中不可或缺的一部分微软的代码审查是一个被广泛采用的工程实践，成千上万的工程师认为这水一项伟大的实践，很多高绩效的团队都会在代码审查中花费大量的时间。\n调查微软的代码审查正是因为代码审查在开发过程中扮演着重要的角色，所以我们需要更深入的挖掘并真正理解这种做法的利弊。在微软代码审查的大规模研究中，我们采访、观察并调查了超过900位工程师，来了解他们在代码审查中的实践。\n我们的目标是弄清楚代码审查究竟发挥着什么样的作用，工程师在做代码审查时面临着哪些挑战，并提炼出他们克服这些挑战的最佳实践。\n我们能从微软的代码审查实践中学到什么？大多数经验告诉我们，小团队和大团队一样有价值。如果你的团队还没有进行代码审查，我会以一种展示最佳实践的方式来提炼我们的发现。我还会解释代码审查的生命周期，以便你在开发过程中加入这样的实践。\n如果你的团队已经在做代码审查了，你可以把你的实践经验与微软的代码审查实践进行对比。看你的代码审查生命周期是否有所不同，在后面的文章中，你会从代码实践的挑战中学习到有用的知识。通过阅读本文，你可以看看你的团队是否实现了我说的所有最佳实践，并克服了相应的挑战。\n微软的工程师多久进行一次代码审查？研究表明，36%的工程师说他们一天会进行多次代码审查，另外39%的工程师说他们每天最少进行一次代码审查，有12%的工程师会在一周进行多次，只有13%的工程师表示他们过去一周一次代码审查都没有做过。\n\n这表示微软的工程师会花费较多的时间来进行代码审查，所以要保证这些时间花费的是值得的。但是，代码审查有哪些好处呢？\n代码审查带来哪些好处呢？代码审查最大的好处就是可以保证代码的质量，并且能够发现代码中的一些缺陷。另一个重要的好处就是知识传输。\n知识传输意味着团队成员审核彼此的代码，以便熟悉代码库中的大部分代码，也是团队开发的最佳实践。另一个好处是，初级程序员和团队新人可以通过审查和被审查代码来快速提升他们的编程技巧。\n工程师可以在代码审查期间讨论替代解决方案，这样做不仅可以改善代码库，也是所有相关人员学习的过程。\n\n工程师一般怎么进行代码审查？代码审查可以通过多种方式执行，有时是一名工程师走到另一名工程师的办公桌前一起看代码，有时是整个团队一起审查代码。但是在微软，最常见的情况是使用代码审查工具来完成代码审查。\n微软的代码审查经常通过内部工具进行代码审查工具有很多种，微软的团队可以自由选择。2016年，89%的工程师使用的是CodeFlow作为代码审查工具。后文我会对此进行进一步的解释。由于Git的兴起，工具库也随之改变，我会尽快增加更新后的数字。但我们要先来讨论常见的代码审查场景：\n假设有一位名叫Rose的微软工程师，Rose刚刚完成了一个功能的一部分，现在想要得到同事的反馈。\nRose怎么在微软发起代码审查Rose这时已经准备好接收反馈了，所以她首先要准备好自己接受审查的代码，然后打开代码审查工具，此时可以看到自己做了哪些修改。\n认真检查这些修改之后，她需要向审查人员描述一下修改内容以及为什么修改。这个描述会帮助审查人员快速理解代码修改的目的，最后准备把代码发送给审查人员。\nRose怎么选择正确的审查人员？大多数有经验的程序员都知道谁应该来进行代码审查。但对于团队新人或者是做了新的领域任务的工程师来说是有一些困惑的。如果Rose不知道她应该添加谁，她应该看一下团队规定或者问一下同事。她也可以使用代码审查工具的推荐功能来帮助她选择审查人员。\n谁是相应的审查人员？Rose选择了她认为合适的审查人员。他们通常是其他工程师，但也有可能是其他相关人员，例如开发工程师、设计人员或者是管理人员。选择一些审查人员是因为他们有专业知识，而有些审查人员被选择则是让他们了解即将发生的变化。\n\nRose请求同事给予反馈选择好审查人员后，Rose会将代码审查发送出去。代码审查工具会自动通知审查人员，有时也会通知管理人员或者其他团队的项目管理者。\n接收反馈是一个迭代过程当Rose的同事有时间时，就会来进行代码审查。每一个审查人员都可以发表评论，评论完成后，审查人员便会把评论发送给Rose，Rose就可以根据审查人员的意见来完善代码了。\n审查人员通常会关注的事情有：代码看起来有没有bug？架构是否有问题？是否存在一些细节问题，比如缺少注释或拼写错误。并不是所有的评论都很有价值，不过有几种最佳实践可以提升代码价值。\nRose准备好了一个新的优化后的版本Rose会按照审查人员的建议修复bug并优化代码。如果Rose发现这其中有一些误解或其他有争议的问题，她会走向同事讨论这些问题。这种方法有时会比使用工具更加高效。\n总之，当她处理完所有的反馈之后，她把最新的代码发送给审查人员。这种新的版本叫做revision。\n如果需要，她会收到更多的反馈，这种循环会持续多次，这主要取决于修改的类型以及代码的质量。对于简单的修改，只需要一次就可以，而对于复杂的修改，往往要经过多轮的review。\n所有的审查人员同意Rose合并代码在审查结束后，审查人员会将代码标记为OK，Rose就可以将代码合并到代码库了。\n有些团队会允许开发人员在审查结束前合入代码，但这只限于修改部分较小的代码，这有利于异步审查和快速开发。\n我所描述的所有步骤都是微软典型的代码审查过程，但不同的团队也会根据情况制定更加宽松或严格的规则。\n不是所有的团队都相同可以想象的是，在微软的6万个工程师，上千个团队并不是完全相同的，有些团队会根据需要增加一些步骤或工具。而我只想向你介绍一些概要的步骤。\n代码审查包括测试结果代码审查中，我们最不想在查bug方面浪费时间，所以需要一套自动化测试流程。你要在提交审查之前保证代码执行结果符合预期。\n这就是为什么有些团队需要在提交任何代码审查之前都跑一遍测试，这是为了防止某些工程师忘了测试，并保证每次提交的代码可用。\n还有些团队会更进一步，在开发人员提交代码后触发一个构建。这个构建包括精确显示修改部分，并开始一系列自动测试。测试结果会反馈到代码审查中。这么做是为了保证公共代码库中代码的可用性。\n代码审查包括用户接口如果修改影响到接口，那么开发人员应该提交截图。这么做可以让审查人员直观的看到改变，其次可以与在自己机器上运行的结果进行比较。\n代码审查包括静态分析静态分析工具可以使审查人员不必浪费时间来检查代码风格是否符合规范。微软有些团队会使用自动话的审查机器人。审查机器人会自动标注代码风格问题，以节省人工审核时间。\n微软代码审查工具多年来，微软使用一款名叫CodeFlow的内部工具作为代码审查工具。这是一款复杂的代码审查工具，它可以引导开发人员发起审查，自动提醒审查人员，并包含丰富的评论和讨论功能。\nCodeFlow的UI是比较重的，很像Word和PowerPoint。\n\nCodeFlow的接口说明如果不感兴趣的话可以跳过这部分，不过为了感兴趣的同学，我还是要介绍一下CodeFlow的各个接口。区域A展示的是所有有影响的文件。\n区域B是已分配的审核人员列表以及审核状态（比如已签名或待处理）。C是文档展示区域，D是所有文档的评论列表。\nF展示的是单条评论，这个评论和具体的代码相关联。最后，E区域是整个代码审查的结果，这里是complete。上方的数字表示之前的几个版本，这里有5个不同版本。\n评论功能评论功能是CodeFlow最棒的功能之一。审查人员可以选择代码中的一部分进行评论，代码的作者和其他审查人员就会收到通知，并且可以在这里开始讨论。\n讨论功能这个功能就像是在社交软件上交流一样（Twitter或Facebook），所以在CodeFlow上使用评论显得非常自然。另一个亮点是评论的状态，它可以是“won’t fix”, “resolved” 或“open”。\n对照两个不同的版本对比两个不同的版本是一个非常实用的功能。你可以清晰的看出作者在不同的版本之间做了哪些修改。这可以帮助审查人员轻松追踪修改过程。\n代码审查分析工具微软开发者会花费大量的时间在执行代码审查上，为了保证这时间花费的是值得的，微软有自己的代码审查分析平台。\n这个平台会存储所有的代码审查数据。包括代码审查的开发者，所有的评论，甚至是每一个修改版本的代码。\n这些数据是代码审查研究的基础，它也会被一些产品团队用来检验自己团队的实践结果。我在这个系列文章中的一些研究和分析也都来自于这些数据。\n微软代码审查的未来随着微软收购了Github，改变是不可避免的。例如，微软内部已经开始广泛采用Git作为代码版本控制工具了。这也意味着未来代码审查可能采用PR的形式。\n","tags":["Code Review"]},{"title":"【译】推荐你使用Vim的三个半理由","url":"/2020/03/11/%E3%80%90%E8%AF%91%E3%80%91%E6%8E%A8%E8%8D%90%E4%BD%A0%E4%BD%BF%E7%94%A8Vim%E7%9A%84%E4%B8%89%E4%B8%AA%E5%8D%8A%E7%90%86%E7%94%B1/","content":"在很久以前，软件行业就有一种争论：究竟要不要使用Vim。这种争论有点像音乐界关于模拟信号与数字信号的争论。\n在大多数工程师眼中，Vim是一种老旧的，已经过时了的文本编辑器。它缺乏很多优秀的功能，用起来也没有现代的IDE舒服。但实际上，Vim绝对不是一种老旧的文本编辑器。\nVim就像是手动挡的汽车一样。\n你必须要学会如何去驾驭它，如何手动换挡。而一旦你习惯了以后，就会获得成倍的收获与乐趣。它可以带着你在蜿蜒的小路上行驶，你需要保持全神贯注，并且精巧的换挡。这时你会觉得你和汽车之间仿佛建立了某种连接，它就像是你身体的一部分一样。\nVim就是这样子，这也是为什么它会如此有趣的原因。\n下面就是我推荐你使用Vim的3个半理由。\n1. 不用鼠标当你使用Vim的时候，你几乎可以不使用鼠标，你必须用键盘去做所有的事情。这会使你成为更强大、更高效的开发者。\n你的手离开键盘的次数越多，你的工作效率就会越低。每次使用鼠标都会拖慢你的工作效率，你的大脑就像CPU一样要进行线程上下文的切换。所以你应该慢慢学会使用快捷键。\nVim的一大好处就是它只能使用快捷键，你想用鼠标都不行。\n你必须学着习惯这一点。\n2. 本地开发在大多数Unix系统中，默认都可以使用Vim或者Vi。这两者是完全运行在命令行上的编辑器看，由于没有GUI，因此它们更加通用和轻便。\n因为Vim是完全由命令行驱动的，当你使用SSH连接远程服务器编辑文本或者在线修改代码时，就会有飞一样的感觉了。\n在使用像Ubuntu Server这样本身并不包含GUI的系统时，VIm更是显得尤为重要。\n有了Vim，你就不需要在系统直接来回传输文件，也不用使用SSHFS来挂载远程系统了。只需要用vim打开你的配置文件，编辑，然后:wq。\n3. 不会分心你是否真的需要自动补全和集成代码分析呢？你又真的需要那些花里胡哨的UI吗？\n你应该学会的是简单思考，Vim没有那些大型IDE臃肿的功能，它能让你专注于问题的本质：代码。\n使用Vim时，你会变得更加有条理和严谨。你会更加清楚你当前在编辑的是哪个文件，它在什么目录下。即使没有展示在你面前，你也会很清楚它们的位置。\n由于编辑器的简单性，在文件之间进行编辑，剪切，粘贴和移动这些动作会变得更加清晰和明确。\n当然了，有时候你可能需要一些额外的功能。可以通过安装Vim插件来达到目的。这些插件都是按需取用的，你只需要选择你真正需要用的即可。\n你可以使用Pathogen来帮助你管理Vim的插件。\n3½ 街头威望其他不使用Vim的开发人员看到你使用Vim开发时会觉得你很疯狂，你可以利用他们的这种心态来展示自己的优势，同时还能提高影响力。\n记住，你可以比别人开发的更快。因为Vim比那些现代的大型IDE占用的内存要少得多。\n无论你是为了什么目的去使用Vim，抑或是被迫去使用的。你都可以学着去爱上它，这样你自己也会成长很多。\nVim是最好的厨师才能用的刀。你可以用它来切东西，或者被它切。。。\n译者点评Vim在开发中的用处还是很大的，不过我本人也只是把它作为一个文本编辑器来用，写代码还是更习惯用IDE。因为我真的需要代码补全功能。用Vim编程也许只有那些初代目大佬才能做到吧。大家还是可以适当学一学使用Vim，毕竟在外行看来可以算是装X利器了。\n原文地址https://medium.com/better-programming/3%C2%BD-reasons-why-you-should-be-using-vim-8202360afa3\n","tags":["技术杂谈"]},{"title":"【译】教你用16个小时从0构建一个Rust应用","url":"/2020/02/23/%E3%80%90%E8%AF%91%E3%80%91%E6%95%99%E4%BD%A0%E7%94%A816%E4%B8%AA%E5%B0%8F%E6%97%B6%E4%BB%8E0%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AARust%E5%BA%94%E7%94%A8/","content":"我们在2019年的最后两天，参加了Prodigy Education举办的黑客马拉松，许多团队聚在一起努力将他们的想法变成现实。\n我们之中有的人只是单纯为了好玩，有的是想学一些新的知识，还有些人可能是想证明一些概念或想法。\n我在过去几周总是被动的获取Rust相关信息或使用Rust的代码，因此我认为hackathon是一次学习Rust的绝佳时机。\nhackathon的时间紧迫性使我更加快速的去学习，同时也会去解决现实世界的一些问题。\n为什么是Rust\n在我职业生涯的前10年中，有8年都在使用C和C++。\n从好的方面来讲，我喜欢像C++这样可以提供静态类型的语言，因为它能在编译期就能够早早的发现错误。\n我个人对于C++的一些看法是：\n\n工程师很容易搬起石头砸自己的脚\n作为一门编程语言，它已经非常臃肿且复杂\n缺乏良好的、标准的广泛适用的包管理系统\n\n自从我改做Web应用以来，一直是做Python和JavaScript开发，使用像Django、Flask和Express这样的框架。\n到目前为止，我在Python和JavaScript中的开发经验是，它们可以提供良好的程序迭代和交付速度，但有时会占用大量的CPU和内存，即使服务是相对空闲的。\n我经常发现自己写好的C++程序，会缺失一些安全性、速度和精简性。\n我想要寻找一种像Rust这样精简的、裸机编程语言来开发web应用。\n没有运行时，没有垃圾回收。直接加载二进制代码，交给内核执行。\n目标我的目标是完成一个后端由Rust编写，前端是JavaScript+React完成的类似于S3作为图床的应用程序，用户可以做以下事情：\n\n浏览图床中所有的图片（分页可选）\n上传图片\n上传图片时可以给图片增加标签\n通过名称进行查询或过滤\n\n所有有趣的hackathon项目都有一个名字，所以我决定将这个项目命名为：\nRustIC -&gt; Rust + Image Contents\n\n我认为如果我做到了以下这些事情，那么这次hackathon之行对我个人来说就是成功的：\n\n对Rust有一个基本的理解，包括它的类型系统和内存模型\n探索S3的对于文件和任意标签的预签名链接功能\n写出一个可以验证的功能正常的应用\n\n由于我的主要目标是开发功能，同时兼顾学习。很多代码是我一边学一边写的，所以代码组织和效率可能并不是最理想的，因为这些属于次要目标。\nRust的原则在我开始之前，我带着好奇心去了解了要学习的语言的设计师在创建这门语言时内心的原则是什么。我找到了一个简化版本和一个详细版本。\n与我在许多博客上读到的内容相反，Rust是有可能发生内存泄露（循环引用）和之行不安全的操作（unsafe代码块中）的，详细描述在上面的FAQ中。\n\n“We [the language creators] do not intend [for Rust] to be 100% static, 100% safe, 100% reflective.”\n\n\n从后端开始Google搜索“Rust web framework“，排在最前面的是Rocket。我进入这个网站，发现文档的示例都一目了然。\n有一点需要注意的是Rocket需要Rust的nightly版本，不过在hackathon上这都是小问题。\nGitHub的代码库中有着非常丰富的例子。完美！\n我使用Cargo创建了一个新的项目，在TOML文件中加入了Rocket依赖，然后跟着Rocket的入门指南，写了第一段代码：\n#[get(&quot;/&quot;)]fn index() -&gt; &amp;&#x27;static str &#123;    &quot;Hello, world!&quot;&#125;fn main() &#123;    rocket::ignite().mount(&quot;/&quot;, routes![index]).launch();&#125;\n对于熟悉Django、Flask、Express等框架等同学来说，这段代码读起来非常容易。作为一名Rocket用户，你可以使用宏作为装饰器来将路由映射到对应的处理函数上。\n在编译时，宏将被扩展。这对开发者是完全透明的。如果你想看扩展后的代码，可以使用cargo-expand。\n以下是我在构建Rust应用程序时的一些有趣的或者有挑战性的亮点：\n指定路由响应我想要以JSON的数据格式返回S3中所有的文件列表。\n你可以看到路由关联的处理函数的代码决定了响应类型。\n设置响应结构非常容易，如果你想要返回JSON格式的数据，并且每个字段都有自己的结构和类型，那对应的就是Rust的struct。\n所以你应该先定义一个结构体struct(S)来接受响应，并且需要进行标注：\n#[derive(Serialize)]\nstruct(s)被标记了#[derive(Serialize)]，因此可以通过rocket_contrib::json::Json将它转换成JSON。\n#[derive(Serialize)]struct BucketContents &#123;    data: Vec&lt;S3Object&gt;,&#125;#[derive(Serialize)]struct S3Object &#123;    file_name: String,    presigned_url: String,    tags: String,    e_tag: String, // AWS generated MD5 checksum hash for object    is_filtered: bool,&#125;#[get(&quot;/contents?&lt;filter&gt;&quot;)]fn get_bucket_contents(    filter: Option&lt;&amp;RawStr&gt;) -&gt; Result&lt;Json&lt;BucketContents&gt;, Custom&lt;String&gt;&gt; &#123;    // Returns either Ok(Json(BucketContents)) or,    // a Custom error with a reason&#125;\n处理分段上传当我意识到我的前端很有可能使用POST方法上传格式为multipart/form-data的表单数据时，我就开始深入研究如何使用Rocket来构建程序了。\n不幸的是，Rocket0.4版本不支持multipart，看起来在0.5版本会支持。\n这意味着我需要使用multipart crate并集成到Rocket中。最终代码可以正常运行，但是如果Rocket支持multipart将会使代码更加简洁。\n#[post(&quot;/upload&quot;, data = &quot;&lt;data&gt;&quot;)]// signature requires the request to have a `Content-Type`. The preferred way to handle the incoming// data would have been to use the FromForm trait as described here: https://rocket.rs/v0.4/guide/requests/#forms// Unfortunately, file uploads are not supported through that mechanism since a file upload is performed as a// multipart upload, and Rocket does not currently (As of v0.4) support this. // https://github.com/SergioBenitez/Rocket/issues/106fn upload_file(cont_type: &amp;ContentType, data: Data) -&gt; Result&lt;Custom&lt;String&gt;, Custom&lt;String&gt;&gt; &#123;    // this and the next check can be implemented as a request guard but it seems like just    // more boilerplate than necessary    if !cont_type.is_form_data() &#123;        return Err(Custom(            Status::BadRequest,            &quot;Content-Type not multipart/form-data&quot;.into()        ));    &#125;    let (_, boundary) = cont_type.params()                                 .find(|&amp;(k, _)| k == &quot;boundary&quot;)                                 .ok_or_else(        || Custom(            Status::BadRequest,            &quot;`Content-Type: multipart/form-data` boundary param not provided&quot;.into()        )    )?;    // The hot mess that ensues is some weird combination of the two links that follow    // and a LOT of hackery to move data between closures.    // https://github.com/SergioBenitez/Rocket/issues/106    // https://github.com/abonander/multipart/blob/master/examples/rocket.rs    let mut d = Vec::new();    data.stream_to(&amp;mut d).expect(&quot;Unable to read&quot;);    let mut mp = Multipart::with_body(Cursor::new(d), boundary);    let mut file_name = String::new();    let mut categories_string = String::new();    let mut raw_file_data = Vec::new();    mp.foreach_entry(|mut entry| &#123;        if *entry.headers.name == *&quot;fileName&quot; &#123;             let file_name_vec = entry.data.fill_buf().unwrap().to_owned();            file_name = from_utf8(&amp;file_name_vec).unwrap().to_string()        &#125; else if *entry.headers.name == *&quot;tags&quot; &#123;            let tags_vec = entry.data.fill_buf().unwrap().to_owned();            categories_string = from_utf8(&amp;tags_vec).unwrap().to_string();        &#125; else if *entry.headers.name == *&quot;file&quot; &#123;            raw_file_data = entry.data.fill_buf().unwrap().to_owned()        &#125;    &#125;).expect(&quot;Unable to iterate&quot;);    let s3_file_manager = s3_interface::S3FileManager::new(None, None, None, None);    s3_file_manager.put_file_in_bucket(file_name.clone(), raw_file_data);    let tag_name_val_pairs = vec![(&quot;tags&quot;.to_string(), categories_string)];    s3_file_manager.put_tags_on_file(file_name, tag_name_val_pairs);    return Ok(        Custom(Status::Ok, &quot;Image Uploaded&quot;.to_string())    );&#125;\n配置CORS路由写好了以后，我就开始用curl或Postman来进行测试了，现在已经是时候开始把前端集成进来了。我需要适当设置响应头以避免跨域问题。\nRocket依旧没有支持这个特性。\n然后我在GitHub代码库中找到了一些解决方案：\n// CORS Solution below comes from: https://github.com/SergioBenitez/Rocket/issues/25extern crate rocket;use std::io::Cursor;use rocket::fairing::&#123;Fairing, Info, Kind&#125;;use rocket::&#123;Request, Response&#125;;use rocket::http::&#123;Header, ContentType, Method&#125;;struct CORS();impl Fairing for CORS &#123;    fn info(&amp;self) -&gt; Info &#123;        Info &#123;            name: &quot;Add CORS headers to requests&quot;,            kind: Kind::Response        &#125;    &#125;    fn on_response(&amp;self, request: &amp;Request, response: &amp;mut Response) &#123;        if request.method() == Method::Options ||            response.content_type() == Some(ContentType::JSON) ||            response.content_type() == Some(ContentType::Plain) &#123;            response.set_header(Header::new(&quot;Access-Control-Allow-Origin&quot;, &quot;http://localhost:3000&quot;));            response.set_header(Header::new(&quot;Access-Control-Allow-Methods&quot;, &quot;POST, GET, OPTIONS&quot;));            response.set_header(Header::new(&quot;Access-Control-Allow-Headers&quot;, &quot;Content-Type&quot;));            response.set_header(Header::new(&quot;Access-Control-Allow-Credentials&quot;, &quot;true&quot;));        &#125;        if request.method() == Method::Options &#123;            response.set_header(ContentType::Plain);            response.set_sized_body(Cursor::new(&quot;&quot;));        &#125;    &#125;&#125;fn main() &#123;        rocket::ignite().attach(        CORS()    ).mount(        &quot;/&quot;,         routes![get_bucket_contents, upload_file]    ).launch();&#125;\n过了一会，我发现了rocket_cors，它帮助我大幅缩减了代码量。\nfn main() -&gt; Result&lt;(), Error&gt; &#123;    let allowed_origins = AllowedOrigins::some_exact(&amp;[&quot;http://localhost:3000&quot;]);    let cors = rocket_cors::CorsOptions &#123;        allowed_origins,        allowed_methods: vec![Method::Get, Method::Post].into_iter().map(From::from).collect(),        allowed_headers: AllowedHeaders::some(&amp;[&quot;Content-Type&quot;, &quot;Authorization&quot;, &quot;Accept&quot;]),        allow_credentials: true,        ..Default::default()    &#125;    .to_cors()?;    rocket::ignite().attach(cors)                    .mount(&quot;/&quot;, routes![get_bucket_contents, upload_file])                    .launch();    Ok(())&#125;\n运行起来我们只需要一个简单的cargo run命令就可以让程序运行起来\n\n我机器上的活动监视器告诉我这个程序正在运行中，并且只消耗了2.7MB内存。\n而且这还只是没有经过优化的调试版本。项目使用- release标签打包的话，运行时只需要1.6MB内存。\n\n基于Rust的后端服务器，我们请求/contents这个路由会得到如下响应：\n&#123;    &quot;data&quot;: [        &#123;            &quot;file_name&quot;: &quot;Duck.gif&quot;,            &quot;presigned_url&quot;: &quot;https://s3.amazonaws.com/rustic-images/Duck.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDWJNDW3U8329UDNJ%2F20200107%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200107T050353Z&amp;X-Amz-Expires=1800&amp;X-Amz-Signature=1369c003b2f54510882bf9982ab56d024d6c9d2655a4d86f8907313c7499b56d&amp;X-Amz-SignedHeaders=host&quot;,            &quot;tags&quot;: &quot;animal&quot;,            &quot;e_tag&quot;: &quot;\\&quot;93c570cadd6b8b2f85b47c2f14fd82a1\\&quot;&quot;,            &quot;is_filtered&quot;: false        &#125;,        &#123;            &quot;file_name&quot;: &quot;GIZMO.png&quot;,            &quot;presigned_url&quot;: &quot;https://s3.amazonaws.com/rustic-images/GIZMO.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDWJNDW3U8329UDNJ%2F20200107%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200107T050353Z&amp;X-Amz-Expires=1800&amp;X-Amz-Signature=040e76c2df5a9a54ed4fbc8490378cf732b32bae78f628448536fc610018c0c3&amp;X-Amz-SignedHeaders=host&quot;,            &quot;tags&quot;: &quot;robots&quot;,            &quot;e_tag&quot;: &quot;\\&quot;2cde221a0c7a72c0a7a60cffce29a0bc\\&quot;&quot;,            &quot;is_filtered&quot;: false        &#125;,        &#123;            &quot;file_name&quot;: &quot;GreenSmile.gif&quot;,            &quot;presigned_url&quot;: &quot;https://s3.amazonaws.com/rustic-images/GreenSmile.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDWJNDW3U8329UDNJ%2F20200107%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200107T050354Z&amp;X-Amz-Expires=1800&amp;X-Amz-Signature=d115b107de530ce15b3590abdbab355c2a9481a81131f88bf4ad2a59ca11bbac&amp;X-Amz-SignedHeaders=host&quot;,            &quot;tags&quot;: &quot;smile-face&quot;,            &quot;e_tag&quot;: &quot;\\&quot;86854a599540f50bdc5e837d30ca34f9\\&quot;&quot;,            &quot;is_filtered&quot;: false        &#125;    ]&#125;\n前端的工作相对简单一些，我们使用的是：\n\nReact\nReact Bootstrap\nreact-grid-gallery\nreact-tags-input\n\n用户可以在我们的页面浏览图片，也可以通过文件名或标签来进行检索或过滤。\n\n用户还可以通过拖拽来上传文件，并且可以在提交上传之前打上标签。\n\n我喜欢使用Rust构建应用程序的原因\nCargo对于依赖和应用管理的程度简直令人惊叹\n编译器对于我们处理编译错误帮助非常大，有位博主在博客中描述了他是如何按照编译器大指导来写代码的。我的经验也比较类似。\n我需要的每一项功能都有crate，这让我感到非常惊喜\n\n\n\n在线的Rust Playground，让我可以运行小的代码片段。\nRust语言服务器，已经很好的集成到了Visual Studio Code，它能够提供实时错误检查、格式设置、符号查找等。这让我可以在几个小时内不编译就能取得不错的进展。\n\n不便、惊喜和麻烦尽管Rust的文档很棒，但我不得不依赖一些crates的文档和例子。有些crates有很棒的集成测试，提供了一些关于如何使用的提示。当然了，Stack Overflow和Reddit也给我提供了很多帮助。\n\n另外还要注意的是：\n\n理解所有权、生命周期和所有权借用会使学习难度陡增，特别是在为期两天的黑客马拉松中努力提供功能时。我将它们与C++做比较并且弄清楚，但有时还是会感到困惑。\n在所有的事情中，Strings拦住了我几分钟，特别是String和&amp;str的区别更是令人困惑——直到我花了些时间来理解所有权、生命周期和所有权借用才搞清楚这些。\n\n其他的一些观察\nRust中没有真正意义上的null类型，通常情况下，空值需要用Option类型的None来表示\n模式匹配非常棒，这是我在Scala中最喜欢的一个特性，在Rust中也一样。这种代码看起来表现力很强，并且允许编译器标记未处理的情况。\n\nmatch bucket_contents &#123;    Err(why) =&gt; match why &#123;        S3ObjectError::FileWithNoName =&gt; Err(Custom(            Status::InternalServerError,            &quot;Encountered bucket objects with no name&quot;.into()        )),        S3ObjectError::MultipleTagsWithSameName =&gt; Err(Custom(            Status::InternalServerError,            &quot;Encountered a file with a more than one tag named &#x27;tags&#x27;&quot;.into()        ))    &#125;,    Ok(s3_objects) =&gt; &#123;        let visible_s3_objects: Vec&lt;S3Object&gt; = s3_objects.into_iter()                                                          .filter(|obj| !obj.is_hidden())                                                          .collect();        Ok(Json(BucketContents::new(visible_s3_objects)))    &#125;&#125;\n\n说起安全和不安全模式，你仍然可以进行更底层的编程，比如说在不安全的模式下可以和C语言代码通过接口交互。尽管Rust中有很多正确性检查，但你仍然可以在不安全模块中做一些骚操作，例如解引用。读代码的人也可以从不安全模块中获取到很多信息。\n通过Box在堆中分配内存空间，而不是new和delete。刚开始感觉比较奇怪，但是也很容易理解。标准库中还定义了其他的一些智能指针，如果你需要使用引用数量或者弱引用时就可以直接使用。\nRust中的异常也很有趣，因为它没有异常。你可以选择使用Result&lt;T, E&gt;表示可以恢复的错误，也可以用panic!宏表示不可恢复的错误。\n\n// This code:// 1. Takes a vector of objects representing S3 contents// 2. Uses filter to remove entries we don&#x27;t care about// 3. Uses map to transform each object into another type, but terminates iteration// .  if the lambda passed to map returns an Err. // 4. If all iterations produced an Ok(S3Object) result, these are collected into a Vec&lt;S3Object&gt;let bucket_contents: Result&lt;Vec&lt;S3Object&gt;, S3ObjectError&gt; = bucket_list        .into_iter()        .filter(|bucket_obj| bucket_obj.size.unwrap_or(0) != 0) // Eliminate folders        .map(|bucket_obj| &#123;            if let None = bucket_obj.key &#123;                return Err(S3ObjectError::FileWithNoName);            &#125;            let file_name = bucket_obj.key.unwrap();            let e_tag = bucket_obj.e_tag.unwrap_or(String::new());            let tag_req_output = s3_file_manager.get_tags_on_file(file_name.clone());            let tags_with_categories: Vec&lt;Tag&gt; = tag_req_output.into_iter()                                                            .filter(|tag| tag.key == &quot;tags&quot;)                                                            .collect();            if tags_with_categories.len() &gt; 1 &#123;                return Err(S3ObjectError::MultipleTagsWithSameName);            &#125;            let tag_value = if tags_with_categories.len() == 0 &#123;                &quot;&quot;.to_string()            &#125; else &#123;                tags_with_categories[0].value.clone()            &#125;;            let presigned_url = s3_file_manager.get_presigned_url_for_file(                file_name.clone()            );            Ok(S3Object::new(                file_name,                e_tag,                tag_value,                presigned_url,                false,            ))        &#125;)        .collect();\n手册中是这样描述的：\n\n在多数情况下，Rust需要你尽可能了解错误，并且在编译之前对其做出相应的处理。这个需求使你的程序更加健壮，保证你在发布之前就可以发现并处理其中的错误。\n\n要点和教训\nJohn Carmack曾经将编写Rust的经历描述为“非常有益”。我同意这种感受，这次hackathon给我的感觉就像是打开了一扇新世界的大门并且发现了很多新鲜事物，这些收获绝不仅仅是停留在代码层面的。\n事后看来，我应该更加严谨的选择网络框架的。再多想一下的话，我可能会走出一条不同的道路。我下次可能会选择iron、actix-web, 或者是 tiny-http。\n我只学到了Rust的皮毛，16个小时是不可能完全成为一名Rustacean的，即使我对这门语言充满了好奇心，也做了一些深入的了解。我对Rust的未来感到兴奋，我认为它为构建应用程序带来了很多规范，它是一种表现力非常丰富的语言，并且能为我们提供与C++性能相当的运行速度和内存性能呢。\n\n资源RustIC后端代码\nRustIC前端代码\nRusoto：一个Rust的AWS SDK\n原文链接https://medium.com/better-programming/learning-to-use-rust-over-a-16-hour-hackathon-5f0ac2f604df\n","tags":["技术杂谈"]},{"title":"【译】浅谈SOLID原则","url":"/2019/12/04/%E3%80%90%E8%AF%91%E3%80%91%E6%B5%85%E8%B0%88SOLID%E5%8E%9F%E5%88%99/","content":"SOLID原则是一种编码的标准，为了避免不良设计，所有的软件开发人员都应该清楚这些原则。SOLID原则是由Robert C Martin推广并被广泛引用于面向对象编程中。正确使用这些规范将提升你的代码的可扩展性、逻辑性和可读性。\n当开发人员按照不好的设计来开发软件时，代码将失去灵活性和健壮性。任何一点点小的修改都非常容易引起bug。因此，我们应该遵循SOLID原则。\n首先我们需要花一些时间来了解SOLID原则，当你能够理解这些原则并正确使用时，你的代码质量将会得到大幅的提高。同时，它可以帮助你更好的理解一些优秀软件的设计。\n为了理解SOLID原则，你必须清楚接口的用法，如果你还不理解接口的概念，建议你先读一读这篇文章。\n下面我将用简单易懂的方式为你描述SOLID原则，希望能帮助你对这些原则有个初步的理解。\n单一责任原则\n一个类只能因为一个理由被修改。\nA class should have one, and only one, reason to change.\n\n一个类应该只为一个目标服务。并不是说每个类都只能有一个方法，但它们都应该与类的责任有直接关系。所有的方法和属性都应该努力做好同一类事情。当一个类具有多个目标或职责时，就应该创建一个新的类出来。\n我们来看一下这段代码：\npublic class OrdersReportService &#123;    public List&lt;OrderVO&gt; getOrdersInfo(Date startDate, Date endDate) &#123;        List&lt;OrderDO&gt; orders = queryDBForOrders(startDate, endDate);        return transform(orders);    &#125;    private List&lt;OrderDO&gt; queryDBForOrders(Date startDate, Date endDate) &#123;        // select * from order where date &gt;= startDate and date &lt; endDate;    &#125;    private List&lt;OrderVO&gt; transform(List&lt;OrderDO&gt; orderDOList) &#123;        //transform DO to VO    &#125;&#125;\n这段代码就违反了单一责任原则。为什么会在这个类中执行sql语句？这样的操作应该放到持久化层，持久化层负责处理数据的持久化的相关操作，包括从数据库中存储或查询数据。所以这个职责不应该属于这个类。\ntransform方法同样不应该属于这个类，因为我们可能需要很多种类型的转换。\n因此我们需要对代码进行重构，重构之后的代码如下（为了节省篇幅）：\npublic class OrdersReportService &#123;    @Autowired    private OrdersReportDao ordersReportDao;    @Autowired    private Formatter formatter;    public List&lt;OrderVO&gt; getOrdersInfo(Date startDate, Date endDate) &#123;        List&lt;OrderDO&gt; orders = ordersReportDao.queryDBForOrders(startDate, endDate);        return formatter.transform(orders);    &#125;&#125;public class OrdersReportDao &#123;        public List&lt;OrderDO&gt; queryDBForOrders(Date startDate, Date endDate) &#123;&#125;&#125;public class Formatter &#123;        private List&lt;OrderVO&gt; transform(List&lt;OrderDO&gt; orderDOList) &#123;&#125;&#125;\n开闭原则\n对扩展开放，对修改关闭。\nEntities should be open for extension, but closed for modification.\n\n软件实体（包括类、模块、函数等）都应该可扩展，而不用因为扩展而修改实体的内容。如果我们严格遵循这个原则，就可以做到修改代码行为时，不需要改动任何原始代码。\n我们还是以一段代码为例：\nclass Rectangle extends Shape &#123;    private int width;    private int height;    public Rectangle(int width, int height) &#123;        this.width = width;        this.height = height;    &#125;&#125;class Circle extends Shape &#123;    private int radius;    public Circle(int radius) &#123;        this.radius = radius;    &#125;&#125;class CostManager &#123;    public double calculate(Shape shape) &#123;        double costPerUnit = 1.5;        double area;        if (shape instanceof Rectangle) &#123;            area = shape.getWidth() * shape.getHeight();        &#125; else &#123;            area = shape.getRadius() * shape.getRadius() * pi();        &#125;        return costPerUnit * area;    &#125;&#125;\n如果你想要计算正方形的面积，那么我们就需要修改calculate方法的代码。这就破坏了开闭原则。根据这个原则，我们不能修改原有代码，但是我们可以进行扩展。\n所以我们可以把计算面积的方法放到Shape类中，再由每个继承它的子类自己去实现自己的计算方法。这样就不用修改原有的代码了。\n里氏替换原则里氏替换原则是由Barbara Liskov在1987年的“数据抽象“大会上提出的。Barbara Liskov和Jeannette Wing在1994年发表了论文对这一原则进行阐述：\n\n如果φ(x)是类型T的属性，并且S是T的子类型，那么φ(y)就是S的属性。\nLet φ(x) be a property provable about objects x of type T. Then φ(y) should be true for objects y of type S where S is a subtype of T.\n\nBarbara Liskov给出了易于理解的版本，但是这一版本更依赖于类型系统：\n\n1. Preconditions cannot be strengthened in a subtype.2. Postconditions cannot be weakened in a subtype.3. Invariants of the supertype must be preserved in a subtype.\n\nRobert Martin在1996年提出了更加简洁、通顺的定义：\n\n使用指向基类指针的函数也可以使用子类。\nFunctions that use pointers of references to base classes must be able to use objects of derived classes without knowing it.\n\n更简单一点讲就是子类可以替代父类。\n根据里氏替换原则，我们可以在接受抽象类（接口）的任何地方用它的子类（实现类）来替代它们。基本上，我们应该注意在编程时不能只关注接口的输入参数，还需要保证接口实现类的返回值都是同一类型的。\n下面这段代码就违反了里氏替换原则：\n&lt;?phpinterface LessonRepositoryInterface&#123;    /**     * Fetch all records.     *     * @return array     */    public function getAll();&#125;class FileLessonRepository implements LessonRepositoryInterface&#123;    public function getAll()    &#123;        // return through file system        return [];    &#125;&#125;class DbLessonRepository implements LessonRepositoryInterface&#123;    public function getAll()    &#123;        /*            Violates LSP because:              - the return type is different              - the consumer of this subclass and FileLessonRepository won&#x27;t work identically         */        // return Lesson::all();        // to fix this        return Lesson::all()-&gt;toArray();    &#125;&#125;\n译者注：这里没想到Java应该怎么实现，因此直接用了作者的代码，大家理解就好\n接口隔离原则\n不能强制客户端实现它不使用的接口。\nA client should not be forced to implement an interface that it doesn’t use.\n\n这个规则告诉我们，应该把接口拆的尽可能小。这样才能更好的满足客户的确切需求。\n与单一责任原则类似，接口隔离原则也是通过将软件拆分为多个独立的部分来最大程度的减少副作用和重复代码。\n我们来看一个例子：\npublic interface WorkerInterface &#123;    void work();    void sleep();&#125;public class HumanWorker implements WorkerInterface &#123;    public void work() &#123;        System.out.println(&quot;work&quot;);    &#125;    public void sleep() &#123;        System.out.println(&quot;sleep&quot;);    &#125;&#125;public class RobotWorker implements WorkerInterface &#123;    public void work() &#123;        System.out.println(&quot;work&quot;);    &#125;    public void sleep() &#123;        // No need    &#125;&#125;\n在上面这段代码中，我们很容易发现问题所在，机器人不需要睡觉，但是由于实现了WorkerInterface接口，它不得不实现sleep方法。这就违背了接口隔离的原则，下面我们一起修复一下这段代码：\npublic interface WorkAbleInterface &#123;    void work();&#125;public interface SleepAbleInterface &#123;    void sleep();&#125;public class HumanWorker implements WorkAbleInterface, SleepAbleInterface &#123;    public void work() &#123;        System.out.println(&quot;work&quot;);    &#125;    public void sleep() &#123;        System.out.println(&quot;sleep&quot;);    &#125;&#125;public class RobotWorker implements WorkerInterface &#123;    public void work() &#123;        System.out.println(&quot;work&quot;);    &#125;&#125;\n依赖倒置原则\n高层模块不应该依赖于低层的模块，它们都应该依赖于抽象。\n抽象不应该依赖于细节，细节应该依赖于抽象。\nHigh-level modules should not depend on low-level modules. Both should depend on abstractions.\nAbstractions should not depend on details. Details should depend on abstractions.\n\n简单来讲就是：抽象不依赖于细节，而细节依赖于抽象。\n通过应用依赖倒置模块，只需要修改依赖模块，其他模块就可以轻松得到修改。同时，低层模块的修改是不会影响到高层模块修改的。\n我们来看这段代码：\npublic class MySQLConnection &#123;    public void connect() &#123;        System.out.println(&quot;MYSQL Connection&quot;);    &#125;&#125;public class PasswordReminder &#123;    private MySQLConnection mySQLConnection;    public PasswordReminder(MySQLConnection mySQLConnection) &#123;        this.mySQLConnection = mySQLConnection;    &#125;&#125;\n有一种常见的误解是，依赖倒置只是依赖注入的另一种表达方式，实际上两者并不相同。\n在上面这段代码中，尽管将MySQLConnection类注入了PasswordReminder类，但它依赖于MySQLConnection。而高层模块PasswordReminder是不应该依赖于低层模块MySQLConnection的。因此这不符合依赖倒置原则。\n如果你想要把MySQLConnection改成MongoConnection，那就要在PasswordReminder中更改硬编码的构造函数注入。\n要想符合依赖倒置原则，PasswordReminder就要依赖于抽象类（接口）而不是细节。那么应该怎么改这段代码呢？我们一起来看一下：\npublic interface ConnectionInterface &#123;    void connect();&#125;public class MySQLConnection implements ConnectionInterface &#123;    public void connect() &#123;        System.out.println(&quot;MYSQL Connection&quot;);    &#125;&#125;public class PasswordReminder &#123;    private ConnectionInterface connection;    public PasswordReminder(ConnectionInterface connection) &#123;        this.connection = connection;    &#125;&#125;\n修改后的代码中，如果我们想要将MySQLConnection改成MongoConnection，就不需要修改PasswordReminder类的构造函数注入，因为这里PasswordReminder类依赖于抽象而非细节。\n感谢阅读！\n原文地址https://medium.com/better-programming/solid-principles-simple-and-easy-explanation-f57d86c47a7f\n译者点评作者对于SOLID原则介绍的还是比较清楚的，但是里氏原则那里我认为说得还不是很明白，举的例子似乎也不是很明确。我理解的里氏替换原则是：子类可以扩展父类的功能，但不能修改父类方法。因此里氏替换原则可以说是开闭原则的一种实现。当然，这篇文章也只是大概介绍了SOLID的每个原则，大家可以通过查资料来进行更详细的了解。我相信理解了这些设计原则之后，你对程序设计就会有更加深入的认识。后面我也会继续推送一些关于设计原则的文章，欢迎关注。\n","tags":["技术杂谈"]},{"title":"【译】教你用50种语言写Hello, World","url":"/2020/01/06/%E3%80%90%E8%AF%91%E3%80%91%E6%95%99%E4%BD%A0%E7%94%A850%E7%A7%8D%E8%AF%AD%E8%A8%80%E5%86%99Hello-World/","content":"当我们学习一门新的语言时，“Hello, World!“通常是我们所写的第一个程序。相信作为一名程序员的你，职业生涯中至少完成了一个“Hello, World!“程序。程序员一般也都会使用多门语言，甚至有多数人都会使用十几种语言。\n甚至有一个名为TTHW的指标来衡量一个程序员接触一门新的编程语言时，成功写出“Hello, World!“并运行所需要的时间。\n然而，如果我问你，你会用多少种编程语言写“Hello, World!“？你的答案会是多少？\n为了刷新你的记忆，我会带你经历一段计算机编程领域的时空之旅。为此，我将向你展示50种不同的编程语言的\n“Hello, World!“程序的写法。你也会了解计算机语言随着时间的推进发生了哪些变化。\n1. 汇编语言 - 1949汇编语言诞生于1949年。本文我向你展示的是适用于Intel 8080的8位处理器的，发布于1974年4月的汇编语言经典代码。\nbdos    equ    0005H    ; BDOS entry pointstart:  mvi    c,9      ; BDOS function: output string        lxi    d,msg$   ; address of msg        call   bdos        ret             ; return to CCP  msg$:   db    &#x27;Hello, world!$&#x27;end     start\n2. Fortran - 1957Fortran是Formula Translation的衍生物，这是一种通用的交互式编程语言，特别适合于数值和科学计算。Fortran创建于1957年，下面是它的第一个“Hello, World!”程序：\nPROGRAM HelloWRITE (*,*) &#x27;Hello, World!&#x27;STOPEND\nFortran 90 或 95的写法有所不同：\nPROGRAM HelloWRITE (*,*) &#x27;Hello, World!&#x27;END PROGRAM Hello\n3. Lisp - 1958Lisp是最古老的编程语言系列，它既是交互式的又是函数式的。1958年，Lisp作为一种实用的演示程序模型被创建出来，但是直到1970和1980年代，Lisp才成为人工智能世界中非常流行的语言。\n下面是Lisp的“Hello, World!”程序：\n(write-line &quot;Hello, World!&quot;)\n4. Cobol - 1959Cobol语言于1959年正式创立，并且在2019年刚刚庆祝过创立60周年。Cobol表示面向通用业务的语言（COmmon Business Oriented Language），目标是成为编程业务应用程序的通用语言。在2019年，Cobol仍被广泛应用于银行和保险的相关系统中。\n下面是Cobol的“Hello, World!”程序：\nIDENTIFICATION DIVISION.PROGRAM-ID. HELLO-WORLD.PROCEDURE DIVISION.    DISPLAY &quot;Hello, World!&quot;    STOP RUN.\n5. BASIC - 1964BASIC是Beginner’s All-purpose Symbolic Instruction Code（初学者通用符号说明代码）的缩写。它是一门高级编程语言，其主要目标在于易用。它的“Hello, World!”程序如下：\nPRINT &quot;Hello, World!&quot;END\n6. Logo - 1968Logo旨在成为易用的Lisp，通常被称为“Lisp without brackets”，Logs并不是面向对象编程语言，但它可以帮助你入门计算机编程。\nprint [Hello World !]\n7. B - 1969B语言创建于1969年，它现在已经过时了，但它的计算机编程语言的发展史上发挥着重要的作用。因为正是B语言启发了现在广泛应用的C语言。\nmain()&#123;  putstr(&quot;Hello world!*n&quot;);  return(0);&#125;\n是不是语法层面就非常像C语言了。\n8. Pascal - 1970Pascal是一种交互式编程语言，它创建于1970年。它被设计出来主要出于教学目的，因为这个语言的特点是清晰，且严格的语法有助于良好的程序结构。\nbegin  writeln(&#x27;Hello, World!&#x27;)end.\nTurbo Pascal是Pascal语言的集成开发环境，在1983年被创建，并在1980年代和1990年代取得了巨大的成功。\nTurbo Pascal的“Hello, World!“程序如下：\nprogram HelloWorld(output);begin  writeln(&#x27;Hello, World!&#x27;);  readln;end.\n9. Forth - 1970Forth是一门基于栈的交互式编程语言，由Charles H. Moore在1960年代发明。但它的第一个大版本是在1970年发布的。它于1994年被ANSI标准化，并于1997年被ISO采纳。2014年的Forth2012为语言的发展又焕发了生机。\n下面是1970年版本Forth语言的“Hello, World!” 程序：\n: HELLO  ( -- )  .&quot; Hello, World!&quot; CR ;  HELLO\n10. C - 19721972年的贝尔实验室中，Dennis Ritchie和Ken Thompson两位大佬为了开发UNIX发明了C语言。Ken Thompson曾开发过B语言，Dennis Ritchie决定通过对B语言添加类型来创造C语言。所以说B语言为C语言提供了灵感。\n#include &lt;stdio.h&gt;int main(void) &#123;  printf(&quot;Hello, World!\\n&quot;);  return 0;&#125;\n11. Smalltalk - 1972Smalltalk受到Lisp的启发，它是一门面向对象的，动态类型的编程语言，它被发明于1972年。Smalltalk是最早具有集成开发环境的编程语言之一。\nTranscript show: &#x27;Hello, world!&#x27;; cr.\n12. Prolog - 1972Prolog是与人工智能和计算语言学相关的逻辑编程语言，被创建于1972年。\n:- write(&#x27;Hello, World!&#x27;),nl.\n13. ML - 1973ML是Meta Language的简称，是一种以Lisp为基础的函数型编程语言。ML通常以Lisp为特征，具有类型。\nprint &quot;Hello, World!\\n&quot;;\n14. Scheme - 1975Scheme创建于1975年，是一种多范式编程语言，支持函数式和交互式编程。它是Lisp三种重要的变种之一，由Common Lisp和Clojure共同开发。\n(display &quot;Hello, World!&quot;) (newline)\n15. SQL - 1978SQL即结构化查询语言，是用于操作关系数据库的标准计算机编程语言。虽然在设计时不能创建简单的“Hello, World!“程序，但我想写出来应该是一个有趣的程序，如果你想学习SQL，这里有一些推荐课程。\nCREATE TABLE message (text char(15));INSERT INTO message (text) VALUES (&#x27;Hello, World!&#x27;);SELECT text FROM message;DROP TABLE message;\n16. C++ - 1980C++是由Bjarne Stroustrup在1980年创建，他为C语言增加了类，在1983年得名C++。现在C++已经通过了ISO标准化，并广泛应用于工业和其他领域。如果你想要学习C++，这里有一些推荐课程。\n#include &lt;iostream&gt;using namespace std;int main() &#123;  cout &lt;&lt; &quot;Hello, World!&quot; &lt;&lt; endl;  return 0;&#125;\n17. Ada - 1983Ada是一种面向对象编程语言，其开发始于1980年初，并在1983年完成发布。之所以叫做Ada是为了纪念Ada Lovelace，这可能是历史上第一位女性计算机科学家。\nAda通常用于需要很高可靠性和安全性的实时系统和嵌入式系统中。\nwith Ada.Text_IO;procedure Hello isbegin   Ada.Text_IO.Put_Line (&quot;Hello, World!&quot;);end Hello;\n18. Common Lisp - 1984Common Lisp，通常缩写为CL，是ANSI标准化的Lisp语言规范。\n(princ &quot;Hello, World!&quot;)\n19. MATLAB - 1984MATLAB是一种用于数值计算的脚本语言，被用于“Matrix Laboratory”。MATLAB是由同名的开发环境模拟的。\ndisp(&#x27;Hello, World!&#x27;)\n20. Eiffel - 1985Eiffel是一种围绕设计方法设计的面向对象编程语言，它具有当下非常流行的概念，例如“按约定编程”或复用。\nclass    HELLO_WORLDcreate    makefeature    make        do            print (&quot;Hello, world!%N&quot;)        endend\n21. Objective-C - 1986Objective-C是一种面向对象编程语言，它像C++一样，是C语言的扩展，而与C++的区别在于它的动态消息分发或动态加载。\n现在，它主要用于Apple操作系统：macOS以及iOS的衍生品。\n#import &lt;Foundation/Foundation.h&gt; int main() &#123;    @autoreleasepool &#123;        NSLog(@&quot;Hello, World!&quot;);    &#125;&#125;\n22. Erlang - 1986Erlang是一种支持多种范式的编程语言：并发、实时、分布式。它基于Actor Model，具有容错能力以及代码热更新能力，所以Erlang开发的应用可用性通常很高。\nio:format(&quot;Hello world!~n&quot;).\n23. Perl - 1987Perl是由Larry Wall在1987年创建的编程语言，它可以轻松处理基于文本的消息。Perl是一种解释型语言，它受C语言的控制和打印结构以及shell脚本的启发。\nprint &quot;Hello, World!\\n&quot;;\n24. Caml - 1987Caml代表Categorical Abstract Machine Language（分类抽象机器语言），是一种通用的编程语言，旨在提高程序的安全性和可靠性。Caml是一种致力于函数式、交互式和面向对象风格，这也是一种非常独特的语言。\nprint_string &quot;Hello, World!\\n&quot;;;\n25. Tcl - 1988Tool Command Language（工具命令行语言），是John Ousterhout在1988年开发的一种脚本语言。它是一种动态类型语言，具有跨平台、可扩展、易学习等特点。并且可以轻松和C语言进行交互。\n在1990年， John Ousterhout又开发了Tcl的扩展——Tk，这是一个可移植的用于创建图形界面的库。我们今天所讨论的Tcl，多数情况是指Tcl和Tk的组合。\nputs &quot;Hello, World!&quot;\n26. Haskell - 1990Haskell是一种基于lambda计算和组合逻辑的函数式编程语言。\nmain = putStrLn &quot;Hello, World!&quot;\n27. Python - 1991相信大家对Python都比较熟悉，即使没有过，基本上也都听说过。它是一门解释型语言，可以跨平台。Python支持结构体，函数和面向对象的交互式编程。随着AI的发展，近几年Python的热度也是持续上涨。\n如果你想学Python，可以参考这份课程推荐。\n下面是Python3.0以后版本“Hello, World“程序的写法。\nprint(&quot;Hello, World!&quot;)\n28. Visual Basic - 1991Visual Basic，通常简称为VB，是第三代事件编程语言。微软也为其创建了集成开发环境。\nPublic Sub Main()    Debug.Print &quot;Hello, World!&quot;End Sub\n29. Lua - 1993Lua创建于1993年，是交互式编程语言。它专注于嵌入其他应用程序以对其进行扩展。\n译者注：我们之前介绍过Lua在Redis中的应用，不知道你是否还记得。\nprint(&quot;Hello, World!&quot;)\n30. Ruby - 1995由于对Smalltalk和Lisp的开发感到沮丧，Yukihiro Matsumoto从1993年起开始在Emacs上设计Ruby语言，并于1995年发布了第一版。Ruby是解释型、面向对象、多范式的编程语言。如果你感兴趣，可以看一下这些课程。\nputs &#x27;Hello, World!&#x27;\n31. Java - 1995Java是James Gosling（我们常说的高司令）在1995年时创建的一门面向对象编程语言，到目前为止，Java仍然是业界最流行、使用最广泛的语言。使用Java可以开发从客户端到服务端到各种应用，Google选择Java作为Android开发语言这件事使得Java得到了更进一步的发展。想要学习Java的话可以关注这几门课程。\nclass HelloWorld &#123;  public static void main(String[] args) &#123;    System.out.println(&quot;Hello, World!&quot;);  &#125;&#125;\n32. JavaScript - 1995JavaScript是一门主要用于开发Web页面的脚本语言，但现在也可以用作服务端开发，如Nodejs。JS是一门面向过程的语言，推荐课程在这里。\ndocument.write(&#x27;Hello, World!&#x27;);\n33. PHP - 19951995年对于编程语言而言绝对是非常重要的一年，在Java和JavaScript之后，PHP也于同年诞生。PHP是一门解释型的面向对象编程语言。\n&lt;? echo &quot;Hello, World!&quot; ?&gt;\n34. Rebol - 1997Rebol是一种高级脚本语言，自称“消息传递语言”。\nprint &quot;Hello, World!&quot;\n35. ActionScript — 1998ActionScript可以用于开发客户端应用程序，如Adobe Flash和Adobe Flex；也可以用于服务端开发（Flash media server, JRun, Macromedia Generator）。现在，ActionScript在Unity图形引擎中被当作脚本语言使用。\npackage &#123;  public class HelloWorld &#123;    public function HelloWorld() &#123;      trace(&quot;Hello World !&quot;);    &#125;  &#125;&#125;\n36. D - 1999D语言是一门面向对象编程语言，它的设计借鉴于许多语言，包括C++、Java和Eiffel。D语言是一门优秀的语言，但是一直都没有像它的创造者期望的那样获得成功。\nimport std.stdio;void main () &#123;  writefln(&quot;Hello, World!&quot;);&#125;\n37. C# - 20002000年，微软就Java语言与Sun公司发生争议，随后便创造了C#，C#是一种被设计为在Microsoft.NET平台上开发的面向对象的编程语言，它是由C++和Java派生而来，使用了许多它们的通用特性和概念。C#也可以用来在ASP.NET平台上开发Web应用。C#课程自取。\nusing System;internal static class HelloWorld &#123;  private static void Main() &#123;    Console.WriteLine(&quot;Hello, World!&quot;);  &#125;&#125;\n38. Groovy - 2003Groovy是运行在Java虚拟机上的一门面向对象编程语言，它是Java的超集，其设计受到了Python、Ruby和Smalltalk的启发。这里有一些学习书籍可以参考。\nprintln &quot;Hello, World!&quot;\n39. Scala - 2003Scala是一种多范式编程语言，旨在以简洁、优雅的形式表达常见的编程模型。Scala通过静态类型集成了面向对象和函数式编程的范例。Scala课程。\nobject HelloWorld extends App &#123;  println(&quot;Hello, World!&quot;)&#125;\n40. F# - 2005F#是一门在.NET平台上开发的函数式、面向对象编程语言。F#源自与它高度兼容的OCaml，它们都属于ML语言这一系列的。\nprintfn &quot;Hello, World!&quot;\n41. Windows PowerShell - 2006Windows PowerShell是Microsoft开发的软件套件，包括命令行界面，称为PowerShell的脚本语言和开发套件。 从Windows 7开始，PowerShell已作为标准配置提供。\necho &quot;Hello, World!&quot;\n42. Clojure - 2007Clojure是编译型、跨平台的函数式编程语言，它可以安全、简单的开发分布式系统。Clojure是Lisp的三种主要方言之一。Clojure可以编译成Java字节码，JavaScript代码或者是.NET字节码，因此它可以运行在JVM、CLR和浏览器上。\n(println &quot;Hello, World!&quot;)\n43. Go - 2009Go是一种编译型的支持并发的编程语言，它的设计受到了C和Pascal的启发，该语言由Google从Robert Griesemer，Rob Pike和Ken Thompson的最初概念开发而来。没错，这个Ken Thompson就是在1969年设计B语言的人！\npackage mainimport &quot;fmt&quot;func main() &#123;  fmt.Println(&quot;Hello, World!&quot;)&#125;\n44. Rust - 2010Rust是由Mozilla设计并开发的一门多范式的编译型编程语言。Rust被设计为“安全、并发、实用的语言”，在某些方面支持函数式编程风格，面向对象编程。Rust被认为有望替代C++。\nfn main() &#123;    println(&quot;Hello, World!&quot;);&#125;\n45. Dart - 2011Dart是由Google开发的用于web应用的编程语言，它最初的目标是替代JavaScript。目前，Dart的目标还没有实现，开发人员的首要任务是可以将Dart转换成与所有现代浏览器兼容的JavaScript代码。Dart也可以用于服务端的开发。\nDart最近的势头很猛，主要是因为它是手机端流行框架Flutter的开发语言。\nmain() &#123;  print(&#x27;Hello, World!&#x27;);&#125;\n46. Kotlin — 2011Kotlin是一门面向对象编程语言，支持静态类型，可以被编译成字节码运行在Java虚拟机上。也可以编译成JavaScript语言，或者在其他多个平台运行的语言。（感谢LLVM）2017年，Google使Kotlin成为继Java之后Android正式支持的第二种语言。如果你想学Kotlin，可以参考这些课程。\nfun main(args: Array&lt;String&gt;) &#123;    println(&quot;Hello, World!&quot;)&#125;\n47. Ceylon - 2011Ceylon是由“红帽”创建的开源的强类型和静态类型的高级编程语言，它的语法类似于Java。可以将其编译成Java字节码和JavaScript。\nvoid hello() &#123;    print(&quot;Hello, World!&quot;);&#125;\n48. TypeScript - 2012TypeScript是由微软开发的免费、开源的编程语言。用于改善和保护JavaScript代码的生产。TypeScript是JavaScript的超集，可以转换为JavaScript，以便由任何Web浏览器或JavaScript引擎解释。\nconsole.log(&quot;Hello, World!&quot;);\n49. Julia - 2012Julia是用于科学计算的高级、强大、动态的编程语言，使用的是其他类似开发环境（例如MATLAB、R或Python）的用户熟悉的语法。\nprintln(&quot;Hello, World!&quot;)\n50. Swift - 2014Swift是一门编译型、多范式的编程语言，它简单、高效、安全。它由苹果开发并开源。使其成为与Objective-C一样开发iOS移动应用的解决方案。\nprint(&quot;Hello, World!&quot;)\n结论我们的旅行到此就要告一段落了，我想你介绍了50种语言的“Hello, World!“程序的写法。本文所提供的编程语言列表远非详尽。希望你能和我分享你喜欢的语言的“Hello, World!“程序。\n译者注50种语言的发展来看，大多数语言的设计思想都是互相借鉴的。语言的发展也是逐渐演进的，但不是越强大的语言就越流行。一门语言想要变得流行也是需要一定的机遇的（例如Python），或者是需要有个优质的大腿（例如Java和Dart）。\n我们把时间粒度再放粗一点，编程语言的发展趋势是逐渐趋近于自然语言的，这也说明编程首先是给人看的，然后才是指导计算机应该做什么操作。所以，请尽力写出整洁的代码。避免同事骂人。\n为了响应作者，我先来一个，最近刚刚创造出来的文言文编程的“Hello, World!“程序。\n吾有一言。曰「「問天地好在。」」。書之。\n原文地址https://medium.com/javarevisited/70-years-of-hello-world-with-50-programming-languages-2400de893a97\n","tags":["技术杂谈"]},{"title":"【译】送给你的代码审查问题手册","url":"/2019/10/15/%E3%80%90%E8%AF%91%E3%80%91%E9%80%81%E7%BB%99%E4%BD%A0%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5%E9%97%AE%E9%A2%98%E6%89%8B%E5%86%8C/","content":"快来领取这份代码审查问题手册！\n\n代码审查列表，是代码审查的明确规则和指导手册，它可以使代码审查为你的团队带来更多好处，并且能够显著提升代码审查的速度。\n研究表明，使用代码审查列表的审阅者的表现要优于不使用的审阅者。所以不管你是新手开发者还是经验丰富的开发者，开始考虑使用代码审查列表吧。\n代码作者应该关注的列表作为代码的作者，你应该保证：\n\n代码编译成功并且通过静态检查（没有警告）\n代码通过所有的测试（单元测试、集成测试和系统测试）\n你已经仔细检查了拼写错误，并做了处理（注释、todo等）\n概述代码修改的原因以及修改了哪些地方\n\n除此之外，作为代码作者，也应该在提交审查之前，按照审查者的列表对自己的代码进行审查。\n代码审查者应该关注的列表作为代码审查者，你的任务是寻找最重要的问题。评论会要对代码的结构性或逻辑性问题更有价值，即使有时候会显得挑剔。\n你应该知道什么是好的代码反馈)。另外需要注意，最好的代码审查反馈不是点评，而是建议。所以不要说“变量名称应该是removeObject“，最好说”调用变量removeObject怎么样？“。\n下面这份列表足够帮助你提出好的代码审查反馈了。\n实现\n此代码更改会执行它应该做的事情吗？\n这种解决方法是最简单的吗？\n这个更改有引入一些不需要的编译时或运行时的依赖吗？\n是否使用了不应该使用的框架、API、库、服务？\n是否存在可以提升解决方法的未使用的框架、API、库、服务？\n代码是否处于正确的抽象级别？\n代码是否的模块化做的是否足够好？\n你是否有其他的解决方案，该方案在代码可维护性、可读性、性能、安全方面表现更好？\n是否已经存在类似功能的函数？如果有，为什么不复用？\n是否有最佳实践、设计模式或特定语言模式可以优化代码？\n代码是否遵循面向对象的分析和设计原则，例如单一责任原则，开闭原则，里氏替换原则，接口隔离，依赖注入？\n\n逻辑错误或Bug\n你能想到代码不按预期运行的任何用例吗？\n你能想到任何可能破坏代码的输入或外部事件吗？\n\n错误处理和日志\n错误都被正确处理了吗？\n是否有需要增加或删除的日志/debug信息？\n错误消息对用户是否友好？\n是否有足够的日志，它们的编写方式是否是易于调试的？\n\n可用性和可访问性\n从可用性角度出发，所提出的解决方案是否设计合理？\nAPI文档是否足够好？\n提出的解决方案是否具备可访问性？\nAPI/UI是否直观易用？\n\n测试与可测试性\n代码是否达到可测试标准？\n是否有足够的自动化测试（单元测试/集成测试/系统测试）？\n现有测试是否合理覆盖代码变更？\n是否有额外的测试用例、输入或边界用例以供测试？\n\n依赖\n如果这个修改需要更新代码以外的文件，例如更新文档，配置，readme文件。是否完成了这些更新？\n这个修改是否会对系统其他地方造成影响？是否能后向后兼容？\n\n安全和隐私数据\n这段代码是否打开软件的安全漏洞？\n权限和身份验证是否被正确处理？\n是否安全处理了敏感数据，例如用户数据、信用卡信息等？是否正确使用加密方法？\n代码更改是否显露了一些私密信息（如迷药，用户名等）？\n如果代码处理用户输入，是否解决了跨站点脚本，SQL注入等安全漏洞，是否进行了输入清洗和验证？\n从外部API或库中获得的数据是否进行了相应的检查？\n\n性能\n这段代码修改是否会对系统性能产生负面影响？\n是否可以进一步提升代码性能？\n\n可读性\n代码是否容易理解？\n哪一部分使你困惑，为什么？\n可以通过减小方法来提高代码可读性吗？\n可以通过使用不同的函数/方法或变量名称来提升代码可读性吗？\n代码是否存放在正确的文件/目录/包？\n你是否认为方法应该重构以拥有更直观的控制流程？\n数据流是否可理解？\n是否有多余的注释？\n某些注释是否可以更好的传达信息？\n是否更多的注释会使你的代码更容易理解？\n是否可以移除一些注释，通过提升代码可读性来理解代码？\n是否存在注释掉的代码？\n\n专家意见\n你是否认为特定专家（如安全专家或可用性专家）应该先检查代码，然后再提交代码？\n这个代码修改会影响其他团队吗？他们也应该发表意见吗？\n\n好了，以上就是最为紧迫的一些问题列表。\n代码风格和约定您的团队或公司必须拥有清晰的编码风格指南，这一点很重要。因为这是在代码库中实施唯一性的唯一方法。并且一致性会使代码审查更快，使人们可以轻松地更改项目，并保持您代码的可读性和可维护性。\nGoogle是做到这一点的很好的例子，无疑，这使Google可以进行快速的代码审查。\n首先，我建议使用现成的编码样式来支持Google提供的多种语言。设定基本规则很重要，但要确保一劳永逸。不要持续争论。\n尽可能自动化确定了代码风格以后，请花一些时间正确安装和配置工具，以便一键格式化代码。\n另外还有很多事情可以做。例如使用静态检查来代替部分人工审核。这是值得为之努力的。\n完整问题列表\n原文链接 https://www.michaelagreiler.com/code-review-checklist/ \n","tags":["Code Review"]},{"title":"代码检查又一利器：ArchUnit","url":"/2019/12/16/%E4%BB%A3%E7%A0%81%E6%A3%80%E6%9F%A5%E5%8F%88%E4%B8%80%E5%88%A9%E5%99%A8%EF%BC%9AArchUnit/","content":"Code Review总是让人又爱又恨，它可以帮助我们在提测之前发现很多代码中比较“丢人”的问题，但是，Code Review通常会比写代码更加耗费精力，因为你需要理解别人的代码，而为了这一目的，往往需要很多次的沟通。\n人们常说“见字如面”。我认为代码也是一样，看到一个人的代码，就会对这个人有一个大概的印象。例如，当你看到一段代码写的非常随意，随意的格式、随意的命名、随意的封装，然后又没有单元测试，那我们一般会认为这段代码的作者是一个不够严谨、做事随意、有些懒惰，又对自己的代码责任心不强的人。如果你不是这样的人，那就需要花费更多的力气向同事证明自己。而如果在代码中做好每一个细节，严格遵循编码规范，单元测试覆盖率比较高，那么同事对你的第一印象一定是这个人还是比较可靠的，跟他合作应该比较愉快。\n说了这么多，其实就是想强调Code Review的重要性。那么既然它这么重要，但又给我们带来了更大的工作量。作为程序员，我们一定会想，能不能自动化？答案当然是可以。事实上现在也有很多公司实现了自动化，例如自动进行静态代码分析来确保代码质量，利用类似Cobertura这样的工具来检查单元测试覆盖程度等等。但是这并不能完全保证代码的整洁性和可靠性。\n有了这些工具之后Code Review轻松了许多，但是这些工具的安装、使用也是需要花费很高的成本的。所以我想给大家介绍的是一个使用简单、方便的工具来帮我完成这些任务。在介绍之前，我们先来想一想我们平时在Review别人代码时可能会注意哪些问题。这里我简单列出来了一些：\n\n抛出的异常不能太过广泛\n不能写System.out，而是要用日志输出\n不能使用java.util.logging\n如果使用贫血模型开发，每个类需要放到对应的包中\n接口不能放在实现类的包中\nService层代码不能访问Controller层代码\n合理使用第三方库\n\n这些事情以前我们都是靠人工来检查，直到我发现了ArchUnit这个库。感觉像是抓住了自动化道路上的救命稻草。\n什么是ArchUnit？ArchUnit的官方网站是 https://www.archunit.org \n官网中原话介绍是\n\n ArchUnit is a free, simple and extensible library for checking the architecture of your Java code using any plain Java unit test framework.\n\n意思是ArchUnit是一款免费、简单可扩展的库，它可以使用任何Java单元测试框架来检查Java代码的架构。\n也就是说，它的主要功能是用来检查代码结构的。那么怎么使用呢？\n如何使用？ArchUnit的简单绝对不是空谈，如果你是maven项目，只需要在pom.xml文件中添加如下依赖：\n&lt;dependency&gt;    &lt;groupId&gt;com.tngtech.archunit&lt;/groupId&gt;    &lt;artifactId&gt;archunit&lt;/artifactId&gt;    &lt;version&gt;0.12.0&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;\n如果你是Gradle项目，使用起来同样非常简单\ndependencies &#123;    testCompile &#x27;com.tngtech.archunit:archunit:0.8.0&#x27;&#125;\n当你添加了依赖以后，就可以为我们前面提到的规则写测试用例了。\n当然，也有一些内建的通用规则，它们定义在\ncom.tngtech.archunit.library.GeneralCodingRules\n这个类中。关于内建规则的细节，可以查看官方文档。\n自定义规则除了内建规则以外，ArchUnit也支持你定义自己需要的规则，至于如何定义规则，文档中都有详细的介绍。当然，也可以参考这个例子来写一些规则。 https://github.com/TNG/ArchUnit-Examples\n如何执行规则定义好以后如何执行呢？我们说ArchUnit使用起来非常简单，如果需要测试，对maven项目来说只需要执行命令\nmvn test\n而对于Gradle项目来说，只要执行命令\ngradle test\n总结ArchUnit看起来是一个很酷的三方库，我并没有在使用层面做过多介绍，因为我也在摸索中，感兴趣的朋友可以和我一起交流。\n","tags":["整洁"]},{"title":"代码洁癖系列（一）：什么是整洁代码","url":"/2018/08/26/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B4%E6%B4%81%E4%BB%A3%E7%A0%81/","content":"作为一个代码洁癖患者，我最大的愿望就是世界和平……对不起，拿错剧本了，最大的愿望就是将对代码的洁癖传播给每一个人，净化所有的代码。这是一个宏大的愿望，但我会一直努力净化我所看到的每一行代码，并且希望能影响更多的人，让大家都写出整洁的代码。\n在阅读本文之前，想先提两个问题：\n\n你是程序员吗\n你想成为更好的程序员吗\n\n如果上面两个回答都是肯定的，那请你继续读下去，否则就可以直接关闭网页了。\n什么是整洁代码相信有过一定工作经验的程序员都读过别人的代码，也一定看到过槽糕的代码，看到那些糟糕的代码时你是不是在想：这写的什么垃圾东西，读懂这段代码所花费的时间我都能把这个功能再实现一遍了。还有另一种情况：引用别人的代码时，为了能按时完成，不得不去适应别人的代码风格，跟着制造混乱。当混乱越来越多时，整个团队的生产力也越来越低，直到最后不得不把所有代码重新整理一遍，而这个工作里可想而知。\n上面说的都是糟糕的代码所带来的影响。那么什么是整洁的代码呢？在我看来可以从两个方面定义整洁代码：\n\n外在美\n外观看起来优雅美观的代码会使人心情愉悦。其中主要体现在：代码所在的位置是否恰当，代码里是否有适当的注释，适当的缩进和空行以及没有重复代码等等。\n\n内在美\n外在美只是“看起来”使人心情愉悦，如果缺乏内在美，那么当别人真正开始读的时候，刚才愉悦的心情会在一瞬间烟消云散。而内在美则体现在：尽量少的API，代码之间的尽量少的依赖，干净利落的抽象和直截了当的控制语句等。\n\n\n在理解了什么是整洁代码后，希望我们在写每一行代码之前都要思考一下，这行代码是否足够整洁，是否让人看了心情愉悦。\n之后的一段时间，我也将会从以下几个方面和大家分享，如何写出整洁的代码。\n有意义的命名什么是无意义的命名？如果代码中所有的变量名，函数名都是abcd之类的，相信你在看到这样一段代码的第一眼就已经失去了读下去的欲望了。也许有人觉得这太极端了，认为不会有人这样写代码。那么我们在考虑一下，我为一个变量命名为name，那么在没有前后语境的情况下，你能想象出这个变量是用来做什么的吗？是一个人的name还是一个物品的name，是firstName还是lastName？所以，有意义的命名对代码阅读是非常重要的。\n如何定义类和函数如何定义相信大家都会，毕竟这是最基本的操作，那么怎么才能算是整洁的类和函数呢？\n要不要写注释大多数程序员都觉得写注释（文档）很麻烦，觉得自己的工作就是实现功能，自己写的代码自己能看懂就行。事实上真的是这样吗，他们真的能看懂自己三个月前写的代码吗？\n另一种程序员知道要写注释，但是他们写的注释都是诸如：“这是一个方法”、“这是一个循环”……这样的注释写出来真的有意义吗？\n如何排版这个属于外在美，第一印象很重要，第一眼看上去很好，才会继续看下去不是吗。\n错误处理代码运行时的错误如何处理吗？全部抛出去，交个用户处理？全部catch住，隐藏起来不处理？这两种程序员相信都会被老板当成错误处理掉吧。\n合格的单元测试什么样的单元测试才算合格，单元测试对代码的覆盖率要达到多少？\n迭代胖子从来都不是一口吃成的，怎么才能让你的代码越来越饱满，系统越来越稳定呢？唯有不断迭代。\n结语上面这些问题，我将在后面的文章中挨个解读。希望你读完之后，也能化身为整洁代码的守护者。\n","tags":["整洁"]},{"title":"代码洁癖系列（三）：整洁的类和函数","url":"/2018/08/29/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E6%95%B4%E6%B4%81%E7%9A%84%E7%B1%BB%E5%92%8C%E5%87%BD%E6%95%B0/","content":"前面我们讨论了什么样的命名更能够让你赏心悦目，今天来讨论一下面向对象编程过程中最重要的环节，编写类和函数。我们仍然用Java来演示，什么样的类和函数才算是整洁的。\n首先讨论函数，函数定义好了，类也就容易了。\n短小相信大家在读代码的时候都会遇到过冗长的函数定义。没有的话可以私信我，我把原来写过的一段300+行的函数发给你，不过不要问我这个函数是做什么的，因为我也忘了，而且不想回顾。当然如果你足够耐心研究出来了，请教教我。\n言归正传，为什么函数要短小呢，如何才能是自己的函数更加短小？第一个问题我也无法证明，只能告诉你短小的函数看起来更加清晰，更加容易理解。那怎么才能让函数变得更加短小呢？很简单，抽离方法。将一些代码抽离成另一个函数。什么样的长度才是合适的呢？我认为不必过于追求短。这里的长度我们可以以代码块的层来定义，对于下面这种代码相信任何人看了都会崩溃吧。\npublic void doSomething() &#123;    for() &#123;        ...        while() &#123;            ...            if() &#123;                ...            &#125;else &#123;                ...            &#125;        &#125;    &#125;&#125;\n所以每个函数中有一层或两层为最佳，每层代码块最好不超过3行。这是我认为最佳的函数长度，当然，这个也可以根据个人习惯稍作调整。\n只做一件事如果说长度还可以根据个人习惯，那么“只做一件事”的要求应该是大家都应该遵守的公约了。如果一个函数中做了太多的事，那么代码阅读起来的难度将会成倍增加，而且文档书写难度同样增大。还有就是给其他代码调用造成不便。比如我定义了函数A做了1和2两件事，函数B想做2和3，怎么办？这时B只能再写一遍A中做2的代码。而这样就会有大量重复代码出现，不但增加工作量，对日后的维护工作也造成很大的负担。而把1和2分别定义为函数C和函数D的话，只需要在AB中分别调用就可以了。\n命名这里不多解释，函数的命名需要具有描述意义，函数越短也就越容易描述。\n函数参数参数数量越少越好（这个我目前也没有做到），究其原因，首先是读代码时每次都要搞清楚每个参数的意义，所以自然越少越好。另一方面就是为测试的同事提供方便，如果有多个函数，测试的同学就需要考虑更多的测试用例对其进行覆盖。如果一个函数有3个以上的参数，那测试的同学可能想要打人了。\n使用异常代替返回错误码这样就可以将Try/catch代码块抽离出来，因为Try/catch代码块影响了正常程序的流程，看起来很丑陋。\n函数的主要规则就是这些，那么如何才能写出这样的函数呢？其实没有什么特别的技巧，就是记住这些规则，在每次写完代码之后再斟酌一番，对代码进行反复的打磨，修改不合适的命名，抽离冗长的函数。久而久之，你的代码一定会被人称赞的。\n说完函数再来说一下如何写好一个类。\n还是短小没错，类也应该短小，不过这里短小的定义和函数短小的定义稍有不同，我们通常以“权责”来衡量。先看下面这个类。\npublic class SuperDashboard extends JFrame implements MetaDataUser &#123;    public Component getLastFocusedComponent()    public void setLastFocused(Component lastFocused)    public int getMajorVersionNumber()    public int getMinorVersionNumber()    public int getBuildNumber()&#125;\n这个类只提供了5个方法，应该不算长，但是我要说，它仍然不满足我们“短小”的条件，原因就是违反了单一权责原则。单一权责指的是一个类只描述一类事。上面这个类有对最后焦点组件的读写方法，还有获取版本号和序列号的方法。只要我们描述一个类时，用到了类似于“还有”这样的字眼时，那么这个类就违反了单一权责原则，就需要对其中的方法进行抽离。\n为了修改而组织大多数系统都会进行持续的迭代，而这也意味着我们需要不断对代码进行修改。而修改代码往往伴随着风险。所以，我们需要做的就是，修改一个方法时，不对其他方法造成影响。当我们开始修改时，就要评估好影响，然后将方法进行抽象，拆分。力求做到每次修改都不影响其他类（即降低耦合）。\n对于写好一个类，总结来说就是“高内聚，低耦合”。\n","tags":["整洁"]},{"title":"代码洁癖系列（七）：单元测试的地位","url":"/2018/09/04/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E7%9A%84%E5%9C%B0%E4%BD%8D/","content":"在许多程序员眼中，单元测试似乎是可有可无的，觉得这应该是测试人员的工作。实际上，测试代码和生成代码同样重要。我们不但需要测试代码，而且需要的是整洁的测试代码。\n测试为什么要整洁我们对待测试代码需要像对待生产代码一样，写之前需要进行严谨的思考、详细的设计。这里分享一下我自己的学习编程的一些经历。\n没有单元测试刚毕业的时候，我的代码可以说是年少轻狂，总是对自己充满自信。根本就不写单元测试，写完之后自测也是随意的点两下就算自测通过了。代码提交测试后，恐怖的事情就出现了，铺天盖地的bug向我袭来。每天工作有一半以上的时间是在和测试同事沟通，其余的时间是在改bug。本来1天的工作可能需要3天才能完成。当我意识到这样做完全是费力不讨好的时候，我决心每次写完代码之后，要写一段单元测试，保证单元测试通过后再提交。\n随意的单元测试在开始写单元测试之后，我的工作效率提高了很多，下班都比原来早了。感觉写单元测试是一个无比正确的决定。随着项目的进行，中间处理过几次紧急的bug fix，当时就没有顾上去写单元测试。然而，当我又一次完成一个新的feature的时候，像往常一样开始跑单元测试，结果是：Failed！就是因为之前的改动导致的。由于手里还有其他比较紧急的工作，单元测试又被放下了。就这样，我又回到了没有单元测试的工作状态。\n现在的我已经不像当初那样盲目的自信了，没有单元测试的代码让我感到恐慌。\n决心重构单元测试曾经有一段可用的单元测试放在我面前，但我没有珍惜，直到失去才追悔莫及。这次我决心重建单元测试，不但要重建，还要写一段好的单元测试。吸取上次的教训，要使我的单元测试可扩展，可维护。把一些公共的方法抽取出来，将不同概念的测试进行拆分。做到“每个概念一个测试”，测试中需要使用断言判断是否成功，而不是人为查看日志。每个测试都要包含构造-操作-检验三个环节，这三个环节要定义清楚。\n这样一来，我就有了一套整洁的单元测试，后来修改代码后，单元测试可以方便的进行扩展和复用，工作效率再次提升。\n整洁测试的规则整洁测试需要遵循F.I.R.S.T规则。什么是F.I.R.S.T规则呢？\n快速（Fast）测试应该足够快，如果测试一次需要等待很长时间，没有人愿意频繁的运行测试，也就没办法快速发现问题。久而久之，我们又会失去测试……\n独立（Independent）测试之间应该相互独立，一个测试的失败不应该影响其他的测试，否则就会导致每次测试出现一大堆问题，我们每次只能解决最上级的测试暴露出来的问题，下级测试需要再次测试才行。这就会大大降低工作效率。\n可重复（Repeatable）测试应该在各种环境中可以重复执行，不论是你的本地环境，测试环境还是生产环境。测试都应该能够跑通。这样才能保证线上的质量，测试也才有意义。\n自足验证（Self-Validating）测试应该有布尔值输出（最好使用断言），我们不应该通过查看日志来判断测试是否通过，更不应该通过人为比较两个文本是否相同来判断测试是否通过。这样不但失去了测试的准确性，也浪费了我们自己的时间。\n及时（Timely）测试应该及时编写，在设计生产代码的同时就应该将测试一并设计好，不然的话，当你写好生产代码，也许会因为某些代码难以测试而放弃。\n结语总结一下今天讨论的内容，我们需要整洁的单元测试，它的地位与生产代码一样，需要我们认真设计。设计测试的时候需要遵循F.I.R.S.T原则。\n","tags":["整洁"]},{"title":"代码洁癖系列（二）：命名的艺术","url":"/2018/08/27/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%91%BD%E5%90%8D%E7%9A%84%E8%89%BA%E6%9C%AF/","content":"不知道大家还记不记得《家有儿女》里有这么一段经典台词：“我叫夏雪”，“我叫夏雨”，“我叫夏冰雹”。\n\n\n刘星自己给自己起的名字承包了我所有笑点。但仔细想想这名字取的竟然还挺不错，不但有意义，还和夏雨夏雪的名字相呼应。\n回到主题，在我们的代码里，命名是随处可见的，比如给包命名，给类命名，方法名，参数名，变量名等等。那么什么样的命名才算是好的命名呢。这就是我们今天要讨论的。\n名副其实首先还是要强调这一点，我读过的糟糕的代码有一个共同的特点，那就是代码中存在大量随意的，无意义的命名。比如：表示消耗多少小时的变量命名为h，同作用域中还有一个想要表示“小时”相关的变量直接命名为h1，一个List类型的变量就命名为list。读这种代码不会令人开心，同时写这种代码的程序员的人身安全是无法保证的。\npublic List&lt;String&gt; getThem() &#123;    List&lt;String&gt; list = new ArrayList&lt;&gt;();    for (String key : keys) &#123;        if (theMap.get(key) == 1) &#123;            list.add(key);        &#125;    &#125;    return list;&#125;\n读一下上面这段代码，能看出有什么问题吗？或者说看完有人明白这段代码要做什么吗？\n我先来说一下我的问题：\n\ngetThem是get什么？\nlist里存的是什么\nkeys指的是什么，为什么遍历它\n常量”1“的意义是什么\n\n这几个问题可能只有写这段代码的人才能解释（没错，就是我写的）。所以我来给大家解释一下，不过我不想一一回答上面的问题，我把代码重写一遍，看看还会不会有这样的问题。\npublic List&lt;String&gt; getPaidOrderIds() &#123;    List&lt;String&gt; paidOrderIds = new ArrayList&lt;&gt;();    for (String orderId : orderIds) &#123;        if (orderStatus.get(orderId) == OrderConstatn.PAID) &#123;            paidOrderIds.add(orderId);        &#125;    &#125;    return paidOrderIds;&#125;\n怎么样，上面的问题都清楚了吗，如果不清楚可以私下和我联系，我会给你推荐一个非常好的英语学习软件。\n这就是名副其实的重要性。\n避免误导命名过程中要注意的第二点就是要避免名称对别人产生误导，例如上面代码中paidOrderIds这个变量，如果我们命名成paidOrderIdList呢，看起来似乎没什么问题，但是如果这个变量是Set类型或者其他类型呢？命名为xxxList还合适吗，别人读代码的时候会下意识的认为这是个List类型的变量（尽量避免名称中出现类型的名字）。\n另一点容易产生误导的是数字和字母相像的情况，比如，我命名一个变量叫做O1，在编辑器里就很难分辨是O1还是01，如果我写一个这样的表达式 \nO0 = l1;\n估计别人会以为我发明了什么新的语言吧。\n第三点是要避免区别较小的两个名称，比如XYZContinentController和XYZCentinentController这样的命名，对于读代码的人来说简直就是灾难，对于写代码的人来说也存在很大的风险。（为什么不是灾难？因为大部分人都是Ctrl+C/Ctrl+V的，这样就减小一些出错的几率。看来善用Ctrl+C/Ctrl+V还是很有必要的）\n做有意义的区分编译器要求我们在同一个区域内不能重复命名。那么有的程序员就会写出a1,a2,a3……这样的命名，这看起来很烂，谁也不知道它们有什么区别。再举过来一个栗子，一个包中有两个类，Product和ProductInfo，这时让你查一下商品名称，你知道要去哪个类里找吗？事实证明，这样的命名与a1,a2,a3之流别无二致。\n既然要做区分，为什么不做些有意义的区分呢，一个类叫做Product，表示商品信息，另一个叫做ProductStocks表示商品的库存信息。这样是不是更清晰一点呢，当然，我这里只是举个栗子，实际上可能不需要这样定义。\n别随意使用简写不要随意使用简写，除非是大家都知道的简写。比如美国的命名写成US，大家都可以理解，而如果generateCode写成genCode就令人费解了。\n使用可以被搜索的名称像我们在一开始的那段代码，为什么要把常量“1”写成OrderConstatn.PAID，不仅是为了可读，也是为了可搜索，试想，如果你要找这段代码，去搜索数字1，会有多少结果？搜索PAID呢？亲自试过之后相信你会回来点赞的。\n类名和方法名类名和方法名也要遵循上面的规范，除此之外，它们还有各自的规范：\n\n类名不应该是动词，避免使用Data、Info这样的词汇。\n方法名应该是动词，比如，saveXXX、deleteXXX\n\n要专一假如你在不同的类中，分别定义了方法getXXX、fetchXXX和findXXX，我要调用的时候怎么知道某个类中应该使用哪种方法？所以，为什么不都用getXXX的形式呢？这样无论是对写代码的人还是对调用的人来说都是莫大的喜讯。\n适当添加语境当你一些变量：firstName、lastName、street、houseNumber、city、state、zipcode。我们可以很轻易的判断出，他们组合在一起表示一个地址。那么把state单独拿出来呢？你还能知道它是什么意思吗？这时，我们可以把变量命名为addrState、addrFirstName……这样即便单独看某个变量，也会理解它要表达什么。当然更好的方法是定义一个名为Address的类，把这些变量放到类中，事实上我们也都是这样做的。\n当然，有时候也不能添加一些无意义的语境。我们要开发一个“画图”的应用，那在每个类名前加Drawing可不是什么好主意。\n结语命名并不能算是什么技术，而是一种写代码的习惯，但这种习惯有可能会决定你作为一名程序员给人的第一印象。所以，养成一个好的命名习惯，也是对自己形象的维护。同时还能让自己的生命安全更有保障（避免被同事……\n","tags":["整洁"]},{"title":"代码洁癖系列（五）：外在的格式美","url":"/2018/08/31/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%A4%96%E5%9C%A8%E7%9A%84%E6%A0%BC%E5%BC%8F%E7%BE%8E/","content":"我们在阅读一些优秀项目的源码时，一定会感叹他们代码的整洁和一致性。而作为第一印象，代码格式的整齐是让人能够继续阅读下去的动力。今天我们分别从垂直格式和横向格式两个方面来讨论代码的格式。修正格式的方法有：间隔、靠近和调整顺序。\n垂直格式在垂直格式方面，我们要向报纸的排版学习。\n\n首先有一个标题告诉你这栏新闻要讲什么，好让你知道是否要继续阅读下去。对应到代码中就是类的名字，我们要力求只通过名字就可以知道这个类要描述什么事情；然后，代码的第一段往往都交代了整个故事的概要，类似于代码中的接口，我们往往通过接口了解类中有哪些函数，每个函数都是干什么的。了解了这些之后，才会去读详细的内容。\n\n\n\nnewspaper\ncode\n\n\n\n\n标题\n类名\n\n\n第一段\n接口\n\n\n内容\n方法体\n\n\n\n此外，还需要注意的就是报纸的排版、段落和段落之间用空行做间隔。写代码也可以用同样的方式，例如，我们习惯于在包声明、导入声明和每个函数之间用空白行来分隔。我们直接拿代码来解释。\npackage com.thoughtworks.selenium;import org.testng.ITestContext;import org.testng.ITestResult;import org.testng.Reporter;import org.testng.internal.IResultListener;import java.io.File;public class ScreenshotListener implements IResultListener &#123;  File outputDirectory;  Selenium selenium;  public ScreenshotListener(File outputDirectory, Selenium selenium) &#123;    this.outputDirectory = outputDirectory;    this.selenium = selenium;  &#125;  public void onTestFailure(ITestResult result) &#123;    Reporter.setCurrentTestResult(result);    try &#123;      outputDirectory.mkdirs();      File outFile = File.createTempFile(&quot;TEST-&quot; + result.getName(), &quot;.png&quot;, outputDirectory);      outFile.delete();      selenium.captureScreenshot(outFile.getAbsolutePath());      Reporter.log(&quot;&lt;a href=&#x27;&quot; +          outFile.getName() +          &quot;&#x27;&gt;screenshot&lt;/a&gt;&quot;);    &#125; catch (Exception e) &#123;      e.printStackTrace();      Reporter.log(&quot;Couldn&#x27;t create screenshot&quot;);      Reporter.log(e.getMessage());    &#125;    Reporter.setCurrentTestResult(null);  &#125;&#125;\npackage com.thoughtworks.selenium;import org.testng.ITestContext;import org.testng.ITestResult;import org.testng.Reporter;import org.testng.internal.IResultListener;import java.io.File;public class ScreenshotListener implements IResultListener &#123;  File outputDirectory;  Selenium selenium;  public ScreenshotListener(File outputDirectory, Selenium selenium) &#123;    this.outputDirectory = outputDirectory;    this.selenium = selenium;  &#125;  public void onTestFailure(ITestResult result) &#123;    Reporter.setCurrentTestResult(result);    try &#123;      outputDirectory.mkdirs();      File outFile = File.createTempFile(&quot;TEST-&quot; + result.getName(), &quot;.png&quot;, outputDirectory);      outFile.delete();      selenium.captureScreenshot(outFile.getAbsolutePath());      Reporter.log(&quot;&lt;a href=&#x27;&quot; +          outFile.getName() +          &quot;&#x27;&gt;screenshot&lt;/a&gt;&quot;);    &#125; catch (Exception e) &#123;      e.printStackTrace();      Reporter.log(&quot;Couldn&#x27;t create screenshot&quot;);      Reporter.log(e.getMessage());    &#125;    Reporter.setCurrentTestResult(null);  &#125;&#125;\n是不是适当增加空白行就提高了代码的可读性呢，这里说的是适当增加空白行，并不代表随意增加。事实上，如果增加一些无意义的空白行反而会使代码的可读性变差。\n我们习惯于使有关联的代码彼此靠近，无明显关联的代码相互分隔。因此我们不但需要使用空白行间隔代码，还要调整代码位置，把有关联的代码放在一起，通常我们把被调用的函数放在调用函数的下面。这样别人在读我们的代码的时候再也不用经历来回“跳跃”的痛苦了。\n最后，我们通常把实体变量定义在类的顶部，这个只是我们Java程序员的习惯操作，如果突然在类的中间位置出现一个变量声明会让人觉得很奇怪。如果你想说，定义在中间是不想让定义位置和调用位置离太远，那么只能说明你的类定义出现了问题。建议你看一下旧文代码洁癖系列（三）：整洁的类和函数。\n横向格式介绍完垂直格式，紧接着自然要来介绍一下它的兄弟，横向格式。或许你会问为什么不叫水平格式，我的回答是：也可以叫水平格式，只要你喜欢。横向格式也是需要间隔和靠近的，这里的间隔主要是为了起到强调的效果。举个例子。\nint lineSize = line.length();\n这里等号两边的空格分别是为了强调左右两边的元素。横向格式另一个比较重要的元素就是缩进，Java程序对缩进没有强制性要求，而Python程序对缩进的要求非常严格，稍有不慎，执行的结果就会不同。但是Java程序员也要注意缩进，因为我们的代码是层级关系，而缩进可以帮我们快速理清层级关系。\n最后，横向代码格式对每行代码长度是有要求的，如果代码过长，那么在阅读的时候就需要左右滑动，而这个操作其实是不受人喜欢的。虽然现在代码编辑器可显示的长度变大了，但我们还是习惯每行代码最多100个字符。\n团队的规则每一个优秀的团队都已一套属于自己的代码格式要求，有些是特定的，有些是使用公共的。我们team所用的代码规范就是Google的代码规范，阿里的代码规范也是比较被大家认可的。这里给大家一个小福利，在我的公众号后台回复【代码规范】，就可以获得一份阿里的Java开发手册。\n","tags":["整洁"]},{"title":"代码洁癖系列（八）：迭代的原则","url":"/2018/09/08/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E8%BF%AD%E4%BB%A3%E7%9A%84%E5%8E%9F%E5%88%99/","content":"我们都知道，一个软件的维护成本往往要高于其研发成本。在维护过程中，我们的代码需要不断的进行迭代。迭代的目的有两个：修复bug和增加新特性。但是迭代也会带来一系列新的问题，比如新的bug，或者是破坏代码的整洁性。这里我们从保持代码整洁性的角度来讨论一下迭代的几个原则。\n运行所有测试没错，首先的要说的还是测试，我们要在每次迭代代码之后，运行所有的测试，如有必要，也要编写新的测试。我们要编写尽量简单的测试，简单的测试会驱使我们降低类与类之间的耦合度。如果还不了解如何编写单元测试，可以参考一下旧文代码洁癖系列（七）：单元测试的地位。良好的测试不但是代码质量的保证，同时也是良好设计的引导。\n不要重复“造轮子”记得我的leader曾经告诉过我：写每一行代码之前，要先思考一下有没有必要写这行代码。在实现一个功能之前，先确认一下这个功能是否已经被实现了。永远不要重复“造轮子”。但是，当我们进行一定的共性抽取时，可能已经违反了SRP原则（Single Responsibility Principle）。因此，抽取出的方法可能需要放在其他类中。\n可读代码是程序员之间的交流工具，要想获得其他程序员的尊重，必须使你的代码具备可读性。这也是我们要保持代码整洁的原因。如何保证代码的可读性呢？首先需要的就是有意义的命名，关于命名规则，可以参考代码洁癖系列（二）：命名的艺术这篇文章，其次就是通过测试用例让别人了解你的代码。\n尽可能少的类和方法在代码洁癖系列（三）：整洁的类和函数一文中，我们说过类和函数都应该尽量短小。有人问了，为了类和函数都足够短小，我要把代码拆分成许多的类吗？这里需要说明一下，在这方面，我们并不需要追求极致。应该根据实际情况，合理的拆分。所以，也要尽量减少类和方法，这可能与“类和函数应该短小”这一原则相矛盾。这需要工程师自己去衡量了，首先要保证“类和函数应该短小”，其次才是尽可能减少类和方法。\n结束语到这里，”代码洁癖系列“的文章要告一段落了，希望大家在写代码的时候可以多思考，保证自己代码的整洁性。文章有什么问题，或者我有哪些遗漏的地方，大家可以通过去我的微信公众号后台留言和我讨论。\n","tags":["整洁"]},{"title":"代码洁癖系列（六）：处理错误","url":"/2018/09/03/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E5%A4%84%E7%90%86%E9%94%99%E8%AF%AF/","content":"程序运行过程中，有些错误是不可避免的，而如何使程序在出现错误时代码仍然正常工作就成了程序员的日常工作之一。那么处理错误和代码整洁有什么关系呢？\n前面我们提到过，try-catch代码块会破坏代码的结构。但是，我们在处理错误时又不得不使用try-catch代码块，这样才能让我们的程序保持健壮。那么如何兼顾整洁和健壮呢，这就是我们今天要讨论的内容。\n不要使用返回码在上古时期，很多语言还都不支持异常，程序员们只能约定好返回的错误码，通过不同的错误码对代码进行错误处理。这么做需要调用者在调用方法后马上判断返回值来决定是否需要处理错误。而这一步骤往往被遗忘，就会导致程序出现不可预估的问题。所以，现在我们有了throw和try-catch这样的“热武器”，就不要再用错误码这样的“冷兵器”了。不然很容易就会被别人给秒杀的。\ntry-catch-finally相信大家都很熟悉，不熟悉的话，可以前往本站达成合作的各大搜索引擎进行搜索，保证有答案。\n异常抛出的层级我们不希望程序中充满了try-catch代码块，这样会使程序的可读性变差。因此，我们要适当的使用throw语句，将异常抛出。交给上一层去处理。但是，如果只由最顶层的代码去处理异常，整个程序看上去是会整洁一些，但当我们需要修改某个底层代码抛出的异常时会发生什么呢？我们需要找到往上抛的每一个方法去修改它的签名，抛出新的异常，并重新发布。这会使我们的工作量增加好几倍。所以，我们的异常最好在本层进行统一处理，或者在抛出后的上一层就进行处理。\n说明发生异常的环境在Java中，系统异常通常会给出调用栈，便于我们进行问题的定位和调试。对于我们自己抛出的异常，也应该给出异常发生的环境，例如列出调用栈，给出请求参数等等。\n远离null值我们在读、写代码时，一定有被好多层的空指针判断折磨的经历。一不小心漏掉一个判断，那么程序就会出现空指针异常，这个异常要么被上层的catch捕获，要么直接抛出。很明显，这两种结果都不是我们想要的。怎么远离空指针异常呢？\n首先，代码的返回值不要返回null。可以选择直接抛出一个异常，或者返回一个特定的值，比如空对象等。\n其次，代码调用时，参数不要传null，因为一旦传null，那么在方法体中就需要对这个参数进行非空判断。而目前还没有什么好的方法可以简单高效的处理参数外部传来的null，只能使用if判断。因此希望大家在调用别的方法时尽量不要传入null。\n小结本文的主要目的是使我们的代码兼顾整洁性和健壮性。这就需要我们将错误处理独立于主逻辑之外，使错误就可以被单独处理，不但能够增强可读性，还能增强可维护性。最后得到的就是我们希望看到的整洁而健壮的代码。\n","tags":["整洁"]},{"title":"代码洁癖系列（四）：可忽略的注释","url":"/2018/08/30/%E4%BB%A3%E7%A0%81%E6%B4%81%E7%99%96%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%8F%AF%E5%BF%BD%E7%95%A5%E7%9A%84%E6%B3%A8%E9%87%8A/","content":"刚开始学编程的时候，老师就告诉我们，注释很重要，但是一直到现在，也没有人真正告诉过我要怎么写注释。还有很多人甚至干脆不写注释。所以今天想聊一下到底如何写注释。\n提到注释就让我想起一个段子：两个程序员去饭店吃饭，点菜的时候程序员甲说：我要吃宫保鸡丁，程序员乙就帮他记。\n宫保鸡丁\n然后程序员甲又说：我不想吃宫保鸡丁了，换成地三鲜吧。程序员乙就说好的，然后又帮他记上了。\n//宫保鸡丁地三鲜\n这个段子也从侧面反映了程序员们习惯性忽略注释的事实。段子讲完了，下面插播一些正文。\n注释不能拯救糟糕的代码首先，我想说的可能和大多数人的观点相左：尽量少用注释！没错，尽量少用。因为注释是会骗人的，而且时间越长的注释越容易骗人，因为大部分人在修改代码的时候都不会去修改注释。少写注释，尽量用代码去描述你要做什么。当你要写注释的时候，就要思考一下，别人为什么不能通过代码理解你想表达什么。这时你需要尝试修改代码，来达到上述目的。\n// Check to see if the employee is eligible for full benefitsif (employee.flags &amp; HOURLY_FLAG) &amp;&amp;    (employee.age &gt; 65)\n看一下这段代码，如果只看代码，可以理解它要表达什么吗？\nif (employee.isEligibleForFUllBenefits())\n花上点时间，把代码改成这样，是不是不用注释也可以读懂了？\n我们这里说尽量少使用注释，并不是完全不用注释，在某些情况下，我们需要注释。那么什么样的注释才算是好的注释呢？\n法律信息有时，公司代码规范会要求注明版权和著作权。那么我们就应该将这些信息放到源文件的开头位置。\n提供信息的注释// Returns an instance of the Responder being tested.protected abstract Responder responderInstance()\n这样的注释就是不错的注释，给读者提供了返回值的信息，不过，如果我们把函数命名为responderBeingTested，那么这个注释也就显得多余了。\n阐释可以用注释把某些难以理解的参数或返回值翻译成可以理解的形式。当前，前提是如果这些代码你无法修改，比如参数或返回值是标准库的一部分。这时阐释就显得很有用。举过来一个栗子。\nassertTrue(a.compareTo(a) == 0);  // a == aassertTrue(a.compareTo(b) != 0);  // a != bassertTrue(a.compareTo(b) == -1); // a &lt; b\n不过这样的阐释也有缺点，那就是它有可能是不正确的，我们需要小心确认其正确性。如果缺失正确性，那么这样的阐释只会起到负面作用。\nTODO注释TODO注释是比较常用的注释，可以在代码里添加工作列表，例如，对一个空实现函数添加TODO注释，就可以解释这里为什么是空实现，以及以后要实现什么。\n公共API的Javadoc这个也许最令人欣赏的注释习惯了。不过目前我们通常用swagger来代替注释。对swagger感兴趣的童鞋可以戳这里。\n所谓见贤思齐焉，见不贤而内自省也。看完了好的注释，就要想想怎么才能写出好的注释；接下来再来看看坏的注释，看的同时需要多反省自己，尽量避免写出坏的注释。\n自说自话写的东西只有自己能看懂，别人都不明白要表达什么。如果读代码时连注释都看不明白，还有人想看下去吗。\n日志式注释几乎把代码的每次修改记录都写到注释里，也许在那个没有代码版本控制工具的远古时代，这么做还有一定的意义。但是现在我们拥有很多健壮的代码版本控制工具，这样的注释也就变得毫无意义。\n在代码里加上自己的签名也是一样的道理，我们都可以通过代码版本控制工具查看具体的创建者和修改者，而不是只记住创建者。\n注释掉代码也是一样，我们用版本控制工具可以轻松找回以前的代码，不需要的代码可以直接删掉，而不是留一个注释掉的代码放在那里。\n废话注释/** The day of the month. */private int dayOfMonth;\n我不想多废话了……\n结语也许文中的观点和大多数人的思维相左，可能我的有些观点是错的，欢迎大家关注我的微信公众号，和我讨论注释究竟是否必要。\n","tags":["整洁"]},{"title":"使用ModelMapper的一次踩坑经历","url":"/2018/09/11/%E4%BD%BF%E7%94%A8ModelMapper%E7%9A%84%E4%B8%80%E6%AC%A1%E8%B8%A9%E5%9D%91%E7%BB%8F%E5%8E%86/","content":"在实际项目中，我们常常需要把两个相似的对象相互转换，其目的是在对外提供数据时需要将一部分敏感数据（例如：密码、加密token等）隐藏起来。最普通的方法是，新建一个对象，将需要的值逐个set进去。如果有多组需要这样转换的对象，那么就需要做很多只是get/set这样无意义的工作。\n在这样的背景下，ModelMapper诞生了，它是一个简单、高效、智能的对象映射工具。它的使用非常简单，首先添加maven依赖\n&lt;dependency&gt;  &lt;groupId&gt;org.modelmapper&lt;/groupId&gt;  &lt;artifactId&gt;modelmapper&lt;/artifactId&gt;  &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt;\n然后就可以直接new出一个ModelMapper对象，并且调用其map方法将指定对象的值映射到另一个对象上了。\n使用方法今天不做过多介绍，大家可以自行Google，找到ModelMapper的相关文档进行学习。今天要分享的时前几天无意间踩到的一个坑。我有两个类，PostDO和PostVO（这里只截取了部分字段，因此两个类的含义也不做解释了）：\npublic class PostDO &#123;    private Long id;    private String commentId;    private Long postId;    private int likeNum;&#125;\npublic class PostVO &#123;    private Long id;    private boolean like;    private int likeNum;&#125;\n在一个方法中，我试图将PostDO的一个对象映射到PostVO，因此我进行如下操作：\npublic class Application &#123;    public static void main(String[] args) &#123;        ModelMapper modelMapper = new ModelMapper();        PostDO postDO = PostDO.builder().id(3L).likeNum(0).build();        PostVO postVO = modelMapper.map(postDO, PostVO.class);        System.out.println(postVO);    &#125;&#125;\n执行结果是这样的：\nPostVO(id=3, like=false, likeNum=0)\n无异常，项目中likeNum字段的值是随着项目的进行递增的。当likeNum增加到2时，异常出现了：\nException in thread &quot;main&quot; org.modelmapper.MappingException: ModelMapper mapping errors:1) Converter org.modelmapper.internal.converter.BooleanConverter@497470ed failed to convert int to boolean.1 error\tat org.modelmapper.internal.Errors.throwMappingExceptionIfErrorsExist(Errors.java:380)\tat org.modelmapper.internal.MappingEngineImpl.map(MappingEngineImpl.java:79)\tat org.modelmapper.ModelMapper.mapInternal(ModelMapper.java:554)\tat org.modelmapper.ModelMapper.map(ModelMapper.java:387)\tat Application.main(Application.java:7)Caused by: org.modelmapper.MappingException: ModelMapper mapping errors:1) Error mapping 2 to boolean1 error\tat org.modelmapper.internal.Errors.toMappingException(Errors.java:258)\tat org.modelmapper.internal.converter.BooleanConverter.convert(BooleanConverter.java:49)\tat org.modelmapper.internal.converter.BooleanConverter.convert(BooleanConverter.java:27)\tat org.modelmapper.internal.MappingEngineImpl.convert(MappingEngineImpl.java:298)\tat org.modelmapper.internal.MappingEngineImpl.map(MappingEngineImpl.java:108)\tat org.modelmapper.internal.MappingEngineImpl.setDestinationValue(MappingEngineImpl.java:238)\tat org.modelmapper.internal.MappingEngineImpl.propertyMap(MappingEngineImpl.java:184)\tat org.modelmapper.internal.MappingEngineImpl.typeMap(MappingEngineImpl.java:148)\tat org.modelmapper.internal.MappingEngineImpl.map(MappingEngineImpl.java:113)\tat org.modelmapper.internal.MappingEngineImpl.map(MappingEngineImpl.java:70)\t... 3 more\n提示int类型不能转换成boolean型，很明显。ModelMapper是将like字段映射到likeNum了。那么ModelMapper究竟是怎样进行映射的呢，我们一起来看一下ModelMapper的源码。\nModelMapper利用反射机制，获取到目标类的字段，并生成期望匹配的键值对，类似于这样。\n\n接着对这些键值对进行遍历，逐个寻找源类中可以匹配的字段。首先会根据目标字段判断是否存在对应的映射，\nMapping existingMapping = this.typeMap.mappingFor(destPath);if (existingMapping == null) &#123;    this.matchSource(this.sourceTypeInfo, mutator);    this.propertyNameInfo.clearSource();    this.sourceTypes.clear();&#125;\n如果不存在，就调用matchSource方法，在源类中根据匹配规则寻找可以匹配的字段。匹配过程中，首先会判断目标字段的类型是否在类型列表中存在，如果存在，则可以根据名称，加入匹配的mappings中。如果不存在，则需要判断converterStore中是否存在能够应用于该字段的转换器。\nif (this.destinationTypes.contains(destinationMutator.getType())) &#123;    this.mappings.add(new PropertyMappingImpl(this.propertyNameInfo.getSourceProperties(), this.propertyNameInfo.getDestinationProperties(), true));&#125; else &#123;    TypeMap&lt;?, ?&gt; propertyTypeMap = this.typeMapStore.get(accessor.getType(), destinationMutator.getType(), (String)null);    PropertyMappingImpl mapping = null;    if (propertyTypeMap != null) &#123;        Converter&lt;?, ?&gt; propertyConverter = propertyTypeMap.getConverter();        if (propertyConverter == null) &#123;            this.mergeMappings(propertyTypeMap);        &#125; else &#123;            this.mappings.add(new PropertyMappingImpl(this.propertyNameInfo.getSourceProperties(), this.propertyNameInfo.getDestinationProperties(), propertyTypeMap.getProvider(), propertyConverter));        &#125;        doneMatching = this.matchingStrategy.isExact();    &#125; else &#123;        Iterator var9 = this.converterStore.getConverters().iterator();        while(var9.hasNext()) &#123;            ConditionalConverter&lt;?, ?&gt; converter = (ConditionalConverter)var9.next();            MatchResult matchResult = converter.match(accessor.getType(), destinationMutator.getType());            if (!MatchResult.NONE.equals(matchResult)) &#123;                mapping = new PropertyMappingImpl(this.propertyNameInfo.getSourceProperties(), this.propertyNameInfo.getDestinationProperties(), false);                if (MatchResult.FULL.equals(matchResult)) &#123;                    this.mappings.add(mapping);                    doneMatching = this.matchingStrategy.isExact();                    break;                &#125;                if (!this.configuration.isFullTypeMatchingRequired()) &#123;                    this.partiallyMatchedMappings.add(mapping);                    break;                &#125;            &#125;        &#125;    &#125;    if (mapping == null) &#123;        this.intermediateMappings.put(accessor, new PropertyMappingImpl(this.propertyNameInfo.getSourceProperties(), this.propertyNameInfo.getDestinationProperties(), false));    &#125;&#125;\n默认的转换器有11中：\n\n找到对应的converter后，converter的map方法返回一个MatchResult，MatchResult有三种结果：FULL、PARTIAL和NONE（即全部匹配，部分匹配和不匹配）。注意，这里有一个部分匹配，也就是我所踩到的坑，在对like进行匹配是，likeNum就被定义为部分匹配。因此，当likeNum大于2时，就不能被转换成boolean类型。\n这里解决方法有两种，一种是在设置中，规定必须字段名完全匹配；另一种就是将匹配策略定义为严格。\n设置方法如下：\nmodelMapper.getConfiguration().setFullTypeMatchingRequired(true);modelMapper.getConfiguration().setMatchingStrategy(MatchingStrategies.STRICT);\n到这里，ModelMapper会选出较为合适的源字段，但是如果匹配要求不高的话，ModelMapper可能会筛选出多个符合条件的字段，因此，还需要进一步过滤。\nPropertyMappingImpl mapping;if (this.mappings.size() == 1) &#123;    mapping = (PropertyMappingImpl)this.mappings.get(0);&#125; else &#123;    mapping = this.disambiguateMappings();    if (mapping == null &amp;&amp; !this.configuration.isAmbiguityIgnored()) &#123;        this.errors.ambiguousDestination(this.mappings);    &#125;&#125;\n这里我们看到，如果匹配到的结果只有1个，那么就返回这个结果；如果有多个，则会调用disambiguateMappings方法，去掉有歧义的结果。我们来看一下这个方法。\nprivate PropertyMappingImpl disambiguateMappings() &#123;    List&lt;ImplicitMappingBuilder.WeightPropertyMappingImpl&gt; weightMappings = new ArrayList(this.mappings.size());    Iterator var2 = this.mappings.iterator();    while(var2.hasNext()) &#123;        PropertyMappingImpl mapping = (PropertyMappingImpl)var2.next();        ImplicitMappingBuilder.SourceTokensMatcher matcher = this.createSourceTokensMatcher(mapping);        ImplicitMappingBuilder.DestTokenIterator destTokenIterator = new ImplicitMappingBuilder.DestTokenIterator(mapping);        while(destTokenIterator.hasNext()) &#123;            matcher.match(destTokenIterator.next());        &#125;        double matchRatio = (double)matcher.matches() / ((double)matcher.total() + (double)destTokenIterator.total());        weightMappings.add(new ImplicitMappingBuilder.WeightPropertyMappingImpl(mapping, matchRatio));    &#125;    Collections.sort(weightMappings);    if (((ImplicitMappingBuilder.WeightPropertyMappingImpl)weightMappings.get(0)).ratio == ((ImplicitMappingBuilder.WeightPropertyMappingImpl)weightMappings.get(1)).ratio) &#123;        return null;    &#125; else &#123;        return ((ImplicitMappingBuilder.WeightPropertyMappingImpl)weightMappings.get(0)).mapping;    &#125;&#125;\nModelMapper定义了一个权重，来判断源字段是否有歧义，这里根据驼峰式的规则（也可以设置为下划线），将源和目标字段名称进行拆分，根据 匹配数量/源token数+目标token数，得到一个匹配的比率，比率越大，说明匹配度越高。最终取得匹配权重最大的那个字段。其他字段被认为是有歧义的。\n截至目前，默认的ModelMapper的map方法的工作原理已经介绍完了，中间可能有些遗漏的细节，或者哪里有说的不明白的地方，欢迎大家和我一起讨论。大家在用到ModelMapper时一定要注意字段名，如果有相近的字段名，必须认真核对匹配是否正确，必要时就采用严格匹配策略。\n","tags":["Java"]},{"title":"吐血推荐珍藏的IDEA插件","url":"/2020/02/10/%E5%90%90%E8%A1%80%E6%8E%A8%E8%8D%90%E7%8F%8D%E8%97%8F%E7%9A%84IDEA%E6%8F%92%E4%BB%B6/","content":"之前给大家推荐了一些我自己常用的VS Code插件，很多同学表示很受用，并私信我说要再推荐一些IDEA插件。作为一名职业Java程序员/业余js开发者，我平时还是用IDEA比较多，所以也确实珍藏了一些IDEA插件。今天就一并分享给大家。\n在最开始，我还是想先介绍一下IDEA中如何安装插件，首先打开Preferences（菜单栏打开或者使用快捷键Command+,）在Windows版本中应该是Settings。然后选择Plugins一栏，就可以从右侧的MarketPlace中选择自己需要的插件进行安装了。\n\nLombok首先向我们走来的是Lombok。作为Java程序员，你还在为不断的写Getter/Setter方法而苦恼吗？你还在为每个Model类都要写类似的构造方法而感到烦恼吗？赶快试试Lombok吧，它可以有效帮助你解决这些问题，只需要一个注解，构造方法和Getter/Setter方法全部搞定，再也不用把时间浪费在无用功上了。\n如果你还不是很了解Lombok的话，可以自己动手，到Lombok官网学习一番，学完记得回来点赞。\n最后展示一个简单的例子供大家参考。\n\nString ManipulationString Manipulation插件是一款非常强大的插件，它可以对代码进行很多操作，如排序、去除空行、字符串格式转换、Encode/Decode。其中我最常用的是字符串格式转换。你可以通过点击右键选择String Manipulation或者使用快捷键Option + M来选择相应的功能。\n\nstackoverflow作为一名高级CtrlCV工程师，我写代码有两大利器，一个是Google，另一个就是stackoverflow。两者相辅相成，帮我在编码的道路上越走越远。相信有不少同学跟我一样离不开stackoverflow，那么这款插件就会给你带来极大的方便，遇到问题怎么办？右键一下，点击「search stackoverflow」，大部分问题都可以轻松搞定。\nRainbow Brackets在推荐VS Code的插件时我们就介绍过一款叫做Bracket Pair Colorizer的插件，它可以把括号变成不同的颜色，我觉得这样分辨括号非常方便，看起来也比较美观。所以在IDEA中也使用了相同效果的插件，就是Rainbow Brackets。\n\nGsonFormat我们在接外部接口时，别人给了一串JSON串，我们在代码中需要将JSON中的字段封装到一个类中，一个一个输入挺麻烦的，这时GsonFormat就可以派上用场了。它可以帮助我们根据JSON中的key快速生成我们需要的类。\n它的使用快捷键是Option + S\n\nMaven Helper如果你的项目使用的构建工具是Maven的话，这个插件就能帮你避免各种依赖冲突，安装好插件之后，打开pom文件，可以看到最下方有一个叫Dependency Analyzer的Tab，这里就可以看到你的哪些依赖是有冲突的，然后在右侧Exclude掉不需要的依赖。\n\nRestfulToolkitRestfulToolkit是一套辅助开发Restful服务的工具集，对于这个插件，我最常用的功能就是快速查找指定的url对应的方法。快捷键是Command + \\\n关于其他的一些功能，大家有兴趣的话可以直接访问该插件的homepage。\n以上这些就是我常用的IDEA插件了，没有太多花里胡哨的东西，大家如果有什么好用的插件也欢迎分享出来。\n","tags":["技术杂谈"]},{"title":"吐血推荐珍藏的Visual Studio Code插件","url":"/2019/11/11/%E5%90%90%E8%A1%80%E6%8E%A8%E8%8D%90%E7%8F%8D%E8%97%8F%E7%9A%84Visual-Studio-Code%E6%8F%92%E4%BB%B6/","content":"作为一名Java工程师，由于工作需要，最近一个月一直在写NodeJS，这种经历可以说是一部辛酸史了。好在有神器Visual Studio Code陪伴，让我的这段经历没有更加困难。眼看这段经历要告一段落了，今天就来给大家分享一下我常用的一些VSC的插件。\nVSC的插件安装方法很简单，只需要点击左侧最下方的插件栏选项，然后就可以搜索你想要的插件了。\n\n下面我们进入正题\nMaterial Theme第一个是Material Theme，这个插件可以帮助我们修改主题色，帮助你摆脱只有黑色和白色的世界。当然你也可以通过修改setting文件来自定义主题颜色。\n\nAuto Import在写Java时，通常我是直接在代码中写出类名，然后使用IDEA自动导入相应的包的，但是使用VSC时没有这个功能，这个问题就让我很困扰，这意味着作为高级crtlCV工程师，粘贴过来的代码无法直接使用，你还要去查一些引用是属于哪个包的，怎么导入。\n而Auto Import帮我解决了这个大问题，它可以自动识别，解析和增加一些对应的包。有了它，我就可以继续做ctrlCV工程师了。\n\nImport Cost写过NodeJS的同学可能都会有一个体会，自己可能只写了几行代码，但是要安装的包竟然达到几个G，可能有些夸张，但是大量的node_modules真的很令人崩溃。\n\n这时你需要的是Import Cost来帮你控制一下你导入包的大小。\n\n当你写了一个导入语句时，它会提醒你这个包的大小，如果你发现某个包太大时，就需要考虑一下你是否真的需要引入整个包了。\nIndent-Rainbow这个插件是帮助你提升读代码的体验的，对于刚开始接触NodeJS的同学来说，读代码的时间往往比写代码的时间要多。如果项目过大时，新同学往往会迷失在很多的代码块中，分辨代码块只能靠行前缩紧数量。但是有时缩紧数量又无法一眼看出。而Indent-Rainbow就是用来帮你快速分辨代码的。\n\nPrettier — Code FormatterPrettier插件是用来格式化代码的。\n符合代码规范的代码可以说是一个工程师的脸面，而Prettier可以说是专门帮你维护脸面的插件。有了它，你在写代码时就可以肆无忌惮了，只需要在写完以后按一下对应的快捷键。你的代码就会马上变漂亮。\n\nSublime Text Keymap and Settings Importer不知道有多少同学和我一样比较喜欢用Sublime Text。虽然ST3也非常强大，可以用来写JS代码，但是我觉得它还是比不上专业的IDE，所以我更喜欢把ST3当作「记事本」来用，如果你已经比较习惯了ST3的快捷键，并且不想因为使用VSC而改变这个习惯，那么就可以使用这个插件，它会在VSC中模仿ST3的快捷键设置。\n\n你可以使用command+P来唤起命令窗口，然后输入&gt;开始像在ST3中那样操作。\nnpm Intellisensenpm Intellisense插件可以帮助你将你想要的node modules补充完整。\n\nFile UtilsFile Utils在我看来是一个非常方（zhuang）便（bi）的插件，它可以帮助你不使用鼠标就可以创建、移动、删除文件。看起来是不是很酷。\n\nBracket Pair Colorizer前面我们提到了缩紧的识别，这里还有一个括号颜色标识的插件。它可以把括号标为不同的颜色，方便识别括号匹配。这种插件我在IDEA中也会用，可以极大的提高读代码的效率。\n\nTrailing Spaces这个插件会帮我们标出一些无用的尾部空格，如果发现，请立即删除它们。\n\nWakaTime这个插件很有意思，它会统计你编码的一些数据，例如各种语言的占比，日平均编码时间等。你可以用它来统计一下你每天大概的有效工作时间是多少，如果数据比较漂亮，可以不经意间让领导看到一下，哈哈哈。\n\nVscode-icons你是否对VSC的默认icon感到厌烦呢？你想直接通过图标看出某个文件的文件格式吗？Vscode-icons插件来帮你实现。\n它会让文件的icon更加友好，也可以下载一些你喜欢的icon。\n\n以上就是我常用的一些VSCode的插件。喜欢的同学可以直接去市场下载体验。这些插件可能大部分都是用于提升读代码，因为我最近也是读代码比较多。如果其他同学有好用的插件也可以分享出来。\n后面我也会考虑分享一些IDEA的插件，做Java的同学可以期待一波。\n","tags":["技术杂谈"]},{"title":"实用的工具，有趣的人","url":"/2018/09/07/%E5%AE%9E%E7%94%A8%E7%9A%84%E5%B7%A5%E5%85%B7%EF%BC%8C%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BA%BA/","content":"1998年的9月4日，拉里·佩奇和谢尔盖·布林将Google带到这个世界。就在前几天，Google刚刚过了20岁生日。经过20年的发展，Google早已不是当初的小作坊了，而是发展成为被大家公认的全球最大的搜索引擎公司。20年间，Google不仅在搜索技术上表现颇为出色，在大数据、云计算、AI等各个方面都有着巨大的贡献。\n我们都知道，如今世界已经从互联网时代走向大数据时代。数据几乎已经成为各个互联网公司的命脉，数据分析师这一职位也应运而生（虽然我本人不是数据分析师），但是我对数据分析很感兴趣。刚刚过完20岁生日的Google这次可以算是给数据分析师和数据分析爱好者们送上了一份大礼（果然是大公司，自己过生日还给我们送礼）——一款叫做Google Dataset Search的搜索引擎。它是一款专门用来搜索数据集的搜索引擎，搜索结果中包括了数据集的创建者，发布时间，数据集的收集方式，数据集的使用条款以及对数据集的描述。\n还记得我当初自学大数据知识的时候，每每到了想要亲自动手试一试的时候，总是为找不到合适的数据集而苦恼。最后，我决定自学Python爬虫，想要什么数据都自己动手爬。然而，这种工作还是挺繁琐的，也许对我这种自学的爱好者无所谓，反而还多掌握一门技术。但对于专门从事数据科学工作的人来说可能会觉得很痛苦。\n随着Google Dataset Search的诞生，这种“巧妇难为无米之炊”的尴尬终于一去不复返了。现在你想要什么数据集都可以直接搜索，简直不要太方便。\n这个网站的地址是：\nhttps://toolbox.google.com/datasetsearch\n可以访问的同学可以上去体验一下，目前还是Beta版，没办法访问的同学，看看这个界面先过过瘾吧。\n\n以上就是今天要介绍的实用的工具。下面再来介绍一个有趣的人，同样是和DataSet Search有关。\n要介绍的这个人呢，作为程序员一定很熟悉，就是Linux之父Linus Torvalds。怀着对大神的崇敬之情，我今天用Dataset Search搜索了一下Linus。结果果然没有令我失望，先来展示一下搜素结果。\n\n第一条是Linus Torvalds Rants，可以理解为Linus的粗话（这是什么鬼？黑人问号脸）。\n我们注意到数据集的描述为Linus2012年到2015年邮件的粗话合集。到这里有些怀疑这个数据集的真实性，不过对大神崇敬之情驱动着我点了进去，看看大神是怎么骂人的，\n\n这是数据集的部分截图，链接也都是可以直接访问的。\n总之，Linus大神说话还是挺文明的，这种做法也非常有Linus的风格。（这里顺便提一下，Linus曾经在Linux内核代码中记录了女儿的生日）怎么样，是不是今天才发现原来Linus是如此有趣的人？\n最后，国内某公司CEO说要“再”次击败Google，我觉得他们首先还是要多做些像这样有意义的事吧。\n","tags":["瞎扯"]},{"title":"敲开通往架构师的门","url":"/2019/11/14/%E6%95%B2%E5%BC%80%E9%80%9A%E5%BE%80%E6%9E%B6%E6%9E%84%E5%B8%88%E7%9A%84%E9%97%A8/","content":"最近学习了一些关于架构设计的知识想分享给大家。俗话说得好，不想当架构师的程序员不是好厨子。那么如何成为一名架构师呢？接下来就聊一聊我的一些想法。\n什么是架构师之前有同学问我，做了几年技术，应该转管理还是转架构师？对于这位同学，我给他的答案是，你要先踏踏实实做好现在的工作。因为就他提的问题来看，应该是刚入行不久或者是在校学生。\n专心做技术的，都想做架构师。但架构师并不是说技术做时间长了可以转的。随着你的知识深度和广度的增加，在工作中会扮演更重要的角色，承担更大的责任，最终自然而然就会接触到架构设计的工作。\n而架构师的主要工作，其实是利用架构设计知识以及丰富的工作经验，在设计架构时，结合实际情况，在不同的选项中做出取舍。\n架构设计的真正目的？为什么要进行架构设计？因为架构设计很重要？可是为什么重要呢？似乎说不清楚。\n因为可以提升开发效率吗？也不一定，因为只有简单的设计才会使开发效率更高。而架构设计出于多方面考虑，不得已会引入一些复杂度，因此架构设计并不一定能提升开发效率。\n是为了大多数口中的“高可用”、“高性能”、“可扩展”吗？其实也不是。我们的系统可能并不一定需要这些。\n那架构设计的真正目的是什么呢？我认为架构设计的真正目的是与系统复杂度做斗争。\n系统复杂度的来源有：高性能、高可用、可扩展性、低成本、安全、规模。\n前面我们聊到有些系统可能不需要高可用、高性能。有些同学可能不理解，这些难道不是软件开发最基本的要求吗？这样的说法是存在一定偏差的。我们举一个简单的例子说明一下。\n如果让你为一所学校设计一个学生信息管理系统。针对上述几个复杂度的来源，你会做出怎样的取舍？我们来逐条分析一下。\n首先是高性能，学校的学生最多也就几万人，而且平时也不可能几万人同时用系统。因此我们并不需要考虑高性能。数据的CRUD直接用关系型数据库就足够了。\n然后是高可用，对于学生系统而言，即使宕机几个小时，影响也不会太大。不过数据的可靠性还是要保证的，如果大量数据丢失而又没有备份的话，数据修复将会是一项繁重的工作。所以这里需要做一些数据高可靠的设计。\n接下来是可扩展性，学生管理系统一般比较稳定，不会出现需要扩展的情况。因此我们也不太需要考虑可扩展性。\n至此，我们在设计系统时习惯考虑的高可用、高性能和可扩展，在这个系统中都不需要过多关注了。我们再来看看剩下的几个复杂度来源。\n关于低成本，由于我们并不需要高可用和高性能的设计，所以几台服务器的成本对于学校来说也不足为虑。\n安全性而言，学生信息需要一定的安全保证，但也不必做到金融级安全。所以只需要做好数据库权限管理，登录密码管理就足够了。\n最后是系统规模，学生管理系统往往不会很复杂。也不会迭代出许多功能。因此规模是比较固定且比较小的，不会带来很多的复杂度。\n从我们的分析中可以看出，学生管理系统是一个并不复杂的系统，我们真正需要着重考虑的就只有数据高可靠和数据安全两方面。面对复杂的系统，我们也应该按照这个步骤来思考并设计出合理的架构。在合理的情况下，尽量减少系统的复杂度。\n架构设计原则前面我们提到，架构师的工作其实就是在多种选项中做出合理的取舍，取舍没有对错之分，只有是否合适一说。为了更好的做出选择，架构设计应该遵循三个原则：合适原则、简单原则、演化原则。下面我来一一介绍这三个原则。\n合适原则我们一直在说，架构设计中架构师要做出取舍，选择合适的架构。之所以一直强调合适，是因为我们在架构设计过程中需要结合实际情况来考虑。\n那么脱离实际情况的设计通常是怎样发生的呢？不知道大家在开发时有没有遇到过这样的需求：“我们决定做一个电商网站，就按照淘宝做一个一模一样的吧。“这时作为开发的你一定是黑人问号脸，心里也会万马奔腾。\n在架构设计时也是一样，最忌讳的就是不顾实际情况，盲目的使用业界最优的架构设计。有同学可能不太理解，使用最优设计有什么错呢？\n这里我们所说的实际情况就是你的业务。试想如果你的业务刚刚起步，QPS刚过百，这时，你设计的架构是能支持1000QPS还是3000QPS对于系统来说没什么区别。但对于开发成本来说就提升了不止3倍。而对于这样的业务体量来说，开发团队一般只有十几人或几十人这样的规模。要让这样的团队来开发的话，大概率是无法完成的。\n演化原则聊完了合适原则，我们再来聊一聊演化原则。就像北京的城市规划一样，它一定是先有二环，慢慢向外扩建，才逐渐有了三四五六环。而我们现在所使用的大多数软件，也都是经过了许多版本的迭代才有了现在的功能。\n对于一名合格的架构师来说，我们首先要遵循合适原则，然后再逐步演化。切不可想着一步到位，从而引起过度设计。当业务发展到一定阶段时，我们不可避免的会需要对架构进行扩展、重构甚至重写。在这一过程中，我们应该保留下好的设计，对不好的设计进行完善。就像淘宝的架构一样，它是经历了多次“双十一”之后，才有了现在这样能支撑每天上千亿成交额的架构。\n因此，我们在设计架构时要遵循的第二个原则就是循序渐进的演化原则，而不是追求一步到位。\n简单原则最后再来说简单原则。前面我们也说了，架构设计其实是在和系统的复杂度做斗争。为什么要有简单原则？我认为原因主要有两点。\n第一，复杂的架构开发成本更高。在开发资源有限的情况下，如果我们的架构设计很复杂，势必会提升开发成本。而对于当今飞速发展的市场来说，时间就是生命。如果你设计的架构开发周期非常长，那么公司也许就会放弃这个项目，那么架构也就没有存在的意义了。\n第二，复杂的架构往往会带来更多的故障。举个栗子，电动牙刷和普通牙刷相比，坏的概率一定会高一点，电动牙刷可能出现刷头磨损，电路问题，充电故障等等，而普通牙刷只会出现刷头磨损的情况。也就是说，系统的组件越多，系统出现故障的概率也就越大。在此基础上还有一个问题就是，一旦出了故障，定位问题的速度而言，简单系统相较于复杂系统也有着很大的优势。\n至此，架构设计的三个原则我们都已经聊完了。细心的同学可能注意到了，我在详细介绍时的顺序和最开始提到的顺序并不一致。这不是我不注意细节。而是我在详细介绍时，对这三个原则的重要程度排了一个顺序。这也是作为架构师的一种取舍，当三种原则无法同时满足时，应该以哪个为重？这里我的答案是合适&gt;演化&gt;简单。\n关于架构设计，我已经有了一个大体的认识，不知道在读完本文以后你是否也有同样的感觉。如果有任何困惑，欢迎和我一起讨论交流。\n最后，架构师是需要有很深的技术积累的，而我在这方面做得还不够。所以后面还是要以技术积累为主，同时也会尝试将架构设计的知识引入到日常工作中。后续有什么新的体会我会继续和大家分享。\n","tags":["架构"]},{"title":"浅谈Redis通信协议","url":"/2019/06/23/%E6%B5%85%E8%B0%88Redis%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/","content":"Redis客户端和服务器端使用的通信协议叫做RESP（Redis Serialization Protocol）。它是特意为Redis设计的，同时也可以用于其他软件工程。\nRESP在以下事项之间进行妥协：\n\n实现简单\n快速解析\n可读性强\n\nRESP可以序列化多种不同的数据类型，比如：整型、字符串、数组。错误是一种特定的类型。Redis客户端把参数用数组来表示。回复的是一种特殊的数据格式。\nRESP是二进制安全的，它不需要处理从一个进程到另一个进程的批量数据，因为采用的是前缀长度来传输批量数据。\n注意：这里的协议只适用用与客户端-服务器通信。Redis集群使用的是不同的协议\n一般情况下，RESP是一种简单的请求-响应式协议。二般情况是：\n\nRedis支持管道，所以有可能一次发送多个命令，然后一起响应\n如果Redis客户端订阅了Pub/Sub频道，那么协议就会变成一种推送协议，当服务器接收到新的数据时会自动推送给客户端\n\nRESP协议支持的数据类型有：Simple Strings，Errors，Integers，Bulk Strings和Arrays。它的使用方法有：\n\n客户端以Bulk Strings数组的形式发送命令\n服务器端返回的结果是协议支持的类型之一\n\nRESP协议中，上述类型是通过首个字节区分的：\n\n+代表简单字符串（Simple Strings）\n-代表错误类型（Errors）\n:代表整型（Integers）\n$代表多行字符串（Bulk Strings）\n*代表数组（Arrays）\n\n此外，每一部分结束时，Redis统一使用“\\r\\n”表示结束。\n看到这里你是否有疑问呢？为什么没有表示null的方法呢？别着急我们一会就会解释。\nRESP简单字符串简单字符串中不允许出现\\r或\\n，只能有一行。它用于以最小开销传输非二进制安全字符串，例如回复的OK\n&quot;+OK\\r\\n&quot;\n如果要发送二进制安全的字符串，应该使用多行字符串。\nRESP错误RESP有特定的错误类型，它与简单字符串类似，只不过是把开头的+换成了-，而两者之间真正的区别是客户端将错误视为异常，而错误中的字符串只是表示错误信息。\n&quot;-Error message\\r\\n&quot;\n当客户端收到错误信息时，通常会抛出一个异常。我们来看一些例子：\n-ERR unknown command &#x27;foobar&#x27;-WRONGTYPE Operation against a key holding the wrong kind of value\n从第一个字符“-”之后，到第一个空格或新的一行，这之间的字符串表示错误类型。这只是Redis的一种约定，并不是RESP的错误格式。\n例如ERR是普通错误，而WRONGTYPE表示客户端试图对错误的数据类型执行操作。\nRESP整型整型只是以\\r\\n结尾，以:开头的纯整数的字符串。\n:1000\\r\\n\n很多Redis命令都会返回整型，例如INCR、LLEN和LASTSAVE。\n返回的整数需要在64位有符号整数范围内，同时也可以用于表示真或假。\nRESP多行字符串多行字符串是二进制安全的，最大长度是512MB。\n多行字符串的编码方式如下：\n\n以$+数字开头，以\\r\\n结束\n数据都是字符串\n结尾是\\r\\n\n\n所以“foobar”应该编码为\n&quot;$6\\r\\nfoobar\\r\\n&quot;\n空字符串表示为：\n&quot;$0\\r\\n\\r\\n&quot;\n多行字符串也可以用来null\n&quot;$-1\\r\\n&quot;\n当服务器返回Null多行字符串时，正常客户端是不应该返回空字符串的，而是应该返回nil对象。\nRESP数组客户端向服务器端发送命令时使用的就是RESP数组。类似的，某些命令返回的元素集合也是RESP数组的类型。\nRESP数组遵循以下规则：\n\n第一个字符是*，后面跟的十进制数字是数组元素的数量，然后跟着\\r\\n。\n每个元素都是RESP类型的\n\n空数组表示为：\n&quot;*0\\r\\n&quot;\n数组中的元素可以是不同类型的：\n*5\\r\\n:1\\r\\n:2\\r\\n:3\\r\\n:4\\r\\n$6\\r\\nfoobar\\r\\n\n第一行的*5\\r\\n表示数组有5个元素，后面每行是一个元素。\nRESP也有NULL数组的表示方法，这是NULL的另一种表示方法，通常用多行字符串的NULL来表示，不过由于历史原因，就保留了两种形式。\n当BLPOP命令超时时，就会返回NULL数组\n&quot;*-1\\r\\n&quot;\n当服务器返回NULL数组时，客户端应该返回null对象而不是空数组。\n数组中的NULL数组中的元素可以是NULL，通常表示数组中某个元素缺失，而不是空字符串：\n*3\\r\\n$3\\r\\nfoo\\r\\n$-1\\r\\n$3\\r\\nbar\\r\\n\n其中第二个元素时NULL，客户端的返回结果应该是：\n[&quot;foo&quot;,nil,&quot;bar&quot;]\n小结到此我们已经了解了RESP协议，RESP中虽然有大量的冗余\\r\\n，但是仍然有很多开源项目使用。\n","tags":["Redis"]},{"title":"玩转Redis集群之Cluster","url":"/2018/11/27/%E7%8E%A9%E8%BD%ACRedis%E9%9B%86%E7%BE%A4%E4%B9%8BCluster/","content":"前面我们介绍了国人自己开发的Redis集群方案——Codis，Codis友好的管理界面以及强大的自动平衡槽位的功能深受广大开发者的喜爱。今天我们一起来聊一聊Redis作者自己提供的集群方案——Cluster。希望读完这篇文章，你能够充分了解Codis和Cluster各自的优缺点，面对不同的应用场景可以从容的做出选择。\nRedis Cluster是去中心化的，这点与Codis有着本质的不同，Redis Cluster划分了16384个slots，每个节点负责其中的一部分数据。slot的信息存储在每个节点中，节点会将slot信息持久化到配置文件中，因此需要保证配置文件是可写的。当客户端连接时，会获得一份slot的信息。这样当客户端需要访问某个key时，就可以直接根据缓存在本地的slot信息来定位节点。这样就会存在客户端缓存的slot信息和服务器的slot信息不一致的问题，这个问题具体怎么解决呢？这里先卖个关子，后面会做解释。\n特性首先我们来看下官方对Redis Cluster的介绍。\n\nHigh performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values.\nAcceptable degree of write safety: the system tries (in a best-effort way) to retain all the writes originating from clients connected with the majority of the master nodes. Usually there are small windows where acknowledged writes can be lost. Windows to lose acknowledged writes are larger when clients are in a minority partition.\nAvailability: Redis Cluster is able to survive partitions where the majority of the master nodes are reachable and there is at least one reachable slave for every master node that is no longer reachable. Moreover using replicas migration, masters no longer replicated by any slave will receive one from a master which is covered by multiple slaves.\n\n是不是不(kan)想(bu)看(dong)？没关系，我来给你掰开了揉碎了解释一下。\n写安全Redis Cluster使用异步的主从同步方式，只能保证最终一致性。所以会引起一些写入数据丢失的问题，在继续阅读之前，可以先自己思考一下在什么情况下写入的数据会丢失。\n先来看一种比较常见的写丢失的情况：\nclient向一个master发送一个写请求，master写成功并通知client。在同步到slave之前，这个master挂了，它的slave代替它成为了新的master。这时前面写入的数据就丢失了。\n此外，还有一种情况。\nmaster节点与大多数节点无法通信，一段时间后，这个master被认为已经下线，并且被它的slave顶替，又过了一段时间，原来的master节点重写恢复了连接。这时如果一个client存有过期的路由表，它就会把写请求发送的这个旧的master节点（已经变成slave了）上，从而导致写数据丢失。\n不过，这种情况一般不会发生，因为当一个master失去连接足够长时间而被认为已经下线时，就会开始拒绝写请求。当它恢复之后，仍然会有一小段时间是拒绝写请求的，这段时间是为了让其他节点更新自己的路由表中的配置信息。\n为了尽可能保证写安全性，Redis Cluster在发生分区时，会尽量使客户端连接到多数节点的那一部分，因为如果连接到少数部分，当master被替换时，会因为多数master不可达而拒绝所有的写请求，这样损失的数据要增大很多。\nRedis Cluster维护了一个NODE_TIMEOUT变量，如果上述情况中，master在NODE_TIMEOUT时间内恢复连接，就不会有数据丢失。\n可用性如果集群的大部分master可达，并且每个不可达的master至少有一个slave，在NODE_TIMEOUT时间后，就会开始进行故障转移（一般1到2秒），故障转移完成后的集群仍然可用。\n如果集群中得N个master节点都有1个slave，当有一个节点挂掉时，集群一定是可用的，如果有2个节点挂掉，那么就会有1/(N*2-1)的概率导致集群不可用。\nRedis Cluster为了提高可用性，新增了一个新的feature，叫做replicas migration（副本迁移，ps：我自己翻译的），这个feature其实就是在每次故障之后，重新布局集群的slave，给没有slave的master配备上slave，以此来更好的应对下次故障。\n性能Redis Cluster不提供代理，而是让client直接重定向到正确的节点。\nclient中会保存一份集群状态的副本，一般情况下就会直接连接到正确的节点。\n由于Redis Cluster是异步备份的，所以节点不需要等待其他节点确认写成功就可以直接返回，除非显式的使用了WAIT命令。\n对于操作多个key的命令，所操作的key必须是在同一节点上的，因为数据是不会移动的。（除非是resharding）\nRedis Cluster设计的主要目标是提高性能和扩展性，只提供弱的数据安全性和可用性（但是要合理）。\nKey分配模型Redis Cluster共划分为16384个槽位。这也意味着一个集群最多可以有16384个master，不过官方建议master的最大数量是1000个。\n如果Cluster不处于重新配置过程，那么就会达到一种稳定状态。在稳定状态下，一个槽位只由一个master提供服务，不过一个master节点会有一个或多个slave，这些slave可以提供缓解master的读请求的压力。\nRedis Cluster会对key使用CRC16算法进行hash，然后对16384取模来确定key所属的槽位（hash tag会打破这种规则）。\nKeys hash tags标签是破坏上述计算规则的实现，Hash tag是一种保证多个键被分配到同一个槽位的方法。\nhash tag的计算规则是：取一对大括号{}之间的字符进行计算，如果key存在多对大括号，那么就取第一个左括号和第一个右括号之间的字符。如果大括号之前没有字符，则会对整个字符串进行计算。\n说了这个多，可能你还是一头雾水。别急，我们来吃几个栗子。\n\n{Jackeyzhe}.following和{Jackeyzhe}.follower这两个key都是计算Jackey的hash值\nfoo这个key就会对{bar进行hash计算\nfollow{}{Jackey}会对整个字符串进行计算\n\n重定向前面聊性能的时候我们提到过，Redis Cluster为了提高性能，不会提供代理，而是使用重定向的方式让client连接到正确的节点。下面我们来详细说明一下Redis Cluster是如何进行重定向的。\nMOVED重定向Redis客户端可以向集群的任意一个节点发送查询请求，节点接收到请求后会对其进行解析，如果是操作单个key的命令或者是包含多个在相同槽位key的命令，那么该节点就会去查找这个key是属于哪个槽位的。\n如果key所属的槽位由该节点提供服务，那么就直接返回结果。否则就会返回一个MOVED错误：\nGET x-MOVED 3999 127.0.0.1:6381\n这个错误包括了对应的key属于哪个槽位（3999）以及该槽位所在的节点的IP地址和端口号。client收到这个错误信息后，就将这些信息存储起来以便可以更准确的找到正确的节点。\n当客户端收到MOVED错误后，可以使用CLUSTER NODES或CLUSTER SLOTS命令来更新整个集群的信息，因为当重定向发生时，很少会是单个槽位的变更，一般都会是多个槽位一起更新。因此，在收到MOVED错误时，客户端应该尽早更新集群的分布信息。当集群达到稳定状态时，客户端保存的槽位和节点的对应信息都是正确的，cluster的性能也会达到非常高效的状态。\n除了MOVED重定向之外，一个完整的集群还应该支持ASK重定向。\nASK重定向对于Redis Cluster来讲，MOVED重定向意味着请求的slot永远由另一个node提供服务，而ASK重定向仅代表下一个请求需要发送到指定的节点。在Redis Cluster迁移的时候会用到ASK重定向，那Redis Cluster迁移的过程究竟是怎样的呢？\nRedis Cluster的迁移是以槽位单位的，迁移过程总共分3步（类似于把大象装进冰箱），我们来举个栗子，看一下一个槽位从节点A迁移到节点B需要经过哪些步骤：\n\n首先打开冰箱门，也就是从A节点获得槽位所有的key列表，再挨个key进行迁移，在这之前，A节点的该槽位被设置为migrating状态，B节点被设置为importing的槽位（都是用CLUSTER SETSLOT命令）。\n第二步，就是要把大象装进去了，对于每个key来说，就是在A节点用dump命令对其进行序列化，再通过客户端在B节点执行restore命令，反序列化到B节点。\n第三步呢，就需要把冰箱门关上，也就是把对应的key从A节点删除。\n\n有同学会问了，说好的用到ASK重定向呢？上面我们所描述的只是迁移的过程，在迁移过程中，Redis还是要对外提供服务的。试想一下，如果在迁移过程中，我向A节点请求查询x的值，A说：我这没有啊，我也不知道是传到B那去了还是我一直就没有存，你还是先问问B吧。然后返回给我们一个-ASK targetNodeAddr的错误，让我们去问B。而这时如果我们直接去问B，B肯定会直接说：这个不归我管，你得去问A。（-MOVED重定向）。因为这时候迁移还没有完成，所以B也没说错，这时候x真的不归它管。但是我们不能让它俩来回踢皮球啊，所以在问B之前，我们先给B发一个asking指令，告诉B：下面我问你一个key的值，你得当成是自己的key来处理，不能说不知道。这样如果x已经迁移到B，就会直接返回结果，如果B也查不到x的下落，说明x不存在。\n容错了解了Redis Cluster的重定向操作之后，我们再来聊一聊Redis Cluster的容错机制，Redis Cluster和大多数集群一样，是通过心跳来判断一个节点是否存活的。\n心跳和gossip消息集群中的节点会不停的互相交换ping pong包，ping pong包具有相同的结构，只是类型不同，ping pong包合在一起叫做心跳包。\n通常节点会发送ping包并接收接收者返回的pong包，不过这也不是绝对，节点也有可能只发送pong包，而不需要让接收者发送返回包，这种操作通常用于广播一个新的配置信息。\n节点会每个几秒钟就发送一定数量的ping包。如果一个节点超过二分之一NODE_TIME时间没有收到来自某个节点ping或pong包，那么就会在NODE_TIMEOUT之前像该节点发送ping包，在NODE_TIMEOUT之前，节点会尝试TCP重连，避免由于TCP连接问题而误以为节点不可达。\n心跳包内容前面我们说了，ping和pong包的结构是相同的，下面就来具体看一下包的内容。\nping和pong包的内容可以分为header和gossip消息两部分，其中header包含以下信息：\n\nNODE ID是一个160bit的伪随机字符串，它是节点在集群中的唯一标识\ncurrentEpoch和configEpoch字段\nnode flag，标识节点是master还是slave，另外还有一些其他的标识位\n节点提供服务的hash slot的bitmap\n发送者的TCP端口\n发送者认为的集群状态（down or ok）\n如果是slave，则包含master的NODE ID\n\ngossip包含了该节点认为的其他节点的状态，不过不是集群的全部节点。具体有以下信息：\n\nNODE ID\n节点的IP和端口\nNODE flags\n\ngossip消息在错误检测和节点发现中起着重要的作用。\n错误检测错误检测用于识别集群中的不可达节点是否已下线，如果一个master下线，会将它的slave提升为master。如果无法提升，则集群会处于错误状态。在gossip消息中，NODE flags的值包括两种PFAIL和FAIL。\nPFAIL flag如果一个节点发现另外一个节点不可达的时间超过NODE_TIMEOUT ，则会将这个节点标记为PFAIL，也就是Possible failure（可能下线）。节点不可达是说一个节点发送了ping包，但是等待了超过NODE_TIMEOUT时间仍然没有收到回应。这也就意味着，NODE_TIMEOUT必须大于一个网络包来回的时间。\nFAIL flagPFAIL标志只是一个节点本地的信息，为了使slave提升为master，需要将PFAIL升级为FAIL。PFAIL升级为FAIL需要满足一些条件：\n\nA节点将B节点标记为PFAIL\nA节点通过gossip消息收集其他大部分master节点标识的B节点的状态\n大部分master节点在NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT时间段内，标识B节点为PFAIL或FAIL\n\n如果满足以上条件，A节点会将B节点标识为FAIL并且向所有节点发送B节点FAIL的消息。收到消息的节点也都会将B标为FAIL。\nFAIL状态是单向的，只能从PFAIL升级为FAIL，而不能从FAIL降为PFAIL。不过存在一些清除FAIL状态的情况：\n\n节点重新可达，并且是slave节点\n节点重新可达，并且是master节点，但是不提供任何slot服务\n节点重新可达，并且是master节点，但是长时间没有slave被提升为master来顶替它\n\nPFAIL提升到FAIL使用的是一种弱协议：\n\n节点收集的状态不在同一时间点，我们会丢弃时间较早的报告信息，但是也只能保证节点的状态在一段时间内大部分master达成了一致\n检测到一个FAIL后，需要通知所有节点，但是没有办法保证每个节点都能成功收到消息\n\n由于是弱协议，Redis Cluster只要求所有节点对某个节点的状态最终保持一致。如果大部分master认为某个节点FAIL，那么最终所有节点都会将其标为FAIL。而如果只有一小部分master节点认为某个节点FAIL，slave并不会被提升为master，因此，FAIL状态将会被清除。\n搭建原理说了这么多，我们一定要来亲自动手搭建一个Redis Cluster，下面演示一个在一台机器上模拟搭建3主3从的Redis Cluster。当然，如果你想了解更多Redis Cluster的其他原理，可以查看官网的介绍。\nRedis环境首先要搭建起我们需要的Redis环境，这里启动6个Redis实例，端口号分别是6379、6380、6479、6480、6579、6580\n拷贝6份Redis配置文件并进行如下修改（以6379为例，端口号和配置文件根据需要修改）：\nport 6379cluster-enabled yescluster-config-file nodes6379.confappendonly yes\n配置文件的名称也需要修改，修改完成后，分别启动6个实例（图片中有一个端口号改错了……）。\n\n创建Redis Cluster实例启动完成后，就可以创建Redis Cluster了，如果Redis的版本是3.x或4.x，需要使用一个叫做redis-trib的工具，而对于Redis5.0之后的版本，Redis Cluster的命令已经集成到了redis-cli中了。这里我用的是Redis5，所以没有再单独安装redis-trib工具。\n接下来执行命令\nredis-cli --cluster create 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6479 127.0.0.1:6480 127.0.0.1:6579 127.0.0.1:6580 --cluster-replicas 1\n当你看到输出了\n[OK] All 16384 slots covered\n就表示Redis Cluster已经创建成功了。\n查看节点信息此时我们使用cluster nodes 命令就可查看Redis Cluster的节点信息了。\n\n可以看到，6379、6380和6479三个节点被配置为master节点。\nreshard接下来我们再来尝试一下reshard操作\n\n如图，输入命令\nredis-cli --cluster reshard 127.0.0.1:6380\nRedis Cluster会问你要移动多少个槽位，这里我们移动1000个，接着会询问你要移动到哪个节点，这里我们输入6479的NODE ID\n\nreshard完成后，可以输入命令查看节点的情况\nredis-cli --cluster check 127.0.0.1:6480\n可以看到6479节点已经多了1000个槽位了，分别是0-498和5461-5961。\n新增master节点我们可以使用add-node命令为Redis Cluster新增master节点，可以看到我们增加的是6679节点，新增成功后，并不会为任何slot提供服务。\n新增slave节点\n我们也可以用add-node命令新增slave节点，只不过需要加上–cluster-slave参数，并且使用–cluster-master-id指明新增的slave属于哪个master。\n总结最后来总结一下，我们介绍了\nRedis Cluster的特性：写安全、可用性、性能\nKey分配模型：使用CRC16算法，如果需要分配到相同的slot，可以使用tag\n两种重定向：MOVED和ASK\n容错机制：PFAIL和FAIL两种状态\n最后又动手搭建了一个实验的Redis Cluster。\n","tags":["Redis"]},{"title":"深入理解Redis的scan命令","url":"/2018/09/26/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Redis%E7%9A%84scan%E5%91%BD%E4%BB%A4/","content":"熟悉Redis的人都知道，它是单线程的。因此在使用一些时间复杂度为O(N)的命令时要非常谨慎。可能一不小心就会阻塞进程，导致Redis出现卡顿。\n有时，我们需要针对符合条件的一部分命令进行操作，比如删除以test_开头的key。那么怎么获取到这些key呢？在Redis2.8版本之前，我们可以使用keys命令按照正则匹配得到我们需要的key。但是这个命令有两个缺点：\n\n没有limit，我们只能一次性获取所有符合条件的key，如果结果有上百万条，那么等待你的就是“无穷无尽”的字符串输出。\nkeys命令是遍历算法，时间复杂度是O(N)。如我们刚才所说，这个命令非常容易导致Redis服务卡顿。因此，我们要尽量避免在生产环境使用该命令。\n\n在满足需求和存在造成Redis卡顿之间究竟要如何选择呢？面对这个两难的抉择，Redis在2.8版本给我们提供了解决办法——scan命令。\n相比于keys命令，scan命令有两个比较明显的优势：\n\nscan命令的时间复杂度虽然也是O(N)，但它是分次进行的，不会阻塞线程。\nscan命令提供了limit参数，可以控制每次返回结果的最大条数。\n\n这两个优势就帮助我们解决了上面的难题，不过scan命令也并不是完美的，它返回的结果有可能重复，因此需要客户端去重。至于为什么会重复，相信你看完本文之后就会有答案了。\n关于scan命令的基本用法，可以参看Redis命令详解：Keys一文中关于SCAN命令的介绍。\n今天我们主要从底层的结构和源码的角度来讨论scan是如何工作的。\nRedis的结构Redis使用了Hash表作为底层实现，原因不外乎高效且实现简单。说到Hash表，很多Java程序员第一反应就是HashMap。没错，Redis底层key的存储结构就是类似于HashMap那样数组+链表的结构。其中第一维的数组大小为2n(n&gt;=0)。每次扩容数组长度扩大一倍。\nscan命令就是对这个一维数组进行遍历。每次返回的游标值也都是这个数组的索引。limit参数表示遍历多少个数组的元素，将这些元素下挂接的符合条件的结果都返回。因为每个元素下挂接的链表大小不同，所以每次返回的结果数量也就不同。\nSCAN的遍历顺序关于scan命令的遍历顺序，我们可以用一个小栗子来具体看一下。\n127.0.0.1:6379&gt; keys *1) &quot;db_number&quot;2) &quot;key1&quot;3) &quot;myKey&quot;127.0.0.1:6379&gt; scan 0 MATCH * COUNT 11) &quot;2&quot;2) 1) &quot;db_number&quot;127.0.0.1:6379&gt; scan 2 MATCH * COUNT 11) &quot;1&quot;2) 1) &quot;myKey&quot;127.0.0.1:6379&gt; scan 1 MATCH * COUNT 11) &quot;3&quot;2) 1) &quot;key1&quot;127.0.0.1:6379&gt; scan 3 MATCH * COUNT 11) &quot;0&quot;2) (empty list or set)\n我们的Redis中有3个key，我们每次只遍历一个一维数组中的元素。如上所示，SCAN命令的遍历顺序是\n0-&gt;2-&gt;1-&gt;3\n这个顺序看起来有些奇怪。我们把它转换成二进制就好理解一些了。\n00-&gt;10-&gt;01-&gt;11\n我们发现每次这个序列是高位加1的。普通二进制的加法，是从右往左相加、进位。而这个序列是从左往右相加、进位的。这一点我们在redis的源码中也得到印证。\n在dict.c文件的dictScan函数中对游标进行了如下处理\nv = rev(v);v++;v = rev(v);\n意思是，将游标倒置，加一后，再倒置，也就是我们所说的“高位加1”的操作。\n这里大家可能会有疑问了，为什么要使用这样的顺序进行遍历，而不是用正常的0、1、2……这样的顺序呢，这是因为需要考虑遍历时发生字典扩容与缩容的情况（不得不佩服开发者考虑问题的全面性）。\n我们来看一下在SCAN遍历过程中，发生扩容时，遍历会如何进行。加入我们原始的数组有4个元素，也就是索引有两位，这时需要把它扩充成3位，并进行rehash。\n\n原来挂接在xx下的所有元素被分配到0xx和1xx下。在上图中，当我们即将遍历10时，dict进行了rehash，这时，scan命令会从010开始遍历，而000和100（原00下挂接的元素）不会再被重复遍历。\n再来看看缩容的情况。假设dict从3位缩容到2位，当即将遍历110时，dict发生了缩容，这时scan会遍历10。这时010下挂接的元素会被重复遍历，但010之前的元素都不会被重复遍历了。所以，缩容时还是可能会有些重复元素出现的。\nRedis的rehashrehash是一个比较复杂的过程，为了不阻塞Redis的进程，它采用了一种渐进式的rehash的机制。\n/* 字典 */typedef struct dict &#123;    // 类型特定函数    dictType *type;    // 私有数据    void *privdata;    // 哈希表    dictht ht[2];    // rehash 索引    // 当 rehash 不在进行时，值为 -1    int rehashidx; /* rehashing not in progress if rehashidx == -1 */    // 目前正在运行的安全迭代器的数量    int iterators; /* number of iterators currently running */&#125; dict;\n在Redis的字典结构中，有两个hash表，一个新表，一个旧表。在rehash的过程中，redis将旧表中的元素逐步迁移到新表中，接下来我们看一下dict的rehash操作的源码。\n/* Performs N steps of incremental rehashing. Returns 1 if there are still * keys to move from the old to the new hash table, otherwise 0 is returned. * * Note that a rehashing step consists in moving a bucket (that may have more * than one key as we use chaining) from the old to the new hash table, however * since part of the hash table may be composed of empty spaces, it is not * guaranteed that this function will rehash even a single bucket, since it * will visit at max N*10 empty buckets in total, otherwise the amount of * work it does would be unbound and the function may block for a long time. */int dictRehash(dict *d, int n) &#123;    int empty_visits = n*10; /* Max number of empty buckets to visit. */    if (!dictIsRehashing(d)) return 0;    while(n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123;        dictEntry *de, *nextde;        /* Note that rehashidx can&#x27;t overflow as we are sure there are more         * elements because ht[0].used != 0 */        assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx);        while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123;            d-&gt;rehashidx++;            if (--empty_visits == 0) return 1;        &#125;        de = d-&gt;ht[0].table[d-&gt;rehashidx];        /* Move all the keys in this bucket from the old to the new hash HT */        while(de) &#123;            uint64_t h;            nextde = de-&gt;next;            /* Get the index in the new hash table */            h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask;            de-&gt;next = d-&gt;ht[1].table[h];            d-&gt;ht[1].table[h] = de;            d-&gt;ht[0].used--;            d-&gt;ht[1].used++;            de = nextde;        &#125;        d-&gt;ht[0].table[d-&gt;rehashidx] = NULL;        d-&gt;rehashidx++;    &#125;    /* Check if we already rehashed the whole table... */    if (d-&gt;ht[0].used == 0) &#123;        zfree(d-&gt;ht[0].table);        d-&gt;ht[0] = d-&gt;ht[1];        _dictReset(&amp;d-&gt;ht[1]);        d-&gt;rehashidx = -1;        return 0;    &#125;    /* More to rehash... */    return 1;&#125;\n通过注释我们就能了解到，rehash的过程是以bucket为基本单位进行迁移的。所谓的bucket其实就是我们前面所提到的一维数组的元素。每次迁移一个列表。下面来解释一下这段代码。\n\n首先判断一下是否在进行rehash，如果是，则继续进行；否则直接返回。\n接着就是分n步开始进行渐进式rehash。同时还判断是否还有剩余元素，以保证安全性。\n在进行rehash之前，首先判断要迁移的bucket是否越界。\n然后跳过空的bucket，这里有一个empty_visits变量，表示最大可访问的空bucket的数量，这一变量主要是为了保证不过多的阻塞Redis。\n接下来就是元素的迁移，将当前bucket的全部元素进行rehash，并且更新两张表中元素的数量。\n每次迁移完一个bucket，需要将旧表中的bucket指向NULL。\n最后判断一下是否全部迁移完成，如果是，则收回空间，重置rehash索引，否则告诉调用方，仍有数据未迁移。\n\n由于Redis使用的是渐进式rehash机制，因此，scan命令在需要同时扫描新表和旧表，将结果返回客户端。\n","tags":["Redis"]},{"title":"玩转Redis集群之Codis","url":"/2018/11/14/%E7%8E%A9%E8%BD%ACRedis%E9%9B%86%E7%BE%A4%E4%B9%8BCodis/","content":"近几年，随着互联网的飞速发展，作为程序员，我们需要处理的数据规模也在不断扩大。如果你使用Redis作为数据库时，面临大数据高并发的场景时，单个Redis实例就会显得力不从心。这时Redis的集群方案应运而生，他将众多Redis实例综合起来，共同应对大数据高并发的场景。\nCodis是Redis集群方案的一种。它是由豌豆荚的中间件团队开发的，所以，它有一套详细的中文版README，方便大家学习。\n\n它的架构如上图所示，由codis-proxy对外提供Redis的服务。ZooKeeper用来存储数据路由表和codis-proxy节点的元信息。codis-proxy会监听所有的redis集群，当Redis集群处理能力达到上限时，可以动态增加Redis实例来实现扩容的需求。\n组件介绍\nCodis Proxy：像刚才所说的，它对外提供Redis服务，除了一些不支持的命令外（不支持的命令列表），表现的和原生的Redis没有区别。由于它是无状态的，所以我们可以部署多个节点，从而保证了可用性。\nCodis Dashboard：集群管理工具，支持Codis Proxy的添加删除以及数据迁移等操作。对于一个Codis集群，Dashboard最多部署一个\nCodis Admin：集群管理的命令行工具\nCodis FE：集群管理界面，多个Codis集群可以共用一个Codis FE，通过配置文件管理后端的codis-dashboard\nStorage：为集群提供外部存储，目前支持ZooKeeper、Etcd、Fs三种。\nCodis Server：基于3.2.8分支开发，增加额外的数据结构，用来支持slot有关的操作及数据迁移指令。\n\nCodis分片原理现在我们已经知道了Codis会将指定key的Redis命令转发给下层的Redis。那么Codis如何知道某个key在哪个Redis上呢。\nCodis采用Pre-sharding的技术来实现数据分片，默认分为1024个slot（0-1023）。Codis在接收到命令时，先对key进行crc32运算，然后再对1024取余，得到的结果就是对应的slot。然后就可以将命令转发给slot对应的Redis实例进行处理了。\n扩容操作Codis的动态扩容/缩容能力是它的一大亮点之一。它可以对Redis客户端透明。在扩容时，Codis提供了SLOTSSCAN指令，这个指令可以扫描指定的slot上的所有key，然后对每个key进行迁移。在扩容过程中，如果有新的key需要转发到正在迁移的slot上，那么codis会判断这个key是否需要迁移，如果需要，则对指定的key进行强制迁移，迁移完成后，再将命令转发到新的Redis上。\n看了上面的介绍是不是觉得扩容是一件很麻烦的事情，Codis已经为我们考虑到这点了，它提供了自动均衡的功能，只需要在界面上点一下”Auto Rebalance”按钮，就可以自动实现slot迁移（可以说非常贴心了）。缩容也比较简单，只需要将需要下线的实例的slot迁移到其他实例上，然后删除group就可以了。\nCodis的缺点当Redis Group的master挂掉时，codis不会自动将某个slave升为master，codis提供了一个叫做codis-ha的工具，这个工具通过dashboard提供RESTful API来实现自动主从切换。但是，当codis将某个slave升为master时，其他的slave并不会改变状态，仍然会从旧的master上同步数据，这就导致了主从数据不一致。因此，当出现主从切换时，需要管理员手动创建新的sync action来完成数据同步。\n此外，Codis还面临一个比较尴尬的情况就是，由于它不是Redis“亲生”的，因此，当Redis发布了new feature时，它总会慢一步，因此，它需要在Redis发布new feature后迅速赶上，以保持竞争力。\n搭建Codis\n安装Go运行环境\n\nMac用户可以参考这个，其他系统的用户也可以看这个教程。\n安装好以后，验证一下是否安装成功\n$ go versiongo version go1.11.2 darwin/amd64\n\n下载Codis源码\n\n需要下载到指定目录：$GOPATH/src/github.com/CodisLabs/codis\n$ mkdir -p $GOPATH/src/github.com/CodisLabs$ cd $_ &amp;&amp; git clone https://github.com/CodisLabs/codis.git -b release3.2\n\n编译源码\n\n进入源码的codis目录，直接执行make命令即可。编译完成后，bin目录下的结构应该是这样的\n$ ll bin total 178584drwxr-xr-x  8 jackey  staff   256B 11 13 10:57 assets-rwxr-xr-x  1 jackey  staff    17M 11 13 10:57 codis-admin-rwxr-xr-x  1 jackey  staff    18M 11 13 10:56 codis-dashboard-rw-r--r--  1 jackey  staff     5B 11 21 18:06 codis-dashboard.pid-rwxr-xr-x  1 jackey  staff    16M 11 13 10:57 codis-fe-rw-r--r--  1 jackey  staff     5B 11 21 18:24 codis-fe.pid-rwxr-xr-x  1 jackey  staff    15M 11 13 10:57 codis-ha-rwxr-xr-x  1 jackey  staff    19M 11 13 10:57 codis-proxy-rw-r--r--  1 jackey  staff     5B 11 21 18:08 codis-proxy.pid-rwxr-xr-x  1 jackey  staff   1.1M 11 13 10:56 codis-server-rwxr-xr-x  1 jackey  staff    98K 11 13 10:56 redis-benchmark-rwxr-xr-x  1 jackey  staff   161K 11 13 10:56 redis-cli-rwxr-xr-x  1 jackey  staff   1.1M 11 13 10:56 redis-sentinel-rw-r--r--  1 jackey  staff   170B 11 13 10:56 version\n到这里为止，我们的准备工作已经完成了。接下来我们来看一下如何在单机环境启动测试集群。\n\n启动codis-dashboard\n\n进入admin目录，执行codis-dashboard-admin.sh脚本\n$ ./codis-dashboard-admin.sh start/Users/jackey/Documents/go_workspace/src/github.com/CodisLabs/codis/admin/../config/dashboard.tomlstarting codis-dashboard ... \n然后查看日志，观察是否启动成功\n$ tail -100 ../log/codis-dashboard.log.2018-11-212018/11/21 18:06:57 main.go:155: [WARN] option --pidfile = /Users/jackey/Documents/go_workspace/src/github.com/CodisLabs/codis/bin/codis-dashboard.pid2018/11/21 18:06:57 topom.go:429: [WARN] admin start service on [::]:180802018/11/21 18:06:57 fsclient.go:195: [INFO] fsclient - create /codis3/codis-demo/topom OK2018/11/21 18:06:58 topom_sentinel.go:169: [WARN] rewatch sentinels = []2018/11/21 18:06:58 main.go:179: [WARN] [0xc000374120] dashboard is working ...\n\n启动codes-proxy\n\n执行codis-proxy-admin.sh脚本\n$ ./codis-proxy-admin.sh start                   /Users/jackey/Documents/go_workspace/src/github.com/CodisLabs/codis/admin/../config/proxy.tomlstarting codis-proxy ...\n查看是否启动成功\n$ tail -100 ../log/codis-proxy.log.2018-11-212018/11/21 18:08:34 proxy_api.go:44: [WARN] [0xc0003262c0] API call /api/proxy/start/212d13827c84455d487036d4bb07ce15 from 10.1.201.43:58800 []2018/11/21 18:08:34 proxy_api.go:44: [WARN] [0xc0003262c0] API call /api/proxy/sentinels/212d13827c84455d487036d4bb07ce15 from 10.1.201.43:58800 []2018/11/21 18:08:34 proxy.go:293: [WARN] [0xc0003262c0] set sentinels = []2018/11/21 18:08:34 main.go:343: [WARN] rpc online proxy seems OK2018/11/21 18:08:35 main.go:233: [WARN] [0xc0003262c0] proxy is working ...\n\n启动codis-server\n\n执行codis-server-admin.sh脚本\n$ ./codis-server-admin.sh start/Users/jackey/Documents/go_workspace/src/github.com/CodisLabs/codis/admin/../config/redis.confstarting codis-server ... \n查看是否启动成功\n$ tail -100 /tmp/redis_6379.log12854:M 21 Nov 18:09:29.172 * Increased maximum number of open files to 10032 (it was originally set to 256).                _._                                                             _.-``__ &#x27;&#x27;-._                                                   _.-``    `.  `_.  &#x27;&#x27;-._           Redis 3.2.11 (de1ad026/0) 64 bit  .-`` .-```.  ```\\/    _.,_ &#x27;&#x27;-._                                    (    &#x27;      ,       .-`  | `,    )     Running in standalone mode |`-._`-...-` __...-.``-._|&#x27;` _.-&#x27;|     Port: 6379 |    `-._   `._    /     _.-&#x27;    |     PID: 12854  `-._    `-._  `-./  _.-&#x27;    _.-&#x27;                                    |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;|                                   |    `-._`-._        _.-&#x27;_.-&#x27;    |           http://redis.io          `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27;                                    |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;|                                   |    `-._`-._        _.-&#x27;_.-&#x27;    |                                    `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27;                                         `-._    `-.__.-&#x27;    _.-&#x27;                                                 `-._        _.-&#x27;                                                         `-.__.-&#x27;                                               12854:M 21 Nov 18:09:29.187 # Server started, Redis version 3.2.1112854:M 21 Nov 18:09:29.187 * The server is now ready to accept connections on port 6379\n如果执行报错，请先确认使用的用户是否有/tmp/redis_6379.log文件的读写权限。\n这里我为了测试Codis的Auto Rebalance功能，所以启动了两个实例。方法很简单，只需要分别将admin/codis-server-admin.sh和config/redis.conf这两个文件复制一份，修改文件中的端口等信息，然后再以同样的方法执行一下新的脚本。\n\n启动codis-fe\n\n执行codis-fe-admin.sh脚本\n$ ./codis-fe-admin.sh startstarting codis-fe ... \n查看是否执行成功\n$ tail -100 ../log/codis-fe.log.2018-11-21 2018/11/21 18:24:33 main.go:101: [WARN] set ncpu = 42018/11/21 18:24:33 main.go:104: [WARN] set listen = 0.0.0.0:90902018/11/21 18:24:33 main.go:120: [WARN] set assets = /Users/jackey/Documents/go_workspace/src/github.com/CodisLabs/codis/bin/assets2018/11/21 18:24:33 main.go:162: [WARN] set --filesystem = /tmp/codis2018/11/21 18:24:33 main.go:216: [WARN] option --pidfile = /Users/jackey/Documents/go_workspace/src/github.com/CodisLabs/codis/bin/codis-fe.pid\n全部启动成功之后，就可以访问http://127.0.0.1:9090，开始设置集群了。\n\n添加group\n\n刚刚我们启动了两个codis-server，因此，我们可以new两个group，然后分别将codis-server加入到两个group中\n\n\n初始化slot\n\n一开始所有的slot都是offline状态。\n\n点击下方的Rebalance All Slots按钮，codis会自动把1024个slot分配给两个group（每个分512个）。\n\n当然，也可以手动分配slot，比如，我们将group-1的10个slot分配给group-2，只需要点击Migrate Some按钮即可。\n\n小结Codis的动态扩容能力简直好用到爆 ，不过目前也存在一些问题（前面我们也介绍过了）。所以你的集群是否要使用Codis还需要看具体的需求。最后还是要为Codis的开发团队点赞，另外他们还开发出了一套分布式数据库——TiDB。有兴趣的同学可以学习一下。\n","tags":["Redis"]},{"title":"玩转Redis集群之Sentinel","url":"/2018/11/04/%E7%8E%A9%E8%BD%ACRedis%E9%9B%86%E7%BE%A4%E4%B9%8BSentinel/","content":"Redis作为内存数据库，需要具备高可用的特点，不然如果服务器宕机，还在内存里的数据就会丢失。我们最常用的高可用方法就是搭建集群，master机器挂了，可以让slave机器顶上，继续提供服务。但是Redis集群是不会自动进行主从切换的，也就是说，如果主节点非常不争气的在凌晨3点挂了，那么运维同学就要马上起床，把从节点改成主节点，这样的操作是非常繁琐低效的。为此，Redis官方提供了一种解决方案：Redis Sentinel\n简介Redis Sentinel集群通常由3到5个节点组成，如果个别节点挂了，集群还可以正常运作。它负责监控Redis集群的健康情况。如果主节点挂掉，Sentinel集群会通过投票选择一个新的主节点。当原来的主节点恢复时，它会被当做新的主节点的从节点重新加入Redis集群。\n基本原理Sentinel集群通过指定的配置文件发现master，对其进行监控，并且会发送info指令获取master的从节点信息。Sentinel集群中的节点通过向其监控的主从节点发送hello信息（包含Sentinel本身的ip、端口和id等内容）来向其他Sentinel宣告自己的存在。\nSentinel集群通过订阅连接来接收其他Sentinel的hello信息。\nSentinel集群通过ping命令来检查监控的实例状态，如果在指定时间内没有返回，则认为该实例下线。\nSentinel触发failover主从切换后，并不会马上进行，只有指定(quorum)Sentinel授权后，master节点被标记为ODOWN状态。这时才真正开始投票选择新的master。\nSentinel选择新的master的原则是：首先判断优先级，选择优先级较小的；如果优先级相同，查看复制下标，选择复制数据较多的；如果复制下标也相同，就选择进程ID较小的。\nSentinel被授权后，它将会获得宕掉的master的一份最新配置版本号(config-epoch)，当failover执行结束以后，这个版本号将会被用于最新的配置，通过广播形式通知其它Sentinel，其它的Sentinel则更新对应master的配置。\n基本使用我们以Python为例，简单说明一下在客户端如何使用Sentinel\nfrom redis.sentinel import Sentinelif __name__ == &#x27;__main__&#x27;:    sentinel = Sentinel([&#x27;localhost&#x27;, 26379], socket_timeout=0.1)    print(sentinel.discover_master(&#x27;mymaster&#x27;))    print(sentinel.discover_slaves(&#x27;mymaster&#x27;))    master = sentinel.master_for(&#x27;mymaster&#x27;, socket_timeout=0.1)    slave = sentinel.slave_for(&#x27;mymaster&#x27;, socket_timeout=0.1)    master.set(&#x27;follow&#x27;, &#x27;Jackeyzhe2018&#x27;)    follow = slave.get(&#x27;follow&#x27;)    print(follow)\nmaster_for和slave_for方法会从连接池中拿出一个连接来使用，如果从地址有多个，则会采用轮询的方法。\n当redis发生了主从切换时，客户端如何知道地址已经变更了呢？我们从redis-py的源码里找一找答案。\n\n\n可以看到，redis在创建一个新的连接时，会调用get_master_address方法来获取主节点地址。get_master_address方法中，客户端先查询主节点地址，然后与内存中的地址进行比较。如果不一致，则会断开连接，然后使用新的地址重新进行连接。\n如果主节点没有挂，而Sentinel主动进行了主从切换，对于这种情况redis-py也做了处理。就是捕获一个ReadOnlyError的异常，然后断开连接，后续指令都需要重新进行连接了。当然，如果没有修改性指令，那么连接就不会切换，不过数据也不会被破坏，所以影响不大。\n动手搭建关于Sentinel的工作原理和使用方法我们已经有了大概的认识，为了加深理解，我们来自己动手搭建一套Sentinel集群。\n首先搭建我们我需要的redis集群环境\n安装好redis后，将redis目录下的配置文件redis.conf复制3份。分别命名为redis6379.conf，redis6380.conf，redis6381.conf。\n在redis6381.conf文件中修改以下几项\nbind 127.0.0.1port 6381logfile &quot;6381.log&quot;dbfilename &quot;dump-6381.rdb&quot;\n在redis6379.conf中修改\nbind 127.0.0.1port 6379logfile &quot;6379.log&quot;dbfilename &quot;dump-6379.rdb&quot;slaveof 127.0.0.1 6381\nredis6380.conf的修改参照redis6379.conf。修改完成后，分别启动三个实例。就搭建好了我们想要的redis主从环境了。\n\n我们连接上master节点，可以看到它的主从配置信息\n\n接着，我们来配置Sentinel集群。这里我们同样配置三个实例。复制3份sentinel.conf文件，分别命名为sentinel-26379.conf，sentinel-26380.conf和sentinel-26381.conf。\nsentinel-26379.conf文件中编辑以下内容\nport 26379  daemonize yes  logfile &quot;26379.log&quot;  dir /home/xxx/redis/data  sentinel monitor mymaster 127.0.0.1 6381 2sentinel down-after-milliseconds mymaster 30000  sentinel parallel-syncs mymaster 1  sentinel failover-timeout mymaster 180000\nsentinel-26380.conf和sentinel-26381.conf的内容与上述类似。配置好后，我们使用命令redis-sentinel来启动3个sentinel实例。\n\n此时，我们用redis-cli命令连接26379的实例，查看sentinel的信息。\n\n发现它已经开始监控我们的3个redis节点了。这时我们的整个集群就部署好了，接下来测试一下。\nkill掉master节点，查看sentinel的日志，会发现sentinel已经按照我们前面说的步骤选择了新的master。\n\n此时再来看sentinel信息。\n\n此时，6380已经成了新的master。\n恭喜你，以后都不需要在凌晨起床切换Redis主从实例了。\n","tags":["Redis"]},{"title":"聊聊Git原理","url":"/2018/08/15/%E8%81%8A%E8%81%8AGit%E5%8E%9F%E7%90%86/","content":"说起Git，相信大家都很熟悉了，毕竟作为程序猿，每天的业余时间除了吃饭睡觉就是逛一下全世界最大的开（tong）源（xing）代（jiao）码（you）网站GitHub了。在那里Git是每个人所要具备的最基本的技能。今天我们不聊Git的基本应用，来聊一聊Git的原理。\nGit给自己的定义是一套内存寻址文件系统，当你在一个目录下执行git init命令时，会生成一个.git目录，它的目录结构是这样的：\n.git/├── branches├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   └── update.sample├── info│   └── exclude├── objects│   ├── info│   └── pack└── refs    ├── heads    └── tags\n其中branches目录已经不再使用，description文件仅供GitWeb程序使用，config文件保存了项目的配置。\n需要我们重点关注的是HEAD和index文件以及objects和refs目录。其中index中保存了暂存区的一些信息，这里不做过多介绍。\nobjects目录这个目录是用来存储Git对象的（包括tree对象、commit对象和blob对象），对于一个初始的Git仓库，objects目录下只有info和pack两个子目录，并没有常规文件。随着项目的进行，我们创建的文件，以及一些操作记录，都会作为Git对象被存储在这个目录下。\n在该目录下，所有对象都会生成一个文件，并且有对应的SHA-1校验和，Git会创建以校验和前两位为名称的子目录，并以剩下的38位为名称来保存文件。\n接下来让我们一起看一下当我们进行一次提交时，Git具体做了哪些事情。\n$ echo &#x27;test content&#x27;&gt;test.txt$ git add .\n执行上述命令后，objects目录结构如下：\n.git/objects/├── d6│   └── 70460b4b4aece5915caf5c68d12f560a9fe3e4├── info└── pack\n这里多了一个文件夹，如上面所述，这个就是Git为我们创建的一个对象，我们可以使用底层命令来看一下这个对象的类型以及它存储的是什么。\n$ git cat-file -t d670460b4b4aece5915caf5c68d12f560a9fe3e4blob$ git cat-file -p d670460b4b4aece5915caf5c68d12f560a9fe3e4test content\n可以看到，这是一个blob对象，存储内容就是我们刚刚创建的文件的内容。接下来继续执行提交操作。\n$ git commit -m &#x27;test message&#x27;[master (root-commit) 2b00dca] test message 1 file changed, 1 insertion(+) create mode 100644 test.txt $ tree .git/objects/.git/objects/├── 2b│   └── 00dcae50af70bb5722033b3fe75281206c74da├── 80│   └── 865964295ae2f11d27383e5f9c0b58a8ef21da├── d6│   └── 70460b4b4aece5915caf5c68d12f560a9fe3e4├── info└── pack\n此时objects目录下又多了两个对象。再用cat-file命令来查看一下这两个文件。\n$ git cat-file -t 2b00dcae50af70bb5722033b3fe75281206c74dacommit$ git cat-file -p 2b00dcae50af70bb5722033b3fe75281206c74datree 80865964295ae2f11d27383e5f9c0b58a8ef21daauthor jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534670725 +0800committer jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534670725 +0800test message$ git cat-file -t 80865964295ae2f11d27383e5f9c0b58a8ef21datree$ git cat-file -p 80865964295ae2f11d27383e5f9c0b58a8ef21da100644 blob d670460b4b4aece5915caf5c68d12f560a9fe3e4\ttest.txt\n可以看到一个是commit对象，一个是tree对象。commit对象通常包括4部分内容：\n\n工作目录快照的Hash，即tree的值\n提交的说明信息\n提交者的信息\n父提交的Hash值\n\n由于我是第一次提交，所以这里没有父提交的Hash值。\ntree对象可以理解为UNIX文件系统中的目录，保存了工作目录的tree对象和blob对象的信息。接下来我们再来看一下Git是如何进行版本控制的。\necho &#x27;version1&#x27;&gt;version.txt$ git add .$ git commit -m &#x27;first version&#x27;[master 702193d] first version 1 file changed, 1 insertion(+) create mode 100644 version.txt$ echo &#x27;version2&#x27;&gt;version.txt$ git add .$ git commit -m &#x27;second version&#x27;[master 5333a75] second version 1 file changed, 1 insertion(+), 1 deletion(-)$ tree .git/objects/.git/objects/├── 1f│   └── a5aab2a3cf025d06479b9eab9a7f66f60dbfc1├── 29│   └── 13bfa5cf9fb6f893bec60ac11d86129d56fcbe├── 2b│   └── 00dcae50af70bb5722033b3fe75281206c74da├── 53│   └── 33a759c4bdcdc6095b4caac19743d9445ca516├── 5b│   └── dcfc19f119febc749eef9a9551bc335cb965e2├── 70│   └── 2193d62ffd797155e4e21eede20897890da12a├── 80│   └── 865964295ae2f11d27383e5f9c0b58a8ef21da├── d6│   └── 70460b4b4aece5915caf5c68d12f560a9fe3e4├── df│   └── 7af2c382e49245443687973ceb711b2b74cb4a├── info└── pack$ git cat-file -p 1fa5aab2a3cf025d06479b9eab9a7f66f60dbfc1100644 blob d670460b4b4aece5915caf5c68d12f560a9fe3e4\ttest.txt100644 blob 5bdcfc19f119febc749eef9a9551bc335cb965e2\tversion.txt$ git cat-file -p 2913bfa5cf9fb6f893bec60ac11d86129d56fcbe100644 blob d670460b4b4aece5915caf5c68d12f560a9fe3e4\ttest.txt100644 blob df7af2c382e49245443687973ceb711b2b74cb4a\tversion.txt\nGit将没有改变的文件的Hash值直接存入tree对象，对于有修改的文件，则会生成一个新的对象，将新的对象存入tree对象。我们再来看一下commit对象的信息。\n$ git cat-file -p 5333a759c4bdcdc6095b4caac19743d9445ca516tree 2913bfa5cf9fb6f893bec60ac11d86129d56fcbeparent 702193d62ffd797155e4e21eede20897890da12aauthor jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672270 +0800committer jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672270 +0800second version$ git cat-file -p 702193d62ffd797155e4e21eede20897890da12atree 1fa5aab2a3cf025d06479b9eab9a7f66f60dbfc1parent 2b00dcae50af70bb5722033b3fe75281206c74daauthor jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672248 +0800committer jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672248 +0800first version\n此时的commit对象已经有parent信息了，这样我们就可以顺着parent一步步往回进行版本回退了。不过这样是比较麻烦的，我们一般习惯用的是git log查看提交记录。\nrefs目录在介绍refs目录之前，我们还是先来看一下该目录结构\n$ tree .git/refs/.git/refs/├── heads│   └── master└── tags2 directories, 1 file$ cat .git/refs/heads/master 5333a759c4bdcdc6095b4caac19743d9445ca516\n在一个刚刚被初始化的Git仓库中，refs目录下只有heads和tags两个子目录，由于我们刚刚有过提交操作，所以git为我们自动生成了一个名为master的引用。master的内容是最后一次提交对象的Hash值。看到这里大家一定在想，如果我们对每次提交都创建一个这样的引用，不就不需要记住每次提交的Hash值了，只要看看引用的值，复制过来就可以退回到对应版本了。没错，这样是可以方便的退回，但是这样做的意义不大，因为我们并不需要频繁的退回，特别是比较古老的版本，退回的概率更是趋近于0。Git用这个引用做了更有意义的事，那就是分支。\n当我新建一个分支时，git就会在.git/refs/heads目录下新建一个文件。当然新建的引用还是指向当前工作目录的最后一次提交，一般情况下我们不会主动去修改这些引用文件，不过如果一定要修改，Git为我们提供了一个update-ref命令。可以改变引用的值，使其指向不同的commit对象。\ntags目录下的文件存储的是标签对应的commit，当为某次提交打上一个tag时，tags目录下就会被创建出一个命名为tag名的文件，值是此次提交的Hash值。\nHEAD新建分支的时候，Git是怎么知道我们当前是在哪个分支的，Git又是如何实现分支切换的呢？答案就在HEAD这个文件中。\n$ cat .git/HEAD ref: refs/heads/master$ git checkout test Switched to branch &#x27;test&#x27;$ cat .git/HEAD ref: refs/heads/test\n很明显，HEAD文件存储的就是我们当前分支的引用，当我们切换分支后再次进行提交操作时，Git就会读取HEAD对应引用的值，作为此次commit的parent。我们也可以通过symbolic-ref命令手动设置HEAD的值，但是不能设置refs以外的形式。\nPackfiles到这里我们在文章开头所说的重点关注的目录和文件都介绍完毕了。但是作为一个文件系统，还存在一个问题，那就是空间。前文介绍过，当文件修改后进行提交时，Git会创建一份新的快照。这样长久下去，必定会占用很大的存储空间。而比较古老的版本的价值已经不大，所以要想办法清理出足够的空间供用户使用。\n好消息是，Git拥有自己的gc（垃圾回收）方法。当仓库中有太多松散对象时，Git会调用git gc命令（当然我们也可以手动调用这个命令），将这些对象进行打包。打包后会出现两个新文件：一个idx索引文件和一个pack文件。索引文件包含了packfile的偏移信息，可以快速定位到文件。打包后，每个文件最新的版本的对象存的是完整的文件内容。而之前的版本只保存差异。这样就达到了压缩空间的目的。\nEnding本文只介绍了Git的原理，如果对Git基本操作不熟悉的话，可以看看Pro Git。\n","tags":["git"]},{"title":"走近源码：Redis命令执行过程（客户端）","url":"/2019/01/12/%E8%B5%B0%E8%BF%91%E6%BA%90%E7%A0%81%EF%BC%9ARedis%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%EF%BC%88%E5%AE%A2%E6%88%B7%E7%AB%AF%EF%BC%89/","content":"前面我们了解过了当Redis执行一个命令时，服务端做了哪些事情，不了解的同学可以看一下这篇文章走近源码：Redis如何执行命令。今天就一起来看看Redis的命令执行过程中客户端都做了什么事情。\n启动客户端首先看redis-cli.c文件的main函数，也就是我们输入redis-cli命令时所要执行的函数。main函数主要是给config变量的各个属性设置默认值。比如：\n\nhostip：要连接的服务端的IP，默认为127.0.0.1\nhostport：要连接的服务端的端口，默认为6379\ninteractive：是否是交互模式，默认为0（非交互模式）\n一些模式的设置，例如：cluster_mode、slave_mode、getrdb_mode、scan_mode等\ncluster相关的参数\n\n……\n接着调用parseOptions()函数来处理参数，例如-p、-c、–verbose等一些用来指定config属性的（可以输入redis-cli –help查看）或是指定启动模式的。\n处理完这些参数后，需要把它们从参数列表中去除，剩下用于在非交互模式中执行的命令。\nparseEnv()用来判断是否需要验证权限，紧接着就是根据刚才的参数判断需要进入哪种模式，是cluster还是slave又或者是RDB……如果没有进入这些模式，并且没有需要执行的命令，那么就进入交互模式，否则会进入非交互模式。\n/* Start interactive mode when no command is provided */if (argc == 0 &amp;&amp; !config.eval) &#123;    /* Ignore SIGPIPE in interactive mode to force a reconnect */    signal(SIGPIPE, SIG_IGN);    /* Note that in repl mode we don&#x27;t abort on connection error.     * A new attempt will be performed for every command send. */    cliConnect(0);    repl();&#125;/* Otherwise, we have some arguments to execute */if (cliConnect(0) != REDIS_OK) exit(1);if (config.eval) &#123;    return evalMode(argc,argv);&#125; else &#123;    return noninteractive(argc,convertToSds(argc,argv));&#125;\n连接服务器cliConnect()函数用于连接服务器，它的参数是一个标志位，如果是CC_FORCE（0）表示强制重连，如果是CC_QUIET（2）表示不打印错误日志。\n如果建立了socket，那么就连接这个socket，否则就去连接指定的IP和端口。\nif (config.hostsocket == NULL) &#123;    context = redisConnect(config.hostip,config.hostport);&#125; else &#123;    context = redisConnectUnix(config.hostsocket);&#125;\nredisConnectredisConnect()（在deps/hiredis/hiredis.c文件中）函数用于连接指定的IP和端口的redis实例。它的返回值是redisContext类型的。这个结构封装了一些客户端与服务端之间的连接状态，obuf是用来存放返回结果的缓冲区，同时还有客户端与服务端的协议。\n//hiredis.h/* Context for a connection to Redis */typedef struct redisContext &#123;    int err; /* Error flags, 0 when there is no error */    char errstr[128]; /* String representation of error when applicable */    int fd;    int flags;    char *obuf; /* Write buffer */    redisReader *reader; /* Protocol reader */    enum redisConnectionType connection_type;    struct timeval *timeout;    struct &#123;        char *host;        char *source_addr;        int port;    &#125; tcp;    struct &#123;        char *path;    &#125; unix_sock;&#125; redisContext;\nredisConnect的实现比较简单，首先初始化一个redisContext变量，然后把客户端的flags字段设置为阻塞状态，接着调用redisContextConnectTcp命令。\nredisContext *redisConnect(const char *ip, int port) &#123;    redisContext *c;    c = redisContextInit();    if (c == NULL)        return NULL;    c-&gt;flags |= REDIS_BLOCK;    redisContextConnectTcp(c,ip,port,NULL);    return c;&#125;\nredisContextConnectTcpredisContextConnectTcp()函数在net.c文件中，它调用的是_redisContextConnectTcp()这个函数，所以我们主要关注这个函数。它用来与服务端创建TCP连接，首先调整了tcp的host和timeout字段，然后getaddrinfo获取要连接的服务信息，这里兼容了IPv6和IPv4。然后尝试连接服务端。\nif (connect(s,p-&gt;ai_addr,p-&gt;ai_addrlen) == -1) &#123;    if (errno == EHOSTUNREACH) &#123;        redisContextCloseFd(c);        continue;    &#125; else if (errno == EINPROGRESS &amp;&amp; !blocking) &#123;        /* This is ok. */    &#125; else if (errno == EADDRNOTAVAIL &amp;&amp; reuseaddr) &#123;        if (++reuses &gt;= REDIS_CONNECT_RETRIES) &#123;            goto error;        &#125; else &#123;            redisContextCloseFd(c);            goto addrretry;        &#125;    &#125; else &#123;        if (redisContextWaitReady(c,timeout_msec) != REDIS_OK)            goto error;    &#125;&#125;\nconnect()函数用于去连接服务器，连接上之后，服务器端会调用accept函数。如果连接失败，也会根据情况决定是否要关闭redisContext文件描述符。\n发送命令并接收返回当客户端和服务端建立连接之后，客户端向服务器端发送命令并接收返回值了。\nrepl我们回到redis-cli.c文件中的repl()函数，这个函数就是用来向服务器端发送命令并且接收到的结果返回。\n这里首先调用了cliInitHelp()和cliIntegrateHelp()这两个函数，初始化了一些帮助信息，然后设置了一些回调的方法。如果是终端模式，则会从rc文件中加载历史命令。然后调用linenoise()函数读取用户输入的命令，并以空格分隔参数。\nnread = read(l.ifd,&amp;c,1);\n接下来是判断是否需要过滤掉重复的参数。\nissueCommandRepeat生成好命令后，就调用issueCommandRepeat()函数开始执行命令。\nstatic int issueCommandRepeat(int argc, char **argv, long repeat) &#123;    while (1) &#123;        config.cluster_reissue_command = 0;        if (cliSendCommand(argc,argv,repeat) != REDIS_OK) &#123;            cliConnect(CC_FORCE);            /* If we still cannot send the command print error.             * We&#x27;ll try to reconnect the next time. */            if (cliSendCommand(argc,argv,repeat) != REDIS_OK) &#123;                cliPrintContextError();                return REDIS_ERR;            &#125;         &#125;         /* Issue the command again if we got redirected in cluster mode */         if (config.cluster_mode &amp;&amp; config.cluster_reissue_command) &#123;            cliConnect(CC_FORCE);         &#125; else &#123;             break;        &#125;    &#125;    return REDIS_OK;&#125;\n这个函数会调用cliSendCommand()函数，将命令发送给服务器端，如果发送失败，会强制重连一次，然后再次发送命令。\nredisAppendCommandArgvcliSendCommand()函数又会调用redisAppendCommandArgv()函数（在hiredis.c文件中）这个函数是按照Redis协议将命令进行编码。\ncliReadReply然后调用cliReadReply()函数，接收服务器端返回的结果，调用cliFormatReplyRaw()函数将结果进行编码并返回。\n举个栗子我们以GET命令为例，具体描述一下，从客户端到服务端，程序是如何运行的。\n我们用gdb调试redis-server，将断点设置到readQueryFromClient函数这里。\ngdb src/redis-server GNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-gitCopyright (C) 2018 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-linux-gnu&quot;.Type &quot;show configuration&quot; for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type &quot;help&quot;.Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...Reading symbols from src/redis-server...done.(gdb) b readQueryFromClientBreakpoint 1 at 0x43c520: file networking.c, line 1379.(gdb) run redis.conf\n然后再调试redis-cli，断点设置cliReadReply函数。\ngdb src/redis-cli GNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-gitCopyright (C) 2018 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-linux-gnu&quot;.Type &quot;show configuration&quot; for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type &quot;help&quot;.Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...Reading symbols from src/redis-cli...done.(gdb) b cliReadReplyBreakpoint 1 at 0x40ffa0: file redis-cli.c, line 845.(gdb) run\n在客户端输入get命令，发现程序在断点处停止。\n127.0.0.1:6379&gt; get jackeyBreakpoint 1, cliReadReply (output_raw_strings=output_raw_strings@entry=0)    at redis-cli.c:845845\tstatic int cliReadReply(int output_raw_strings) &#123;\n我们可以看到这时Redis已经准备好将命令发送给服务端了，先来查看一下要发送的内容。\n(gdb) p context-&gt;obuf$1 = 0x684963 &quot;*2\\r\\n$3\\r\\nget\\r\\n$6\\r\\njackey\\r\\n&quot;\n把\\r\\n替换成换行符看的后是这样：\n*2$3get$6jackey\n*2表示命令参数的总数，包括命令的名字，也就是告诉服务端应该处理两个参数。\n$3表示第一个参数的长度。\nget是命令名，也就是第一个参数。\n$6表示第二个参数的长度。\njackey是第二个参数。\n当程序运行到redisGetReply时就会把命令发送给服务端了，这时我们再来看服务端的运行情况。\nThread 1 &quot;redis-server&quot; hit Breakpoint 1, readQueryFromClient (    el=0x7ffff6a41050, fd=7, privdata=0x7ffff6b1e340, mask=1)    at networking.c:13791379\tvoid readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) &#123;(gdb) \n程序调整到\nsdsIncrLen(c-&gt;querybuf,nread);\n这时nread的内容会被加到c-&gt;querybuf中，我们来看一下是不是我们发送过来的命令。\n(gdb) p c-&gt;querybuf$1 = (sds) 0x7ffff6a75cc5 &quot;*2\\r\\n$3\\r\\nget\\r\\n$6\\r\\njackey\\r\\n&quot;\n到这里，Redis的服务端已经接受到请求了。接下来就是处理命令的过程，前文我们提到Redis是在processCommand()函数中处理的。\nprocessCommand()函数会调用lookupCommand()函数，从redisCommandTable表中查询出要执行的函数。然后调用c-&gt;cmd-&gt;proc(c)执行这个函数，这里我们get命令对应的是getCommand函数，getCommand里只是调用了getGenericCommand()函数。\n//t_string.cint getGenericCommand(client *c) &#123;    robj *o;    if ((o = lookupKeyReadOrReply(c,c-&gt;argv[1],shared.null[c-&gt;resp])) == NULL)        return C_OK;    if (o-&gt;type != OBJ_STRING) &#123;        addReply(c,shared.wrongtypeerr);        return C_ERR;    &#125; else &#123;        addReplyBulk(c,o);        return C_OK;    &#125;&#125;\nlookupKeyReadOrReply()用来查找指定key存储的内容。并返回一个Redis对象，它的实现在db.c文件中。\nrobj *lookupKeyReadOrReply(client *c, robj *key, robj *reply) &#123;    robj *o = lookupKeyRead(c-&gt;db, key);    if (!o) addReply(c,reply);    return o;&#125;\n在lookupKeyReadWithFlags函数中，会先判断这个key是否过期，如果没有过期，则会继续调用lookupKey()函数进行查找。\nrobj *lookupKey(redisDb *db, robj *key, int flags) &#123;    dictEntry *de = dictFind(db-&gt;dict,key-&gt;ptr);    if (de) &#123;        robj *val = dictGetVal(de);        /* Update the access time for the ageing algorithm.         * Don&#x27;t do it if we have a saving child, as this will trigger         * a copy on write madness. */        if (server.rdb_child_pid == -1 &amp;&amp;            server.aof_child_pid == -1 &amp;&amp;            !(flags &amp; LOOKUP_NOTOUCH))        &#123;            if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_LFU) &#123;                updateLFU(val);            &#125; else &#123;                val-&gt;lru = LRU_CLOCK();            &#125;        &#125;        return val;    &#125; else &#123;        return NULL;    &#125;&#125;\n在这个函数中，先调用了dictFind函数，找到key对应的entry，然后再从entry中取出val。\n找到val后，我们回到getGenericCommand函数中，它会调用addReplyBulk函数，将返回值添加到client结构的buf字段。\n(gdb) p c-&gt;buf$18 = &quot;$3\\r\\nzhe\\r\\n\\n$8\\r\\nflushall\\r\\n:-1\\r\\n&quot;, &#x27;\\000&#x27; &lt;repeats 16354 times&gt;\n到这里，get命令的处理过程已经完结了，剩下的事情就是将结果返回给客户端，并且等待下次命令。\n客户端收到返回值后，如果是控制台输出，则会调用cliFormatReplyTTY对结果进行解析\n(gdb) n912\t                out = cliFormatReplyTTY(reply,&quot;&quot;);(gdb) n918\t        fwrite(out,sdslen(out),1,stdout);(gdb) p out$5 = (sds) 0x6949b3 &quot;\\&quot;zhe\\&quot;\\n&quot;\n最后将结果输出。\n推荐阅读走近源码：Redis如何执行命令\nMore Redis internals: Tracing a GET &amp; SET\nGDB cheatsheet \n","tags":["Redis"]},{"title":"走近源码：Redis如何执行命令","url":"/2019/01/05/%E8%B5%B0%E8%BF%91%E6%BA%90%E7%A0%81%EF%BC%9ARedis%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/","content":"前文我们了解了Redis的启动过程。在initServer()函数中创建了一些循环事件来监听TCP端口和Unix的Sockets，从而使Redis服务器可以接收新的连接。今天我们再一起来看一下Redis究竟是如何处理命令并返回结果的。\n处理新连接Redis在initServer()函数中创建循环事件调用了acceptTcpHandler和acceptUnixHandler函数（都在networking.c文件中）来处理接收到的TCP连接和Unix的Sockets连接。这两个函数又调用了acceptCommonHandler()函数，在这个函数中调用了createClient()函数创建一个新的client对象，用来表示一个新的客户端连接。\ncreateClient()函数具体做了哪些事情呢？\n首先为变量c分配了内存，接着将Socket连接置为非阻塞状态，并且设置了TCP无延迟。然后创建了File循环事件（aeCreateFileEvent）来调用readQueryFromClient函数。新建的客户端默认连接的是服务器的第一个数据库（编码为0），最后需要设置好客户端的各种属性和状态。\n读一个客户端的命令刚刚我们提到了readQueryFromClient函数，从名称上就能看出来这个函数是用来从客户端读取命令的。下面来看看函数的具体实现。\nRedis会先将命令读入缓冲区，一次最多读取的大小是PROTO_IOBUF_LEN（1024*16）bit。然后调用processInputBufferAndReplicate()函数，来处理缓冲区中的数据，如果客户端是master（主从同步过程），那么Redis会计算处理前后缓冲区的不同部分，以确定从节点接收了多少数据。processInputBufferAndReplicate()函数会处理客户端向服务器发送命令和主节点向从节点发送命令这两种情况，不过最后都需要调用processInputBuffer()函数。\nprocessInputBuffer()函数会先判断客户端是否正常，如果出现连接中断或者客户端阻塞等情况，就会立即停止处理命令，不做无用功。然后根据读取的请求生成相应的Redis可以执行的命令（包括参数）。不同的请求类型分别调用processInlineBuffer()和processMultibulkBuffer()函数。生成好命令之后，交给processCommand()（server.c文件中）函数执行，如果返回C_OK则重置客户端，等待下一个命令。如果返回的是C_ERR，则客户端会被销毁（比如执行QUIT命令）。\nprocessCommand()函数会从Redis启动时加载的命令表中查找命令，然后检查命令的执行权限。\n如果是cluster，这时会判断key是否属于当前的master，不属于需要返回重定向信息。\n如果内存不够用，这里也需要判断一下是否有可以释放的内存，如果没有，就不能执行命令，返回错误信息。\n接下来会判断一些不能接收写命令的情况：\n\n服务器不能进行持久化\n作为master，没有足够的可用的slave\n此服务器为只读的slave，只有它的master可以接收写命令\n\n在订阅模式中，只能接收指定的命令：(P)SUBSCRIBE / (P)UNSUBSCRIBE / PING / QUIT。\n当slave和master失联时，只能接收有flag “t”的命令，例如，INFO，SLAVEOF等。\n如果命令没有CMD_LOADING标志，并且当前服务器正在加载数据，则不能接收此命令。\n对lua脚本的长度进行限制。\n进行完上面的各种条件判断之后，才可以真正开始调用call()函数执行命令。\n执行命令并返回call()函数的参数是client类型的，取出cmd成员进行执行。\n/* Call the command. */dirty = server.dirty;start = ustime();c-&gt;cmd-&gt;proc(c);duration = ustime()-start;dirty = server.dirty-dirty;if (dirty &lt; 0) dirty = 0;\n如果是写命令，就会使服务器变“脏”，也就是服务器需要标记一下内存中的某些页有了改变。这对于Redis的持久化来说非常重要，它可以知道这个命令影响了多少个key。命令执行完之后并没有结束，call函数还会做一些其他操作。例如记录日志，写AOF文件，向从节点同步命令等。\n至于返回值，每个命令有各自的处理方法，我们后面在介绍。\n到这里，Redis处理命令的过程也就完成了。\n后面我们会再通过具体的命令来对这个过程做一个更清晰的介绍。\n","tags":["Redis"]},{"title":"走近源码：Redis如何清除过期key","url":"/2020/03/30/%E8%B5%B0%E8%BF%91%E6%BA%90%E7%A0%81%EF%BC%9ARedis%E5%A6%82%E4%BD%95%E6%B8%85%E9%99%A4%E8%BF%87%E6%9C%9Fkey/","content":"“叮……”，美好的周六就这么被一阵钉钉消息吵醒了。\n业务组的同学告诉我说很多用户的帐号今天被强制下线。我们的帐号系统正常的逻辑是用户登录一次后，token的有效期可以维持一天的时间。现在的问题是用户大概每10分钟左右就需要重新登录一次。这种情况一般有两种原因：1、token生成时出问题。2、验证token时出现问题。\n通过检查日志，我发现是验证token时，Redis中已经没有对应的token了。并且确定了生成新的token时，set到Redis中的有效期是正确的，那么就基本可以确定是Redis的问题了。\n于是又去检查了Redis的监控，发现在那段时间Redis由于内存占用过高强制清理了几次key。但从日志上来看，这段时间并没有出现流量暴涨的情况，而且Redis中key的数量也没有显著增加。那是什么原因导致Redis内存占用过高呢？确定了Redis内存升高不是我们造成的之后，我们又联系了业务组的同学协助他们，他们表示最近确实有上线，并且新上线的功能有使用到Redis。但我仍然感觉很奇怪，为什么Redis中的key没有增多，并且没看到有其他业务的key。经过一番询问，才了解到，业务组同学使用的是这个Redis的db1，而我用的（和刚查的）是db0。这里确实是我在排查问题时出现了疏忽。\n那么Redis的不同db之间会互相影响吗？通常情况下，我们使用不同的db进行数据隔离，这没问题。但Redis进行清理时，并不是只清理数据量占用最大的那个db，而是会对所有的db进行清理。在这之前我并不是很了解这方面知识，这里也只是根据现象进行的猜测。\n好奇心驱使我来验证一下这个想法。于是我决定直接来看Redis的源码。清理key相关的代码在evict.c文件中。\nRedis中会保存一个“过期key池”，这个池子中存放了一些可能会被清理的key。其中保存的数据结构如下：\nstruct evictionPoolEntry &#123;    unsigned long long idle;    /* Object idle time (inverse frequency for LFU) */    sds key;                    /* Key name. */    sds cached;                 /* Cached SDS object for key name. */    int dbid;                   /* Key DB number. */&#125;;\n其中idle是对象空闲时间，在Reids中，key的过期算法有两种：一种是近似LRU，一种是LFU。默认使用的是近似LRU。\n近似LRU在解释近似LRU之前，先来简单了解一下LRU。当Redis的内存占用超过我们设置的maxmemory时，会把长时间没有使用的key清理掉。按照LRU算法，我们需要对所有key（也可以设置成只淘汰有过期时间的key）按照空闲时间进行排序，然后淘汰掉空闲时间最大的那部分数据，使得Redis的内存占用降到一个合理的值。\nLRU算法的缺点是，我们需要维护一个全部（或只有过期时间）key的列表，还要按照最近使用时间排序。这会消耗大量内存，并且每次使用key时更新排序也会占用额外的CPU资源。对于Redis这样对性能要求很高的系统来说是不被允许的。\n因此，Redis采用了一种近似LRU的算法。当Redis接收到新的写入命令，而内存又不够时，就会触发近似LRU算法来强制清理一些key。具体清理的步骤是，Redis会对key进行采样，通常是取5个，然后会把过期的key放到我们上面说的“过期池”中，过期池中的key是按照空闲时间来排序的，Redis会优先清理掉空闲时间最长的key，直到内存小于maxmemory。\n近似LRU算法的清理效果图如图（图片来自Redis官方文档）\n\n这么说可能不够清楚，我们直接上代码。\n源码分析\n上图展示了代码中近似LRU算法的主要逻辑调用路径。\n其中主要逻辑是在freeMemoryIfNeeded函数中\n首先调用getMaxmemoryState函数判断当前内存的状态\nint getMaxmemoryState(size_t *total, size_t *logical, size_t *tofree, float *level) &#123;    size_t mem_reported, mem_used, mem_tofree;    mem_reported = zmalloc_used_memory();    if (total) *total = mem_reported;    int return_ok_asap = !server.maxmemory || mem_reported &lt;= server.maxmemory;    if (return_ok_asap &amp;&amp; !level) return C_OK;    mem_used = mem_reported;    size_t overhead = freeMemoryGetNotCountedMemory();    mem_used = (mem_used &gt; overhead) ? mem_used-overhead : 0;    if (level) &#123;        if (!server.maxmemory) &#123;            *level = 0;        &#125; else &#123;            *level = (float)mem_used / (float)server.maxmemory;        &#125;    &#125;    if (return_ok_asap) return C_OK;    if (mem_used &lt;= server.maxmemory) return C_OK;    mem_tofree = mem_used - server.maxmemory;    if (logical) *logical = mem_used;    if (tofree) *tofree = mem_tofree;    return C_ERR;&#125;\n如果使用内存低于maxmemory的话，就返回C_OK，否则返回C_ERR。另外，这个函数还通过传递指针型的参数来返回一些额外的信息。\n\ntotal：已使用的字节总数，无论是C_OK还是C_ERR都有效。\nlogical：已使用的内存减去slave或AOF缓冲区后的大小，只有返回C_ERR时有效。\ntofree：需要释放的内存大小，只有返回C_ERR时有效。\nlevel：已使用内存的比例，通常是0到1之间，当超出内存限制时，就大于1。无论是C_OK还是C_ERR都有效。\n\n判断完内存状态以后，如果内存没有超过使用限制就会直接返回，否则就继续向下执行。此时我们已经知道需要释放多少内存空间了，下面就开始进行释放内存的操作了。每次释放内存都会记录释放内存的大小，直到释放的内存不小于tofree。\n首先根据maxmemory_policy进行判断，对于不同的清除策略有不同的实现方法，我们来看LRU的具体实现。\nfor (i = 0; i &lt; server.dbnum; i++) &#123;  db = server.db+i;  dict = (server.maxmemory_policy &amp; MAXMEMORY_FLAG_ALLKEYS) ?    db-&gt;dict : db-&gt;expires;  if ((keys = dictSize(dict)) != 0) &#123;    evictionPoolPopulate(i, dict, db-&gt;dict, pool);    total_keys += keys;  &#125;&#125;\n首先是填充“过期池”，这里遍历了每一个db（验证了我最开始的想法），调用evictionPoolPopulate函数进行填充。\nvoid evictionPoolPopulate(int dbid, dict *sampledict, dict *keydict, struct evictionPoolEntry *pool) &#123;    int j, k, count;    dictEntry *samples[server.maxmemory_samples];    count = dictGetSomeKeys(sampledict,samples,server.maxmemory_samples);    for (j = 0; j &lt; count; j++) &#123;        unsigned long long idle;        sds key;        robj *o;        dictEntry *de;        de = samples[j];        key = dictGetKey(de);\t\t\t\t/* some code */        if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_LRU) &#123;            idle = estimateObjectIdleTime(o);        &#125;        /* some code */        k = 0;        while (k &lt; EVPOOL_SIZE &amp;&amp;               pool[k].key &amp;&amp;               pool[k].idle &lt; idle) k++;        if (k == 0 &amp;&amp; pool[EVPOOL_SIZE-1].key != NULL) &#123;            continue;        &#125; else if (k &lt; EVPOOL_SIZE &amp;&amp; pool[k].key == NULL) &#123;        &#125; else &#123;            if (pool[EVPOOL_SIZE-1].key == NULL) &#123;                sds cached = pool[EVPOOL_SIZE-1].cached;                memmove(pool+k+1,pool+k,                    sizeof(pool[0])*(EVPOOL_SIZE-k-1));                pool[k].cached = cached;            &#125; else &#123;                k--;                sds cached = pool[0].cached; /* Save SDS before overwriting. */                if (pool[0].key != pool[0].cached) sdsfree(pool[0].key);                memmove(pool,pool+1,sizeof(pool[0])*k);                pool[k].cached = cached;            &#125;        &#125;        /* some code */    &#125;&#125;\n由于篇幅原因，我截取了部分代码，通过这段代码我们可以看到，Redis首先是采样了一部分key，这里采样数量maxmemory_samples通常是5，我们也可以自己设置，采样数量越大，结果就越接近LRU算法的结果，带来的影响是性能随之变差。\n采样之后我们需要获得每个key的空闲时间，然后将其填充到“过期池”中的指定位置。这里“过期池”是按照空闲时间从小到大排序的，也就是说，idle大大key排在最右边。\n填充完“过期池”之后，会从后向前获取到最适合清理的key。\n/* Go backward from best to worst element to evict. */for (k = EVPOOL_SIZE-1; k &gt;= 0; k--) &#123;  if (pool[k].key == NULL) continue;  bestdbid = pool[k].dbid;  if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_ALLKEYS) &#123;    de = dictFind(server.db[pool[k].dbid].dict,                  pool[k].key);  &#125; else &#123;    de = dictFind(server.db[pool[k].dbid].expires,                  pool[k].key);  &#125;  /* some code */  if (de) &#123;    bestkey = dictGetKey(de);    break;  &#125;&#125;\n找到需要删除的key后，就需要根据设置清理策略进行同步/异步清理。\nif (server.lazyfree_lazy_eviction)  dbAsyncDelete(db,keyobj);else  dbSyncDelete(db,keyobj)\n最后记下本次清理的空间大小，用来在循环条件判断是否要继续清理。\ndelta -= (long long) zmalloc_used_memory();mem_freed += delta;\n清理策略最后我们来看一下Redis支持的几种清理策略\n\nnoeviction：不会继续处理写请求（DEL可以继续处理）。\nallkeys-lru：对所有key的近似LRU\nvolatile-lru：使用近似LRU算法淘汰设置了过期时间的key\nallkeys-random：从所有key中随机淘汰一些key\nvolatile-random：对所有设置了过期时间的key随机淘汰\nvolatile-ttl：淘汰有效期最短的一部分key\n\nRedis4.0开始支持了LFU策略，和LRU类似，它分为两种：\n\nvolatile-lfu：使用LFU算法淘汰设置了过期时间的key\nallkeys-lfu：从全部key中进行淘汰，使用LFU\n\n写在最后现在我知道了Redis在内存达到上限时做了哪些事了。以后出问题时也就不会只检查自己的db了。\n关于这次事故的后续处理，我首先是让业务同学回滚了代码，然后让他们使用一个单独的Redis，这样业务再出现类似问题就不会影响到我们的帐号服务了，整体的影响范围也会变得更加可控。\n","tags":["Redis"]},{"title":"走近源码：Redis的启动过程","url":"/2019/01/04/%E8%B5%B0%E8%BF%91%E6%BA%90%E7%A0%81%EF%BC%9ARedis%E7%9A%84%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/","content":"当我们对不断加深对某一项技术的了解时，一定会在一个特定的时间对它的实现方式产生兴趣。没错，这就是我现在的状态，所以，多年没有读/写C语言的我，决定要啃一下Redis的源码。\nRedis大体上可以分为两部分：服务器和客户端（读者吐槽：你这分的也太大体了吧）。在使用时，我们先启动服务器，然后再启动客户端。由客户端向服务器发送命令，服务器处理后将结果返回给客户端。我们从“头”开始，一起来了解一下Redis服务器在启动的时候都做了哪些事情。\n对于C语言来说，main函数是一个程序的的入口，Redis也不例外。Redis的main函数写在server.c文件中。由于redis启动过程相当复杂，需要判断许多条件，例如是否在集群中，或者是否是哨兵模式等等，因此我们只介绍单机redis启动过程中一些比较重要的步骤。\n初始化全局服务器状态如果redis-server命令启动时使用了test参数，那么就会先进行指定的测试。接下来调用了initServerConfig()函数，这个函数初始化了一个类型为redisServer的全局变量server。redisServer这个结构包含了非常多的字段，由于篇幅限制，我们不在这里列出，如果按类别划分的话，可以分为以下类别：\n\nGeneral\nModules\nNetworking\nRDB / AOF loading information\nFast pointers to often looked up command\nFields used only for stats\nConfiguration\nAOF / RDB persistence\nLogging\nReplication\nSynchronous replication\nLimits\nBlocked clients\nSort parameters\nZip structure config\ntime cache\nPubsub\nCluster\nScripting\nLazy free\nLatency monitor\nAssert &amp; bug reporting\nSystem hardware info\n\n如果用一句话来概括initServerConfig()函数作用，它就是用来给可以在配置文件（通常命名为redis.conf）中配置的变量初始化一个默认值。比较常用的变量有服务器端口号、日志等级等等。\n设置commend table在initServerConfig()函数中，会调用populateCommandTable()函数来设置服务器的命令表，命令表的结构如下。\nstruct redisCommand redisCommandTable[] = &#123;    &#123;&quot;module&quot;,moduleCommand,-2,&quot;as&quot;,0,NULL,0,0,0,0,0&#125;,    &#123;&quot;get&quot;,getCommand,2,&quot;rF&quot;,0,NULL,1,1,1,0,0&#125;,    &#123;&quot;set&quot;,setCommand,-3,&quot;wm&quot;,0,NULL,1,1,1,0,0&#125;,    &#123;&quot;setnx&quot;,setnxCommand,3,&quot;wmF&quot;,0,NULL,1,1,1,0,0&#125;,    ...&#125;\n每一项代表的含义是：\n\nname：命令的名称\nfunction：命令对应的函数名。redis-server处理命令时要执行的函数\narity：命令的参数个数，如果是-N代表大于等于N\nsflags：命令标志，标识命令的类型（read/write/admin…）\nflags：位掩码，由Redis根据sflags计算\nget_keys_proc：可选函数，当下面三个项不能指定哪些参数是key时使用\nfirst_key_index：第一个是key的参数\nlast_key_index：最后一个是key的参数\nkey_step：key的“步长”，比如MSET的key_step是2，因为它的参数是key,val,key,val这样的形式\nmicroseconds：执行命令所需要的微秒数\ncalls：该命令被调用总次数\n\n设置好命令表后，redis-server还会对一些常用的命令设置快速查找方式，直接赋予server的成员指针。\nserver.delCommand = lookupCommandByCString(&quot;del&quot;);server.multiCommand = lookupCommandByCString(&quot;multi&quot;);server.lpushCommand = lookupCommandByCString(&quot;lpush&quot;);server.lpopCommand = lookupCommandByCString(&quot;lpop&quot;);server.rpopCommand = lookupCommandByCString(&quot;rpop&quot;);server.zpopminCommand = lookupCommandByCString(&quot;zpopmin&quot;);server.zpopmaxCommand = lookupCommandByCString(&quot;zpopmax&quot;);server.sremCommand = lookupCommandByCString(&quot;srem&quot;);server.execCommand = lookupCommandByCString(&quot;exec&quot;);server.expireCommand = lookupCommandByCString(&quot;expire&quot;);server.pexpireCommand = lookupCommandByCString(&quot;pexpire&quot;);server.xclaimCommand = lookupCommandByCString(&quot;xclaim&quot;);server.xgroupCommand = lookupCommandByCString(&quot;xgroup&quot;);\n初始化哨兵模式变量初始化以后，就会将启动命令的路径和参数保存起来，以备下次重启的时候使用。如果启动的服务是哨兵模式，那么就会调用initSentinelConfig()和initSentinel()这两个方法来初始化哨兵模式。对sentinel不了解的同学可以看这里。initSentinelConfig()和initSentinel()都在sentinel.c文件中。initSentinelConfig函数负责初始化sentinel的端口号，以及解除服务器的保护模式。initSentinel函数负责将command table设置为只支持sentinel命令，以及初始化sentinelState数据格式。\n修复持久化文件启动模式如果是redis-check-rdb/aof，那么就会执行redis_check_rdb_main()或redis_check_aof_main()这两个函数来修复持久化文件，不过redis_check_rdb_main函数所做的事情在Redis启动过程中已经做了，所以这里不需要做，直接使这个函数加载错误就可以了。\n处理参数如果是简单的参数例如-v或–version、-h或–help，就会直接调用相应的方法，打印信息。如果是使用其他配置文件，则修改server.exec_argv。对于其他信息，会将他们转换成字符串，然后添加进配置文件，例如“–port 6380”就会被转换成“port 6380\\n”加进配置文件。这时，redis就会调用loadServerConfig()函数来加载配置文件，这个过程会覆盖掉前面初始化默认配置文件的变量的值。\ninitServer()initServer()函数负责结束server变量初始化工作。首先设置处理信号（SIGHUP和SIGPIPE除外），接着会创建一些双向列表用来跟踪客户端、从节点等。\nserver.current_client = NULL;server.clients = listCreate();server.clients_index = raxNew();server.clients_to_close = listCreate();server.slaves = listCreate();server.monitors = listCreate();server.clients_pending_write = listCreate();server.slaveseldb = -1; /* Force to emit the first SELECT command. */server.unblocked_clients = listCreate();server.ready_keys = listCreate();server.clients_waiting_acks = listCreate();\nShared objectcreateSharedObjects()函数会创建一些shared对象保存在全局的shared变量中，对于不同的命令，可能会有相同的返回值（比如报错）。这样在返回时就不必每次都去新增对象了，保存到内存中了。这个设计就是以Redis启动时多消耗一些时间为代价，换取运行的更小的延迟。\nshared.crlf = createObject(OBJ_STRING,sdsnew(&quot;\\r\\n&quot;));shared.ok = createObject(OBJ_STRING,sdsnew(&quot;+OK\\r\\n&quot;));shared.err = createObject(OBJ_STRING,sdsnew(&quot;-ERR\\r\\n&quot;));shared.emptybulk = createObject(OBJ_STRING,sdsnew(&quot;$0\\r\\n\\r\\n&quot;));shared.czero = createObject(OBJ_STRING,sdsnew(&quot;:0\\r\\n&quot;));shared.cone = createObject(OBJ_STRING,sdsnew(&quot;:1\\r\\n&quot;));shared.cnegone = createObject(OBJ_STRING,sdsnew(&quot;:-1\\r\\n&quot;));shared.nullbulk = createObject(OBJ_STRING,sdsnew(&quot;$-1\\r\\n&quot;));shared.nullmultibulk = createObject(OBJ_STRING,sdsnew(&quot;*-1\\r\\n&quot;));shared.emptymultibulk = createObject(OBJ_STRING,sdsnew(&quot;*0\\r\\n&quot;));shared.pong = createObject(OBJ_STRING,sdsnew(&quot;+PONG\\r\\n&quot;));shared.queued = createObject(OBJ_STRING,sdsnew(&quot;+QUEUED\\r\\n&quot;));shared.emptyscan = createObject(OBJ_STRING,sdsnew(&quot;*2\\r\\n$1\\r\\n0\\r\\n*0\\r\\n&quot;));shared.wrongtypeerr = createObject(OBJ_STRING,sdsnew(    &quot;-WRONGTYPE Operation against a key holding the wrong kind of value\\r\\n&quot;));shared.nokeyerr = createObject(OBJ_STRING,sdsnew(    &quot;-ERR no such key\\r\\n&quot;));\nShared integers除了上述的一些返回值以外，createSharedObjects()函数还会创建一些共享的整数对象。对Redis来说，有许多类型（比如lists或者sets）都需要一些整数（比如数量），这时就可以复用这些已经创建好的整数对象，而不需要重新分配内存并创建。这同样是牺牲了启动时间来换取运行时间。\n新增循环事件initServer()函数调用aeCreateEventLoop()函数(ae.c文件)来增加循环事件，并将结果返回给server的el成员。Redis使用不同的函数来兼容各个平台，在Linux平台使用epoll，在BSD使用kqueue，都不是的话，最终会使用select。Redis轮询新的连接以及I/O事件，有新的事件到来时就会及时作出响应。\n分配数据库Redis初始化需要的数据库，并将结果赋给server的db成员。\nserver.db = zmalloc(sizeof(redisDb)*server.dbnum);\n监听TCP端口listenToPort()用来初始化一些文件描述符，从而监听server配置的地址和端口。listenToPort函数会根据参数中的地址判断要监听的是IPv4还是IPv6，对应的调用anetTcpServer()或anetTcp6Server()函数，如果参数中未指明地址，则会强行绑定0.0.0.0\n初始化LRU键池evictionPoolAlloc()（evict.c文件中）用于初始化LRU的键池，Redis的key过期策略是近似LRU算法。\nvoid evictionPoolAlloc(void) &#123;    struct evictionPoolEntry *ep;    int j;    ep = zmalloc(sizeof(*ep)*EVPOOL_SIZE);    for (j = 0; j &lt; EVPOOL_SIZE; j++) &#123;        ep[j].idle = 0;        ep[j].key = NULL;        ep[j].cached = sdsnewlen(NULL,EVPOOL_CACHED_SDS_SIZE);        ep[j].dbid = 0;    &#125;    EvictionPoolLRU = ep;&#125;\nServer croninitServer()函数接下来会为数据库和pub/sub再生成一些列表和字典，重置一些状态，标记系统启动时间。在这之后，Redis会执行aeCreateTimeEvent()（在ae.c文件中）函数，用来新建一个循环执行serverCron()函数的事件。serverCron()默认每100毫秒执行一次。\n /* Create the timer callback, this is our way to process many background  * operations incrementally, like clients timeout, eviction of unaccessed  * expired keys and so forth. */if (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) &#123;    serverPanic(&quot;Can&#x27;t create event loop timers.&quot;);    exit(1);&#125;\n可以看到，代码中创建循环事件时指定每毫秒执行一次serverCron()函数，这是为了使循环马上启动，但是serverCron()函数的返回值又会被作为下次执行的时间间隔。默认为1000/server.hz。server.hz随着客户端数量的增加而增加。\nserverCron()函数做了许多定时执行的任务，包括rehash、后台持久化，AOF重新与清理、清理过期key，交换虚拟内存、同步主从节点等等。总之能想到的Redis的定时任务几乎都在serverCron()函数中处理。\n打开AOF文件/* Open the AOF file if needed. */if (server.aof_state == AOF_ON) &#123;    server.aof_fd = open(server.aof_filename,                         O_WRONLY|O_APPEND|O_CREAT,0644);    if (server.aof_fd == -1) &#123;        serverLog(LL_WARNING, &quot;Can&#x27;t open the append-only file: %s&quot;,                  strerror(errno));        exit(1);    &#125;&#125;\n最大内存限制对于32位系统，最大内存是4GB，如果用户没有明确指出Redis可使用的最大内存，那么这里默认限制为3GB。\n/* 32 bit instances are limited to 4GB of address space, so if there is     * no explicit limit in the user provided configuration we set a limit     * at 3 GB using maxmemory with &#x27;noeviction&#x27; policy&#x27;. This avoids     * useless crashes of the Redis instance for out of memory. */if (server.arch_bits == 32 &amp;&amp; server.maxmemory == 0) &#123;    serverLog(LL_WARNING,&quot;Warning: 32 bit instance detected but no memory limit set. Setting 3 GB maxmemory limit with &#x27;noeviction&#x27; policy now.&quot;);    server.maxmemory = 3072LL*(1024*1024); /* 3 GB */    server.maxmemory_policy = MAXMEMORY_NO_EVICTION;&#125;\nRedis Server启动如果Redis被设置为后台运行，此时Redis会尝试写pid文件，默认路径是/var/run/redis.pid。这时，Redis服务器已经启动，不过还有一些事情要做。\n从磁盘加载数据如果存在AOF文件或者dump文件（都有的话AOF文件的优先级高），loadDataFromDisk()函数负责将数据从磁盘加载到内存。\n最后的设置每次进入循环事件时，要调用beforeSleep()函数，它做了以下这些事情：\n\n如果server是cluster中的一个节点，调用clusterBeforeSleep()函数\n执行一个快速的周期\n如果有客户端在前一个循环事件被阻塞了，向所有的从节点发送ACK请求\n取消在同步备份过程中被阻塞的客户端的阻塞状态\n检查是否有因为阻塞命令而被阻塞的客户端，如果有，解除\n把AOF缓冲区写到磁盘\n线程释放GIL\n\n进入主循环事件程序调用aeMain()函数，进入主循环，这时其他的一些循环事件也会分别被调用\nvoid aeMain(aeEventLoop *eventLoop) &#123;    eventLoop-&gt;stop = 0;    while (!eventLoop-&gt;stop) &#123;        if (eventLoop-&gt;beforesleep != NULL)            eventLoop-&gt;beforesleep(eventLoop);        aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP);    &#125;&#125;\n到此，Redis服务器已经完全准备好处理各种事件了。后面我们会继续了解Redis命令执行过程究竟做了哪些事情。\n参考：Redis: under the hood\n","tags":["Redis"]},{"title":"走近源码：Redis跳跃列表究竟怎么跳","url":"/2019/04/18/%E8%B5%B0%E8%BF%91%E6%BA%90%E7%A0%81%EF%BC%9ARedis%E8%B7%B3%E8%B7%83%E5%88%97%E8%A1%A8%E7%A9%B6%E7%AB%9F%E6%80%8E%E4%B9%88%E8%B7%B3/","content":"在前面介绍压缩列表ziplist的时候我们提到过，zset内部有两种存储结构，一种是ziplist，另一种是跳跃列表skiplist。为了彻底理解zset的内部结构，我们就再来介绍一下skiplist。\nskiplist介绍顾名思义，skiplist本质上是一个有序的多维的list。我们先回顾一下一维列表是如何进行查找的。\n\n如上图，我们要查找一个元素，就需要从头节点开始遍历，直到找到对应的节点或者是第一个大于要查找的元素的节点（没找到）。时间复杂度为O(N)。\n这个查找效率是比较低的，如果我们把列表的某些节点拔高一层，例如把每两个节点中有一个节点变成两层。那么第二层的节点只有第一层的一半，查找效率也就会提高。\n\n查找的步骤是从头节点的顶层开始，查到第一个大于指定元素的节点时，退回上一节点，在下一层继续查找。\n例如我们要在上面的列表中查询16。\n\n从头节点的最顶层开始，先到节点7。\n7的下一个节点是39，大于16，因此我们退回到7\n从7开始，在下一层继续查找，就可以找到16。\n\n这个例子中遍历的节点不比一维列表少，但是当节点更多，查找的数字更大时，这种做法的优势就体现出来了。还是上面的例子，如果我们要查找的是39，那么只需要访问两个节点（7、39）就可以找到了。这比一维列表要减少一半的数量。\n为了避免插入操作的时间复杂度是O(N)，skiplist每层的数量不会严格按照2:1的比例，而是对每个要插入的元素随机一个层数。\n随机层数的计算过程如下：\n\n每个节点都有第一层\n那么它有第二层的概率是p，有第三层的概率是p*p\n不能超过最大层数\n\nRedis中的实现是\n/* Returns a random level for the new skiplist node we are going to create. * The return value of this function is between 1 and ZSKIPLIST_MAXLEVEL * (both inclusive), with a powerlaw-alike distribution where higher * levels are less likely to be returned. */int zslRandomLevel(void) &#123;    int level = 1;    while ((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF))        level += 1;    return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;&#125;\n其中ZSKIPLIST_P的值是0.25，存在上一层的概率是1/4，也就是说相对于我们上面的例子更加扁平化一些。ZSKIPLIST_MAXLEVEL的值是64，即最高允许64层。\nRedis中的skiplistRedis中的skiplist是作为zset的一种内部存储结构\n/* ZSETs use a specialized version of Skiplists */typedef struct zskiplistNode &#123;    sds ele;    double score;    struct zskiplistNode *backward;    struct zskiplistLevel &#123;        struct zskiplistNode *forward;        unsigned long span;    &#125; level[];&#125; zskiplistNode;typedef struct zskiplist &#123;    struct zskiplistNode *header, *tail;    unsigned long length;    int level;&#125; zskiplist;typedef struct zset &#123;    dict *dict;    zskiplist *zsl;&#125; zset;\n可以看到zset是由一个hash和一个skiplist组成。\nskiplist的结构包括头尾指针，长度和当前跳跃列表的层数。\n而在zskiplistNode，也就是跳跃列表的节点中包括\n\nele，即节点存储的数据\n节点的分数score\n回溯指针是在第一层指向前一个节点的指针，也就是说Redis的skiplist第一层是一个双向列表\n节点各层级的指针level[]，每层对应一个指针forward，以及这个指针跨越了多少个节点span。span用于计算元素的排名\n\n了解了zset和skiplist的结构之后，我们就来看一下zset的基本操作的实现。\n插入过程前面我们介绍压缩列表的插入过程的时候就有提到过skiplist的插入，在zsetAdd函数中，Redis对zset的编码方式进行了判断，分别处理skiplist和ziplist。ziplist的部分前文已经介绍过了，今天就来看一下skiplist的部分。\nif (zobj-&gt;encoding == OBJ_ENCODING_SKIPLIST) &#123;    zset *zs = zobj-&gt;ptr;    zskiplistNode *znode;    dictEntry *de;    de = dictFind(zs-&gt;dict,ele);    if (de != NULL) &#123;        /* NX? Return, same element already exists. */        if (nx) &#123;            *flags |= ZADD_NOP;            return 1;        &#125;        curscore = *(double*)dictGetVal(de);        /* Prepare the score for the increment if needed. */        if (incr) &#123;            score += curscore;            if (isnan(score)) &#123;                *flags |= ZADD_NAN;                return 0;            &#125;            if (newscore) *newscore = score;        &#125;        /* Remove and re-insert when score changes. */        if (score != curscore) &#123;            znode = zslUpdateScore(zs-&gt;zsl,curscore,ele,score);            /* Note that we did not removed the original element from             * the hash table representing the sorted set, so we just             * update the score. */            dictGetVal(de) = &amp;znode-&gt;score; /* Update score ptr. */            *flags |= ZADD_UPDATED;        &#125;        return 1;    &#125; else if (!xx) &#123;        ele = sdsdup(ele);        znode = zslInsert(zs-&gt;zsl,score,ele);        serverAssert(dictAdd(zs-&gt;dict,ele,&amp;znode-&gt;score) == DICT_OK);        *flags |= ZADD_ADDED;        if (newscore) *newscore = score;        return 1;    &#125; else &#123;        *flags |= ZADD_NOP;        return 1;    &#125;&#125;\n首先是查找对应元素是否存在，如果存在并且没有参数NX，就记录下这个元素当前的分数。这里可以看出zset中的hash字典是用来根据元素获取分数的。\n接着判断是不是要执行increment命令，如果是的话，就用当前分数加上指定分数，得到新的分数newscore。如果分数发生了变化，就调用zslUpdateScore函数，来更新skiplist中的节点，另外还要多一步操作来更新hash字典中的分数。\n如果要插入的元素不存在，那么就直接调用zslInsert函数。\nzskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele) &#123;    zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x;    unsigned int rank[ZSKIPLIST_MAXLEVEL];    int i, level;    serverAssert(!isnan(score));    x = zsl-&gt;header;    for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123;        /* store rank that is crossed to reach the insert position */        rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1];        while (x-&gt;level[i].forward &amp;&amp;                (x-&gt;level[i].forward-&gt;score &lt; score ||                    (x-&gt;level[i].forward-&gt;score == score &amp;&amp;                    sdscmp(x-&gt;level[i].forward-&gt;ele,ele) &lt; 0)))        &#123;            rank[i] += x-&gt;level[i].span;            x = x-&gt;level[i].forward;        &#125;        update[i] = x;    &#125;    /* we assume the element is not already inside, since we allow duplicated     * scores, reinserting the same element should never happen since the     * caller of zslInsert() should test in the hash table if the element is     * already inside or not. */    level = zslRandomLevel();    if (level &gt; zsl-&gt;level) &#123;        for (i = zsl-&gt;level; i &lt; level; i++) &#123;            rank[i] = 0;            update[i] = zsl-&gt;header;            update[i]-&gt;level[i].span = zsl-&gt;length;        &#125;        zsl-&gt;level = level;    &#125;    x = zslCreateNode(level,score,ele);    for (i = 0; i &lt; level; i++) &#123;        x-&gt;level[i].forward = update[i]-&gt;level[i].forward;        update[i]-&gt;level[i].forward = x;        /* update span covered by update[i] as x is inserted here */        x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]);        update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1;    &#125;    /* increment span for untouched levels */    for (i = level; i &lt; zsl-&gt;level; i++) &#123;        update[i]-&gt;level[i].span++;    &#125;    x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0];    if (x-&gt;level[0].forward)        x-&gt;level[0].forward-&gt;backward = x;    else        zsl-&gt;tail = x;    zsl-&gt;length++;    return x;&#125;\n函数一开始定义了两个数组，update数组用来存储搜索路径，rank数组用来存储节点跨度。\n第一步操作是找出要插入节点的搜索路径，并且记录节点跨度数。\n接着开始插入，先随机一个层数。如果随机出的层数大于当前的层数，就需要继续填充update和rank数组，并更新skiplist的最大层数。\n然后调用zslCreateNode函数创建新的节点。\n创建好节点后，就根据搜索路径数据提供的位置，从第一层开始，逐层插入节点（更新指针），并其他节点的span值。\n最后还要更新回溯节点，以及将skiplist的长度加一。\n这就是插入新元素的整个过程。\n更新过程了解了插入过程以后我们再回过头来看更新过程\nzskiplistNode *zslUpdateScore(zskiplist *zsl, double curscore, sds ele, double newscore) &#123;    zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x;    int i;    /* We need to seek to element to update to start: this is useful anyway,     * we&#x27;ll have to update or remove it. */    x = zsl-&gt;header;    for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123;        while (x-&gt;level[i].forward &amp;&amp;                (x-&gt;level[i].forward-&gt;score &lt; curscore ||                    (x-&gt;level[i].forward-&gt;score == curscore &amp;&amp;                     sdscmp(x-&gt;level[i].forward-&gt;ele,ele) &lt; 0)))        &#123;            x = x-&gt;level[i].forward;        &#125;        update[i] = x;    &#125;    /* Jump to our element: note that this function assumes that the     * element with the matching score exists. */    x = x-&gt;level[0].forward;    serverAssert(x &amp;&amp; curscore == x-&gt;score &amp;&amp; sdscmp(x-&gt;ele,ele) == 0);    /* If the node, after the score update, would be still exactly     * at the same position, we can just update the score without     * actually removing and re-inserting the element in the skiplist. */    if ((x-&gt;backward == NULL || x-&gt;backward-&gt;score &lt; newscore) &amp;&amp;        (x-&gt;level[0].forward == NULL || x-&gt;level[0].forward-&gt;score &gt; newscore))    &#123;        x-&gt;score = newscore;        return x;    &#125;    /* No way to reuse the old node: we need to remove and insert a new     * one at a different place. */    zslDeleteNode(zsl, x, update);    zskiplistNode *newnode = zslInsert(zsl,newscore,x-&gt;ele);    /* We reused the old node x-&gt;ele SDS string, free the node now     * since zslInsert created a new one. */    x-&gt;ele = NULL;    zslFreeNode(x);    return newnode;&#125;\n和插入过程一样，先保存了搜索路径。并且定位到要更新的节点，如果更新后节点位置不变，则直接返回。否则，就要先调用zslDeleteNode函数删除该节点，再插入新的节点。\n删除过程Redis中skiplist的更新过程还是比较容易理解的，就是先删除再插入，那么我们接下来就看看它是如何删除节点的。\nvoid zslDeleteNode(zskiplist *zsl, zskiplistNode *x, zskiplistNode **update) &#123;    int i;    for (i = 0; i &lt; zsl-&gt;level; i++) &#123;        if (update[i]-&gt;level[i].forward == x) &#123;            update[i]-&gt;level[i].span += x-&gt;level[i].span - 1;            update[i]-&gt;level[i].forward = x-&gt;level[i].forward;        &#125; else &#123;            update[i]-&gt;level[i].span -= 1;        &#125;    &#125;    if (x-&gt;level[0].forward) &#123;        x-&gt;level[0].forward-&gt;backward = x-&gt;backward;    &#125; else &#123;        zsl-&gt;tail = x-&gt;backward;    &#125;    while(zsl-&gt;level &gt; 1 &amp;&amp; zsl-&gt;header-&gt;level[zsl-&gt;level-1].forward == NULL)        zsl-&gt;level--;    zsl-&gt;length--;&#125;\n删除过程的代码也比较容易理解，首先按照搜索路径，从下到上，逐层更新前向指针。然后更新回溯指针。如果删除节点的层数是最大的层数，那么还需要更新skiplist的level字段。最后长度减一。\n总结skiplist是节点有层级的list，节点的查找过程可以跨越多个节点，从而节省查找时间。\nRedis的zset由hash字典和skiplist组成，hash字典负责数据到分数的对应，skiplist负责根据分数查找数据。\nRedis中skiplist插入和删除操作都依赖于搜索路径，更新操作是先删除再插入。\n推荐阅读Skip Lists: A Probabilistic Alternative to Balanced Trees\n《Redis 深度历险：核心原理与应用实践》\n","tags":["Redis"]},{"title":"走近源码：压缩列表是怎样炼成的","url":"/2019/03/23/%E8%B5%B0%E8%BF%91%E6%BA%90%E7%A0%81%EF%BC%9A%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8%E6%98%AF%E6%80%8E%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84/","content":"经过前面对Redis源码的了解，令人印象深刻的也许就是Redis各种节约内存手段。而Redis对于内存的节约可以说是费尽心思，今天我就再来介绍一种Redis为了节约内存而创造的存储结构——压缩列表（ziplist）。\n存储结构ziplist是zset和hash在元素数量较少时使用的一种存储结构。它的特点存储于一块连续的内存，元素与元素之间没有空隙。我们可以用DEBUG OBJECT命令来查看一个zset的编码格式：\n127.0.0.1:6379&gt; ZADD db 1.0 mysql 2.0 mongo 3.0 redis(integer) 3127.0.0.1:6379&gt; DEBUG OBJECT dbValue at:0x7f5bf1908070 refcount:1 encoding:ziplist serializedlength:39 lru:9589668 lru_seconds_idle:12\n那么ziplist究竟是一种怎样的结构的，话不多说，直接看图。\nZIPLIST OVERALL LAYOUT\n接下来我们挨个解释一下每一部分存储的内容：\n\nzlbytes：32位无符号整数，存储的是包括它自己在内的整个ziplist所占用的字节数\nzltail：32位无符号整数，存储的是最后一个entry的偏移量，用来快速定位最后一个元素\nzllen：16位无符号整数，用于存储entry的数量，当元素数量大于216-2时，这个值就被设置为216-1。我们想知道元素的数量就需要遍历整个列表\nentry：表示存储的元素\nzlend：8位无符号整数，用于标识整个ziplist的结尾。它的值是255。\n\nZIPLIST ENTRIES了解了ziplist的大概结构以后，我们剖析更深一层的entry的结构。\n对于每个entry都有两个前缀\n\nprevlen：表示前一个元素的长度，它与zltail字段结合使用可以实现快速的从后向前定位元素\nencoding：表示元素的编码格式，它用来表示元素是整数还是字符串，如果是字符串，也表示字符串的长度\nentry-data：元素的数据，它并不是一定存在，对于某些编码而言，编码本身也是数据，因此这一部分可以省略\n\n这里要解释一点，prevlen是一个变长的整数，当前一个元素的长度小于254时，它仅需要一个字节（8位无符号整数）表示，如果元素的长度大于（或等于）254字节，prevlen就用5个字节来表示，其中第一个字节是254，后4个字节表示前一个元素的长度。\nencoding字段决定了元素的内容。如果entry存储的是字符串，那么就通过encoding的前两位来区分不同长度的字符串，如果entry存储的内容是整数，那么前两位都会被设置为1，再后面两位用来区分整数的类型。\n\n|00pppppp|：字符串长度小于63字节，pppppp是6位无符号整数，用来表示字符串长度\n|01pppppp|qqqqqqqq|：字符串长度小于等于16383字节，后面14位表示字符串长度\n|10000000|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt|：字符串长度大于等于16384字节，后4个字节表示字符串长度\n|11000000|：16位整数，后面跟2个字节存储整数\n|11010000|：32位整数，后面跟4个字节存储整数\n|11100000|：64位整数，后面跟8个字节存储整数\n|11110000|：24位整数，后面跟3个字节存储整数\n|11111110|：8位整数，后面跟1个字节存储整数\n|1111xxxx|：(xxxx 取值从0000到1101)表示0到12的整数，读到的xxxx减1为实际表示的整数。这就是前面提到的省略entry-data的情况\n|11111111|：ziplist的结束值，也就是zlend的值\n\n说了这么多，也许你还是不太清楚ziplist存储的内容究竟要表示什么，我们还是来举一个栗子\n[0f 00 00 00] [0c 00 00 00] [02 00] [00 f3] [02 f6] [ff]\n这是一个实际的ziplist存储的内容，我们就一起来解读一下。\n首先是4个字节的zlbytes，ziplist一共是15个字节，因此zlbytes的值是0x0f；接下来是4个字节的zltail，偏移量是12，因此zltail的值是0x0c；后两个字节是zllen，也就是一共两个元素；第一个元素的prevlen为00，0xf3表示元素值是2：1111 0011符合上述第9条，读到xxxx为3，需要减1，因此实际值是2；第二个元素同理，0xf6表示的值是5，最后0xff表示这个ziplist结束。\n这时，我向这个ziplist中又加了一个元素，是一个字符串，请大家自行解读下面的entry（注意，只是entry）。友情提示：需要查询ASCII码表来解读\n[02] [0b] [48 65 6c 6c 6f 20 57 6f 72 6c 64]\n增加元素了解了ziplist的存储之后，我们再来看一下ziplist是如何增加元素的。前面提到过，ziplist存储结构用于元素数量少的zset和hash。那么我们就以zset为例，一起追踪源码，了解ziplist增加元素的过程。\n我们从ZADD命令执行的函数zaddCommand()开始。\nvoid zaddCommand(client *c) &#123;    zaddGenericCommand(c,ZADD_NONE);&#125;\n它只是简单调用了zaddGenericCommand()函数，传入了客户端对象c和一个标志位，表示要执行ZADD命令，因为这个函数同样也是ZINCRBY要执行的函数（传入的标志是ZADD_INCR）。\n而在zaddGenericCommand()函数中，首先对参数进行了处理，并且做了一些校验。\n/* Lookup the key and create the sorted set if does not exist. */zobj = lookupKeyWrite(c-&gt;db,key);if (zobj == NULL) &#123;    if (xx) goto reply_to_client; /* No key + XX option: nothing to do. */    if (server.zset_max_ziplist_entries == 0 ||        server.zset_max_ziplist_value &lt; sdslen(c-&gt;argv[scoreidx+1]-&gt;ptr))    &#123;        zobj = createZsetObject();    &#125; else &#123;        zobj = createZsetZiplistObject();    &#125;    dbAdd(c-&gt;db,key,zobj);&#125; else &#123;    if (zobj-&gt;type != OBJ_ZSET) &#123;        addReply(c,shared.wrongtypeerr);        goto cleanup;    &#125;&#125;\n然后判断key是否存在，如果存在，验证数据类型；否则创建一个新的zset对象。这里可以看到，当\nzset_max_ziplist_entries为0或者第一个元素的长度大于zset_max_ziplist_value时，创建zset对象，否则创建ziplist对象。创建好对象之后，就开始遍历元素，执行zsetAdd函数了：\nfor (j = 0; j &lt; elements; j++) &#123;    double newscore;    score = scores[j];    int retflags = flags;    ele = c-&gt;argv[scoreidx+1+j*2]-&gt;ptr;    int retval = zsetAdd(zobj, score, ele, &amp;retflags, &amp;newscore);    if (retval == 0) &#123;        addReplyError(c,nanerr);        goto cleanup;    &#125;    if (retflags &amp; ZADD_ADDED) added++;    if (retflags &amp; ZADD_UPDATED) updated++;    if (!(retflags &amp; ZADD_NOP)) processed++;    score = newscore;&#125;\n这个函数用来增加新元素或者更新元素的score。这个函数中判断了zset对象的编码方式，对压缩列表ziplist和跳跃列表skiplist分开处理，跳跃列表是zset的另一种编码方式，这个我们以后再介绍，本文我们只关注ziplist。\nif (zobj-&gt;encoding == OBJ_ENCODING_ZIPLIST) &#123;    unsigned char *eptr;    if ((eptr = zzlFind(zobj-&gt;ptr,ele,&amp;curscore)) != NULL) &#123;        /* NX? Return, same element already exists. */        if (nx) &#123;            *flags |= ZADD_NOP;            return 1;        &#125;        /* Prepare the score for the increment if needed. */        if (incr) &#123;            score += curscore;            if (isnan(score)) &#123;                *flags |= ZADD_NAN;                return 0;            &#125;            if (newscore) *newscore = score;        &#125;        /* Remove and re-insert when score changed. */        if (score != curscore) &#123;            zobj-&gt;ptr = zzlDelete(zobj-&gt;ptr,eptr);            zobj-&gt;ptr = zzlInsert(zobj-&gt;ptr,ele,score);            *flags |= ZADD_UPDATED;        &#125;        return 1;    &#125; else if (!xx) &#123;        /* Optimize: check if the element is too large or the list             * becomes too long *before* executing zzlInsert. */        zobj-&gt;ptr = zzlInsert(zobj-&gt;ptr,ele,score);        if (zzlLength(zobj-&gt;ptr) &gt; server.zset_max_ziplist_entries)            zsetConvert(zobj,OBJ_ENCODING_SKIPLIST);        if (sdslen(ele) &gt; server.zset_max_ziplist_value)            zsetConvert(zobj,OBJ_ENCODING_SKIPLIST);        if (newscore) *newscore = score;        *flags |= ZADD_ADDED;        return 1;    &#125; else &#123;        *flags |= ZADD_NOP;        return 1;    &#125;&#125;\n可以看到，这里首先调用zzlFind()函数查找对应的元素，如果元素存在，那么就判断是否包含参数NX或者是否是INCR操作。如果修改了元素的分数，则先删除原有的元素，再重新增加；如果元素不存在，就直接执行zzlInsert()函数，再insert之后，会判断是否需要改为跳跃列表存储。这里有两个条件：\n\nzset元素数量大于zset_max_ziplist_entries（默认128）\n添加的元素长度大于zset_max_ziplist_value（默认64）\n\n满足任意一个条件，zset都会使用跳跃列表来存储。\n我们继续追踪zzlInsert()函数。\nunsigned char *zzlInsert(unsigned char *zl, sds ele, double score) &#123;    unsigned char *eptr = ziplistIndex(zl,0), *sptr;    double s;    while (eptr != NULL) &#123;        sptr = ziplistNext(zl,eptr);        serverAssert(sptr != NULL);        s = zzlGetScore(sptr);        if (s &gt; score) &#123;            /* First element with score larger than score for element to be             * inserted. This means we should take its spot in the list to             * maintain ordering. */            zl = zzlInsertAt(zl,eptr,ele,score);            break;        &#125; else if (s == score) &#123;            /* Ensure lexicographical ordering for elements. */            if (zzlCompareElements(eptr,(unsigned char*)ele,sdslen(ele)) &gt; 0) &#123;                zl = zzlInsertAt(zl,eptr,ele,score);                break;            &#125;        &#125;        /* Move to next element. */        eptr = ziplistNext(zl,sptr);    &#125;    /* Push on tail of list when it was not yet inserted. */    if (eptr == NULL)        zl = zzlInsertAt(zl,NULL,ele,score);    return zl;&#125;\n它首先定位了zset的第一个元素，如果该元素不为空，就比较该元素的分数s与要插入的元素分数score，如果s&gt;score，就插入到当前位置，如果分数相同，则比较元素（按字典序）。插入后，将后面的元素依次移到下一位。\nunsigned char *zzlInsertAt(unsigned char *zl, unsigned char *eptr, sds ele, double score) &#123;    unsigned char *sptr;    char scorebuf[128];    int scorelen;    size_t offset;    scorelen = d2string(scorebuf,sizeof(scorebuf),score);    if (eptr == NULL) &#123;        zl = ziplistPush(zl,(unsigned char*)ele,sdslen(ele),ZIPLIST_TAIL);        zl = ziplistPush(zl,(unsigned char*)scorebuf,scorelen,ZIPLIST_TAIL);    &#125; else &#123;        /* Keep offset relative to zl, as it might be re-allocated. */        offset = eptr-zl;        zl = ziplistInsert(zl,eptr,(unsigned char*)ele,sdslen(ele));        eptr = zl+offset;        /* Insert score after the element. */        serverAssert((sptr = ziplistNext(zl,eptr)) != NULL);        zl = ziplistInsert(zl,sptr,(unsigned char*)scorebuf,scorelen);    &#125;    return zl;&#125;\n在zzlInsertAt()函数中，主要是调用了ziplistPush()或者ziplistInsert()将元素和分数插入列表尾部或中间。插入顺序是先插入元素，然后插入分数。\n接下来就到了ziplist.c文件中，真正向压缩列表中插入元素了。关键代码在__ziplistInsert()函数中。\n首先需要计算插入位置前一个元素的长度，存储到当前entry的prevlen。\nif (p[0] != ZIP_END) &#123;    ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);&#125; else &#123;    unsigned char *ptail = ZIPLIST_ENTRY_TAIL(zl);    if (ptail[0] != ZIP_END) &#123;        prevlen = zipRawEntryLength(ptail);    &#125;&#125;\n这里区分了是否是在尾部插入元素的情况，如果是在尾部，就可以通过ziplist中的zltail字段直接定位。接下来就是尝试对插入的元素进行编码，判断是否可以存储为整数，如果不能，就按照字符串的编码格式来存储。\nif (zipTryEncoding(s,slen,&amp;value,&amp;encoding)) &#123;    /* &#x27;encoding&#x27; is set to the appropriate integer encoding */    reqlen = zipIntSize(encoding);&#125; else &#123;    /* &#x27;encoding&#x27; is untouched, however zipStoreEntryEncoding will use the         * string length to figure out how to encode it. */    reqlen = slen;&#125;\n这一步判断是节省内存的关键，它会使用我们前面介绍的尽量小的编码格式来进行编码。编码完成后就要计算当前entry的长度，包括prevlen、encoding和entry-data，并且需要保证后一个entry（如果有的话）的prevlen能够保存当前entry的长度。这里调用的是zipPrevLenByteDiff()函数，需要的prevlen的长度和现有的prevlen的长度的差值，也就是说如果返回为整数，表示需要更多空间。\n在这之后就要调用zrealloc()来扩展空间了。这里有可能会在原来的基础上进行扩展，也有可能重新分配一块内存，然后将原来的ziplist整体迁移。如果ziplist占用较大内存时，整体迁移的代价是很高的。有了足够的空间之后，就是把当前位置的entry向后移一位了，然后要修改这个entry的prevlen。更新zltail。\nif (nextdiff != 0) &#123;    offset = p-zl;    zl = __ziplistCascadeUpdate(zl,p+reqlen);    p = zl+offset;&#125;\nnextdiff是前面zipPrevLenByteDiff()函数的返回值，它不为0表示需要更多空间（小于0时被置为0）。这时后面的元素需要级联更新。所有的这些处理完毕之后，我们终于可以把要插入的entry写入当前位置了，并且将ziplist的长度加1。\n级联更新如果一个entry的长度小于254字节，那么后一个元素的prevlen就用一个字节来存储，否则就要用5个字节存储。当我们插入一个元素时，如果它的长度大于253字节，那么原来的entry就可能从1个字节变成5个字节，而如果由于这一变化导致这个entry的长度大于254字节，那么后面的元素也要更新。到后面甚至有可能导致重新分配内存的问题，所以级联更新是一件很可怕的事情。\n接下来就通过源码，看一下级联更新的具体步骤。（查看ziplist.c文件的__ziplistCascadeUpdate函数）\n首先，判断当前entry是否是最后一个，如果是，则跳出级联更新。\nif (p[rawlen] == ZIP_END) break;\n接着判断了下一个entry的prevlen长度是否发生变化，如果没有变化，也不用继续进行级联更新。\nif (next.prevrawlen == rawlen) break;\n而如果下一个entry的prevlen长度需要扩展，那么就先调用ziplistResize扩展内存，然后要更新zltail。要将后面的entry向后移动，再开始判断下一个entry是否需要更新。\nif (next.prevrawlensize &lt; rawlensize) &#123;    /* The &quot;prevlen&quot; field of &quot;next&quot; needs more bytes to hold     * the raw length of &quot;cur&quot;. */    offset = p-zl;    extra = rawlensize-next.prevrawlensize;    zl = ziplistResize(zl,curlen+extra);    p = zl+offset;    /* Current pointer and offset for next element. */    np = p+rawlen;    noffset = np-zl;    /* Update tail offset when next element is not the tail element. */    if ((zl+intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))) != np) &#123;        ZIPLIST_TAIL_OFFSET(zl) =            intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+extra);    &#125;    /* Move the tail to the back. */    memmove(np+rawlensize,            np+next.prevrawlensize,            curlen-noffset-next.prevrawlensize-1);    zipStorePrevEntryLength(np,rawlen);    /* Advance the cursor */    p += rawlen;    curlen += extra;&#125;\n如果后面的entry的prevlen大于需要的长度呢，此时应该收缩prevlen，如果要进行收缩，那么可能会继续级联更新。这太麻烦了，所以这里选择了浪费一些空间，用5个字节的空间来存储1个字节可以存储的内容。如果prevlen的长度等于需要的长度，就直接更新内容。\nif (next.prevrawlensize &gt; rawlensize) &#123;    /* This would result in shrinking, which we want to avoid.     * So, set &quot;rawlen&quot; in the available bytes. */    zipStorePrevEntryLengthLarge(p+rawlen,rawlen);&#125; else &#123;    zipStorePrevEntryLength(p+rawlen,rawlen);&#125;/* Stop here, as the raw length of &quot;next&quot; has not changed. */break;\n除了新增操作以外，删除操作也有可能引起级联更新。假设我们有3个entry是下面的情况\n\n我们可以知道，entry2的prevlen需要5个字节，entry3的prevlen只需要1个字节。而如果我们删除了entry2，那么entry3的prevlen就需要扩展到5个字节，这一操作就有可能引起级联更新，后面的情况和新增节点时一样。\n总结最后做一个总结：\n\n压缩列表是zset和hash元素个数较少时的存储结构\nziplist由zlbytes、zltail、zllen、entry、zlend这五部分组成\n每个entry由prevlen、encoding和entry-data三部分组成\nziplist增加元素时，需要重新计算插入位置的entry的prevlen（prevlen的长度为1字节或5字节），这一操作有可能引起级联更新。\n\n","tags":["Redis"]},{"title":"走近源码：神奇的HyperLogLog","url":"/2019/02/26/%E8%B5%B0%E8%BF%91%E6%BA%90%E7%A0%81%EF%BC%9A%E7%A5%9E%E5%A5%87%E7%9A%84HyperLogLog/","content":"HyperLogLog是Redis的高级数据结构，是统计基数的利器。前文我们已经介绍过HyperLogLog的基本用法，如果只求会用，只需要掌握HyperLogLog的三个命令即可，如果想要更进一步了解HyperLogLog的原理以及源码实现，相信这篇文章会给你带来一些启发。\n基数\n在数学上，基数或势，即集合中包含的元素的“个数”（参见势的比较），是日常交流中基数的概念在数学上的精确化（并使之不再受限于有限情形）。有限集合的基数，其意义与日常用语中的“基数”相同，例如{\\displaystyle {a,b,c}}的基数是3。无限集合的基数，其意义在于比较两个集的大小，例如整数集和有理数集的基数相同；整数集的基数比实数集的小。\n\n在介绍HyperLogLog的原理之前，请你先来思考一下，如果让你来统计基数，你会用什么方法。\nSet熟悉Redis数据结构的同学一定首先会想到Set这个结构，我们只需要把数据都存入Set，然后用scard命令就可以得到结果，这是一种思路，但是存在一定的问题。如果数据量非常大，那么将会耗费很大的内存空间，如果这些数据仅仅是用来统计基数，那么无疑是造成了巨大的浪费，因此，我们需要找到一种占用内存较小的方法。\nbitmapbitmap同样是一种可以统计基数的方法，可以理解为用bit数组存储元素，例如011010001，表示的是[1,2,4,8]，bitmap中1的个数就是基数。bitmap也可以轻松合并多个集合，只需要将多个数组进行异或操作就可以了。bitmap相比于Set也大大节省了内存，我们来粗略计算一下，统计1亿个数据的基数，需要的内存是：100000000/8/1024/1024 ≈ 12M。\n虽然bitmap在节省空间方面已经有了不错的表现，但是如果需要统计1000个对象，就需要大约12G的内存，显然这个结果仍然不能令我们满意。在这种情况下，HyperLogLog将会出来拯救我们。\nHyperLogLog原理HyperLogLog实际上不会存储每个元素的值，它使用的是概率算法，通过存储元素的hash值的第一个1的位置，来计算元素数量。这么说不太容易理解，容我先搬出来一个栗子。\n有一天Jack和丫丫玩抛硬币的游戏，规则是丫丫负责抛硬币，每次抛到正面为一回合，丫丫可以自己决定进行几个回合。最后需要告诉Jack最长的那个回合抛了多少次，再由Jack来猜丫丫一共进行了几个回合。Jack心想：这可不好猜啊，我得算算概率了。于是在脑海中绘制这样一张图。\n\nk是每回合抛到1所用的次数，我们已知的是最大的k值，可以用kmax表示，由于每次抛硬币的结果只有0和1两种情况，因此，kmax在任意回合出现的概率即为(1/2)kmax，因此可以推测n=2kmax。概率学把这种问题叫做伯努利实验。此时丫丫已经完成了n个回合，并且告诉Jack最长的一次抛了3次，Jack此时也胸有成竹，马上说出他的答案8，最后的结果是：丫丫只抛了一回合，Jack输了，要负责刷碗一个月。\n终于，我们的Philippe Flajolet教授遇到了Jack一样的问题，他决心吸取Jack的教训，要让这个算法更加准确，于是引入了桶的概念，计算m个桶的加权平均值，这样就能得到比较准确的答案了（实际上还要进行其他修正）。最终的公式如图\n\n其中m是桶的数量，const是修正常数，它的取值会根据m而变化。p=log2m\nswitch (p) &#123;   case 4:       constant = 0.673 * m * m;   case 5:       constant = 0.697 * m * m;   case 6:       constant = 0.709 * m * m;   default:       constant = (0.7213 / (1 + 1.079 / m)) * m * m;&#125;\n我们回到Redis，对于一个输入的字符串，首先得到64位的hash值，用前14位来定位桶的位置（共有214，即16384个桶）。后面50位即为伯努利过程，每个桶有6bit，记录第一次出现1的位置count，如果count&gt;oldcount，就用count替换oldcount。\n了解原理之后，我们再来聊一下HyperLogLog的存储。HyperLogLog的存储结构分为密集存储结构和稀疏存储结构两种，默认为稀疏存储结构，而我们常说的占用12K内存的则是密集存储结构。\n密集存储结构密集存储比较简单，就是连续16384个6bit的串成的位图。由于每个桶是6bit，因此对桶的定位要麻烦一些。\n#define HLL_BITS 6 /* Enough to count up to 63 leading zeroes. */#define HLL_REGISTER_MAX ((1&lt;&lt;HLL_BITS)-1)/* Store the value of the register at position &#x27;regnum&#x27; into variable &#x27;target&#x27;. * &#x27;p&#x27; is an array of unsigned bytes. */#define HLL_DENSE_GET_REGISTER(target,p,regnum) do &#123; \\    uint8_t *_p = (uint8_t*) p; \\    unsigned long _byte = regnum*HLL_BITS/8; \\    unsigned long _fb = regnum*HLL_BITS&amp;7; \\    unsigned long _fb8 = 8 - _fb; \\    unsigned long b0 = _p[_byte]; \\    unsigned long b1 = _p[_byte+1]; \\    target = ((b0 &gt;&gt; _fb) | (b1 &lt;&lt; _fb8)) &amp; HLL_REGISTER_MAX; \\&#125; while(0)/* Set the value of the register at position &#x27;regnum&#x27; to &#x27;val&#x27;. * &#x27;p&#x27; is an array of unsigned bytes. */#define HLL_DENSE_SET_REGISTER(p,regnum,val) do &#123; \\    uint8_t *_p = (uint8_t*) p; \\    unsigned long _byte = regnum*HLL_BITS/8; \\    unsigned long _fb = regnum*HLL_BITS&amp;7; \\    unsigned long _fb8 = 8 - _fb; \\    unsigned long _v = val; \\    _p[_byte] &amp;= ~(HLL_REGISTER_MAX &lt;&lt; _fb); \\    _p[_byte] |= _v &lt;&lt; _fb; \\    _p[_byte+1] &amp;= ~(HLL_REGISTER_MAX &gt;&gt; _fb8); \\    _p[_byte+1] |= _v &gt;&gt; _fb8; \\&#125; while(0)\n如果我们要定位的桶编号为regnum，它的偏移字节量为(regnum  6) / 8，起始bit偏移为(regnum  6) % 8，例如，我们要定位编号为5的桶，字节偏移是3，位偏移也是6，也就是说，从第4个字节的第7位开始是编号为3的桶。这里需要注意，字节序和我们平时的字节序相反，因此需要进行倒置。我们用一张图来说明Redis是如何定位桶并且得到存储的值（即HLL_DENSE_GET_REGISTER函数的解释）。\n\n对于编号为5的桶，我们已经得到了字节偏移_byte和为偏移_fb，b0 &gt;&gt; _fb和b1 &lt;&lt; _fb8操作是将字节倒置，然后进行拼接，并且保留最后6位。\n稀疏存储结构你以为Redis真的会用16384个6bit存储每一个HLL对象吗，那就too naive了，虽然它只占用了12K内存，但是Redis对于内存的节约已经到了丧心病狂的地步了。因此，如果比较多的计数值都是0，那么就会采用稀疏存储的结构。\n对于连续多个计数值为0的桶，Redis使用的存储方式是：00xxxxxx，前缀两个0，后面6位的值加1表示有连续多少个桶的计数值为0，由于6bit最大能表示64个桶，所以Redis又设计了另一种表示方法：01xxxxxx yyyyyyyy，这样后面14bit就可以表示16384个桶了，而一个初始状态的HyperLogLog对象只需要用2个字节来存储。\n如果连续的桶数都不是0，那么Redis的表示方式为1vvvvvxx，即为连续(xx+1)个桶的计数值都是(vvvvv+1)。例如，10011110表示连续3个8。这里使用5bit，最大只能表示32。因此，当某个计数值大于32时，Redis会将这个HyperLogLog对象调整为密集存储。\nRedis用三条指令来表达稀疏存储的方式：\n\nZERO:len 单个字节表示 00[len-1]，连续最多64个零计数值\nVAL:value,len 单个字节表示 1[value-1][len-1]，连续 len 个值为 value 的计数值\nXZERO:len 双字节表示 01[len-1]，连续最多16384个零计数值\n\nRedis从稀疏存储转换到密集存储的条件是：\n\n任意一个计数值从 32 变成 33，因为VAL指令已经无法容纳，它能表示的计数值最大为 32\n稀疏存储占用的总字节数超过 3000 字节，这个阈值可以通过 hll_sparse_max_bytes 参数进行调整。\n\n源码解析接下来通过源码来看一下pfadd和pfcount两个命令的具体流程。在这之前我们首先要了解的是HyperLogLog的头结构体和创建一个HyperLogLog对象的步骤。\nHyperLogLog头结构体struct hllhdr &#123;    char magic[4];      /* &quot;HYLL&quot; */    uint8_t encoding;   /* HLL_DENSE or HLL_SPARSE. */    uint8_t notused[3]; /* Reserved for future use, must be zero. */    uint8_t card[8];    /* Cached cardinality, little endian. */    uint8_t registers[]; /* Data bytes. */&#125;;\n创建HyperLogLog对象#define HLL_P 14 /* The greater is P, the smaller the error. */#define HLL_REGISTERS (1&lt;&lt;HLL_P) /* With P=14, 16384 registers. */#define HLL_SPARSE_XZERO_MAX_LEN 16384#define HLL_SPARSE_XZERO_SET(p,len) do &#123; \\    int _l = (len)-1; \\    *(p) = (_l&gt;&gt;8) | HLL_SPARSE_XZERO_BIT; \\    *((p)+1) = (_l&amp;0xff); \\&#125; while(0)/* Create an HLL object. We always create the HLL using sparse encoding. * This will be upgraded to the dense representation as needed. */robj *createHLLObject(void) &#123;    robj *o;    struct hllhdr *hdr;    sds s;    uint8_t *p;    int sparselen = HLL_HDR_SIZE +                    (((HLL_REGISTERS+(HLL_SPARSE_XZERO_MAX_LEN-1)) /                     HLL_SPARSE_XZERO_MAX_LEN)*2);    int aux;    /* Populate the sparse representation with as many XZERO opcodes as     * needed to represent all the registers. */    aux = HLL_REGISTERS;    s = sdsnewlen(NULL,sparselen);    p = (uint8_t*)s + HLL_HDR_SIZE;    while(aux) &#123;        int xzero = HLL_SPARSE_XZERO_MAX_LEN;        if (xzero &gt; aux) xzero = aux;        HLL_SPARSE_XZERO_SET(p,xzero);        p += 2;        aux -= xzero;    &#125;    serverAssert((p-(uint8_t*)s) == sparselen);    /* Create the actual object. */    o = createObject(OBJ_STRING,s);    hdr = o-&gt;ptr;    memcpy(hdr-&gt;magic,&quot;HYLL&quot;,4);    hdr-&gt;encoding = HLL_SPARSE;    return o;&#125;\n这里sparselen=HLL_HDR_SIZE+2，因为初始化时默认所有桶的计数值都是0。其他过程不难理解，用的存储方式是我们前面提到过的稀疏存储，创建的对象实质上是一个字符串对象，这也是字符串命令可以操作HyperLogLog对象的原因。\nPFADD命令/* PFADD var ele ele ele ... ele =&gt; :0 or :1 */void pfaddCommand(client *c) &#123;    robj *o = lookupKeyWrite(c-&gt;db,c-&gt;argv[1]);    struct hllhdr *hdr;    int updated = 0, j;    if (o == NULL) &#123;        /* Create the key with a string value of the exact length to         * hold our HLL data structure. sdsnewlen() when NULL is passed         * is guaranteed to return bytes initialized to zero. */        o = createHLLObject();        dbAdd(c-&gt;db,c-&gt;argv[1],o);        updated++;    &#125; else &#123;        if (isHLLObjectOrReply(c,o) != C_OK) return;        o = dbUnshareStringValue(c-&gt;db,c-&gt;argv[1],o);    &#125;    /* Perform the low level ADD operation for every element. */    for (j = 2; j &lt; c-&gt;argc; j++) &#123;        int retval = hllAdd(o, (unsigned char*)c-&gt;argv[j]-&gt;ptr,                               sdslen(c-&gt;argv[j]-&gt;ptr));        switch(retval) &#123;        case 1:            updated++;            break;        case -1:            addReplySds(c,sdsnew(invalid_hll_err));            return;        &#125;    &#125;    hdr = o-&gt;ptr;    if (updated) &#123;        signalModifiedKey(c-&gt;db,c-&gt;argv[1]);        notifyKeyspaceEvent(NOTIFY_STRING,&quot;pfadd&quot;,c-&gt;argv[1],c-&gt;db-&gt;id);        server.dirty++;        HLL_INVALIDATE_CACHE(hdr);    &#125;    addReply(c, updated ? shared.cone : shared.czero);&#125;\nPFADD命令会先判断key是否存在，如果不存在，则创建一个新的HyperLogLog对象；如果存在，会调用isHLLObjectOrReply()函数检查这个对象是不是HyperLogLog对象，检查方法主要是检查魔数是否正确，存储结构是否正确以及头结构体的长度是否正确等。\n一切就绪后，才可以调用hllAdd()函数添加元素。hllAdd函数很简单，只是根据存储结构判断需要调用hllDenseAdd()函数还是hllSparseAdd()函数。\n密集存储结构只是比较新旧计数值，如果新计数值大于就计数值，就将其替代。\n而稀疏存储结构要复杂一些：\n\n判断是否需要调整为密集存储结构，如果不需要则继续进行，否则就先调整为密集存储结构，然后执行添加操作\n我们需要先定位要修改的字节段，通过循环计算每一段表示的桶的范围是否包括要修改的桶\n定位到桶后，如果这个桶已经是VAL，并且计数值大于当前要添加的计数值，则返回0，如果小于当前计数值，就进行更新\n如果是ZERO，并且长度为1，那么可以直接把它替换为VAL，并且设置计数值\n如果不是上述两种情况，则需要对现有的存储进行拆分\n\nPFCOUNT命令/* PFCOUNT var -&gt; approximated cardinality of set. */void pfcountCommand(client *c) &#123;    robj *o;    struct hllhdr *hdr;    uint64_t card;    /* Case 1: multi-key keys, cardinality of the union.     *     * When multiple keys are specified, PFCOUNT actually computes     * the cardinality of the merge of the N HLLs specified. */    if (c-&gt;argc &gt; 2) &#123;        uint8_t max[HLL_HDR_SIZE+HLL_REGISTERS], *registers;        int j;        /* Compute an HLL with M[i] = MAX(M[i]_j). */        memset(max,0,sizeof(max));        hdr = (struct hllhdr*) max;        hdr-&gt;encoding = HLL_RAW; /* Special internal-only encoding. */        registers = max + HLL_HDR_SIZE;        for (j = 1; j &lt; c-&gt;argc; j++) &#123;            /* Check type and size. */            robj *o = lookupKeyRead(c-&gt;db,c-&gt;argv[j]);            if (o == NULL) continue; /* Assume empty HLL for non existing var.*/            if (isHLLObjectOrReply(c,o) != C_OK) return;            /* Merge with this HLL with our &#x27;max&#x27; HHL by setting max[i]             * to MAX(max[i],hll[i]). */            if (hllMerge(registers,o) == C_ERR) &#123;                addReplySds(c,sdsnew(invalid_hll_err));                return;            &#125;        &#125;        /* Compute cardinality of the resulting set. */        addReplyLongLong(c,hllCount(hdr,NULL));        return;    &#125;    /* Case 2: cardinality of the single HLL.     *     * The user specified a single key. Either return the cached value     * or compute one and update the cache. */    o = lookupKeyWrite(c-&gt;db,c-&gt;argv[1]);    if (o == NULL) &#123;        /* No key? Cardinality is zero since no element was added, otherwise         * we would have a key as HLLADD creates it as a side effect. */        addReply(c,shared.czero);    &#125; else &#123;        if (isHLLObjectOrReply(c,o) != C_OK) return;        o = dbUnshareStringValue(c-&gt;db,c-&gt;argv[1],o);        /* Check if the cached cardinality is valid. */        hdr = o-&gt;ptr;        if (HLL_VALID_CACHE(hdr)) &#123;            /* Just return the cached value. */            card = (uint64_t)hdr-&gt;card[0];            card |= (uint64_t)hdr-&gt;card[1] &lt;&lt; 8;            card |= (uint64_t)hdr-&gt;card[2] &lt;&lt; 16;            card |= (uint64_t)hdr-&gt;card[3] &lt;&lt; 24;            card |= (uint64_t)hdr-&gt;card[4] &lt;&lt; 32;            card |= (uint64_t)hdr-&gt;card[5] &lt;&lt; 40;            card |= (uint64_t)hdr-&gt;card[6] &lt;&lt; 48;            card |= (uint64_t)hdr-&gt;card[7] &lt;&lt; 56;        &#125; else &#123;            int invalid = 0;            /* Recompute it and update the cached value. */            card = hllCount(hdr,&amp;invalid);            if (invalid) &#123;                addReplySds(c,sdsnew(invalid_hll_err));                return;            &#125;            hdr-&gt;card[0] = card &amp; 0xff;            hdr-&gt;card[1] = (card &gt;&gt; 8) &amp; 0xff;            hdr-&gt;card[2] = (card &gt;&gt; 16) &amp; 0xff;            hdr-&gt;card[3] = (card &gt;&gt; 24) &amp; 0xff;            hdr-&gt;card[4] = (card &gt;&gt; 32) &amp; 0xff;            hdr-&gt;card[5] = (card &gt;&gt; 40) &amp; 0xff;            hdr-&gt;card[6] = (card &gt;&gt; 48) &amp; 0xff;            hdr-&gt;card[7] = (card &gt;&gt; 56) &amp; 0xff;            /* This is not considered a read-only command even if the             * data structure is not modified, since the cached value             * may be modified and given that the HLL is a Redis string             * we need to propagate the change. */            signalModifiedKey(c-&gt;db,c-&gt;argv[1]);            server.dirty++;        &#125;        addReplyLongLong(c,card);    &#125;&#125;\n如果要计算多个HyperLogLog的基数，则需要将多个HyperLogLog对象合并，这里合并方法是将所有的HyperLogLog对象合并到一个名为max的对象中，max采用的是密集存储结构，如果被合并的对象也是密集存储结构，则循环比较每一个计数值，将大的那个存入max。如果被合并的是稀疏存储，则只需要比较VAL即可。\n如果计算单个HyperLogLog对象的基数，则先判断对象头结构体中的基数缓存是否有效，如果有效，可直接返回。如果已经失效，则需要重新计算基数，并修改原有缓存，这也是PFCOUNT命令不被当做只读命令的原因。\n结语最后，给大家推荐一个帮助理解HyperLogLog原理的工具：http://content.research.neustar.biz/blog/hll.html，有兴趣的话可以去学习一下。\n\n参考阅读 \nRedis new data structure: the HyperLogLog\n探索HyperLogLog算法（含Java实现）\nRedis 深度历险：核心原理与应用实践\n","tags":["Redis"]},{"title":"送给你的算法敲门砖","url":"/2018/09/12/%E9%80%81%E7%BB%99%E4%BD%A0%E7%9A%84%E7%AE%97%E6%B3%95%E6%95%B2%E9%97%A8%E7%A0%96/","content":"在学校的时候老师一直跟我们强调算法的重要性，我和大多数同学一样不以为然，觉得学起来又难又枯燥。直到开始找工作以后，被各种算法面试题打击得体无完肤的时候，才算明白算法的重要性。\n近年来，人工智能、区块链、大数据等概念非常火热，特别是AlphaGo之后，各大公司也都在积极开展人工智能领域的相关业务，而其中算法工程师这个职位更是供不应求，最近在网上见到好多人说要转算法工程师，不管是学医的、学法律的、学建筑的甚至是学经济的，都有做算法工程师的想法。可以说已经达到了人人转算法的地步了，那么为什么大家都想做算法工程师呢？我们先来看一组数据。\n首先是人工智能类岗位的招聘需求\n\n可以看到算法工程师的需求遥遥领先，需求大，机会就多，大家也就都愿意去尝试。但是只是这个原因似乎还不足以吸引这么多人去学算法。我们再来看一下互联网高薪清单\n\n其中，很大一部分都是算法工程师，我们都知道最近几年互联网行业的薪资已经超过了曾经的“高富帅”金融行业。而算法工程师在互联网中又是高薪岗位，这个吸引力可想而知，毕竟没有谁会跟钱过不去。\n上面说的都是国内的情形，在国外，大公司更是注重算法，据说面试Google的Android工程师的时候，会问几道特别难的算法题，而只要这几道题你答得好，Google甚至不在乎你究竟有没有做过Android，因为在他们看来，这么难的算法题都能学会，那么学习Android的技术根本不在话下。\n总得来说，算法可以被称为大公司的敲门砖，说到这，有的人可能已经等不及了，算法是大公司的敲门砖这件事我已经知道了，那你说的算法的敲门砖到底是什么啊？\n其实，今天要推荐的是一个算法学习的网站，对于算法初学者，可能脑子里还不能很好的理清每个算法究竟是怎么一回事，或者说算法的工作原理到底什么样的。这个网站就是将算法可视化，把它的工作原理摆在你面前。\n\n可以看到这里有很多算法和数据结构，包括排序算法、链表、Hash表、二叉搜索树等等。可以说应有尽有，只要你想学，就可以直接搜索相关的算法。点进去之后，会有相关的提示，然后就可以观看相关算法的工作视频了。这对于我们理解算法非常有帮助，把算法做到可视化，并且为你演示每一步如何工作，侧边栏还有相应的讲解。\n\n总之，这个网站对于初学算法的同学来说算是一个非常好用的敲门砖了。这个网站的地址是：\nhttps://visualgo.net/\n建议大家最好看英文的，因为中文的翻译还是有些问题。可能在学习过程中造成一定的困惑。\n最后，希望大家不要像我一样从入门到放弃，坚持下去，年薪百万不是梦。\n","tags":["瞎扯"]},{"title":"速度不够，管道来凑——Redis管道技术","url":"/2019/04/27/%E9%80%9F%E5%BA%A6%E4%B8%8D%E5%A4%9F%EF%BC%8C%E7%AE%A1%E9%81%93%E6%9D%A5%E5%87%91%E2%80%94%E2%80%94Redis%E7%AE%A1%E9%81%93%E6%8A%80%E6%9C%AF/","content":"Redis客户端与服务器之间使用TCP协议进行通信，并且很早就支持管道（pipelining）技术了。在某些高并发的场景下，网络开销成了Redis速度的瓶颈，所以需要使用管道技术来实现突破。\n在介绍管道之前，先来想一下单条命令的执行步骤：\n\n客户端把命令发送到服务器，然后阻塞客户端，等待着从socket读取服务器的返回结果\n服务器处理命令并将结果返回给客户端\n\n按照这样的描述，每个命令的执行时间 = 客户端发送时间+服务器处理和返回时间+一个网络来回的时间\n其中一个网络来回的时间是不固定的，它的决定因素有很多，比如客户端到服务器要经过多少跳，网络是否拥堵等等。但是这个时间的量级也是最大的，也就是说一个命令的完成时间的长度很大程度上取决于网络开销。如果我们的服务器每秒可以处理10万条请求，而网络开销是250毫秒，那么实际上每秒钟只能处理4个请求。最暴力的优化方法就是使客户端和服务器在一台物理机上，这样就可以将网络开销降低到1ms以下。但是实际的生产环境我们并不会这样做。而且即使使用这种方法，当请求非常频繁时，这个时间和服务器处理时间比较仍然是很长的。\nRedis Pipelining为了解决这种问题，Redis在很早就支持了管道技术。也就是说客户端可以一次发送多条命令，不用逐条等待命令的返回值，而是到最后一起读取返回结果，这样只需要一次网络开销，速度就会得到明显的提升。管道技术其实已经非常成熟并且得到广泛应用了，例如POP3协议由于支持管道技术，从而显著提高了从服务器下载邮件的速度。\n在Redis中，如果客户端使用管道发送了多条命令，那么服务器就会将多条命令放入一个队列中，这一操作会消耗一定的内存，所以管道中命令的数量并不是越大越好（太大容易撑爆内存），而是应该有一个合理的值。\n深入理解Redis交互流程管道并不只是用来网络开销延迟的一种方法，它实际上是会提升Redis服务器每秒操作总数的。在解释原因之前，需要更深入的了解Redis命令处理过程。\n\n一个完整的交互流程如下：\n\n客户端进程调用write()把消息写入到操作系统内核为Socket分配的send buffer中\n操作系统会把send buffer中的内容写入网卡，网卡再通过网关路由把内容发送到服务器端的网卡\n服务端网卡会把接收到的消息写入操作系统为Socket分配的recv buffer\n服务器进程调用read()读取消息然后进行处理\n处理完成后调用write()把返回结果写入到服务器端的send buffer\n服务器操作系统再将send buffer中的内容写入网卡，然后发送到客户端\n客户端操作系统将网卡内容读到recv buffer中\n客户端进程调用read()从recv buffer中读取消息并返回\n\n现在我们把命令执行的时间进一步细分：\n命令的执行时间 = 客户端调用write并写网卡时间+一次网络开销的时间+服务读网卡并调用read时间++服务器处理数据时间+服务端调用write并写网卡时间+客户端读网卡并调用read时间\n这其中除了网络开销，花费时间最长的就是进行系统调用write()和read()了，这一过程需要操作系统由用户态切换到内核态，中间涉及到的上下文切换会浪费很多时间。\n使用管道时，多个命令只会进行一次read()和wrtie()系统调用，因此使用管道会提升Redis服务器处理命令的速度，随着管道中命令的增多，服务器每秒处理请求的数量会线性增长，最后会趋近于不使用管道的10倍。\n\n和Scripting对比对于管道的大部分应用场景而言，使用Redis脚本（Redis2.6及以后的版本）会使服务器端有更好的表现。使用脚本最大的好处就是可以以最小的延迟读写数据。\n有时我们也需要在管道中使用EVAL和EVALSHA命令，这是完全有可能的。因此Redis提供了SCRIPT LOAD命令来支持这种情况。\n眼见为实多说无益，还是眼见为实。下面就来对比一下使用管道和不使用管道的速度差异。\npublic class JedisDemo &#123;    private static int COMMAND_NUM = 1000;        private static String REDIS_HOST = &quot;Redis服务器IP&quot;;    public static void main(String[] args) &#123;        Jedis jedis = new Jedis(REDIS_HOST, 6379);        withoutPipeline(jedis);        withPipeline(jedis);    &#125;    private static void withoutPipeline(Jedis jedis) &#123;        Long start = System.currentTimeMillis();        for (int i = 0; i &lt; COMMAND_NUM; i++) &#123;            jedis.set(&quot;no_pipe_&quot; + String.valueOf(i), String.valueOf(i), SetParams.setParams().ex(60));        &#125;        long end = System.currentTimeMillis();        long cost = end - start;        System.out.println(&quot;withoutPipeline cost : &quot; + cost + &quot; ms&quot;);    &#125;    private static void withPipeline(Jedis jedis) &#123;        Pipeline pipe = jedis.pipelined();        long start_pipe = System.currentTimeMillis();        for (int i = 0; i &lt; COMMAND_NUM; i++) &#123;            pipe.set(&quot;pipe_&quot; + String.valueOf(i), String.valueOf(i), SetParams.setParams().ex(60));        &#125;        pipe.sync(); // 获取所有的response        long end_pipe = System.currentTimeMillis();        long cost_pipe = end_pipe - start_pipe;        System.out.println(&quot;withPipeline cost : &quot; + cost_pipe + &quot; ms&quot;);    &#125;&#125;\n结果也符合我们的预期：\nwithoutPipeline cost : 11791 mswithPipeline cost : 55 ms\n总结\n使用管道技术可以显著提升Redis处理命令的速度，其原理就是将多条命令打包，只需要一次网络开销，在服务器端和客户端各一次read()和write()系统调用，以此来节约时间。\n\n管道中的命令数量要适当，并不是越多越好。\n\nRedis2.6版本以后，脚本在大部分场景中的表现要优于管道。\n\n\n扩展前面我们提到，为了解决网络开销带来的延迟问题，可以把客户端和服务器放到一台物理机上。但是有时用benchmark进行压测的时候发现这仍然很慢。\n这时客户端和服务端实际是在一台物理机上的，所有的操作都在内存中进行，没有网络延迟，按理来说这样的操作应该是非常快的。为什么会出现上面的情况的呢？\n实际上，这是由内核调度导致的。比如说，benchmark运行时，读取了服务器返回的结果，然后写了一个新的命令。这个命令就在回环接口的send buffer中了，如果要执行这个命令，内核需要唤醒Redis服务器进程。所以在某些情况下，本地接口也会出现类似于网络延迟的延迟。其实是内核特别繁忙，一直没有调度到Redis服务器进程。\n参考Redis官方文档\nRedis源码\n掘金小册：《Redis 深度历险：核心原理与应用实践》\n","tags":["Redis"]}]