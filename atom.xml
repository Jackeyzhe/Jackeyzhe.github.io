<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jackeyzhe&#39;s Blog</title>
  
  <subtitle>靠脸吃饭</subtitle>
  <link href="https://jackeyzhe.github.io/atom.xml" rel="self"/>
  
  <link href="https://jackeyzhe.github.io/"/>
  <updated>2026-01-25T15:13:10.985Z</updated>
  <id>https://jackeyzhe.github.io/</id>
  
  <author>
    <name>Jackey Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>从零学习Kafka：集群架构和基本概念</title>
    <link href="https://jackeyzhe.github.io/2026/01/16/%E4%BB%8E%E9%9B%B6%E5%AD%A6%E4%B9%A0Kafka%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>https://jackeyzhe.github.io/2026/01/16/%E4%BB%8E%E9%9B%B6%E5%AD%A6%E4%B9%A0Kafka%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</id>
    <published>2026-01-16T08:48:07.000Z</published>
    <updated>2026-01-25T15:13:10.985Z</updated>
    
    <content type="html"><![CDATA[<p>在前文中，我们从源码层面介绍了 Flink 的 Kafka Connector 的实现。从本文开始，我们的目标也正式从 Flink 过渡到 Kafka。<span id="more"></span></p><h3 id="什么是-Kafka"><a href="#什么是-Kafka" class="headerlink" title="什么是 Kafka"></a>什么是 Kafka</h3><p>Kafka 官方文档给出的定义是：Apache Kafka 是一个事件流平台。它的关键能力如下：</p><ol><li><p>发布（写入）和订阅（读取）事件流，包括从其他系统持续导入和导出数据。</p></li><li><p>将事件流持久可靠的存储，可以自定义时长。</p></li><li><p>实时处理事件流或历史回溯。</p></li></ol><p>我们通过一个最基础的场景来理解一下这些定义：系统 A 将数据发送给 Kafka，Kafka 会将数据进行持久化存储，系统 B 从 Kafka 中读取系统 A 发送的数据。那么这里有两个关键问题：一是如何设计事件的编码格式，二是使用什么样的传输协议。</p><p>关于编码格式，目前有很多成熟的方案，比如 Protocol Buffer、Thrift、JSON 等。而 Kafka 使用的是纯二进制的字节序列。</p><p>确定好了编码格式之后，我们再来看传输协议。常见的传输协议有两种：</p><ol><li><p>点对点模型：点对点模型也叫消息队列模型，即上游发送的数据只能由一个下游接收。对上面的场景来说，就是系统 A 发送的消息，只能由系统 B 接收。</p></li><li><p>发布/订阅模型：在这个模型中有 topic 的概念，会存在多个发布者向同一个 topic 发送消息，多个订阅者从 topic 读取消息的情况。</p></li></ol><p>Kafka 同时支持这两种模型，这点我们在后面会具体阐述。</p><p>现在知道了什么是 Kafka 之后，我们再来考虑另一个问题：为什么要使用 Kafka？</p><p>这个问题每个人可能会有不同的答案，对我而言，使用 Kafka 最主要的原因是做流量缓冲和数据同步。</p><p>流量缓冲是 Kafka 的应用场景之一，在上面的例子中，当系统 A 有很大的突增流量时，如果直接对接系统 B，那么瞬时流量很有可能直接把系统 B 打挂。而 Kafka 就可以在中间起到一个缓冲的作用。给系统 B 留出充足的处理时间，同时也避免了因为系统 B 崩溃可能导致的整个链路的雪崩问题。</p><p>数据同步在实际应用场景中更多与 Flink 结合，完成离线数据链路流转或者离线到在线的数据传输。</p><p>有了这些基本的背景信息之后，我们再来看一些 Kafka 的核心概念。</p><h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><p>我们从一张图开始，对图中的概念逐个进行解释。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1769137576/Blog/Kafka/0/KafkaCluster.png" alt="KafkaCluster"></p><p>首先是 Topic，在 Kafka 中发布和订阅的对象就是 Topic。通常我们按照业务来拆分，把不同业务的数据放在不同的 Topic 中。</p><p>Topic 的数据是由 Producer 生产的。Producer 即生产者程序，它和 Consumer 都可以被称为 Kafka 集群的客户端。Producer 负责把消息写入对应的 Topic 中，在写入时会选择对应的 Partition。</p><p>Partition 类似于 HBase 中的 region，它是用来支撑横向扩展的。当我们的消息量太大时，一个 Partition 存不下或者处理不过来，我们可以选择扩 Partition 的数量。利用多分区分散存储和处理请求的压力。你可能还注意到，Partition 还有 Leader 和 Follower 之分，这其实是 Kafka 的副本机制（Replica），它是 Kafka 高可用手段之一。每个分区都会有 1 个 Leader 副本和 n 个 Follower 副本，写入消息和消费消息都是由 Leader 副本执行，Follower 只负责向 Leader 副本发送请求，将最新的消息发送给它，这样与 Leader 副本保持同步。</p><p>在 Kafka 集群的服务端，由 Broker 进程对客户端的请求进行处理。将多个 Broker 集群分别部署在不同的机器上是 Kafka 的高可用手段之一。</p><p>前面我们提到了 Kafka 除了发布/订阅模型之外，还支持点对点模型，对于点对点模型的支持，Kafka 是引入了 Consumer Group 的概念。Consumer Group 是由多个 Consumer 组成，负责消费一组主题，这组主题中的每一个 Partition 只能由其中的一个 Consumer 消费。如果有新增的 Consumer 加入或者现有的 Consumer 崩溃，那么就会进行 Rebalance。</p><p>在消息写入和消费流程中，分别有 Offset 和 Consumer Offset 两个概念。Offset 是在分区中递增的。Consumer Offset 是消费者用来记录自己消费到了哪个 Offset。</p><p>最后再多提一点，Kafka Broker 是使用 Log 保存数据，Log 底层又分成了多个 Log Segment，消息在 Log Segment 上只能追加写入，当一个 Log Segment 写满后，就会创建新的 Log Segment 继续写入。Kafka 还会定期清理旧的 Log Segment，以此来节省磁盘空间。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们先是简单介绍了什么是 Kafka，接着又介绍了几个核心概念。包括 Topic、Producer、Consumer、Consumer Group、Partition、Replica、Broker、Rebalance等。我们从这里开始一起来学习 Kafka 相关的知识。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在前文中，我们从源码层面介绍了 Flink 的 Kafka Connector 的实现。从本文开始，我们的目标也正式从 Flink 过渡到 Kafka。</summary>
    
    
    
    
    <category term="Kafka" scheme="https://jackeyzhe.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：Kafka Connector</title>
    <link href="https://jackeyzhe.github.io/2026/01/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AKafka-Connector/"/>
    <id>https://jackeyzhe.github.io/2026/01/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AKafka-Connector/</id>
    <published>2026-01-12T06:39:21.000Z</published>
    <updated>2026-01-14T11:51:36.671Z</updated>
    
    <content type="html"><![CDATA[<p>本文我们来梳理 Kafka Connector 相关的源码。<span id="more"></span></p><h3 id="自定义-Source-和-Sink"><a href="#自定义-Source-和-Sink" class="headerlink" title="自定义 Source 和 Sink"></a>自定义 Source 和 Sink</h3><p>在介绍 Kafka Connector 之前，我们先来看一下在 Flink 中是如何支持自定义 Source 和 Sink 的。我们来看一张 Flink 官方文档提供的图。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768288302/Blog/flink/22/table_connectors.png" alt="table-connector"></p><p>这张图展示了 Connector 的基本体系结构，三层架构也非常清晰。</p><h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><p>首先是最上层的 MetaData，CREATE TABLE 会更新 Catalog，然后被转换为 TableAPI 的 CatalogTable，CatalogTable 实例用于表示动态表（Source 或 Sink 表）的元信息。</p><h4 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h4><p>在解析和优化程序时，会将 CatalogTable 转换为 DynamicTableSource 和 DynamicTableSink，分别用于查询和插入数据，这两个实例的创建都需要对应的工厂类，工厂类的完整路径需要放到这个配置文件中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">META-INF/services/org.apache.flink.table.factories.Factory</span><br></pre></td></tr></table></figure><p>如果有需要的话，我们还可以在解析过程中配置编码和解码方法。</p><p>在 Source 端，通过三个接口支持不同的查询能力。</p><ul><li><p>ScanTableSource：用于消费 changelog 流，扫描的数据支持 insert、updata、delete 三种类型。ScanTableSource 还支持很多其他的功能， 都是通过接口提供的。具体可以看参考这个连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/sourcessinks/#source-abilities</span><br></pre></td></tr></table></figure></li><li><p>LookupTableSource：LookupTableSource 不会全量读取表的数据，它在需要时会发送请求，懒加载数据。目前只支持 insert-only 变更模式。</p></li><li><p>VectorSearchTableSource：使用一个输入向量来搜索数据，并返回最相似的 Top-K 行数据。</p></li></ul><p>在 Sink 端，通过 DynamicTableSink 来实现具体的写入逻辑，这里也提供了一些用于扩展能力的接口。具体参考</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/sourcessinks/#sink-abilities</span><br></pre></td></tr></table></figure><h4 id="Runtime"><a href="#Runtime" class="headerlink" title="Runtime"></a>Runtime</h4><p>逻辑解析完成后，会到 Runtime 层。这里就是定义几个 Provider，在 Provider 中实现和连接器具体的交互逻辑。</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>当我们需要创建一个自定义的 Source 和 Sink 时，就可以通过以下步骤实现。</p><ol><li><p>定义 Flink SQL 的 DDL，需要定义相应的 Options。</p></li><li><p>实现 DynamicTableSourceFactory 和 DynamicTableSinkFactory，并把实现类的具体路径写到配置文件中。</p></li><li><p>实现 DynamicTableSource 和 DynamicTableSink，这里需要处理 SQL 层的元数据。</p></li><li><p>提供 Provider，将逻辑层与底层 DataStream 关联起来。</p></li><li><p>编写底层算子，实现 Source 和 Sink 接口。</p></li></ol><h3 id="Kafka-Connector-的实现"><a href="#Kafka-Connector-的实现" class="headerlink" title="Kafka Connector 的实现"></a>Kafka Connector 的实现</h3><p>带着这些知识，我们一起来看一下 Kafka Connector 相关的源码。</p><p>Kafka Connector 代码目前已经是一个独立的项目了。项目地址是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/apache/flink-connector-kafka</span><br></pre></td></tr></table></figure><h4 id="Factory"><a href="#Factory" class="headerlink" title="Factory"></a>Factory</h4><p>我们首先找到定义的工厂类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory</span><br><span class="line">org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory</span><br></pre></td></tr></table></figure><p>以 KafkaDynamicTableFactory 为例，它同时实现了 DynamicTableSourceFactory 和 DynamicTableSinkFactory 两个接口。</p><p>KafkaDynamicTableFactory 包含以下几个方法。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768294001/Blog/flink/22/KafkaDynamicTableFactory.png" alt="KafkaDynamicTableFactory"></p><ul><li><p>factoryIdentifier：返回一个唯一标识符，对应 Flink SQL 中 connector=’xxx’ 这个配置。</p></li><li><p>requiredOptions：必填配置集合。</p></li><li><p>optionalOptions：选填配置集合。</p></li><li><p>forwardOptions：直接传递到 Runtime 层的配置集合。</p></li><li><p>createDynamicTableSource：创建 DynamicTableSource。</p></li><li><p>createDynamicTableSink：创建 DynamicTableSink。</p></li></ul><h4 id="Source-端"><a href="#Source-端" class="headerlink" title="Source 端"></a>Source 端</h4><p>工厂类的 createDynamicTableSource 方法创建了 DynamicTableSource，我们来看一下创建的逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DynamicTableSource <span class="title function_">createDynamicTableSource</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">TableFactoryHelper</span> <span class="variable">helper</span> <span class="operator">=</span> FactoryUtil.createTableFactoryHelper(<span class="built_in">this</span>, context);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Optional&lt;DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt;&gt; keyDecodingFormat =</span><br><span class="line">            getKeyDecodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; valueDecodingFormat =</span><br><span class="line">            getValueDecodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    helper.validateExcept(PROPERTIES_PREFIX);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">ReadableConfig</span> <span class="variable">tableOptions</span> <span class="operator">=</span> helper.getOptions();</span><br><span class="line"></span><br><span class="line">    validateTableSourceOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    validatePKConstraints(</span><br><span class="line">            context.getObjectIdentifier(),</span><br><span class="line">            context.getPrimaryKeyIndexes(),</span><br><span class="line">            context.getCatalogTable().getOptions(),</span><br><span class="line">            valueDecodingFormat);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">StartupOptions</span> <span class="variable">startupOptions</span> <span class="operator">=</span> getStartupOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">BoundedOptions</span> <span class="variable">boundedOptions</span> <span class="operator">=</span> getBoundedOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> getKafkaProperties(context.getCatalogTable().getOptions());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add topic-partition discovery</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Duration</span> <span class="variable">partitionDiscoveryInterval</span> <span class="operator">=</span></span><br><span class="line">            tableOptions.get(SCAN_TOPIC_PARTITION_DISCOVERY);</span><br><span class="line">    properties.setProperty(</span><br><span class="line">            KafkaSourceOptions.PARTITION_DISCOVERY_INTERVAL_MS.key(),</span><br><span class="line">            Long.toString(partitionDiscoveryInterval.toMillis()));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">DataType</span> <span class="variable">physicalDataType</span> <span class="operator">=</span> context.getPhysicalRowDataType();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] keyProjection = createKeyFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] valueProjection = createValueFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">keyPrefix</span> <span class="operator">=</span> tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Integer</span> <span class="variable">parallelism</span> <span class="operator">=</span> tableOptions.getOptional(SCAN_PARALLELISM).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> createKafkaTableSource(</span><br><span class="line">            physicalDataType,</span><br><span class="line">            keyDecodingFormat.orElse(<span class="literal">null</span>),</span><br><span class="line">            valueDecodingFormat,</span><br><span class="line">            keyProjection,</span><br><span class="line">            valueProjection,</span><br><span class="line">            keyPrefix,</span><br><span class="line">            getTopics(tableOptions),</span><br><span class="line">            getTopicPattern(tableOptions),</span><br><span class="line">            properties,</span><br><span class="line">            startupOptions.startupMode,</span><br><span class="line">            startupOptions.specificOffsets,</span><br><span class="line">            startupOptions.startupTimestampMillis,</span><br><span class="line">            boundedOptions.boundedMode,</span><br><span class="line">            boundedOptions.specificOffsets,</span><br><span class="line">            boundedOptions.boundedTimestampMillis,</span><br><span class="line">            context.getObjectIdentifier().asSummaryString(),</span><br><span class="line">            parallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个方法中，首先要获取到 key 和 value 的解码格式。接着是各种参数校验和获取必要的属性。最后创建 KafkaDynamicSource 实例。</p><p>获取解码格式需要用到 DeserializationFormatFactory 工厂，DeserializationFormatFactory 有多个实现类，对应了多种格式的反序列化方法。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768297428/Blog/flink/22/DeserializationFormatFactory.png" alt="DeserializationFormatFactory"></p><p>我们来看比较常见的 Json 格式的工厂 JsonFormatFactory。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; <span class="title function_">createDecodingFormat</span><span class="params">(</span></span><br><span class="line"><span class="params">        DynamicTableFactory.Context context, ReadableConfig formatOptions)</span> &#123;</span><br><span class="line">    FactoryUtil.validateFactoryOptions(<span class="built_in">this</span>, formatOptions);</span><br><span class="line">    JsonFormatOptionsUtil.validateDecodingFormatOptions(formatOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">failOnMissingField</span> <span class="operator">=</span> formatOptions.get(FAIL_ON_MISSING_FIELD);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">ignoreParseErrors</span> <span class="operator">=</span> formatOptions.get(IGNORE_PARSE_ERRORS);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">jsonParserEnabled</span> <span class="operator">=</span> formatOptions.get(DECODE_JSON_PARSER_ENABLED);</span><br><span class="line">    <span class="type">TimestampFormat</span> <span class="variable">timestampOption</span> <span class="operator">=</span> JsonFormatOptionsUtil.getTimestampFormat(formatOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ProjectableDecodingFormat</span>&lt;DeserializationSchema&lt;RowData&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> DeserializationSchema&lt;RowData&gt; <span class="title function_">createRuntimeDecoder</span><span class="params">(</span></span><br><span class="line"><span class="params">                DynamicTableSource.Context context,</span></span><br><span class="line"><span class="params">                DataType physicalDataType,</span></span><br><span class="line"><span class="params">                <span class="type">int</span>[][] projections)</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="type">DataType</span> <span class="variable">producedDataType</span> <span class="operator">=</span></span><br><span class="line">                    Projection.of(projections).project(physicalDataType);</span><br><span class="line">            <span class="keyword">final</span> <span class="type">RowType</span> <span class="variable">rowType</span> <span class="operator">=</span> (RowType) producedDataType.getLogicalType();</span><br><span class="line">            <span class="keyword">final</span> TypeInformation&lt;RowData&gt; rowDataTypeInfo =</span><br><span class="line">                    context.createTypeInformation(producedDataType);</span><br><span class="line">            <span class="keyword">if</span> (jsonParserEnabled) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">JsonParserRowDataDeserializationSchema</span>(</span><br><span class="line">                        rowType,</span><br><span class="line">                        rowDataTypeInfo,</span><br><span class="line">                        failOnMissingField,</span><br><span class="line">                        ignoreParseErrors,</span><br><span class="line">                        timestampOption,</span><br><span class="line">                        toProjectedNames(</span><br><span class="line">                                (RowType) physicalDataType.getLogicalType(), projections));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">JsonRowDataDeserializationSchema</span>(</span><br><span class="line">                        rowType,</span><br><span class="line">                        rowDataTypeInfo,</span><br><span class="line">                        failOnMissingField,</span><br><span class="line">                        ignoreParseErrors,</span><br><span class="line">                        timestampOption);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> ChangelogMode <span class="title function_">getChangelogMode</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> ChangelogMode.insertOnly();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">supportsNestedProjection</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> jsonParserEnabled;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在创建解码格式时，最重要的是创建运行时的解码器，也就是 DeserializationSchema，在 JsonFormatFactory 中，有 JsonParserRowDataDeserializationSchema 和 JsonRowDataDeserializationSchema 两种实现，分别是用于将 JsonParser 和 JsonNode 转换成为 RowData，具体的逻辑都在 createNotNullConverter 方法中。</p><p>了解完解码格式后，我们把视角拉回到 KafkaDynamicSource，它实现了三个接口 ScanTableSource、SupportsReadingMetadata、SupportsWatermarkPushDown。分别用于消费数据，读取元数据和生成水印。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ScanRuntimeProvider <span class="title function_">getScanRuntimeProvider</span><span class="params">(ScanContext context)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> DeserializationSchema&lt;RowData&gt; keyDeserialization =</span><br><span class="line">            createDeserialization(context, keyDecodingFormat, keyProjection, keyPrefix);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> DeserializationSchema&lt;RowData&gt; valueDeserialization =</span><br><span class="line">            createDeserialization(context, valueDecodingFormat, valueProjection, <span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> TypeInformation&lt;RowData&gt; producedTypeInfo =</span><br><span class="line">            context.createTypeInformation(producedDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> KafkaSource&lt;RowData&gt; kafkaSource =</span><br><span class="line">            createKafkaSource(keyDeserialization, valueDeserialization, producedTypeInfo);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">DataStreamScanProvider</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> DataStream&lt;RowData&gt; <span class="title function_">produceDataStream</span><span class="params">(</span></span><br><span class="line"><span class="params">                ProviderContext providerContext, StreamExecutionEnvironment execEnv)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (watermarkStrategy == <span class="literal">null</span>) &#123;</span><br><span class="line">                watermarkStrategy = WatermarkStrategy.noWatermarks();</span><br><span class="line">            &#125;</span><br><span class="line">            DataStreamSource&lt;RowData&gt; sourceStream =</span><br><span class="line">                    execEnv.fromSource(</span><br><span class="line">                            kafkaSource, watermarkStrategy, <span class="string">&quot;KafkaSource-&quot;</span> + tableIdentifier);</span><br><span class="line">            providerContext.generateUid(KAFKA_TRANSFORMATION).ifPresent(sourceStream::uid);</span><br><span class="line">            <span class="keyword">return</span> sourceStream;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isBounded</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> kafkaSource.getBoundedness() == Boundedness.BOUNDED;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Optional&lt;Integer&gt; <span class="title function_">getParallelism</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> Optional.ofNullable(parallelism);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 ScanRuntimeProvider 的逻辑中，先获取到反序列化器，也就是刚刚我们提到的 DeserializationSchema。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768318497/Blog/flink/22/KafkaSource.png" alt="KafkaSource"></p><p>然后开始创建 KafkaSource 实例，它是 Source 的实现类，也就是执行引擎层了，这个过程会依次创建图中这些类。</p><p>KafkaSource 中主要是创建 KafkaSourceReader 和 KafkaSourceEnumerator，KafkaSourceEnumerator 是负责和分片相关的逻辑，包括分片分配和分片发现等。</p><p>KafkaSourceReader 中主要是和 State 相关的逻辑，包括触发快照和完成 Checkpoint 通知的方法。当做 Snapshot 时，会记录活跃 split 的 offset，同时将 split 作为状态提交。当 Checkpoint 完成时，会调用 <code>KafkaSourceFetcherManager.commitOffsets</code> 提交 offset。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;KafkaPartitionSplit&gt; <span class="title function_">snapshotState</span><span class="params">(<span class="type">long</span> checkpointId)</span> &#123;</span><br><span class="line">    List&lt;KafkaPartitionSplit&gt; splits = <span class="built_in">super</span>.snapshotState(checkpointId);</span><br><span class="line">    <span class="keyword">if</span> (!commitOffsetsOnCheckpoint) &#123;</span><br><span class="line">        <span class="keyword">return</span> splits;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (splits.isEmpty() &amp;&amp; offsetsOfFinishedSplits.isEmpty()) &#123;</span><br><span class="line">        offsetsToCommit.put(checkpointId, Collections.emptyMap());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        Map&lt;TopicPartition, OffsetAndMetadata&gt; offsetsMap =</span><br><span class="line">                offsetsToCommit.computeIfAbsent(checkpointId, id -&gt; <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;());</span><br><span class="line">        <span class="comment">// Put the offsets of the active splits.</span></span><br><span class="line">        <span class="keyword">for</span> (KafkaPartitionSplit split : splits) &#123;</span><br><span class="line">            <span class="comment">// If the checkpoint is triggered before the partition starting offsets</span></span><br><span class="line">            <span class="comment">// is retrieved, do not commit the offsets for those partitions.</span></span><br><span class="line">            <span class="keyword">if</span> (split.getStartingOffset() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                offsetsMap.put(</span><br><span class="line">                        split.getTopicPartition(),</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(split.getStartingOffset()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Put offsets of all the finished splits.</span></span><br><span class="line">        offsetsMap.putAll(offsetsOfFinishedSplits);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">notifyCheckpointComplete</span><span class="params">(<span class="type">long</span> checkpointId)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Committing offsets for checkpoint &#123;&#125;&quot;</span>, checkpointId);</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    ((KafkaSourceFetcherManager) splitFetcherManager)</span><br><span class="line">            .commitOffsets(</span><br><span class="line">                    committedPartitions,</span><br><span class="line">                    (ignored, e) -&gt; &#123;...&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>KafkaSourceFetcherManager 负责管理 fetcher 线程，提交 Offset。</p><p>KafkaPartitionSplitReader 的 fetch 方法用来消费 Kafka 的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> RecordsWithSplitIds&lt;ConsumerRecord&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt;&gt; fetch() <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    ConsumerRecords&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt; consumerRecords;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        consumerRecords = consumer.poll(Duration.ofMillis(POLL_TIMEOUT));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (WakeupException | IllegalStateException e) &#123;</span><br><span class="line">        <span class="comment">// IllegalStateException will be thrown if the consumer is not assigned any partitions.</span></span><br><span class="line">        <span class="comment">// This happens if all assigned partitions are invalid or empty (starting offset &gt;=</span></span><br><span class="line">        <span class="comment">// stopping offset). We just mark empty partitions as finished and return an empty</span></span><br><span class="line">        <span class="comment">// record container, and this consumer will be closed by SplitFetcherManager.</span></span><br><span class="line">        <span class="type">KafkaPartitionSplitRecords</span> <span class="variable">recordsBySplits</span> <span class="operator">=</span></span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">KafkaPartitionSplitRecords</span>(</span><br><span class="line">                        ConsumerRecords.empty(), kafkaSourceReaderMetrics);</span><br><span class="line">        markEmptySplitsAsFinished(recordsBySplits);</span><br><span class="line">        <span class="keyword">return</span> recordsBySplits;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">KafkaPartitionSplitRecords</span> <span class="variable">recordsBySplits</span> <span class="operator">=</span></span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">KafkaPartitionSplitRecords</span>(consumerRecords, kafkaSourceReaderMetrics);</span><br><span class="line">    List&lt;TopicPartition&gt; finishedPartitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : consumer.assignment()) &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">stoppingOffset</span> <span class="operator">=</span> getStoppingOffset(tp);</span><br><span class="line">        <span class="type">long</span> <span class="variable">consumerPosition</span> <span class="operator">=</span> getConsumerPosition(tp, <span class="string">&quot;retrieving consumer position&quot;</span>);</span><br><span class="line">        <span class="comment">// Stop fetching when the consumer&#x27;s position reaches the stoppingOffset.</span></span><br><span class="line">        <span class="comment">// Control messages may follow the last record; therefore, using the last record&#x27;s</span></span><br><span class="line">        <span class="comment">// offset as a stopping condition could result in indefinite blocking.</span></span><br><span class="line">        <span class="keyword">if</span> (consumerPosition &gt;= stoppingOffset) &#123;</span><br><span class="line">            LOG.debug(</span><br><span class="line">                    <span class="string">&quot;Position of &#123;&#125;: &#123;&#125;, has reached stopping offset: &#123;&#125;&quot;</span>,</span><br><span class="line">                    tp,</span><br><span class="line">                    consumerPosition,</span><br><span class="line">                    stoppingOffset);</span><br><span class="line">            recordsBySplits.setPartitionStoppingOffset(tp, stoppingOffset);</span><br><span class="line">            finishSplitAtRecord(</span><br><span class="line">                    tp, stoppingOffset, consumerPosition, finishedPartitions, recordsBySplits);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Only track non-empty partition&#x27;s record lag if it never appears before</span></span><br><span class="line">    consumerRecords</span><br><span class="line">            .partitions()</span><br><span class="line">            .forEach(</span><br><span class="line">                    trackTp -&gt; &#123;</span><br><span class="line">                        kafkaSourceReaderMetrics.maybeAddRecordsLagMetric(consumer, trackTp);</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line">    markEmptySplitsAsFinished(recordsBySplits);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unassign the partitions that has finished.</span></span><br><span class="line">    <span class="keyword">if</span> (!finishedPartitions.isEmpty()) &#123;</span><br><span class="line">        finishedPartitions.forEach(kafkaSourceReaderMetrics::removeRecordsLagMetric);</span><br><span class="line">        unassignPartitions(finishedPartitions);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Update numBytesIn</span></span><br><span class="line">    kafkaSourceReaderMetrics.updateNumBytesInCounter();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> recordsBySplits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此，Source 端相关的源码我们就梳理完了。接下来我们再看 Sink 端的代码。</p><h4 id="Sink-端"><a href="#Sink-端" class="headerlink" title="Sink 端"></a>Sink 端</h4><p>我们从工厂类中的 createDynamicTableSink 方法开始。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DynamicTableSink <span class="title function_">createDynamicTableSink</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">TableFactoryHelper</span> <span class="variable">helper</span> <span class="operator">=</span></span><br><span class="line">            FactoryUtil.createTableFactoryHelper(</span><br><span class="line">                    <span class="built_in">this</span>, autoCompleteSchemaRegistrySubject(context));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Optional&lt;EncodingFormat&lt;SerializationSchema&lt;RowData&gt;&gt;&gt; keyEncodingFormat =</span><br><span class="line">            getKeyEncodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> EncodingFormat&lt;SerializationSchema&lt;RowData&gt;&gt; valueEncodingFormat =</span><br><span class="line">            getValueEncodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    helper.validateExcept(PROPERTIES_PREFIX);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">ReadableConfig</span> <span class="variable">tableOptions</span> <span class="operator">=</span> helper.getOptions();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">DeliveryGuarantee</span> <span class="variable">deliveryGuarantee</span> <span class="operator">=</span> validateDeprecatedSemantic(tableOptions);</span><br><span class="line">    validateTableSinkOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    KafkaConnectorOptionsUtil.validateDeliveryGuarantee(tableOptions);</span><br><span class="line"></span><br><span class="line">    validatePKConstraints(</span><br><span class="line">            context.getObjectIdentifier(),</span><br><span class="line">            context.getPrimaryKeyIndexes(),</span><br><span class="line">            context.getCatalogTable().getOptions(),</span><br><span class="line">            valueEncodingFormat);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">DataType</span> <span class="variable">physicalDataType</span> <span class="operator">=</span> context.getPhysicalRowDataType();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] keyProjection = createKeyFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] valueProjection = createValueFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">keyPrefix</span> <span class="operator">=</span> tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Integer</span> <span class="variable">parallelism</span> <span class="operator">=</span> tableOptions.getOptional(SINK_PARALLELISM).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> createKafkaTableSink(</span><br><span class="line">            physicalDataType,</span><br><span class="line">            keyEncodingFormat.orElse(<span class="literal">null</span>),</span><br><span class="line">            valueEncodingFormat,</span><br><span class="line">            keyProjection,</span><br><span class="line">            valueProjection,</span><br><span class="line">            keyPrefix,</span><br><span class="line">            getTopics(tableOptions),</span><br><span class="line">            getTopicPattern(tableOptions),</span><br><span class="line">            getKafkaProperties(context.getCatalogTable().getOptions()),</span><br><span class="line">            getFlinkKafkaPartitioner(tableOptions, context.getClassLoader()).orElse(<span class="literal">null</span>),</span><br><span class="line">            deliveryGuarantee,</span><br><span class="line">            parallelism,</span><br><span class="line">            tableOptions.get(TRANSACTIONAL_ID_PREFIX),</span><br><span class="line">            tableOptions.get(TRANSACTION_NAMING_STRATEGY));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和 Source 的流程很相似，这里首先是获取 key 和 value 的编码格式，然后做了很多校验，最后是创建 KafkaDynamicSink 实例。</p><p>获取编码格式用到的工厂类是 SerializationFormatFactory，我们前面介绍的 JsonFormatFactory 也实现了 SerializationFormatFactory，因此它既提供了解码格式，又提供了编码格式。编码格式用到的编码器是 JsonRowDataSerializationSchema，通过 RowDataToJsonConverters 将 RowData 转换成 JsonNode。</p><p>在 KafkaDynamicSink 的 getSinkRuntimeProvider 方法中，主要就是创建 KafkaSink 实例。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768363734/Blog/flink/22/KafkaSink.png" alt="KafkaSink"></p><p>KafkaSink 类实现了 TwoPhaseCommittingStatefulSink 接口，即支持两阶段提交。它创建了 KafkaWrter 和 KafkaCommiter。</p><p>创建 KafkaWriter 时，如果配置的是 ExactlyOnce 模式，则会创建出 ExactlyOnceKafkaWriter，否则创建 KafkaWriter。Writer 真正实现两阶段提交的是 ExactlyOnceKafkaWriter。它在启动时，会调用 <code>producer.beginTransaction</code> 开启一个事务。数据写入时会调用 <code>KafkaWriter.write</code> 方法，此操作会被标记为事务内的操作。当 Sink 收到 Barrier 时，会先调用 flush 方法，将缓冲区的数据都发送到 Kafka Broker，然后调用 prepareCommit 方法预提交。预提交方法中记录 epoch 和 transactionalId 返回给框架层。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Collection&lt;KafkaCommittable&gt; <span class="title function_">prepareCommit</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// only return a KafkaCommittable if the current transaction has been written some data</span></span><br><span class="line">    <span class="keyword">if</span> (currentProducer.hasRecordsInTransaction()) &#123;</span><br><span class="line">        <span class="type">KafkaCommittable</span> <span class="variable">committable</span> <span class="operator">=</span> KafkaCommittable.of(currentProducer);</span><br><span class="line">        LOG.debug(<span class="string">&quot;Prepare &#123;&#125;.&quot;</span>, committable);</span><br><span class="line">        currentProducer.precommitTransaction();</span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(committable);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// otherwise, we recycle the producer (the pool will reset the transaction state)</span></span><br><span class="line">    producerPool.recycle(currentProducer);</span><br><span class="line">    <span class="keyword">return</span> Collections.emptyList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>状态保存时，会将预提交的 transactionalId 存到状态中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;KafkaWriterState&gt; <span class="title function_">snapshotState</span><span class="params">(<span class="type">long</span> checkpointId)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">// recycle committed producers</span></span><br><span class="line">    TransactionFinished finishedTransaction;</span><br><span class="line">    <span class="keyword">while</span> ((finishedTransaction = backchannel.poll()) != <span class="literal">null</span>) &#123;</span><br><span class="line">        producerPool.recycleByTransactionId(</span><br><span class="line">                finishedTransaction.getTransactionId(), finishedTransaction.isSuccess());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// persist the ongoing transactions into the state; these will not be aborted on restart</span></span><br><span class="line">    Collection&lt;CheckpointTransaction&gt; ongoingTransactions =</span><br><span class="line">            producerPool.getOngoingTransactions();</span><br><span class="line">    currentProducer = startTransaction(checkpointId + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> createSnapshots(ongoingTransactions);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> List&lt;KafkaWriterState&gt; <span class="title function_">createSnapshots</span><span class="params">(</span></span><br><span class="line"><span class="params">        Collection&lt;CheckpointTransaction&gt; ongoingTransactions)</span> &#123;</span><br><span class="line">    List&lt;KafkaWriterState&gt; states = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="type">int</span>[] subtaskIds = <span class="built_in">this</span>.ownedSubtaskIds;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> <span class="number">0</span>; index &lt; subtaskIds.length; index++) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">ownedSubtask</span> <span class="operator">=</span> subtaskIds[index];</span><br><span class="line">        states.add(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">KafkaWriterState</span>(</span><br><span class="line">                        transactionalIdPrefix,</span><br><span class="line">                        ownedSubtask,</span><br><span class="line">                        totalNumberOfOwnedSubtasks,</span><br><span class="line">                        transactionNamingStrategy.getOwnership(),</span><br><span class="line">                        <span class="comment">// new transactions are only created with the first owned subtask id</span></span><br><span class="line">                        index == <span class="number">0</span> ? ongoingTransactions : List.of()));</span><br><span class="line">    &#125;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Snapshotting state &#123;&#125;&quot;</span>, states);</span><br><span class="line">    <span class="keyword">return</span> states;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当 Checkpoint 完成时，会调用 <code>KafkaCommitter.commit</code> 方法。在 commit 方法中会调用 <code>producer.commitTransaction</code> 正式提交事务。</p><p>FlinkKafkaInternalProducer 是 Flink 内部封装的与 Kafka 生产者的交互类，所有与 Kafka 生产者的交互都通过它执行。</p><p>关于 Kafka Connector 的 Sink 端的源码我们就梳理到这里。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后还是总结一下。本文我们先了解了 Flink 中自定义 Source 和 Sink 的流程。按照这个流程，我们梳理了 Kafka Connector 的源码。在 Source 端，Flink Kafka 封装了对消费者 Offset 的提交逻辑。在 Sink 端结合了 Kafka 提供的事务支持实现了两阶段提交的逻辑。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文我们来梳理 Kafka Connector 相关的源码。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：Mailbox线程模型</title>
    <link href="https://jackeyzhe.github.io/2026/01/09/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AMailbox%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/"/>
    <id>https://jackeyzhe.github.io/2026/01/09/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AMailbox%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/</id>
    <published>2026-01-09T07:34:54.000Z</published>
    <updated>2026-01-12T12:47:23.935Z</updated>
    
    <content type="html"><![CDATA[<p>本文我们来梳理 Flink 的线程模型——Mailbox。<span id="more"></span></p><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>在以前的线程模型中，Flink 通过 checkpointLock 来隔离保证不同线程在修改内部状态时的正确性。通过 checkpointLock 控制并发会在代码中出现大量的 <code>synchronize(lock)</code> 这样非常不利于阅读和调试。Flink 也提供了一些 API 将锁对象暴露给用户，如果没有正确使用锁，很容易导致线程安全问题。</p><p>为了解决这些问题，Flink 社区提出了基于 Mailbox 的线程模型。它是通过单线程加阻塞队列来实现。这样内部状态的修改就由单线程来完成了。</p><p>旧的线程模型中，checkpointLock 主要用在三个地方：</p><ul><li><p>Event Process：包括 event、watermark、barrier 的处理和发送</p></li><li><p>Checkpoint：包括 Checkpoint 的触发和完成通知</p></li><li><p>ProcessTime Timer：ProcessTime 的回调通常涉及对状态的修改</p></li></ul><p>在 Mailbox 模型中，将所有需要处理的事件都封装成 Mail 投递到 Mailbox 中，然后由单线程按照顺序处理。</p><h3 id="相关定义"><a href="#相关定义" class="headerlink" title="相关定义"></a>相关定义</h3><p>下面我们来看 Mailbox 的具体实现，具体涉及到以下这些类。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768183924/Blog/flink/21/Mailbox.png" alt="Mailbox"></p><p>我们来逐个看一下这些类的定义和作用。</p><h4 id="Mail"><a href="#Mail" class="headerlink" title="Mail"></a>Mail</h4><p>在 Mailbox 线程模型中，Mail 是最基础的一个类，它用来封装需要处理的消息和执行的动作。Checkpoint Trigger 和 ProcessTime Trigger 都是通过 Mail 来触发的。Mail 中包含以下属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 选项，包括两个选项：isUrgent 和 deferrable</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MailOptionsImpl mailOptions;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 要执行的动作</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ThrowingRunnable&lt;? <span class="keyword">extends</span> <span class="title class_">Exception</span>&gt; runnable;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 优先级，这里的优先级不决定执行顺序，而是避免上下游之间的死锁问题</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> priority;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 描述信息</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String descriptionFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Object[] descriptionArgs;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用于执行 runnable 的执行器</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> StreamTaskActionExecutor actionExecutor;</span><br></pre></td></tr></table></figure><h3 id="TaskMailbox"><a href="#TaskMailbox" class="headerlink" title="TaskMailbox"></a>TaskMailbox</h3><p>有了 Mail 之后，Flink 用 TaskMailbox 来存储它，在需要执行时，再从 TaskMailbox 中取出。具体的处理逻辑在 TaskMailboxImpl 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 内部对于 queue 和 state 的并发访问都需要被这个锁保护</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">ReentrantLock</span> <span class="variable">lock</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ReentrantLock</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实际存储 Mail 的队列</span></span><br><span class="line"><span class="meta">@GuardedBy(&quot;lock&quot;)</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Deque&lt;Mail&gt; queue = <span class="keyword">new</span> <span class="title class_">ArrayDeque</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与 lock 关联的 Condition，主要用于队列从空变为非空时唤醒等待获取 Mail 的线程</span></span><br><span class="line"><span class="meta">@GuardedBy(&quot;lock&quot;)</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Condition</span> <span class="variable">notEmpty</span> <span class="operator">=</span> lock.newCondition();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 状态，包括 OPEN/QUIESCED/CLOSED</span></span><br><span class="line"><span class="meta">@GuardedBy(&quot;lock&quot;)</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">State</span> <span class="variable">state</span> <span class="operator">=</span> OPEN;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定的邮箱线程的引用</span></span><br><span class="line"><span class="meta">@Nonnull</span> <span class="keyword">private</span> <span class="keyword">final</span> Thread taskMailboxThread;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用于性能优化的设计</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Deque&lt;Mail&gt; batch = <span class="keyword">new</span> <span class="title class_">ArrayDeque</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// queue队列是否为空，用于性能优化，避免频繁访问主队列</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">boolean</span> <span class="variable">hasNewMail</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否有紧急邮件，同样用于性能优化，减少检查队列中是否有紧急邮件的次数</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">boolean</span> <span class="variable">hasNewUrgentMail</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br></pre></td></tr></table></figure><p>通过上面的属性，我们知道 TaskMailbox 底层是用 ArrayDeque 来存储 Mail 的，它内部包含了一个状态字段 state，state 的状态包括：</p><ul><li><p>OPEN：可以正常访问，接收和发送 Mail。</p></li><li><p>QUIESCED：处于静默状态，不接收新的 Mail，已有的 Mail 仍然可以被取出。</p></li><li><p>CLOSED：关闭状态，不能进行任何操作。</p></li></ul><p>在 TaskMailbox 内部，并发访问 queue 队列和 state 状态都需要 lock 这个锁的保护。此外 TaskMailbox 内部还做了一些性能优化，比如增加了 batch 队列，在处理 Mail 时，先将一批 Mail 从 queue 队列转移到 batch，之后会优先从 batch 队列中取，这样就减少了访问 queue 队列的次数，缓解了锁竞争压力。</p><h4 id="MailboxProcessor"><a href="#MailboxProcessor" class="headerlink" title="MailboxProcessor"></a>MailboxProcessor</h4><p>MailboxProcessor 可以认为是 Mailbox 相关的核心入口，MailboxProcessor 的核心方法就是事件循环，这个循环中主要是从 TaskMailbox 中取出 Mail 执行相应动作和执行默认动作（MailboxDefaultAction）。</p><p>MailboxProcessor 还对外提供了 MailboxExecutor，其他组件可以利用 MailboxExecutor 来提交事件。</p><h4 id="MailboxExecutor"><a href="#MailboxExecutor" class="headerlink" title="MailboxExecutor"></a>MailboxExecutor</h4><p>我们接着来看 MailboxExecutor，它的实现类是 MailboxExecutorImpl。包括以下属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 实际存储的 mailbox 实例</span></span><br><span class="line"><span class="meta">@Nonnull</span> <span class="keyword">private</span> <span class="keyword">final</span> TaskMailbox mailbox;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 优先级，MailboxExecutor 提供的默认优先级，提交 mail 时会带上这个字段</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> priority;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行器，运行 mail 的动作</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> StreamTaskActionExecutor actionExecutor;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行 MailboxProcessor，主要用于 isIdle 方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MailboxProcessor mailboxProcessor;</span><br></pre></td></tr></table></figure><p>MailboxExecutor 的主要作用是向 TaskMailbox 中投递 mail，核心方法是 execute。这个方法可以在任意线程中执行，因为 mailbox 内部控制了并发。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">execute</span><span class="params">(</span></span><br><span class="line"><span class="params">        MailOptions mailOptions,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> ThrowingRunnable&lt;? extends Exception&gt; command,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> String descriptionFormat,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> Object... descriptionArgs)</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        mailbox.put(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Mail</span>(</span><br><span class="line">                        mailOptions,</span><br><span class="line">                        command,</span><br><span class="line">                        priority,</span><br><span class="line">                        actionExecutor,</span><br><span class="line">                        descriptionFormat,</span><br><span class="line">                        descriptionArgs));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (MailboxClosedException mbex) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RejectedExecutionException</span>(mbex);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>除了 execute 方法以外，MailboxExecutor 中还有一个重要的方法，就是 yield。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">yield</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">    <span class="type">Mail</span> <span class="variable">mail</span> <span class="operator">=</span> mailbox.take(priority);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        mail.run();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">        <span class="keyword">throw</span> WrappingRuntimeException.wrapIfNecessary(ex);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个方法的主要目的是为了让出对当前事件的处理。这么做的原因有二：</p><ol><li><p>如果不考虑优先级的因素，Mailbox 队列是 FIFO 的顺序处理，如果当前事件依赖后面的事件完成，则有可能造成”死锁“。</p></li><li><p>当前事件处理事件较长，会阻塞其他事件。因此需要让出执行权，让相同或更高优先级的事件有机会执行。</p></li></ol><p>需要注意的是 yield 方法只能有 mailbox 线程自身调用。另外，Flink 也提供了非阻塞版本的方法，就是 tryYield。</p><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><h4 id="主流程"><a href="#主流程" class="headerlink" title="主流程"></a>主流程</h4><p>在创建 StreamTask 时，会创建 mailboxProcessor，同时也会持有 mainMailboxExecutor。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="title class_">TaskMailboxImpl</span>(Thread.currentThread()));</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="built_in">this</span>.mailboxProcessor =</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MailboxProcessor</span>(</span><br><span class="line">                <span class="built_in">this</span>::processInput, mailbox, actionExecutor, mailboxMetricsControl);</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="built_in">this</span>.mainMailboxExecutor = mailboxProcessor.getMainMailboxExecutor();</span><br></pre></td></tr></table></figure><p>可以看到这里将 processInput 作为 MailboxDefaultAction 传入 MailboxProcessor。在 StreamTask 启动时，会调用 MailboxProcessor 的核心方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title function_">invoke</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// Allow invoking method &#x27;invoke&#x27; without having to call &#x27;restore&#x27; before it.</span></span><br><span class="line">    <span class="keyword">if</span> (!isRunning) &#123;</span><br><span class="line">        LOG.debug(<span class="string">&quot;Restoring during invoke will be called.&quot;</span>);</span><br><span class="line">        restoreInternal();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// final check to exit early before starting to run</span></span><br><span class="line">    ensureNotCanceled();</span><br><span class="line"></span><br><span class="line">    scheduleBufferDebloater();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let the task do its work</span></span><br><span class="line">    getEnvironment().getMetricGroup().getIOMetricGroup().markTaskStart();</span><br><span class="line">    runMailboxLoop();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if this left the run() method cleanly despite the fact that this was canceled,</span></span><br><span class="line">    <span class="comment">// make sure the &quot;clean shutdown&quot; is not attempted</span></span><br><span class="line">    ensureNotCanceled();</span><br><span class="line"></span><br><span class="line">    afterInvoke();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">runMailboxLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    mailboxProcessor.runMailboxLoop();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>runMailboxLoop 的核心逻辑是一个 while 循环，在循环中处理 mail 并执行默认动作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">runMailboxLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    suspended = !mailboxLoopRunning;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">TaskMailbox</span> <span class="variable">localMailbox</span> <span class="operator">=</span> mailbox;</span><br><span class="line"></span><br><span class="line">    checkState(</span><br><span class="line">            localMailbox.isMailboxThread(),</span><br><span class="line">            <span class="string">&quot;Method must be executed by declared mailbox thread!&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> localMailbox.getState() == TaskMailbox.State.OPEN : <span class="string">&quot;Mailbox must be opened!&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">MailboxController</span> <span class="variable">mailboxController</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MailboxController</span>(<span class="built_in">this</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (isNextLoopPossible()) &#123;</span><br><span class="line">        <span class="comment">// The blocking `processMail` call will not return until default action is available.</span></span><br><span class="line">        processMail(localMailbox, <span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">if</span> (isNextLoopPossible()) &#123;</span><br><span class="line">            mailboxDefaultAction.runDefaultAction(</span><br><span class="line">                    mailboxController); <span class="comment">// lock is acquired inside default action as needed</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">isNextLoopPossible</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// &#x27;Suspended&#x27; can be false only when &#x27;mailboxLoopRunning&#x27; is true.</span></span><br><span class="line">    <span class="keyword">return</span> !suspended;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先是做了前置检查，包括确保 TaskMailbox 是指定的 mailbox 线程，TaskMailbox 的状态是 OPEN。接着创建了 MailboxController，它用于 MailboxDefaultAction 与 MailboxProcessor 的交互。</p><p>然后就进入到 <code>while (isNextLoopPossible())</code> 循环了，循环中调用了 processMail，在这个方法中对 mail 进行处理。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">processMail</span><span class="params">(TaskMailbox mailbox, <span class="type">boolean</span> singleStep)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// Doing this check is an optimization to only have a volatile read in the expected hot</span></span><br><span class="line">    <span class="comment">// path, locks are only</span></span><br><span class="line">    <span class="comment">// acquired after this point.</span></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">isBatchAvailable</span> <span class="operator">=</span> mailbox.createBatch();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Take mails in a non-blockingly and execute them.</span></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">processed</span> <span class="operator">=</span> isBatchAvailable &amp;&amp; processMailsNonBlocking(singleStep);</span><br><span class="line">    <span class="keyword">if</span> (singleStep) &#123;</span><br><span class="line">        <span class="keyword">return</span> processed;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If the default action is currently not available, we can run a blocking mailbox execution</span></span><br><span class="line">    <span class="comment">// until the default action becomes available again.</span></span><br><span class="line">    processed |= processMailsWhenDefaultActionUnavailable();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> processed;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>processMail 方法中先创建 batch，然后非阻塞的处理这批 mail。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">processMailsNonBlocking</span><span class="params">(<span class="type">boolean</span> singleStep)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">long</span> <span class="variable">processedMails</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    Optional&lt;Mail&gt; maybeMail;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (isNextLoopPossible() &amp;&amp; (maybeMail = mailbox.tryTakeFromBatch()).isPresent()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (processedMails++ == <span class="number">0</span>) &#123;</span><br><span class="line">            maybePauseIdleTimer();</span><br><span class="line">        &#125;</span><br><span class="line">        runMail(maybeMail.get());</span><br><span class="line">        <span class="keyword">if</span> (singleStep) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (processedMails &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        maybeRestartIdleTimer();</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">runMail</span><span class="params">(Mail mail)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    mailboxMetricsControl.getMailCounter().inc();</span><br><span class="line">    mail.run();</span><br><span class="line">    <span class="keyword">if</span> (!suspended) &#123;</span><br><span class="line">        <span class="comment">// start latency measurement on first mail that is not suspending mailbox execution,</span></span><br><span class="line">        <span class="comment">// i.e., on first non-poison mail, otherwise latency measurement is not started to avoid</span></span><br><span class="line">        <span class="comment">// overhead</span></span><br><span class="line">        <span class="keyword">if</span> (!mailboxMetricsControl.isLatencyMeasurementStarted()</span><br><span class="line">                &amp;&amp; mailboxMetricsControl.isLatencyMeasurementSetup()) &#123;</span><br><span class="line">            mailboxMetricsControl.startLatencyMeasurement();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>processMailsNonBlocking 直接调用 runMail 方法，最终是调用 <code>mail.run</code> 执行具体动作。</p><p>processMailsWhenDefaultActionUnavailable 的逻辑是如果当前默认动作不可用，会接着调用 runMail 尝试处理 Mail，这里会阻塞的等待，直到有新的需要处理的 Mail 或者默认动作可用。</p><p>当默认动作可用时，就会执行默认动作，也就是 <code>Stream.processInput</code>，这里就是处理 StreamRecord 了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">processInput</span><span class="params">(MailboxDefaultAction.Controller controller)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">DataInputStatus</span> <span class="variable">status</span> <span class="operator">=</span> inputProcessor.processInput();</span><br><span class="line">    <span class="keyword">switch</span> (status) &#123;</span><br><span class="line">        <span class="keyword">case</span> MORE_AVAILABLE:</span><br><span class="line">            <span class="keyword">if</span> (taskIsAvailable()) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> NOTHING_AVAILABLE:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> END_OF_RECOVERY:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalStateException</span>(<span class="string">&quot;We should not receive this event here.&quot;</span>);</span><br><span class="line">        <span class="keyword">case</span> STOPPED:</span><br><span class="line">            endData(StopMode.NO_DRAIN);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">case</span> END_OF_DATA:</span><br><span class="line">            endData(StopMode.DRAIN);</span><br><span class="line">            notifyEndOfData();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">case</span> END_OF_INPUT:</span><br><span class="line">            <span class="comment">// Suspend the mailbox processor, it would be resumed in afterInvoke and finished</span></span><br><span class="line">            <span class="comment">// after all records processed by the downstream tasks. We also suspend the default</span></span><br><span class="line">            <span class="comment">// actions to avoid repeat executing the empty default operation (namely process</span></span><br><span class="line">            <span class="comment">// records).</span></span><br><span class="line">            controller.suspendDefaultAction();</span><br><span class="line">            mailboxProcessor.suspend();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当 status 是 MORE_AVAILABLE，表示还有更多数据可用立即处理，判断当前任务可用就立即返回。当 status 是 END_OF_INPUT 时，表示所有的输入都结束了，这时就会暂停循环事件的调用。</p><h4 id="Checkpoint-流程"><a href="#Checkpoint-流程" class="headerlink" title="Checkpoint 流程"></a>Checkpoint 流程</h4><p>触发 Checkpoint 的流程是调用 <code>Stream.triggerCheckpointAsync</code> 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> CompletableFuture&lt;Boolean&gt; <span class="title function_">triggerCheckpointAsync</span><span class="params">(</span></span><br><span class="line"><span class="params">        CheckpointMetaData checkpointMetaData, CheckpointOptions checkpointOptions)</span> &#123;</span><br><span class="line">    checkForcedFullSnapshotSupport(checkpointOptions);</span><br><span class="line"></span><br><span class="line">    MailboxExecutor.<span class="type">MailOptions</span> <span class="variable">mailOptions</span> <span class="operator">=</span></span><br><span class="line">            CheckpointOptions.AlignmentType.UNALIGNED == checkpointOptions.getAlignment()</span><br><span class="line">                    ? MailboxExecutor.MailOptions.urgent()</span><br><span class="line">                    : MailboxExecutor.MailOptions.options();</span><br><span class="line"></span><br><span class="line">    CompletableFuture&lt;Boolean&gt; result = <span class="keyword">new</span> <span class="title class_">CompletableFuture</span>&lt;&gt;();</span><br><span class="line">    mainMailboxExecutor.execute(</span><br><span class="line">            mailOptions,</span><br><span class="line">            () -&gt; &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="type">boolean</span> <span class="variable">noUnfinishedInputGates</span> <span class="operator">=</span></span><br><span class="line">                            Arrays.stream(getEnvironment().getAllInputGates())</span><br><span class="line">                                    .allMatch(InputGate::isFinished);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (noUnfinishedInputGates) &#123;</span><br><span class="line">                        result.complete(</span><br><span class="line">                                triggerCheckpointAsyncInMailbox(</span><br><span class="line">                                        checkpointMetaData, checkpointOptions));</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        result.complete(</span><br><span class="line">                                triggerUnfinishedChannelsCheckpoint(</span><br><span class="line">                                        checkpointMetaData, checkpointOptions));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">                    <span class="comment">// Report the failure both via the Future result but also to the mailbox</span></span><br><span class="line">                    result.completeExceptionally(ex);</span><br><span class="line">                    <span class="keyword">throw</span> ex;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;checkpoint %s with %s&quot;</span>,</span><br><span class="line">            checkpointMetaData,</span><br><span class="line">            checkpointOptions);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过调用 <code>mainMailboxExecutor.execute</code> 方法来向 Mailbox 中提交 Mail。Checkpoint 完成的通知也是一样放在 Mailbox 中执行的，不过这里提交的是一个高优先级的操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Future&lt;Void&gt; <span class="title function_">notifyCheckpointOperation</span><span class="params">(</span></span><br><span class="line"><span class="params">        RunnableWithException runnable, String description)</span> &#123;</span><br><span class="line">    CompletableFuture&lt;Void&gt; result = <span class="keyword">new</span> <span class="title class_">CompletableFuture</span>&lt;&gt;();</span><br><span class="line">    mailboxProcessor</span><br><span class="line">            .getMailboxExecutor(TaskMailbox.MAX_PRIORITY)</span><br><span class="line">            .execute(</span><br><span class="line">                    () -&gt; &#123;</span><br><span class="line">                        <span class="keyword">try</span> &#123;</span><br><span class="line">                            runnable.run();</span><br><span class="line">                        &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">                            result.completeExceptionally(ex);</span><br><span class="line">                            <span class="keyword">throw</span> ex;</span><br><span class="line">                        &#125;</span><br><span class="line">                        result.complete(<span class="literal">null</span>);</span><br><span class="line">                    &#125;,</span><br><span class="line">                    description);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们梳理了 Mailbox 相关的源码。Flink 通过 Mailbox 线程模型来简化相关代码逻辑。我们先了解了几个核心类：Mail、TaskMailbox、MailboxProcessor、MailboxExecutor。然后梳理了具体的事件处理和触发的流程。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文我们来梳理 Flink 的线程模型——Mailbox。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：JobManager的HA机制</title>
    <link href="https://jackeyzhe.github.io/2026/01/06/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AJobManager%E7%9A%84HA%E6%9C%BA%E5%88%B6/"/>
    <id>https://jackeyzhe.github.io/2026/01/06/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AJobManager%E7%9A%84HA%E6%9C%BA%E5%88%B6/</id>
    <published>2026-01-06T14:15:45.000Z</published>
    <updated>2026-01-09T10:57:22.375Z</updated>
    
    <content type="html"><![CDATA[<p>JobManager 在 Flink 集群中发挥着重要的作用，包括任务调度和资源管理等工作。如果 JobManager 宕机，那么整个集群的任务都将失败。为了解决 JobManager 的单点问题，Flink 也设计了 HA 机制来保障整个集群的稳定性。<span id="more"></span></p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>在 JobManager 启动时，调用 <code>HighAvailabilityServicesUtils.createHighAvailabilityServices</code> 来创建 HA 服务，HA 依赖的服务都被封装在 HighAvailabilityServices 中。当前 Flink 内部支持两种高可用模式，分别是 ZooKeeper 和 KUBERNETES。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> ZOOKEEPER:</span><br><span class="line">    <span class="keyword">return</span> createZooKeeperHaServices(configuration, executor, fatalErrorHandler);</span><br><span class="line"><span class="keyword">case</span> KUBERNETES:</span><br><span class="line">    <span class="keyword">return</span> createCustomHAServices(</span><br><span class="line">            <span class="string">&quot;org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory&quot;</span>,</span><br><span class="line">            configuration,</span><br><span class="line">            executor);</span><br></pre></td></tr></table></figure><p>HighAvailabilityServices 中提供的关键组件包括：</p><ul><li><p>LeaderRetrievalService：服务发现，用于获取当前 leader 的地址。目前用到服务发现的组件有 ResourceManager、Dispatcher、JobManager、ClusterRestEndpoint。</p></li><li><p>LeaderElection：选举服务，从多个候选者中选出一个作为 leader。用到选举服务的同样是 ResourceManager、Dispatcher、JobManager、ClusterRestEndpoint 这四个。</p></li><li><p>CheckpointRecoveryFactory：Checkpoint 恢复组件的工厂类，提供了创建 CompletedCheckpointStore 和 CheckpointIDCounter 的方法。CompletedCheckpointStore 是用于存储已完成的 checkpoint 的元信息，CheckpointIDCounter 是用于生成 checkpoint ID。</p></li><li><p>ExecutionPlanStore：用于存储执行计划。</p></li><li><p>JobResultStore：用于存储作业结果，这里有两种状态，一种是 dirty，表示作业没有被完全清理，另一种是 clean，表示作业清理工作已经执行完成了。</p></li><li><p>BlobStore：存储作业运行期间的一些二进制文件。</p></li></ul><h4 id="选举服务"><a href="#选举服务" class="headerlink" title="选举服务"></a>选举服务</h4><p>Flink 的选举是依靠 LeaderElection 和 LeaderContender 配合完成的。LeaderElection 是 LeaderElectionService 的代理接口，提供了注册候选者、确认 leader 和 判断候选者是否是 leader 三个接口。LeaderContender 则是用来表示候选者对象。当一个 LeaderContender 当选 leader 后，LeaderElectionService 会为其生成一个 leaderSessionId，LeaderContender 会调用 confirmLeadershipAsync 发布自己的地址。选举服务的具体实现在 LeaderElectionDriver 接口中。</p><h4 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h4><p>服务发现的作用是获取各组件的 leader 地址。服务发现依赖 LeaderRetrievalService 和 LeaderRetrievalListener。LeaderRetrievalService 可以启动一个监听，当有新的 leader 当选时，会调用 LeaderRetrievalListener 的 notifyLeaderAddress 方法。</p><h4 id="信息保存"><a href="#信息保存" class="headerlink" title="信息保存"></a>信息保存</h4><p>当 leader 发生切换时，新的 leader 需要获取到旧 leader 存储的信息，这就需要旧 leader 把这些信息存在一个公共的存储上。它可以是 ZooKeeper 或 Kubernetes 的存储，也可以是分布式文件系统的存储。</p><h3 id="基于-ZooKeeper-的-HA"><a href="#基于-ZooKeeper-的-HA" class="headerlink" title="基于 ZooKeeper 的 HA"></a>基于 ZooKeeper 的 HA</h3><h4 id="选举服务-1"><a href="#选举服务-1" class="headerlink" title="选举服务"></a>选举服务</h4><p>前面我们提到了选举服务主要依赖 LeaderElection 和 LeaderContender 配合完成。我们就以 JobManager 为例，看一下机遇 ZooKeeper 的选举流程的具体实现。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767924499/Blog/flink/20/ZKLeaderElection.png" alt="ZKLeaderElection"></p><p>图中 JobMasterServiceLeadershipRunner 是 LeaderContender 的实现类。在启动服务时，会向 LeaderElection 注册自己的信息，实际执行者是 DefaultLeaderElectionService。它先创建了 LeaderElectionDriver，然后将 LeaderContender 保存在 leaderContenderRegistry 中。选举的核心逻辑封装在 LeaderElectionDriver 中。</p><p>在创建 LeaderElectionDriver 时，会创建 LeaderLatch 对象和 TreeCache 对象， LeaderLatch 封装了与 ZooKeeper 关联的回调，会接收一个 LeaderElectionDriver 作为监听。TreeCache 主要用于监听 ZooKeeper 中 leader 节点的变更。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">ZooKeeperLeaderElectionDriver</span><span class="params">(</span></span><br><span class="line"><span class="params">        CuratorFramework curatorFramework, LeaderElectionDriver.Listener leaderElectionListener)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">this</span>.leaderLatch = <span class="keyword">new</span> <span class="title class_">LeaderLatch</span>(curatorFramework, ZooKeeperUtils.getLeaderLatchPath());</span><br><span class="line">    <span class="built_in">this</span>.treeCache =</span><br><span class="line">            ZooKeeperUtils.createTreeCache(</span><br><span class="line">                    curatorFramework,</span><br><span class="line">                    <span class="string">&quot;/&quot;</span>,</span><br><span class="line">                    <span class="keyword">new</span> <span class="title class_">ZooKeeperLeaderElectionDriver</span>.ConnectionInfoNodeSelector());</span><br><span class="line"></span><br><span class="line">    treeCache</span><br><span class="line">            .getListenable()</span><br><span class="line">            .addListener(</span><br><span class="line">                    (client, event) -&gt; &#123;</span><br><span class="line">                        <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">                            <span class="keyword">case</span> NODE_ADDED:</span><br><span class="line">                            <span class="keyword">case</span> NODE_UPDATED:</span><br><span class="line">                                Preconditions.checkNotNull(</span><br><span class="line">                                        event.getData(),</span><br><span class="line">                                        <span class="string">&quot;The ZooKeeper event data must not be null.&quot;</span>);</span><br><span class="line">                                handleChangedLeaderInformation(event.getData());</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            <span class="keyword">case</span> NODE_REMOVED:</span><br><span class="line">                                Preconditions.checkNotNull(</span><br><span class="line">                                        event.getData(),</span><br><span class="line">                                        <span class="string">&quot;The ZooKeeper event data must not be null.&quot;</span>);</span><br><span class="line">                                handleRemovedLeaderInformation(event.getData().getPath());</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line">    leaderLatch.addListener(<span class="built_in">this</span>);</span><br><span class="line">    ...</span><br><span class="line">    leaderLatch.start();</span><br><span class="line">    treeCache.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们进入到 LeaderLatch 的 start 方法。它的内部是在 ZooKeeper 上创建 latch-xxx 节点。xxx 是当前 LeaderLatch 的 ID，它由 ZooKeeper 生成，ID 最小的当选 Leader。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">checkLeadership</span><span class="params">(List&lt;String&gt; children)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.debugCheckLeaderShipLatch != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="built_in">this</span>.debugCheckLeaderShipLatch.await();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> <span class="variable">localOurPath</span> <span class="operator">=</span> (String)<span class="built_in">this</span>.ourPath.get();</span><br><span class="line">    List&lt;String&gt; sortedChildren = LockInternals.getSortedChildren(<span class="string">&quot;latch-&quot;</span>, sorter, children);</span><br><span class="line">    <span class="type">int</span> <span class="variable">ourIndex</span> <span class="operator">=</span> localOurPath != <span class="literal">null</span> ? sortedChildren.indexOf(ZKPaths.getNodeFromPath(localOurPath)) : -<span class="number">1</span>;</span><br><span class="line">    <span class="built_in">this</span>.log.debug(<span class="string">&quot;checkLeadership with id: &#123;&#125;, ourPath: &#123;&#125;, children: &#123;&#125;&quot;</span>, <span class="keyword">new</span> <span class="title class_">Object</span>[]&#123;<span class="built_in">this</span>.id, localOurPath, sortedChildren&#125;);</span><br><span class="line">    <span class="keyword">if</span> (ourIndex &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">this</span>.log.error(<span class="string">&quot;Can&#x27;t find our node. Resetting. Index: &quot;</span> + ourIndex);</span><br><span class="line">        <span class="built_in">this</span>.reset();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ourIndex == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">this</span>.lastPathIsLeader.set(localOurPath);</span><br><span class="line">        <span class="built_in">this</span>.setLeadership(<span class="literal">true</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.setLeadership(<span class="literal">false</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">watchPath</span> <span class="operator">=</span> (String)sortedChildren.get(ourIndex - <span class="number">1</span>);</span><br><span class="line">        <span class="type">Watcher</span> <span class="variable">watcher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (LeaderLatch.<span class="built_in">this</span>.state.get() == LeaderLatch.State.STARTED &amp;&amp; event.getType() == EventType.NodeDeleted) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        LeaderLatch.<span class="built_in">this</span>.getChildren();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">                        ThreadUtils.checkInterrupted(ex);</span><br><span class="line">                        LeaderLatch.<span class="built_in">this</span>.log.error(<span class="string">&quot;An error occurred checking the leadership.&quot;</span>, ex);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="type">BackgroundCallback</span> <span class="variable">callback</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BackgroundCallback</span>() &#123;</span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processResult</span><span class="params">(CuratorFramework client, CuratorEvent event)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">if</span> (event.getResultCode() == Code.NONODE.intValue()) &#123;</span><br><span class="line">                    LeaderLatch.<span class="built_in">this</span>.getChildren();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        ((ErrorListenerPathable)((BackgroundPathable)<span class="built_in">this</span>.client.getData().usingWatcher(watcher)).inBackground(callback)).forPath(ZKPaths.makePath(<span class="built_in">this</span>.latchPath, watchPath));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当选 Leader 后，会回调 LeaderElectionDriver 的 isLeader 方法，如果未当选，则继续监听 latch 节点的变更。isLeader 会继续回调 LeaderElection 的 onGrantLeadership 方法，接着调用 LeaderContender 的 grantLeadership。这时会启动 JobMaster 服务，然后调用 LeaderElection 的 confirmLeadershipAsync 来确认当选成功。确认的过程是由 LeaderElectionDriver 来执行的。主要作用是把当前 leader 的信息写回到 ZooKeeper 的 connection_info 节点。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">publishLeaderInformation</span><span class="params">(String componentId, LeaderInformation leaderInformation)</span> &#123;</span><br><span class="line">    Preconditions.checkState(running.get());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!leaderLatch.hasLeadership()) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">connectionInformationPath</span> <span class="operator">=</span></span><br><span class="line">            ZooKeeperUtils.generateConnectionInformationPath(componentId);</span><br><span class="line"></span><br><span class="line">    LOG.debug(</span><br><span class="line">            <span class="string">&quot;Write leader information &#123;&#125; for component &#x27;&#123;&#125;&#x27; to &#123;&#125;.&quot;</span>,</span><br><span class="line">            leaderInformation,</span><br><span class="line">            componentId,</span><br><span class="line">            ZooKeeperUtils.generateZookeeperPath(</span><br><span class="line">                    curatorFramework.getNamespace(), connectionInformationPath));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ZooKeeperUtils.writeLeaderInformationToZooKeeper(</span><br><span class="line">                leaderInformation,</span><br><span class="line">                curatorFramework,</span><br><span class="line">                leaderLatch::hasLeadership,</span><br><span class="line">                connectionInformationPath);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        leaderElectionListener.onError(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="服务发现-1"><a href="#服务发现-1" class="headerlink" title="服务发现"></a>服务发现</h4><p>梳理完选举服务的源码后，我们再来看一下服务发现的过程。我们以 TaskManager 获取 JobManager 的 leader 为例。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767929447/Blog/flink/20/ZKLeaderRetrieval.png" alt="ZKLeaderRetrieval"></p><p>当我们往 TaskManager 添加任务时，会调用 JobLeaderService 的 addJob 方法。这里会先获取 LeaderRetrieval，然后调用 start 方法注册 LeaderRetrievalListener 监听，并创建 LeaderRetrievalDriver。在 LeaderRetrievalDriver 中主要是向 ZooKeeper 注册 connection_info 节点的变更。</p><p>如果发生变更，ZooKeeper 会回调 <code>LeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper</code> 方法。我们从 ZooKeeper 获取到 leader 的地址和 sessionId 后，就回调 <code>LeaderRetrievalService.notifyLeaderAddress</code> 方法。最终调用到 JobLeaderService 的 notifyLeaderAddress 方法，这个方法中就是断开与旧 leader 的连接，增加与新 leader 的连接。</p><h4 id="信息保存-1"><a href="#信息保存-1" class="headerlink" title="信息保存"></a>信息保存</h4><p>最后我们再来看信息保存相关的源码。在 JobManager 完成一次 Checkpoint 时，会执行 <code>CheckpointCoordinator.completePendingCheckpoint</code> 方法，跟随调用链路可以找到 <code>ZooKeeperStateHandleStore.addAndLock</code> 方法，这里会把状态写入到文件系统中，然后把文件路径保存在 ZooKeeper 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> RetrievableStateHandle&lt;T&gt; <span class="title function_">addAndLock</span><span class="params">(String pathInZooKeeper, T state)</span></span><br><span class="line">        <span class="keyword">throws</span> PossibleInconsistentStateException, Exception &#123;</span><br><span class="line">    checkNotNull(pathInZooKeeper, <span class="string">&quot;Path in ZooKeeper&quot;</span>);</span><br><span class="line">    checkNotNull(state, <span class="string">&quot;State&quot;</span>);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> normalizePath(pathInZooKeeper);</span><br><span class="line">    <span class="keyword">final</span> Optional&lt;Stat&gt; maybeStat = getStat(path);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (maybeStat.isPresent()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (isNotMarkedForDeletion(maybeStat.get())) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">AlreadyExistException</span>(</span><br><span class="line">                    String.format(<span class="string">&quot;ZooKeeper node %s already exists.&quot;</span>, path));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Preconditions.checkState(</span><br><span class="line">                releaseAndTryRemove(path),</span><br><span class="line">                <span class="string">&quot;The state is marked for deletion and, therefore, should be deletable.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> RetrievableStateHandle&lt;T&gt; storeHandle = storage.store(state);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">byte</span>[] serializedStoreHandle = serializeOrDiscard(storeHandle);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        writeStoreHandleTransactionally(path, serializedStoreHandle);</span><br><span class="line">        <span class="keyword">return</span> storeHandle;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (KeeperException.NodeExistsException e) &#123;</span><br><span class="line">        <span class="comment">// Transactions are not idempotent in the curator version we&#x27;re currently using, so it</span></span><br><span class="line">        <span class="comment">// is actually possible that we&#x27;ve re-tried a transaction that has already succeeded.</span></span><br><span class="line">        <span class="comment">// We&#x27;ve ensured that the node hasn&#x27;t been present prior executing the transaction, so</span></span><br><span class="line">        <span class="comment">// we can assume that this is a result of the retry mechanism.</span></span><br><span class="line">        <span class="keyword">return</span> storeHandle;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">if</span> (indicatesPossiblyInconsistentState(e)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">PossibleInconsistentStateException</span>(e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// In case of any other failure, discard the state and rethrow the exception.</span></span><br><span class="line">        storeHandle.discardState();</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此，基于 ZooKeeper 的 HA 逻辑我们就梳理完了。从 1.12 版本开始，Flink 还支持了 Kubernetes 高可用，下面我们再来一下它是如何实现的。</p><h3 id="基于-Kubernetes-的-HA"><a href="#基于-Kubernetes-的-HA" class="headerlink" title="基于 Kubernetes 的 HA"></a>基于 Kubernetes 的 HA</h3><h4 id="选举服务-2"><a href="#选举服务-2" class="headerlink" title="选举服务"></a>选举服务</h4><p>通过前面的学习，我们已经了解到，选举的主要逻辑是在 LeaderElectionDriver 中，因此，我们直接来看 KubernetesLeaderElectionDriver 的逻辑即可。创建 KubernetesLeaderElectionDriver 时，创建并启动了 KubernetesLeaderElector。这个类似于 ZooKeeper 逻辑中 LeaderLatch，会跟 Kubernetes 底层的选举逻辑交互，同时注册监听。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">KubernetesLeaderElector</span><span class="params">(</span></span><br><span class="line"><span class="params">        NamespacedKubernetesClient kubernetesClient,</span></span><br><span class="line"><span class="params">        KubernetesLeaderElectionConfiguration leaderConfig,</span></span><br><span class="line"><span class="params">        LeaderCallbackHandler leaderCallbackHandler,</span></span><br><span class="line"><span class="params">        ExecutorService executorService)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.kubernetesClient = kubernetesClient;</span><br><span class="line">    <span class="built_in">this</span>.leaderElectionConfig =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">LeaderElectionConfigBuilder</span>()</span><br><span class="line">                    .withName(leaderConfig.getConfigMapName())</span><br><span class="line">                    .withLeaseDuration(leaderConfig.getLeaseDuration())</span><br><span class="line">                    .withLock(</span><br><span class="line">                            <span class="keyword">new</span> <span class="title class_">ConfigMapLock</span>(</span><br><span class="line">                                    <span class="keyword">new</span> <span class="title class_">ObjectMetaBuilder</span>()</span><br><span class="line">                                            .withNamespace(kubernetesClient.getNamespace())</span><br><span class="line">                                            .withName(leaderConfig.getConfigMapName())</span><br><span class="line">                                            <span class="comment">// Labels will be used to clean up the ha related</span></span><br><span class="line">                                            <span class="comment">// ConfigMaps.</span></span><br><span class="line">                                            .withLabels(</span><br><span class="line">                                                    KubernetesUtils.getConfigMapLabels(</span><br><span class="line">                                                            leaderConfig.getClusterId()))</span><br><span class="line">                                            .build(),</span><br><span class="line">                                    leaderConfig.getLockIdentity()))</span><br><span class="line">                    .withRenewDeadline(leaderConfig.getRenewDeadline())</span><br><span class="line">                    .withRetryPeriod(leaderConfig.getRetryPeriod())</span><br><span class="line">                    .withReleaseOnCancel(<span class="literal">true</span>)</span><br><span class="line">                    .withLeaderCallbacks(</span><br><span class="line">                            <span class="keyword">new</span> <span class="title class_">LeaderCallbacks</span>(</span><br><span class="line">                                    leaderCallbackHandler::isLeader,</span><br><span class="line">                                    leaderCallbackHandler::notLeader,</span><br><span class="line">                                    newLeader -&gt;</span><br><span class="line">                                            LOG.info(</span><br><span class="line">                                                    <span class="string">&quot;New leader elected &#123;&#125; for &#123;&#125;.&quot;</span>,</span><br><span class="line">                                                    newLeader,</span><br><span class="line">                                                    leaderConfig.getConfigMapName())))</span><br><span class="line">                    .build();</span><br><span class="line">    <span class="built_in">this</span>.executorService = executorService;</span><br><span class="line"></span><br><span class="line">    LOG.info(</span><br><span class="line">            <span class="string">&quot;Create KubernetesLeaderElector on lock &#123;&#125;.&quot;</span>,</span><br><span class="line">            leaderElectionConfig.getLock().describe());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>选举成功后，会回调 <code>LeaderElectionListener.onGrantLeadership</code> 方法。后续的调用链路还是会调用到 <code>KubernetesLeaderElectionDriver.publishLeaderInformation</code> 方法。这个方法是把 leader 信息写到 Kubernetes 的 configMap 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">publishLeaderInformation</span><span class="params">(String componentId, LeaderInformation leaderInformation)</span> &#123;</span><br><span class="line">    Preconditions.checkState(running.get());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        kubeClient</span><br><span class="line">                .checkAndUpdateConfigMap(</span><br><span class="line">                        configMapName,</span><br><span class="line">                        updateConfigMapWithLeaderInformation(componentId, leaderInformation))</span><br><span class="line">                .get();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">        leaderElectionListener.onError(e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.debug(</span><br><span class="line">            <span class="string">&quot;Successfully wrote leader information &#123;&#125; for leader &#123;&#125; into the config map &#123;&#125;.&quot;</span>,</span><br><span class="line">            leaderInformation,</span><br><span class="line">            componentId,</span><br><span class="line">            configMapName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="服务发现-2"><a href="#服务发现-2" class="headerlink" title="服务发现"></a>服务发现</h4><p>服务发现的逻辑在 KubernetesLeaderRetrievalDriver 类中，在创建时，会将内部类 ConfigMapCallbackHandlerImpl 注册为监听回调类。</p><p>当 configMap 有新增或变更后，会回调 <code>LeaderRetrievalService.notifyLeaderAddress</code> 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">ConfigMapCallbackHandlerImpl</span></span><br><span class="line">        <span class="keyword">implements</span> <span class="title class_">FlinkKubeClient</span>.WatchCallbackHandler&lt;KubernetesConfigMap&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onAdded</span><span class="params">(List&lt;KubernetesConfigMap&gt; configMaps)</span> &#123;</span><br><span class="line">        <span class="comment">// The ConfigMap is created by KubernetesLeaderElectionDriver with</span></span><br><span class="line">        <span class="comment">// empty data. We don&#x27;t really need to process anything unless the retriever was started</span></span><br><span class="line">        <span class="comment">// after the leader election has already succeeded.</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">KubernetesConfigMap</span> <span class="variable">configMap</span> <span class="operator">=</span> getOnlyConfigMap(configMaps, configMapName);</span><br><span class="line">        <span class="keyword">final</span> <span class="type">LeaderInformation</span> <span class="variable">leaderInformation</span> <span class="operator">=</span> leaderInformationExtractor.apply(configMap);</span><br><span class="line">        <span class="keyword">if</span> (!leaderInformation.isEmpty()) &#123;</span><br><span class="line">            leaderRetrievalEventHandler.notifyLeaderAddress(leaderInformation);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onModified</span><span class="params">(List&lt;KubernetesConfigMap&gt; configMaps)</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">KubernetesConfigMap</span> <span class="variable">configMap</span> <span class="operator">=</span> getOnlyConfigMap(configMaps, configMapName);</span><br><span class="line">        leaderRetrievalEventHandler.notifyLeaderAddress(</span><br><span class="line">                leaderInformationExtractor.apply(configMap));</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="信息保存-2"><a href="#信息保存-2" class="headerlink" title="信息保存"></a>信息保存</h4><p>信息保存的逻辑和 ZooKeeper 也非常类似。即先把 state 保存在文件系统，然后把存储路径写到 Kubernetes 写到 configMap 中。具体可以看 <code>KubernetesStateHandleStore.addAndLock</code> 方法。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们一起梳理了 Flink 中 JobManager 的 HA 机制相关源码。目前 Flink 支持 ZooKeeper 和 Kubernetes 两种实现。在梳理过程中，我们以 JobManager 为例，其他几个用到高可用的服务的选举逻辑也是一样的。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;JobManager 在 Flink 集群中发挥着重要的作用，包括任务调度和资源管理等工作。如果 JobManager 宕机，那么整个集群的任务都将失败。为了解决 JobManager 的单点问题，Flink 也设计了 HA 机制来保障整个集群的稳定性。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：Netty通信</title>
    <link href="https://jackeyzhe.github.io/2026/01/05/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ANetty%E9%80%9A%E4%BF%A1/"/>
    <id>https://jackeyzhe.github.io/2026/01/05/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ANetty%E9%80%9A%E4%BF%A1/</id>
    <published>2026-01-05T02:23:02.000Z</published>
    <updated>2026-01-06T12:48:54.328Z</updated>
    
    <content type="html"><![CDATA[<p>前文中我们了解了 Flink 的数据交互过程，上游的 Task 将数据写入到 ResultSubpartition 的 buffers 队列中。下游的 Task 通过 LocalInputChannel 和 RemoteInputChannel 消费上游的数据。<span id="more"></span></p><p>LocalInputChannel 是上下游的 Task 部署在同一个 TaskManager 时使用的，在本地即可完成数据交换，无需网络通信。当上下游的 Task 部署在不同的 TaskManager 时，就需要用到 RemoteInputChannel，Flink 利用 Netty 来进行数据交互。本文我们来一起梳理一下 Netty 相关的源码。</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>我们先来看 NettyServer 和 NettyClient 的初始化过程。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767601135/Blog/flink/19/NettyInit.png" alt="NettyInit"></p><p>Netty 的初始化阶段是在 TaskManager 启动的过程中执行的。在 <code>TaskManagerServices.fromConfiguration</code> 方法中，会创建并启动 ShuffleEnvironment。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> TaskManagerServices <span class="title function_">fromConfiguration</span><span class="params">(</span></span><br><span class="line"><span class="params">        TaskManagerServicesConfiguration taskManagerServicesConfiguration,</span></span><br><span class="line"><span class="params">        PermanentBlobService permanentBlobService,</span></span><br><span class="line"><span class="params">        MetricGroup taskManagerMetricGroup,</span></span><br><span class="line"><span class="params">        ExecutorService ioExecutor,</span></span><br><span class="line"><span class="params">        ScheduledExecutor scheduledExecutor,</span></span><br><span class="line"><span class="params">        FatalErrorHandler fatalErrorHandler,</span></span><br><span class="line"><span class="params">        WorkingDirectory workingDirectory)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> ShuffleEnvironment&lt;?, ?&gt; shuffleEnvironment =</span><br><span class="line">            createShuffleEnvironment(</span><br><span class="line">                    taskManagerServicesConfiguration,</span><br><span class="line">                    taskEventDispatcher,</span><br><span class="line">                    taskManagerMetricGroup,</span><br><span class="line">                    ioExecutor,</span><br><span class="line">                    scheduledExecutor);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">listeningDataPort</span> <span class="operator">=</span> shuffleEnvironment.start();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们顺着调用链路可以一直找到 <code>NettyShuffleServiceFactory.createNettyShuffleEnvironment</code> 方法，这个方法中创建了 NettyConnectionManager，在 NettyConnectionManager 中有几个很重要的对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">NettyConnectionManager</span><span class="params">(</span></span><br><span class="line"><span class="params">        NettyBufferPool bufferPool,</span></span><br><span class="line"><span class="params">        ResultPartitionProvider partitionProvider,</span></span><br><span class="line"><span class="params">        TaskEventPublisher taskEventPublisher,</span></span><br><span class="line"><span class="params">        NettyConfig nettyConfig,</span></span><br><span class="line"><span class="params">        <span class="type">boolean</span> connectionReuseEnabled)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.server = <span class="keyword">new</span> <span class="title class_">NettyServer</span>(nettyConfig);</span><br><span class="line">    <span class="built_in">this</span>.client = <span class="keyword">new</span> <span class="title class_">NettyClient</span>(nettyConfig);</span><br><span class="line">    <span class="built_in">this</span>.bufferPool = checkNotNull(bufferPool);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.partitionRequestClientFactory =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">PartitionRequestClientFactory</span>(</span><br><span class="line">                    client, nettyConfig.getNetworkRetries(), connectionReuseEnabled);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.nettyProtocol =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">NettyProtocol</span>(</span><br><span class="line">                    checkNotNull(partitionProvider), checkNotNull(taskEventPublisher));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>server 和 client 不需要多介绍，就是 Netty 的服务端和客户端。bufferPool 是缓冲池，用于存储要传输的数据。nettyProtocol 提供了 NettyClient 和 NettyServer 引导启动注册的 Channel Handler。</p><p>后面就是创建 NettyShuffleEnvironment 及其需要的对象了。在创建完成后，会调用它的 start 方法启动。这个启动方法就是调用了 <code>connectionManager.start</code>，在 NettyConnectionManager 中，就是初始化客户端和服务端。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">start</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    client.init(nettyProtocol, bufferPool);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> server.init(nettyProtocol, bufferPool);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="client-初始化"><a href="#client-初始化" class="headerlink" title="client 初始化"></a>client 初始化</h4><p>client 的初始化过程是先创建并初始化 Bootstrap。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">initEpollBootstrap</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// Add the server port number to the name in order to distinguish</span></span><br><span class="line">    <span class="comment">// multiple clients running on the same host.</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span></span><br><span class="line">            NettyConfig.CLIENT_THREAD_GROUP_NAME + <span class="string">&quot; (&quot;</span> + config.getServerPortRange() + <span class="string">&quot;)&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">EpollEventLoopGroup</span> <span class="variable">epollGroup</span> <span class="operator">=</span></span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">EpollEventLoopGroup</span>(</span><br><span class="line">                    config.getClientNumThreads(), NettyServer.getNamedThreadFactory(name));</span><br><span class="line">    bootstrap.group(epollGroup).channel(EpollSocketChannel.class);</span><br><span class="line"></span><br><span class="line">    config.getTcpKeepIdleInSeconds()</span><br><span class="line">            .ifPresent(idle -&gt; bootstrap.option(EpollChannelOption.TCP_KEEPIDLE, idle));</span><br><span class="line">    config.getTcpKeepInternalInSeconds()</span><br><span class="line">            .ifPresent(</span><br><span class="line">                    interval -&gt; bootstrap.option(EpollChannelOption.TCP_KEEPINTVL, interval));</span><br><span class="line">    config.getTcpKeepCount()</span><br><span class="line">            .ifPresent(count -&gt; bootstrap.option(EpollChannelOption.TCP_KEEPCNT, count));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>初始化过程重要设置 EventLoopGroup 和 channel，可以用 epoll 的话就用 epoll，否则就用 nio。设置好这些后就是设置了一些通道参数（连接超时时间、Bufffer 池等）。</p><p>到这里 client 的初始化其实并没有结束，还需要设置 Handler 流水线，这些工作是在 Task 启动时执行了。</p><h4 id="server-初始化"><a href="#server-初始化" class="headerlink" title="server 初始化"></a>server 初始化</h4><p>server 的初始化过程是先创建并初始化了 ServerBootstrap。之后同样也是设置 EventLoopGroup 和 channel，以及通道相关的各种参数。</p><p>设置好之后，会添加 ChannelHandler 流水线，这里的 ChannelHandler 流水线就是我们前面创建的 NettyProtocol 提供的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ChannelHandler[] getServerChannelHandlers() &#123;</span><br><span class="line">    <span class="type">PartitionRequestQueue</span> <span class="variable">queueOfPartitionQueues</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PartitionRequestQueue</span>();</span><br><span class="line">    <span class="type">PartitionRequestServerHandler</span> <span class="variable">serverHandler</span> <span class="operator">=</span></span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">PartitionRequestServerHandler</span>(</span><br><span class="line">                    partitionProvider, taskEventPublisher, queueOfPartitionQueues);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ChannelHandler</span>[] &#123;</span><br><span class="line">        messageEncoder,</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">NettyMessage</span>.NettyMessageDecoder(),</span><br><span class="line">        serverHandler,</span><br><span class="line">        queueOfPartitionQueues</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>流水线上包含了消息编码器、解码器、PartitionRequestServerHandler 请求服务端处理器和 PartitionRequestQueue 分区请求队列。</p><p>这些都设置好之后，就开始启动 NettyServer 服务了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Iterator&lt;Integer&gt; portsIterator = config.getServerPortRange().getPortsIterator();</span><br><span class="line"><span class="keyword">while</span> (portsIterator.hasNext() &amp;&amp; bindFuture == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="type">Integer</span> <span class="variable">port</span> <span class="operator">=</span> portsIterator.next();</span><br><span class="line">    LOG.debug(<span class="string">&quot;Trying to bind Netty server to port: &#123;&#125;&quot;</span>, port);</span><br><span class="line"></span><br><span class="line">    bootstrap.localAddress(config.getServerAddress(), port);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        bindFuture = bootstrap.bind().syncUninterruptibly();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="comment">// syncUninterruptibly() throws checked exceptions via Unsafe</span></span><br><span class="line">        <span class="comment">// continue if the exception is due to the port being in use, fail early</span></span><br><span class="line">        <span class="comment">// otherwise</span></span><br><span class="line">        <span class="keyword">if</span> (isBindFailure(e)) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;Failed to bind Netty server&quot;</span>, e);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (bindFuture == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">BindException</span>(</span><br><span class="line">            <span class="string">&quot;Could not start rest endpoint on any port in port range &quot;</span></span><br><span class="line">                    + config.getServerPortRange());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">localAddress = (InetSocketAddress) bindFuture.channel().localAddress();</span><br></pre></td></tr></table></figure><h3 id="客户端请求远端子分区"><a href="#客户端请求远端子分区" class="headerlink" title="客户端请求远端子分区"></a>客户端请求远端子分区</h3><p>服务端和客户端初始化之后，在 Task 运行时，会先完成 Client 的 ChannelHandler 的配置，然后请求 Netty 远端服务。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767668875/Blog/flink/19/NettyRequestPartition.png" alt="NettyRequestPartition"></p><p>我们来看具体过程，在 Task 初始化时，会调用 <code>StreamTask.invoke</code> 方法，其内部会调用 <code>StreamTask.restoreStateAndGate</code> 方法，这里会便利 Task 的所有 InputGate，然后调用 requestPartitions。在 InputGate 的 requestPartitions 逻辑中，又便利所有的 InputChannel，调用 requestSubpartitions。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (InputGate inputGate : inputGates) &#123;</span><br><span class="line">    recoveredFutures.add(inputGate.getStateConsumedFuture());</span><br><span class="line"></span><br><span class="line">    inputGate</span><br><span class="line">            .getStateConsumedFuture()</span><br><span class="line">            .thenRun(</span><br><span class="line">                    () -&gt;</span><br><span class="line">                            mainMailboxExecutor.execute(</span><br><span class="line">                                    inputGate::requestPartitions,</span><br><span class="line">                                    <span class="string">&quot;Input gate request partitions&quot;</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">internalRequestPartitions</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (InputChannel inputChannel : inputChannels()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            inputChannel.requestSubpartitions();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            inputChannel.setError(t);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们来看 <code>RemoteInputChannel.requestSubpartitions</code> 的逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">requestSubpartitions</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    <span class="keyword">if</span> (partitionRequestClient == <span class="literal">null</span>) &#123;</span><br><span class="line">        LOG.debug(</span><br><span class="line">                <span class="string">&quot;&#123;&#125;: Requesting REMOTE subpartitions &#123;&#125; of partition &#123;&#125;. &#123;&#125;&quot;</span>,</span><br><span class="line">                <span class="built_in">this</span>,</span><br><span class="line">                consumedSubpartitionIndexSet,</span><br><span class="line">                partitionId,</span><br><span class="line">                channelStatePersister);</span><br><span class="line">        <span class="comment">// Create a client and request the partition</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            partitionRequestClient =</span><br><span class="line">                    connectionManager.createPartitionRequestClient(connectionId);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="comment">// IOExceptions indicate that we could not open a connection to the remote</span></span><br><span class="line">            <span class="comment">// TaskExecutor</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">PartitionConnectionException</span>(partitionId, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        partitionRequestClient.requestSubpartition(</span><br><span class="line">                partitionId, consumedSubpartitionIndexSet, <span class="built_in">this</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里主要有两个步骤，先是创建 partitionRequestClient，然后调用 requestSubpartition。</p><h4 id="创建请求客户端"><a href="#创建请求客户端" class="headerlink" title="创建请求客户端"></a>创建请求客户端</h4><p>PartitionRequestClient 是在 <code>PartitionRequestClientFactory.connect</code> 中创建的。先调用了 <code>NettyClient.connect</code>，同步等待客户端连接到服务端，这个过程中会进行 ChannelHandler 配置，也就是我们在初始化的过程中介绍的，NettyClient 没有完成的步骤。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ChannelHandler[] getClientChannelHandlers() &#123;</span><br><span class="line">    <span class="type">NetworkClientHandler</span> <span class="variable">networkClientHandler</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CreditBasedPartitionRequestClientHandler</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ChannelHandler</span>[] &#123;</span><br><span class="line">        messageEncoder,</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">NettyMessageClientDecoderDelegate</span>(networkClientHandler),</span><br><span class="line">        networkClientHandler</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Handler 包括了消息编码器、解码器和 CreditBasedPartitionRequestClientHandler 这个基于 Credit 的分区请求客户端处理器。Handler 配置好之后会利用 bootstrap 连接到服务端。</p><p>在获取到 Channel 和 NetworkClientHandler 之后，就直接创建了 NettyPartitionRequestClient。</p><h4 id="请求子分区数据"><a href="#请求子分区数据" class="headerlink" title="请求子分区数据"></a>请求子分区数据</h4><p>让我们再回到 RemoteInputChannel 的 requestSubpartitions 方法中，现在我们创建好了 NettyPartitionRequestClient，接下来就是调用它的 requestSubpartition 方法来发起请求。</p><p>这里逻辑也比较简单：</p><ol><li><p>向 NetworkClientHandler 注册当前 RemoteInputChannel。</p></li><li><p>构造请求对象 PartitionRequest，这里包含了分区 ID、子分区索引、inputChannel ID 以及初识的 Credit。</p></li><li><p>调用 <code>tcpChannel.writeAndFlush</code> 发起请求，并添加请求失败的监听。</p></li><li><p>如果请求失败，移除当前 inputChannel。</p></li></ol><h3 id="服务端响应"><a href="#服务端响应" class="headerlink" title="服务端响应"></a>服务端响应</h3><p>现在数据到了服务端，我们来看服务端处理的具体过程。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767684663/Blog/flink/19/NettyServerHandler.png" alt="NettyServerHandler"></p><p>在 NettyServer 初始化的过程中，我们添加了两个重要的 Handler，分别是 PartitionRequestServerHandler 和 PartitionRequestQueue。服务端响应数据的过程就是这两个 Handler 在发挥作用。</p><p>PartitionRequestServerHandler 负责处理 Client 端通过 PartitionRequestClient 发送的请求，处理过程是先创建 CreditBasedSequenceNumberingViewReader 类型的 reader，然后将它放入 PartitionRequestQueue 维护的 reader 队列中。PartitionRequestQueue 会监听 Netty Channel 的可写入状态，当 Netty Channel 可写入时，会消费数据并写入网络。</p><p>下面我们来看具体的源码。</p><p>服务端的响应入口在 <code>PartitionRequestServerHandler.channelRead0</code> 方法，这里在处理 PartitionRequest 请求时，先是创建 CreditBasedSequenceNumberingViewReader，然后调用 requestSubpartitionViewOrRegisterListener。</p><p>requestSubpartitionViewOrRegisterListener 的逻辑是创建 ResultSubpartitionView，并提醒 PartitionRequestQueue 有数据可用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Optional&lt;ResultSubpartitionView&gt; subpartitionViewOptional =</span><br><span class="line">        partitionProvider.createSubpartitionViewOrRegisterListener(</span><br><span class="line">                resultPartitionId,</span><br><span class="line">                subpartitionIndexSet,</span><br><span class="line">                <span class="built_in">this</span>,</span><br><span class="line">                partitionRequestListener);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">notifyDataAvailable(subpartitionView);</span><br></pre></td></tr></table></figure><p>ResultSubpartitionView 就是用来消费 ResultSubpartition 的数据。</p><p>notifyDataAvailable 内部调用了 notifyReaderNonEmpty，notifyReaderNonEmpty 又触发了 userEventTriggered，这里调用 enqueueAvailableReader 将 reader 放入到可用队列 availableReaders 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">enqueueAvailableReader</span><span class="params">(<span class="keyword">final</span> NetworkSequenceViewReader reader)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="keyword">if</span> (reader.isRegisteredAsAvailable()) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ResultSubpartitionView.<span class="type">AvailabilityWithBacklog</span> <span class="variable">availabilityWithBacklog</span> <span class="operator">=</span></span><br><span class="line">            reader.getAvailabilityAndBacklog();</span><br><span class="line">    <span class="keyword">if</span> (!availabilityWithBacklog.isAvailable()) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">backlog</span> <span class="operator">=</span> availabilityWithBacklog.getBacklog();</span><br><span class="line">        <span class="keyword">if</span> (backlog &gt; <span class="number">0</span> &amp;&amp; reader.needAnnounceBacklog()) &#123;</span><br><span class="line">            announceBacklog(reader, backlog);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Queue an available reader for consumption. If the queue is empty,</span></span><br><span class="line">    <span class="comment">// we try trigger the actual write. Otherwise this will be handled by</span></span><br><span class="line">    <span class="comment">// the writeAndFlushNextMessageIfPossible calls.</span></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">triggerWrite</span> <span class="operator">=</span> availableReaders.isEmpty();</span><br><span class="line">    registerAvailableReader(reader);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (triggerWrite) &#123;</span><br><span class="line">        writeAndFlushNextMessageIfPossible(ctx.channel());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果 reader 是队列中的第一个元素，会触发数据写入网络。</p><p>writeAndFlushNextMessageIfPossible 的处理步骤如下：</p><ol><li><p>取出可用 reader。</p></li><li><p>调用 <code>reader.getNextBuffer</code> 获取数据。</p></li><li><p>如果 reader 仍然可用，将其加回队列。</p></li><li><p>向下游写入数据并添加下次写入的监听。</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> BufferAndAvailability <span class="title function_">getNextBuffer</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="type">BufferAndBacklog</span> <span class="variable">next</span> <span class="operator">=</span> subpartitionView.getNextBuffer();</span><br><span class="line">    <span class="keyword">if</span> (next != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (next.buffer().isBuffer() &amp;&amp; --numCreditsAvailable &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalStateException</span>(<span class="string">&quot;no credit available&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> Buffer.<span class="type">DataType</span> <span class="variable">nextDataType</span> <span class="operator">=</span> getNextDataType(next);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">BufferAndAvailability</span>(</span><br><span class="line">                next.buffer(), nextDataType, next.buffersInBacklog(), next.getSequenceNumber());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 getNextBuffer 方法中，会将 credit 值减 1，并判断是否小于 0。如果小于 0 会抛出异常，reader 是否可用也是根据 numCreditsAvailable 是否大于 0 来判断的。</p><h3 id="客户端接收数据"><a href="#客户端接收数据" class="headerlink" title="客户端接收数据"></a>客户端接收数据</h3><p>NettyClient 在消费数据时，同样也是以 ChannelHandler 作为入口。这里的入口方法是 <code>CreditBasedPartitionRequestClientHandler.channelRead</code> 。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767689791/Blog/flink/19/NettyClientHandler.png" alt="NettyClientHandler"></p><p>在 decodeMsg 方法中，先解码 msg，判断 InputChannel 是否可用，如果不可用，则取消当前 InputChannel 的订阅。如果可用，继续调用 decodeBufferOrEvent 进行处理。decodeBufferOrEvent 的核心逻辑是调用 <code>RemoteInputChannel.onBuffer</code> 方法，将数据加入到 receivedBuffers 队列。</p><p>如果 receivedBuffers 队列在此之前处于空闲状态，会调用 notifyChannelNonEmpty，将当前 RemoteInputChannel 加入到 inputChannelsWithData 队列中，同时还会唤醒 inputChannelsWithData 上的阻塞线程，让 inputGate 可以消费 RemoteInputChannel 的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">queueChannelUnsafe</span><span class="params">(InputChannel channel, <span class="type">boolean</span> priority)</span> &#123;</span><br><span class="line">    <span class="keyword">assert</span> Thread.holdsLock(inputChannelsWithData);</span><br><span class="line">    <span class="keyword">if</span> (channelsWithEndOfPartitionEvents.get(channel.getChannelIndex())) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">alreadyEnqueued</span> <span class="operator">=</span></span><br><span class="line">            enqueuedInputChannelsWithData.get(channel.getChannelIndex());</span><br><span class="line">    <span class="keyword">if</span> (alreadyEnqueued</span><br><span class="line">            &amp;&amp; (!priority || inputChannelsWithData.containsPriorityElement(channel))) &#123;</span><br><span class="line">        <span class="comment">// already notified / prioritized (double notification), ignore</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当前 inputChannel 加入到 inputChannelsWithData</span></span><br><span class="line">    inputChannelsWithData.add(channel, priority, alreadyEnqueued);</span><br><span class="line">    <span class="keyword">if</span> (!alreadyEnqueued) &#123;</span><br><span class="line">        enqueuedInputChannelsWithData.set(channel.getChannelIndex());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 唤醒线程</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">notifyDataAvailable</span><span class="params">()</span> &#123;</span><br><span class="line">    availabilityMonitor.notifyAll();</span><br><span class="line">    toNotify = inputGate.availabilityHelper.getUnavailableToResetAvailable();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果客户端有积压，还需要根据积压申请 Buffer 并更新 Credit 值。这里申请的 buffer 数量为积压数量+初识 Credit 值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onSenderBacklog</span><span class="params">(<span class="type">int</span> backlog)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    notifyBufferAvailable(bufferManager.requestFloatingBuffers(backlog + initialCredit));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果 RemoteInputChannel 没有足够的 buffer，则会向 LocalBufferPool 申请新的 buffer，如果申请不到，会加一个监听，等 LocalBufferPool 有空闲时再触发申请 buffer。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="title function_">tryRequestBuffers</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">assert</span> Thread.holdsLock(bufferQueue);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> <span class="variable">numRequestedBuffers</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (bufferQueue.getAvailableBufferSize() &lt; numRequiredBuffers</span><br><span class="line">            &amp;&amp; !isWaitingForFloatingBuffers) &#123;</span><br><span class="line">        <span class="type">BufferPool</span> <span class="variable">bufferPool</span> <span class="operator">=</span> inputChannel.inputGate.getBufferPool();</span><br><span class="line">        <span class="type">Buffer</span> <span class="variable">buffer</span> <span class="operator">=</span> bufferPool.requestBuffer();</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="literal">null</span>) &#123;</span><br><span class="line">            bufferQueue.addFloatingBuffer(buffer);</span><br><span class="line">            numRequestedBuffers++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (bufferPool.addBufferListener(<span class="built_in">this</span>)) &#123;</span><br><span class="line">            isWaitingForFloatingBuffers = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> numRequestedBuffers;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当 RemoteInputChannel 申请到了需要的 buffer 之后，就会向 NettyServer 发送 AddCredit 消息，请求更新 Credit 值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">notifyCreditAvailable</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    checkPartitionRequestQueueInitialized();</span><br><span class="line"></span><br><span class="line">    partitionRequestClient.notifyCreditAvailable(<span class="built_in">this</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">notifyCreditAvailable</span><span class="params">(RemoteInputChannel inputChannel)</span> &#123;</span><br><span class="line">    sendToChannel(<span class="keyword">new</span> <span class="title class_">AddCreditMessage</span>(inputChannel));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>NettyServer 收到请求后，会将对应的 Credit 值进行更新。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (msgClazz == AddCredit.class) &#123;</span><br><span class="line">    <span class="type">AddCredit</span> <span class="variable">request</span> <span class="operator">=</span> (AddCredit) msg;</span><br><span class="line"></span><br><span class="line">    outboundQueue.addCreditOrResumeConsumption(</span><br><span class="line">            request.receiverId, reader -&gt; reader.addCredit(request.credit));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">addCreditOrResumeConsumption</span><span class="params">(</span></span><br><span class="line"><span class="params">        InputChannelID receiverId, Consumer&lt;NetworkSequenceViewReader&gt; operation)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="keyword">if</span> (fatalError) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">NetworkSequenceViewReader</span> <span class="variable">reader</span> <span class="operator">=</span> obtainReader(receiverId);</span><br><span class="line"></span><br><span class="line">    operation.accept(reader);</span><br><span class="line">    enqueueAvailableReader(reader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addCredit</span><span class="params">(<span class="type">int</span> creditDeltas)</span> &#123;</span><br><span class="line">    numCreditsAvailable += creditDeltas;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此外，Flink 还有两种场景会更新 Credit 值。</p><p>一种是 LocalBufferPool 回收空闲 buffer 时，会将 buffer 分配给申请者，分配之后会调用对应 InputChannel 的 notifyBufferAvailable 方法通知更新 Credit。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767694630/Blog/flink/19/LocalBufferPoolRecycle.png" alt="LocalBufferPool"></p><p>另一种是 RemoteInputChannel 独占的 buffer 队列释放 buffer 时，会触发 Credit 更新。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767700038/Blog/flink/19/BufferManagerRecycle.png" alt="BufferManager"></p><h3 id="反压"><a href="#反压" class="headerlink" title="反压"></a>反压</h3><p>通过前面的学习，我们其实已经理解了反压的机制了。当前 Flink 的反压就是通过 Credit 来实现反压的，如果下游数据处理速度慢，Credit 会被耗尽，上游也就不会继续处理和下发数据了。直到下游处理完成，有了空闲的 buffer，此时向上游反馈更新 Credit 值，上游就会继续处理数据。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后我们总结一下，本文我们一起梳理了 Flink Netty 相关的源码。包括 NettyClient 和 NettyServer 的初始化，初始化过程中会创建一系列 ChannelHandler，之后利用这些Handler 处理数据，数据处理包括 Client 端的发送和接收消息，Server 端处理消息的过程。中间还穿插着 Credit 的处理。Flink 的反压逻辑就是依赖于 Credit 来实现的。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前文中我们了解了 Flink 的数据交互过程，上游的 Task 将数据写入到 ResultSubpartition 的 buffers 队列中。下游的 Task 通过 LocalInputChannel 和 RemoteInputChannel 消费上游的数据。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：Task数据交互</title>
    <link href="https://jackeyzhe.github.io/2025/12/29/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ATask%E6%95%B0%E6%8D%AE%E4%BA%A4%E4%BA%92/"/>
    <id>https://jackeyzhe.github.io/2025/12/29/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ATask%E6%95%B0%E6%8D%AE%E4%BA%A4%E4%BA%92/</id>
    <published>2025-12-29T02:44:46.000Z</published>
    <updated>2026-01-04T12:52:01.194Z</updated>
    
    <content type="html"><![CDATA[<p>经过前面的学习，Flink 的几个核心概念相关的源码实现我们已经了解了。本文我们来梳理 Task 的数据交互相关的源码。<span id="more"></span></p><h3 id="数据输出"><a href="#数据输出" class="headerlink" title="数据输出"></a>数据输出</h3><p>话不多说，我们直接进入正题。首先来看 Task 的数据输出，在进入流程之前，我们先介绍几个基本概念。</p><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ul><li><p>RecordWriterOutput：它是 Output 接口的一个具体实现类，底层使用 RecordWriter 来发送数据。</p></li><li><p>RecordWriter：数据写入的执行者，负责将数据写到 ResultPartition。</p></li><li><p>ResultPartition 和 ResultSubpartition：ResultPartition 是 ExecutionGraph 中一个节点的输出结果，下游的每个需要从当前 ResultPartition 消费数据的 Task 都会有一个 ResultSubpartition。</p></li><li><p>ChannelSelector：用来决定一个 Record 要被写到哪个 Subpartition 中。</p></li><li><p>LocalBufferPool：用来管理 Buffer 的缓冲池。在介绍反压的原理时，我们提到过。</p></li></ul><p>对这些基本概念有了一定的了解之后，我们来看数据输出的具体流程。</p><h4 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h4><p>我们以 map 为例，看一下数据的输出过程。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767175113/Blog/flink/18/RecordOutput.png" alt="DataOutput"></p><p>在 <code>StreamMap.processElement</code> 方法中，调用完 map 方法之后，就会调用 <code>output.collect</code> 方法将数据输出，这里的 output 就是 RecordWriterOutput。在 RecordWriterOutput 中，会调用 RecordWriter 的 emit 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;X&gt; <span class="keyword">void</span> <span class="title function_">pushToRecordWriter</span><span class="params">(StreamRecord&lt;X&gt; record)</span> &#123;</span><br><span class="line">    serializationDelegate.setInstance(record);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        recordWriter.emit(serializationDelegate);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UncheckedIOException</span>(e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的 serializationDelegate 是用来对 record 进行序列化的。RecordWriter 有两个实现类，一个是 ChannelSelectorRecordWriter，另一个是 BroadcastRecordWriter。ChannelSelectorRecordWriter 需要先调用 ChannelSelector 选择对应的 subparition，然后进行写入。BroadcastRecordWriter 则是写到所有的 subparition。</p><p>接下来就是调用 <code>BufferWritingResultPartition.emitRecord</code> 来写入数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">emitRecord</span><span class="params">(ByteBuffer record, <span class="type">int</span> targetSubpartition)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    totalWrittenBytes += record.remaining();</span><br><span class="line"></span><br><span class="line">    <span class="type">BufferBuilder</span> <span class="variable">buffer</span> <span class="operator">=</span> appendUnicastDataForNewRecord(record, targetSubpartition);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (record.hasRemaining()) &#123;</span><br><span class="line">        <span class="comment">// full buffer, partial record</span></span><br><span class="line">        finishUnicastBufferBuilder(targetSubpartition);</span><br><span class="line">        buffer = appendUnicastDataForRecordContinuation(record, targetSubpartition);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (buffer.isFull()) &#123;</span><br><span class="line">        <span class="comment">// full buffer, full record</span></span><br><span class="line">        finishUnicastBufferBuilder(targetSubpartition);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// partial buffer, full record</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里把 record 写入到 buffer 中，如果 buffer 不够，则会从 LocalBufferPool 中申请新的 buffer，申请到之后就会继续写入。下面是具体的申请过程。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> MemorySegment <span class="title function_">requestMemorySegment</span><span class="params">(<span class="type">int</span> targetChannel)</span> &#123;</span><br><span class="line">    <span class="type">MemorySegment</span> <span class="variable">segment</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">synchronized</span> (availableMemorySegments) &#123;</span><br><span class="line">        checkDestroyed();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!availableMemorySegments.isEmpty()) &#123;</span><br><span class="line">            segment = availableMemorySegments.poll();</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isRequestedSizeReached()) &#123;</span><br><span class="line">            <span class="comment">// Only when the buffer request reaches the upper limit(i.e. current pool size),</span></span><br><span class="line">            <span class="comment">// requests an overdraft buffer.</span></span><br><span class="line">            segment = requestOverdraftMemorySegmentFromGlobal();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (segment == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (targetChannel != UNKNOWN_CHANNEL) &#123;</span><br><span class="line">            <span class="keyword">if</span> (++subpartitionBuffersCount[targetChannel] == maxBuffersPerChannel) &#123;</span><br><span class="line">                unavailableSubpartitionsCount++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        checkAndUpdateAvailability();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> segment;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果有可用内存，就直接从队列中出队。如果达到了本地 BufferPool 的上限，就从全局的 NetworkBufferPool 中申请，申请不到就会阻塞写入过程，等待申请。最后还会检查并更新可用内存状态。</p><p>有了可用的 buffer 之后，就会调用 addToSubpartition，最终数据存储在 PipelinedSubpartition 的 buffers 队列中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">addToSubpartition</span><span class="params">(</span></span><br><span class="line"><span class="params">        BufferBuilder buffer,</span></span><br><span class="line"><span class="params">        <span class="type">int</span> targetSubpartition,</span></span><br><span class="line"><span class="params">        <span class="type">int</span> partialRecordLength,</span></span><br><span class="line"><span class="params">        <span class="type">int</span> minDesirableBufferSize)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">desirableBufferSize</span> <span class="operator">=</span></span><br><span class="line">            subpartitions[targetSubpartition].add(</span><br><span class="line">                    buffer.createBufferConsumerFromBeginning(), partialRecordLength);</span><br><span class="line"></span><br><span class="line">    resizeBuffer(buffer, desirableBufferSize, minDesirableBufferSize);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h3><p>看完了数据输出的过程之后，我们再来看一下数据输入的过程。首先还是了解几个基本概念。</p><h4 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h4><ul><li><p>InputGate：InputGate 是对输入的封装，与 JobGraph 中的 JobEdge 一一对应，每个 InputGate 消费上游一个或多个 Resultpartition。</p></li><li><p>InputChannel：InputChannel 是和 ExecutionGraph 中的 ExecutionEdge 一一对应的。每个 InputChannel 接收一个 ResultSubpartition 的输出，InputChannel 主要关注 LocalInputChannel 和 RemoteInputChannel 两种实现。</p></li></ul><h4 id="执行流程-1"><a href="#执行流程-1" class="headerlink" title="执行流程"></a>执行流程</h4><p>了解了具体概念之后，我们再看数据输入的具体流程。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1767531100/Blog/flink/18/RecordInput.png" alt="RecordInput"></p><p>数据输入的入口是 <code>StreamTask.processInput</code> 方法，这个方法中主要是调用 <code>inputProcessor.processInput</code> 方法，我们以 StreamOneInputProcessor 为例。这个方法就是调用 <code>input.emitNext</code> 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DataInputStatus <span class="title function_">emitNext</span><span class="params">(DataOutput&lt;T&gt; output)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// get the stream element from the deserializer</span></span><br><span class="line">        <span class="keyword">if</span> (currentRecordDeserializer != <span class="literal">null</span>) &#123;</span><br><span class="line">            RecordDeserializer.DeserializationResult result;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                result = currentRecordDeserializer.getNextRecord(deserializationDelegate);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(</span><br><span class="line">                        String.format(<span class="string">&quot;Can&#x27;t get next record for channel %s&quot;</span>, lastChannel), e);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (result.isBufferConsumed()) &#123;</span><br><span class="line">                currentRecordDeserializer = <span class="literal">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (result.isFullRecord()) &#123;</span><br><span class="line">                <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">breakBatchEmitting</span> <span class="operator">=</span></span><br><span class="line">                        processElement(deserializationDelegate.getInstance(), output);</span><br><span class="line">                <span class="keyword">if</span> (canEmitBatchOfRecords.check() &amp;&amp; !breakBatchEmitting) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> DataInputStatus.MORE_AVAILABLE;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Optional&lt;BufferOrEvent&gt; bufferOrEvent = checkpointedInputGate.pollNext();</span><br><span class="line">        <span class="keyword">if</span> (bufferOrEvent.isPresent()) &#123;</span><br><span class="line">            <span class="comment">// return to the mailbox after receiving a checkpoint barrier to avoid processing of</span></span><br><span class="line">            <span class="comment">// data after the barrier before checkpoint is performed for unaligned checkpoint</span></span><br><span class="line">            <span class="comment">// mode</span></span><br><span class="line">            <span class="keyword">if</span> (bufferOrEvent.get().isBuffer()) &#123;</span><br><span class="line">                processBuffer(bufferOrEvent.get());</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">DataInputStatus</span> <span class="variable">status</span> <span class="operator">=</span> processEvent(bufferOrEvent.get(), output);</span><br><span class="line">                <span class="keyword">if</span> (status == DataInputStatus.MORE_AVAILABLE &amp;&amp; canEmitBatchOfRecords.check()) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> status;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (checkpointedInputGate.isFinished()) &#123;</span><br><span class="line">                checkState(</span><br><span class="line">                        checkpointedInputGate.getAvailableFuture().isDone(),</span><br><span class="line">                        <span class="string">&quot;Finished BarrierHandler should be available&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> DataInputStatus.END_OF_INPUT;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> DataInputStatus.NOTHING_AVAILABLE;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里是调用 <code>checkpointedInputGate.pollNext</code> 来获取输入的数据。它的内部就是调用 InputGate 的 pollNext 方法来获取数据。当获取到完整数据之后，就会调用 processElement 来处理数据。</p><p>我们以 SingleInputGate 为例看 InputGate 的 pollNext 方法。它的内部调用链路可用一直追踪到 readBufferFromInputChannel 方法，这个方法内会调用 <code>inputChannel.getNextBuffer</code>，这里交给 InputChannel 来具体执行数据读取。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Optional&lt;BufferAndAvailability&gt; <span class="title function_">getNextBuffer</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    checkError();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!toBeConsumedBuffers.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> getBufferAndAvailability(toBeConsumedBuffers.removeFirst());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">ResultSubpartitionView</span> <span class="variable">subpartitionView</span> <span class="operator">=</span> <span class="built_in">this</span>.subpartitionView;</span><br><span class="line">    <span class="keyword">if</span> (subpartitionView == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// There is a possible race condition between writing a EndOfPartitionEvent (1) and</span></span><br><span class="line">        <span class="comment">// flushing (3) the Local</span></span><br><span class="line">        <span class="comment">// channel on the sender side, and reading EndOfPartitionEvent (2) and processing flush</span></span><br><span class="line">        <span class="comment">// notification (4). When</span></span><br><span class="line">        <span class="comment">// they happen in that order (1 - 2 - 3 - 4), flush notification can re-enqueue</span></span><br><span class="line">        <span class="comment">// LocalInputChannel after (or</span></span><br><span class="line">        <span class="comment">// during) it was released during reading the EndOfPartitionEvent (2).</span></span><br><span class="line">        <span class="keyword">if</span> (isReleased) &#123;</span><br><span class="line">            <span class="keyword">return</span> Optional.empty();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// this can happen if the request for the partition was triggered asynchronously</span></span><br><span class="line">        <span class="comment">// by the time trigger</span></span><br><span class="line">        <span class="comment">// would be good to avoid that, by guaranteeing that the requestPartition() and</span></span><br><span class="line">        <span class="comment">// getNextBuffer() always come from the same thread</span></span><br><span class="line">        <span class="comment">// we could do that by letting the timer insert a special &quot;requesting channel&quot; into the</span></span><br><span class="line">        <span class="comment">// input gate&#x27;s queue</span></span><br><span class="line">        subpartitionView = checkAndWaitForSubpartitionView();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">BufferAndBacklog</span> <span class="variable">next</span> <span class="operator">=</span> subpartitionView.getNextBuffer();</span><br><span class="line">    <span class="comment">// ignore the empty buffer directly</span></span><br><span class="line">    <span class="keyword">while</span> (next != <span class="literal">null</span> &amp;&amp; next.buffer().readableBytes() == <span class="number">0</span>) &#123;</span><br><span class="line">        next.buffer().recycleBuffer();</span><br><span class="line">        next = subpartitionView.getNextBuffer();</span><br><span class="line">        numBuffersIn.inc();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (next == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (subpartitionView.isReleased()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">CancelTaskException</span>(</span><br><span class="line">                    <span class="string">&quot;Consumed partition &quot;</span> + subpartitionView + <span class="string">&quot; has been released.&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> Optional.empty();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">Buffer</span> <span class="variable">buffer</span> <span class="operator">=</span> next.buffer();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (buffer <span class="keyword">instanceof</span> FullyFilledBuffer) &#123;</span><br><span class="line">        List&lt;Buffer&gt; partialBuffers = ((FullyFilledBuffer) buffer).getPartialBuffers();</span><br><span class="line">        <span class="type">int</span> <span class="variable">seq</span> <span class="operator">=</span> next.getSequenceNumber();</span><br><span class="line">        <span class="keyword">for</span> (Buffer partialBuffer : partialBuffers) &#123;</span><br><span class="line">            toBeConsumedBuffers.add(</span><br><span class="line">                    <span class="keyword">new</span> <span class="title class_">BufferAndBacklog</span>(</span><br><span class="line">                            partialBuffer,</span><br><span class="line">                            next.buffersInBacklog(),</span><br><span class="line">                            buffer.getDataType(),</span><br><span class="line">                            seq++));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> getBufferAndAvailability(toBeConsumedBuffers.removeFirst());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> getBufferAndAvailability(next);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们先来看 LocalInputChannel，先获取到了 subpartitionView，并调用 getNextBuffer，这里其实就是从 PipelinedSubpartition 的 buffers 队列中读取数据。</p><p>RemoteInputChannel 则需要从 receivedBuffers 中读取数据，这个队列的数据就是消费上游数据后保存的。</p><p>至此，Flink 中 Task 的数据输入和输出过程的源码就梳理完了，更加底层的 Netty 相关代码我们在后面继续梳理。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后简单总结一下，本文我们梳理了 Task 的数据输出和输入的过程。输出过程主要是利用 RecordWriter 将数据写入到 Buffer 中，输入过程则是利用 InputChannel 从 Buffer 消费的过程。如果你的 Flink 任务数据量特别大，并且没什么复杂的逻辑，可以考虑适当调整 localBufferPool 的大小来调优任务的吞吐。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;经过前面的学习，Flink 的几个核心概念相关的源码实现我们已经了解了。本文我们来梳理 Task 的数据交互相关的源码。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：双流操作</title>
    <link href="https://jackeyzhe.github.io/2025/12/24/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%8F%8C%E6%B5%81%E6%93%8D%E4%BD%9C/"/>
    <id>https://jackeyzhe.github.io/2025/12/24/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%8F%8C%E6%B5%81%E6%93%8D%E4%BD%9C/</id>
    <published>2025-12-24T08:24:14.000Z</published>
    <updated>2025-12-27T04:45:37.366Z</updated>
    
    <content type="html"><![CDATA[<p>今天来梳理一下 Flink 双流操作相关的源码。<span id="more"></span></p><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>通过<a href="https://jackeyzhe.github.io/2025/07/19/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%A4%9A%E6%B5%81join/">Flink学习笔记：多流 Join</a>一文的介绍，我们知道 Flink 有三种数据关联的方式，分别是 Window Join、Interval Join 和 CoGroup。下面我们分别看下这三种关联方式的源码实现。</p><h3 id="Window-Join"><a href="#Window-Join" class="headerlink" title="Window Join"></a>Window Join</h3><p>我们先回顾一下 window join 的使用方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Double&gt;&gt; result = source1.join(source2)</span><br><span class="line">        .where(record -&gt; record.f0)</span><br><span class="line">        .equalTo(record -&gt; record.f0)</span><br><span class="line">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">2L</span>)))</span><br><span class="line">        .apply(<span class="keyword">new</span> <span class="title class_">JoinFunction</span>&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Double&gt; <span class="title function_">join</span><span class="params">(Tuple2&lt;String, Double&gt; record1, Tuple2&lt;String, Double&gt; record2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(record1.f0, record1.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>上述调用链路类的流转如下：</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766656374/Blog/flink/17/window-join.png" alt="window-join"></p><p>在 WithWindow 的 apply 方法中，是构建了一个 coGroupedWindowedStream，然后调用它的 apply 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;T&gt; SingleOutputStreamOperator&lt;T&gt; <span class="title function_">apply</span><span class="params">(</span></span><br><span class="line"><span class="params">        JoinFunction&lt;T1, T2, T&gt; function, TypeInformation&lt;T&gt; resultType)</span> &#123;</span><br><span class="line">    <span class="comment">// clean the closure</span></span><br><span class="line">    function = input1.getExecutionEnvironment().clean(function);</span><br><span class="line"></span><br><span class="line">    coGroupedWindowedStream =</span><br><span class="line">            input1.coGroup(input2)</span><br><span class="line">                    .where(keySelector1)</span><br><span class="line">                    .equalTo(keySelector2)</span><br><span class="line">                    .window(windowAssigner)</span><br><span class="line">                    .trigger(trigger)</span><br><span class="line">                    .evictor(evictor)</span><br><span class="line">                    .allowedLateness(allowedLateness);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> coGroupedWindowedStream.apply(<span class="keyword">new</span> <span class="title class_">JoinCoGroupFunction</span>&lt;&gt;(function), resultType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里可以看出，Window Join 的底层是转换成 coGroup 进行处理的。</p><p>在 JoinCoGroupFunction 中，coGroup 方法就是对两个流进行两层遍历，然后将其应用到我们自定义的 JoinFunction 上。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">JoinCoGroupFunction</span>&lt;T1, T2, T&gt;</span><br><span class="line">        <span class="keyword">extends</span> <span class="title class_">WrappingFunction</span>&lt;JoinFunction&lt;T1, T2, T&gt;&gt;</span><br><span class="line">        <span class="keyword">implements</span> <span class="title class_">CoGroupFunction</span>&lt;T1, T2, T&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">JoinCoGroupFunction</span><span class="params">(JoinFunction&lt;T1, T2, T&gt; wrappedFunction)</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>(wrappedFunction);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">coGroup</span><span class="params">(Iterable&lt;T1&gt; first, Iterable&lt;T2&gt; second, Collector&lt;T&gt; out)</span></span><br><span class="line">            <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">for</span> (T1 val1 : first) &#123;</span><br><span class="line">            <span class="keyword">for</span> (T2 val2 : second) &#123;</span><br><span class="line">                out.collect(wrappedFunction.join(val1, val2));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CoGroup"><a href="#CoGroup" class="headerlink" title="CoGroup"></a>CoGroup</h3><p>CoGroup 的整体用法和流程与 Join 都类似，我们就不逐个介绍了。我们直接来看 apply 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;T&gt; SingleOutputStreamOperator&lt;T&gt; <span class="title function_">apply</span><span class="params">(</span></span><br><span class="line"><span class="params">        CoGroupFunction&lt;T1, T2, T&gt; function, TypeInformation&lt;T&gt; resultType)</span> &#123;</span><br><span class="line">    <span class="comment">// clean the closure</span></span><br><span class="line">    function = input1.getExecutionEnvironment().clean(function);</span><br><span class="line"></span><br><span class="line">    UnionTypeInfo&lt;T1, T2&gt; unionType =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">UnionTypeInfo</span>&lt;&gt;(input1.getType(), input2.getType());</span><br><span class="line">    UnionKeySelector&lt;T1, T2, KEY&gt; unionKeySelector =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">UnionKeySelector</span>&lt;&gt;(keySelector1, keySelector2);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;TaggedUnion&lt;T1, T2&gt;&gt; taggedInput1 =</span><br><span class="line">            input1.map(<span class="keyword">new</span> <span class="title class_">Input1Tagger</span>&lt;T1, T2&gt;());</span><br><span class="line">    taggedInput1.getTransformation().setParallelism(input1.getParallelism(), <span class="literal">false</span>);</span><br><span class="line">    taggedInput1.returns(unionType);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;TaggedUnion&lt;T1, T2&gt;&gt; taggedInput2 =</span><br><span class="line">            input2.map(<span class="keyword">new</span> <span class="title class_">Input2Tagger</span>&lt;T1, T2&gt;());</span><br><span class="line">    taggedInput2.getTransformation().setParallelism(input2.getParallelism(), <span class="literal">false</span>);</span><br><span class="line">    taggedInput2.returns(unionType);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;TaggedUnion&lt;T1, T2&gt;&gt; unionStream = taggedInput1.union(taggedInput2);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we explicitly create the keyed stream to manually pass the key type information in</span></span><br><span class="line">    windowedStream =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">KeyedStream</span>&lt;TaggedUnion&lt;T1, T2&gt;, KEY&gt;(</span><br><span class="line">                            unionStream, unionKeySelector, keyType)</span><br><span class="line">                    .window(windowAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (trigger != <span class="literal">null</span>) &#123;</span><br><span class="line">        windowedStream.trigger(trigger);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (evictor != <span class="literal">null</span>) &#123;</span><br><span class="line">        windowedStream.evictor(evictor);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (allowedLateness != <span class="literal">null</span>) &#123;</span><br><span class="line">        windowedStream.allowedLateness(allowedLateness);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> windowedStream.apply(</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">CoGroupWindowFunction</span>&lt;T1, T2, T, KEY, W&gt;(function), resultType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 apply 方法中，先把两个流进行合并，然后创建了 windowedStream，并把窗口相关的属性设置好，最后是调用 windowedStream 的 apply 方法。</p><p>在调用 <code>windowedStream.apply</code> 方法时，又将 function 包装成了 CoGroupWindowFunction。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">CoGroupWindowFunction</span>&lt;T1, T2, T, KEY, W <span class="keyword">extends</span> <span class="title class_">Window</span>&gt;</span><br><span class="line">        <span class="keyword">extends</span> <span class="title class_">WrappingFunction</span>&lt;CoGroupFunction&lt;T1, T2, T&gt;&gt;</span><br><span class="line">        <span class="keyword">implements</span> <span class="title class_">WindowFunction</span>&lt;TaggedUnion&lt;T1, T2&gt;, T, KEY, W&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">CoGroupWindowFunction</span><span class="params">(CoGroupFunction&lt;T1, T2, T&gt; userFunction)</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>(userFunction);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(KEY key, W window, Iterable&lt;TaggedUnion&lt;T1, T2&gt;&gt; values, Collector&lt;T&gt; out)</span></span><br><span class="line">            <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        List&lt;T1&gt; oneValues = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        List&lt;T2&gt; twoValues = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TaggedUnion&lt;T1, T2&gt; val : values) &#123;</span><br><span class="line">            <span class="keyword">if</span> (val.isOne()) &#123;</span><br><span class="line">                oneValues.add(val.getOne());</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                twoValues.add(val.getTwo());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        wrappedFunction.coGroup(oneValues, twoValues, out);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 CoGroupWindowFunction 的 apply 方法中是将主键为 key 的流分开两个流，再去调用 JoinCoGroupFunction 的 coGroup 方法。这里的 values 都是相同的 key，原因是在 window 中维护的 windowState，它内部是一个 stateTable，窗口的 namespace 和 key 共同维护一个 state，当窗口触发时，就会对相同 key 的数据调用 apply 方法。</p><h3 id="Interval-Join"><a href="#Interval-Join" class="headerlink" title="Interval Join"></a>Interval Join</h3><p>梳理完了 Window Join 和 CoGroup 之后，我们再接着看 Interval Join。还是先来回顾一下用法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Double&gt;&gt; intervalJoinResult = source1.keyBy(record -&gt; record.f0)</span><br><span class="line">        .intervalJoin(source2.keyBy(record -&gt; record.f0))</span><br><span class="line">        .between(Time.seconds(-<span class="number">2</span>), Time.seconds(<span class="number">2</span>))</span><br><span class="line">        .process(<span class="keyword">new</span> <span class="title class_">ProcessJoinFunction</span>&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(Tuple2&lt;String, Double&gt; record1, Tuple2&lt;String, Double&gt; record2, ProcessJoinFunction&lt;Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;, Tuple2&lt;String, Double&gt;&gt;.Context context, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                out.collect(Tuple2.of(record1.f0, record1.f1 + record2.f1));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>通过用法可以看出，interval join 传入的对象是两个 KeyedStream，接着使用 between 方法定义 interval join 的上下边界，最后调用 process 方法执行计算逻辑。</p><p>在调用过程中，类型的转换如下图。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766736225/Blog/flink/17/interval-join.png" alt="interval-join"></p><p>我们主要关注 process 的逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;OUT&gt; SingleOutputStreamOperator&lt;OUT&gt; <span class="title function_">process</span><span class="params">(</span></span><br><span class="line"><span class="params">        ProcessJoinFunction&lt;IN1, IN2, OUT&gt; processJoinFunction,</span></span><br><span class="line"><span class="params">        TypeInformation&lt;OUT&gt; outputType)</span> &#123;</span><br><span class="line">    Preconditions.checkNotNull(processJoinFunction);</span><br><span class="line">    Preconditions.checkNotNull(outputType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> ProcessJoinFunction&lt;IN1, IN2, OUT&gt; cleanedUdf =</span><br><span class="line">            left.getExecutionEnvironment().clean(processJoinFunction);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isEnableAsyncState) &#123;</span><br><span class="line">        <span class="keyword">final</span> AsyncIntervalJoinOperator&lt;KEY, IN1, IN2, OUT&gt; operator =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">AsyncIntervalJoinOperator</span>&lt;&gt;(</span><br><span class="line">                        lowerBound,</span><br><span class="line">                        upperBound,</span><br><span class="line">                        lowerBoundInclusive,</span><br><span class="line">                        upperBoundInclusive,</span><br><span class="line">                        leftLateDataOutputTag,</span><br><span class="line">                        rightLateDataOutputTag,</span><br><span class="line">                        left.getType()</span><br><span class="line">                                .createSerializer(</span><br><span class="line">                                        left.getExecutionConfig().getSerializerConfig()),</span><br><span class="line">                        right.getType()</span><br><span class="line">                                .createSerializer(</span><br><span class="line">                                        right.getExecutionConfig().getSerializerConfig()),</span><br><span class="line">                        cleanedUdf);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left.connect(right)</span><br><span class="line">                .keyBy(keySelector1, keySelector2)</span><br><span class="line">                .transform(<span class="string">&quot;Interval Join [Async]&quot;</span>, outputType, operator);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> IntervalJoinOperator&lt;KEY, IN1, IN2, OUT&gt; operator =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">IntervalJoinOperator</span>&lt;&gt;(</span><br><span class="line">                        lowerBound,</span><br><span class="line">                        upperBound,</span><br><span class="line">                        lowerBoundInclusive,</span><br><span class="line">                        upperBoundInclusive,</span><br><span class="line">                        leftLateDataOutputTag,</span><br><span class="line">                        rightLateDataOutputTag,</span><br><span class="line">                        left.getType()</span><br><span class="line">                                .createSerializer(</span><br><span class="line">                                        left.getExecutionConfig().getSerializerConfig()),</span><br><span class="line">                        right.getType()</span><br><span class="line">                                .createSerializer(</span><br><span class="line">                                        right.getExecutionConfig().getSerializerConfig()),</span><br><span class="line">                        cleanedUdf);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left.connect(right)</span><br><span class="line">                .keyBy(keySelector1, keySelector2)</span><br><span class="line">                .transform(<span class="string">&quot;Interval Join&quot;</span>, outputType, operator);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Interval join 是基于 ConnectedStream 实现的，ConnectedStream 提供了更加通用的双流操作，它将两个流组合成一个 TwoInputTransformation，然后加入执行图中。</p><p>具体的 Operator 是 IntervalJoinOperator 或 AsyncIntervalJoinOperator，它们都是 TwoInputStreamOperator 的实现类，提供 <code>processElement1</code>  和 <code>processElement2</code> 两个方法分别处理两个输入源的数据，最终都调用的是 processElement。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;THIS, OTHER&gt; <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> StreamRecord&lt;THIS&gt; record,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;THIS&gt;&gt;&gt; ourBuffer,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;OTHER&gt;&gt;&gt; otherBuffer,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> <span class="type">long</span> relativeLowerBound,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> <span class="type">long</span> relativeUpperBound,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> <span class="type">boolean</span> isLeft)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">THIS</span> <span class="variable">ourValue</span> <span class="operator">=</span> record.getValue();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">long</span> <span class="variable">ourTimestamp</span> <span class="operator">=</span> record.getTimestamp();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ourTimestamp == Long.MIN_VALUE) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">FlinkException</span>(</span><br><span class="line">                <span class="string">&quot;Long.MIN_VALUE timestamp: Elements used in &quot;</span></span><br><span class="line">                        + <span class="string">&quot;interval stream joins need to have timestamps meaningful timestamps.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isLate(ourTimestamp)) &#123;</span><br><span class="line">        sideOutput(ourValue, ourTimestamp, isLeft);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    addToBuffer(ourBuffer, ourValue, ourTimestamp);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Long, List&lt;BufferEntry&lt;OTHER&gt;&gt;&gt; bucket : otherBuffer.entries()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> bucket.getKey();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (timestamp &lt; ourTimestamp + relativeLowerBound</span><br><span class="line">                || timestamp &gt; ourTimestamp + relativeUpperBound) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (BufferEntry&lt;OTHER&gt; entry : bucket.getValue()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isLeft) &#123;</span><br><span class="line">                collect((T1) ourValue, (T2) entry.element, ourTimestamp, timestamp);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                collect((T1) entry.element, (T2) ourValue, timestamp, ourTimestamp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">long</span> <span class="variable">cleanupTime</span> <span class="operator">=</span></span><br><span class="line">            (relativeUpperBound &gt; <span class="number">0L</span>) ? ourTimestamp + relativeUpperBound : ourTimestamp;</span><br><span class="line">    <span class="keyword">if</span> (isLeft) &#123;</span><br><span class="line">        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_LEFT, cleanupTime);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_RIGHT, cleanupTime);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 IntervalJoinOperator 中维护了两个 MapState，每个消息进来的时候，都会加入到 MapState 中，key 是 timestamp，value 是一个元素的列表。然后遍历另一个 MapState，得到符合条件的数据。最后是为每条数据注册一个定时器，当时间超过有效范围后，会从 MapState 中清除这个时间戳的数据。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们梳理了 Flink 的三种双流操作的源码，我们了解到 Window Join 底层是通过 CoGroup 实现的。CoGroup 本身是将两个流合并成 WindowedStream 并依赖于 WindowState 进行数据 join。最后 Interval Join 是通过 ConnectedStreams 实现的，内部的 IntervalJoinOperator 会维护两个 MapState，通过 MapState 进行数据关联。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天来梳理一下 Flink 双流操作相关的源码。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：窗口</title>
    <link href="https://jackeyzhe.github.io/2025/12/22/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E7%AA%97%E5%8F%A3/"/>
    <id>https://jackeyzhe.github.io/2025/12/22/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E7%AA%97%E5%8F%A3/</id>
    <published>2025-12-22T02:49:15.000Z</published>
    <updated>2025-12-23T13:04:01.933Z</updated>
    
    <content type="html"><![CDATA[<p>前文我们梳理了 Watermark 相关的源码，Watermark 的作用就是用来触发窗口，本文我们就一起看一下窗口相关的源码。<span id="more"></span></p><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>在<a href="https://jackeyzhe.github.io/2025/07/19/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%AA%97%E5%8F%A3/">Flink学习笔记：窗口</a>一文中，我们介绍了窗口的分类以及基本的用法。按照处理数据流的类型划分，Flink 可以分为 Keyed Window 和 Non-Keyed Window，它们的用法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  仅 keyed 窗口需要</span><br><span class="line">       .window(...)              &lt;-  必填项：<span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  可选项：<span class="string">&quot;trigger&quot;</span> (省略则使用默认 trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  可选项：<span class="string">&quot;evictor&quot;</span> (省略则不使用 evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  可选项：<span class="string">&quot;lateness&quot;</span> (省略则为 <span class="number">0</span>)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  可选项：<span class="string">&quot;output tag&quot;</span> (省略则不对迟到数据使用 side output)</span><br><span class="line">       .reduce/aggregate/apply()      &lt;-  必填项：<span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  可选项：<span class="string">&quot;output tag&quot;</span></span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  必填项：<span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  可选项：<span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  可选项：<span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  可选项：<span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  可选项：<span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/apply()      &lt;-  必填项：<span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  可选项：<span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure><p>下面我们根据用法，分别来看两种窗口的源码。</p><h3 id="Keyed-Window"><a href="#Keyed-Window" class="headerlink" title="Keyed Window"></a>Keyed Window</h3><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766394316/Blog/flink/16/KeyedWindow.png" alt="KeyedWindow"></p><h4 id="WindowAssigner"><a href="#WindowAssigner" class="headerlink" title="WindowAssigner"></a>WindowAssigner</h4><p>在示例代码中，数据流类型流转过程如图。我们聚焦于 WindowedStream，它是在调用 <code>KeyedStream.window</code> 方法之后生成的。window 方法需要传入一个 WindowAssigner，用来确定一条消息属于哪几个窗口，各个类型的窗口都有不同的实现。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766472754/Blog/flink/16/windowAssigner.png" alt="windowAssigner"></p><p>我们以 TumblingEventTimeWindows 为例，看一下它具体的分配逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title function_">assignWindows</span><span class="params">(</span></span><br><span class="line"><span class="params">        Object element, <span class="type">long</span> timestamp, WindowAssignerContext context)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (timestamp &gt; Long.MIN_VALUE) &#123;</span><br><span class="line">        <span class="keyword">if</span> (staggerOffset == <span class="literal">null</span>) &#123;</span><br><span class="line">            staggerOffset =</span><br><span class="line">                    windowStagger.getStaggerOffset(context.getCurrentProcessingTime(), size);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Long.MIN_VALUE is currently assigned when no timestamp is present</span></span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span></span><br><span class="line">                TimeWindow.getWindowStartWithOffset(</span><br><span class="line">                        timestamp, (globalOffset + staggerOffset) % size, size);</span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(<span class="keyword">new</span> <span class="title class_">TimeWindow</span>(start, start + size));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(</span><br><span class="line">                <span class="string">&quot;Record has Long.MIN_VALUE timestamp (= no timestamp marker). &quot;</span></span><br><span class="line">                        + <span class="string">&quot;Did you forget to call &#x27;DataStream.assignTimestampsAndWatermarks(...)&#x27;?&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里就是根据消息的 timestamp 来确定窗口的开始和结束时间，然后返回消息所属的窗口。这里还有个 windowStagger 变量，它是窗口触发是否错峰的配置，如果你的任务有成千上万个子任务，同时触发窗口计算带来的瞬时流量可能会对服务器本身和下游造成稳定性的影响，这时就可以通过修改 WindowStagger 配置将流量打散。</p><p>将我们自己定义好的 WindowAssigner 传入 window 方法后，会创建一个 WindowOperatorBuilder，它负责创建一个 WindowOperator 对象，WindowOperator 来执行窗口具体的计算逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">WindowedStream</span><span class="params">(KeyedStream&lt;T, K&gt; input, WindowAssigner&lt;? <span class="built_in">super</span> T, W&gt; windowAssigner)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.input = input;</span><br><span class="line">    <span class="built_in">this</span>.isEnableAsyncState = input.isEnableAsyncState();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.builder =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">WindowOperatorBuilder</span>&lt;&gt;(</span><br><span class="line">                    windowAssigner,</span><br><span class="line">                    windowAssigner.getDefaultTrigger(),</span><br><span class="line">                    input.getExecutionConfig(),</span><br><span class="line">                    input.getType(),</span><br><span class="line">                    input.getKeySelector(),</span><br><span class="line">                    input.getKeyType());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Trigger"><a href="#Trigger" class="headerlink" title="Trigger"></a>Trigger</h4><p>有了 WindowOperatorBuilder 之后，我们可以对它进行一些设置，如 trigger、evictor 等，trigger 中提供了一些回调函数，这些回调函数的返回结果 TriggerResult 决定了是否触发窗口计算。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Trigger</span>&lt;T, W <span class="keyword">extends</span> <span class="title class_">Window</span>&gt; <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> -<span class="number">4104633972991191369L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title function_">onElement</span><span class="params">(T element, <span class="type">long</span> timestamp, W window, TriggerContext ctx)</span></span><br><span class="line">            <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title function_">onProcessingTime</span><span class="params">(<span class="type">long</span> time, W window, TriggerContext ctx)</span></span><br><span class="line">            <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title function_">onEventTime</span><span class="params">(<span class="type">long</span> time, W window, TriggerContext ctx)</span></span><br><span class="line">            <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">canMerge</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMerge</span><span class="params">(W window, OnMergeContext ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UnsupportedOperationException</span>(<span class="string">&quot;This trigger does not support merging.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">clear</span><span class="params">(W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>回调函数有三个，分别是 onElement、onProcessingTime、onEventTime，onElement 是在处理每条消息的时候触发，onProcessingTime 和 onEventTime 都是与定时器配合触发，上一篇文章我们提到过，在处理 Watermark 的时候会注册定时器，触发时就会回调这两个方法。</p><p>此外，Trigger 类中还有三个方法，我们简单介绍一下。canMerge 是用来判断窗口是否可以被合并，onMerge 则是在合并窗口时的回调方法。clear 方法用于清除窗口的状态数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">TriggerResult</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** No action is taken on the window. */</span></span><br><span class="line">    CONTINUE(<span class="literal">false</span>, <span class="literal">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** &#123;<span class="doctag">@code</span> FIRE_AND_PURGE&#125; evaluates the window function and emits the window result. */</span></span><br><span class="line">    FIRE_AND_PURGE(<span class="literal">true</span>, <span class="literal">true</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * On &#123;<span class="doctag">@code</span> FIRE&#125;, the window is evaluated and results are emitted. The window is not purged,</span></span><br><span class="line"><span class="comment">     * though, all elements are retained.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    FIRE(<span class="literal">true</span>, <span class="literal">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * All elements in the window are cleared and the window is discarded, without evaluating the</span></span><br><span class="line"><span class="comment">     * window function or emitting any elements.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    PURGE(<span class="literal">false</span>, <span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>说回 TriggerResult，它有四种枚举：</p><ul><li><p>CONTINUE：什么也不做</p></li><li><p>FIRE_AND_PURGE：触发窗口计算并清除窗口中的元素</p></li><li><p>FIRE：只触发窗口计算</p></li><li><p>PURGE：清除窗口中的元素，不触发计算</p></li></ul><h4 id="Evictor"><a href="#Evictor" class="headerlink" title="Evictor"></a>Evictor</h4><p>Evictor 是用来自定义删除窗口中元素的的接口，如果设置了 evictor，WindowOperatorBuilder 就会创建 EvictingWindowOperator。在执行窗口计算逻辑前后，都会调用 evictBefore 和 evictAfter。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">emitWindowContents</span><span class="params">(</span></span><br><span class="line"><span class="params">        W window, Iterable&lt;StreamRecord&lt;IN&gt;&gt; contents, ListState&lt;StreamRecord&lt;IN&gt;&gt; windowState)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    ...</span><br><span class="line">    evictorContext.evictBefore(recordsWithTimestamp, Iterables.size(recordsWithTimestamp));</span><br><span class="line"></span><br><span class="line">    FluentIterable&lt;IN&gt; projectedContents =</span><br><span class="line">            recordsWithTimestamp.transform(</span><br><span class="line">                    <span class="keyword">new</span> <span class="title class_">Function</span>&lt;TimestampedValue&lt;IN&gt;, IN&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="keyword">public</span> IN <span class="title function_">apply</span><span class="params">(TimestampedValue&lt;IN&gt; input)</span> &#123;</span><br><span class="line">                            <span class="keyword">return</span> input.getValue();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line">    processContext.window = triggerContext.window;</span><br><span class="line">    userFunction.process(</span><br><span class="line">            triggerContext.key,</span><br><span class="line">            triggerContext.window,</span><br><span class="line">            processContext,</span><br><span class="line">            projectedContents,</span><br><span class="line">            timestampedCollector);</span><br><span class="line">    evictorContext.evictAfter(recordsWithTimestamp, Iterables.size(recordsWithTimestamp));</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="allowedLateness-amp-sideOutputLateData"><a href="#allowedLateness-amp-sideOutputLateData" class="headerlink" title="allowedLateness &amp; sideOutputLateData"></a>allowedLateness &amp; sideOutputLateData</h4><p>allowedLateness 和 sideOutputLateData 都是针对迟到数据的，allowedLateness 是用来指定允许的最大迟到时长，sideOutputLateData 则是将迟到数据输出到指定 outputTag。</p><p>判断是否迟到的方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="type">boolean</span> <span class="title function_">isElementLate</span><span class="params">(StreamRecord&lt;IN&gt; element)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> (windowAssigner.isEventTime())</span><br><span class="line">            &amp;&amp; (element.getTimestamp() + allowedLateness</span><br><span class="line">                    &lt;= internalTimerService.currentWatermark());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果是迟到数据，则进行如下处理：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (isSkippedElement &amp;&amp; isElementLate(element)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (lateDataOutputTag != <span class="literal">null</span>) &#123;</span><br><span class="line">        sideOutput(element);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.numLateRecordsDropped.inc();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="WindowOperator"><a href="#WindowOperator" class="headerlink" title="WindowOperator"></a>WindowOperator</h4><p>设置好 WindowOperatorBuilder 之后，接着就可以调用 process/aggregate/reduce 等方法进行数据计算。</p><p>我们以 process 方法为例，来看下具体的处理逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title function_">process</span><span class="params">(</span></span><br><span class="line"><span class="params">        ProcessWindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType)</span> &#123;</span><br><span class="line">    function = input.getExecutionEnvironment().clean(function);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">opName</span> <span class="operator">=</span> builder.generateOperatorName();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">opDesc</span> <span class="operator">=</span> builder.generateOperatorDescription(function, <span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    OneInputStreamOperator&lt;T, R&gt; operator =</span><br><span class="line">            isEnableAsyncState ? builder.asyncProcess(function) : builder.process(function);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input.transform(opName, resultType, operator).setDescription(opDesc);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>WindowedStream.process</code> 方法中，就是调用 WindowOperatorBuilder 的 process 方法（如果是异步则调用异步方法）生成 WindowOperator，再将 WindowOperator 加入到执行图中。</p><p>下面我们来看 WindowOperator 中几个重要的方法。</p><h5 id="open"><a href="#open" class="headerlink" title="open"></a>open</h5><p>首先是 open 方法，它主要负责进行初始化，包括创建 timerService，创建 windowState 等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="built_in">super</span>.open();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.numLateRecordsDropped = metrics.counter(LATE_ELEMENTS_DROPPED_METRIC_NAME);</span><br><span class="line">    timestampedCollector = <span class="keyword">new</span> <span class="title class_">TimestampedCollector</span>&lt;&gt;(output);</span><br><span class="line"></span><br><span class="line">    internalTimerService = getInternalTimerService(<span class="string">&quot;window-timers&quot;</span>, windowSerializer, <span class="built_in">this</span>);</span><br><span class="line"></span><br><span class="line">    triggerContext = <span class="keyword">new</span> <span class="title class_">Context</span>(<span class="literal">null</span>, <span class="literal">null</span>);</span><br><span class="line">    processContext = <span class="keyword">new</span> <span class="title class_">WindowContext</span>(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    windowAssignerContext =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">WindowAssigner</span>.WindowAssignerContext() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getCurrentProcessingTime</span><span class="params">()</span> &#123;</span><br><span class="line">                    <span class="keyword">return</span> internalTimerService.currentProcessingTime();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create (or restore) the state that hold the actual window contents</span></span><br><span class="line">    <span class="comment">// NOTE - the state may be null in the case of the overriding evicting window operator</span></span><br><span class="line">    <span class="keyword">if</span> (windowStateDescriptor != <span class="literal">null</span>) &#123;</span><br><span class="line">        windowState =</span><br><span class="line">                (InternalAppendingState&lt;K, W, IN, ACC, ACC&gt;)</span><br><span class="line">                        getOrCreateKeyedState(windowSerializer, windowStateDescriptor);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the typed and helper states for merging windows</span></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner) &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="processElement"><a href="#processElement" class="headerlink" title="processElement"></a>processElement</h5><p>processElement 是负责处理进入窗口的数据，这里首先调用 <code>WindowAssigner.assignWindows</code> 方法确认元素属于哪些窗口。然后遍历窗口进行处理，包括向 windowState 中添加元素，调用 trigger 的 onElement 方法获取 TriggerResult。如果触发了窗口计算，调用 emitWindowContents 执行计算逻辑。最后是处理迟到数据，我们前面提到过。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(StreamRecord&lt;IN&gt; element)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="keyword">final</span> Collection&lt;W&gt; elementWindows =</span><br><span class="line">            windowAssigner.assignWindows(</span><br><span class="line">                    element.getValue(), element.getTimestamp(), windowAssignerContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if element is handled by none of assigned elementWindows</span></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">isSkippedElement</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">K</span> <span class="variable">key</span> <span class="operator">=</span> <span class="built_in">this</span>.&lt;K&gt;getKeyedStateBackend().getCurrentKey();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner) &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (W window : elementWindows) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// drop if the window is already late</span></span><br><span class="line">            <span class="keyword">if</span> (isWindowLate(window)) &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            isSkippedElement = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">            windowState.setCurrentNamespace(window);</span><br><span class="line">            windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">            triggerContext.key = key;</span><br><span class="line">            triggerContext.window = window;</span><br><span class="line"></span><br><span class="line">            <span class="type">TriggerResult</span> <span class="variable">triggerResult</span> <span class="operator">=</span> triggerContext.onElement(element);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (triggerResult.isFire()) &#123;</span><br><span class="line">                <span class="type">ACC</span> <span class="variable">contents</span> <span class="operator">=</span> windowState.get();</span><br><span class="line">                <span class="keyword">if</span> (contents != <span class="literal">null</span>) &#123;</span><br><span class="line">                    emitWindowContents(window, contents);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (triggerResult.isPurge()) &#123;</span><br><span class="line">                windowState.clear();</span><br><span class="line">            &#125;</span><br><span class="line">            registerCleanupTimer(window);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// side output input event if</span></span><br><span class="line">    <span class="comment">// element not handled by any window</span></span><br><span class="line">    <span class="comment">// late arriving tag has been set</span></span><br><span class="line">    <span class="comment">// windowAssigner is event time and current timestamp + allowed lateness no less than</span></span><br><span class="line">    <span class="comment">// element timestamp</span></span><br><span class="line">    <span class="keyword">if</span> (isSkippedElement &amp;&amp; isElementLate(element)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (lateDataOutputTag != <span class="literal">null</span>) &#123;</span><br><span class="line">            sideOutput(element);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">this</span>.numLateRecordsDropped.inc();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="onEventTime"><a href="#onEventTime" class="headerlink" title="onEventTime"></a>onEventTime</h5><p>onEventTime 方法是 eventTime 触发窗口计算时调用的。主要逻辑就是获取 TriggerResult，然后触发计算逻辑，以及对 windowState 的处理。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onEventTime</span><span class="params">(InternalTimer&lt;K, W&gt; timer)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    triggerContext.key = timer.getKey();</span><br><span class="line">    triggerContext.window = timer.getNamespace();</span><br><span class="line"></span><br><span class="line">    MergingWindowSet&lt;W&gt; mergingWindows;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner) &#123;</span><br><span class="line">        mergingWindows = getMergingWindowSet();</span><br><span class="line">        <span class="type">W</span> <span class="variable">stateWindow</span> <span class="operator">=</span> mergingWindows.getStateWindow(triggerContext.window);</span><br><span class="line">        <span class="keyword">if</span> (stateWindow == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="comment">// Timer firing for non-existent window, this can only happen if a</span></span><br><span class="line">            <span class="comment">// trigger did not clean up timers. We have already cleared the merging</span></span><br><span class="line">            <span class="comment">// window and therefore the Trigger state, however, so nothing to do.</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            windowState.setCurrentNamespace(stateWindow);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        windowState.setCurrentNamespace(triggerContext.window);</span><br><span class="line">        mergingWindows = <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">TriggerResult</span> <span class="variable">triggerResult</span> <span class="operator">=</span> triggerContext.onEventTime(timer.getTimestamp());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (triggerResult.isFire()) &#123;</span><br><span class="line">        <span class="type">ACC</span> <span class="variable">contents</span> <span class="operator">=</span> windowState.get();</span><br><span class="line">        <span class="keyword">if</span> (contents != <span class="literal">null</span>) &#123;</span><br><span class="line">            emitWindowContents(triggerContext.window, contents);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (triggerResult.isPurge()) &#123;</span><br><span class="line">        windowState.clear();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner.isEventTime()</span><br><span class="line">            &amp;&amp; isCleanupTime(triggerContext.window, timer.getTimestamp())) &#123;</span><br><span class="line">        clearAllState(triggerContext.window, windowState, mergingWindows);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mergingWindows != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// need to make sure to update the merging state in state</span></span><br><span class="line">        mergingWindows.persist();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="onProcessingTime"><a href="#onProcessingTime" class="headerlink" title="onProcessingTime"></a>onProcessingTime</h5><p>onProcessingTime 和 onEventTime 逻辑基本一致，只是触发条件不同，这里就不再赘述了。</p><p>至此，Keyed Window 从设置到使用的源码我们就梳理完成了，下面再来看另外一种窗口 Non-Keyed Window。</p><h3 id="Non-Keyed-Window"><a href="#Non-Keyed-Window" class="headerlink" title="Non-Keyed Window"></a>Non-Keyed Window</h3><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766484574/Blog/flink/16/Non-KeyedWindow.png" alt="AllWindow"></p><p>我们调用 windowAll 得到 AllWindowedStream，在构造函数中，会给对 input 调用 keyBy 方法，传入 NullByteKeySelector， NullByteKeySelector 对每个 key 都返回0，因此所有的 key 都会被分配到同一个节点。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">NullByteKeySelector</span>&lt;T&gt; <span class="keyword">implements</span> <span class="title class_">KeySelector</span>&lt;T, Byte&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">614256539098549020L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Byte <span class="title function_">getKey</span><span class="params">(T value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Non-Keyed Window 后续的逻辑都和 Keyed Window 比较类似。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们梳理了窗口相关的源码，几个重点概念包括 WindowAssginer、WindowOperator、Trigger、Evictor。其中 WindowAssigner 是用来确定一条消息属于哪些窗口，WindowOperator 则是窗口计算逻辑的具体执行层。Trigger 和 Evictor 分别用于触发窗口和清理窗口中数据。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前文我们梳理了 Watermark 相关的源码，Watermark 的作用就是用来触发窗口，本文我们就一起看一下窗口相关的源码。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：Watermark机制</title>
    <link href="https://jackeyzhe.github.io/2025/12/17/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AWatermark%E6%9C%BA%E5%88%B6/"/>
    <id>https://jackeyzhe.github.io/2025/12/17/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AWatermark%E6%9C%BA%E5%88%B6/</id>
    <published>2025-12-17T07:15:22.000Z</published>
    <updated>2025-12-20T13:52:07.904Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们已经梳理了 Flink 状态和 Checkpoint 相关的源码。从本文开始，我们再来关注另外几个核心概念，即时间、Watermark 和窗口。<span id="more"></span></p><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>在 Flink 中 Watermark 是用来解决数据乱序问题的，它也是窗口关闭的触发条件。对于 Watermark 的概念和用法还不熟悉的同学可以先阅读<a href="https://jackeyzhe.github.io/2025/06/30/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%97%B6%E9%97%B4%E4%B8%8EWatermark/">Flink学习笔记：时间与Watermark</a>一文。下面我们进入正题，开始梳理 Watermark 相关的源码。</p><h3 id="Watermark-定义"><a href="#Watermark-定义" class="headerlink" title="Watermark 定义"></a>Watermark 定义</h3><p>Watermark 的定义非常简单，它继承了 <code>StreamElement</code> 类，内部只有一个 timestamp 变量。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Watermark</span> <span class="keyword">extends</span> <span class="title class_">StreamElement</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The watermark that signifies end-of-event-time. */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Watermark</span> <span class="variable">MAX_WATERMARK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watermark</span>(Long.MAX_VALUE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The watermark that signifies is used before any actual watermark has been generated. */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Watermark</span> <span class="variable">UNINITIALIZED</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watermark</span>(Long.MIN_VALUE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The timestamp of the watermark in milliseconds. */</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">long</span> timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Creates a new watermark with the given timestamp in milliseconds. */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Watermark</span><span class="params">(<span class="type">long</span> timestamp)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.timestamp = timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Returns the timestamp associated with this &#123;<span class="doctag">@link</span> Watermark&#125; in milliseconds. */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getTimestamp</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span> == o</span><br><span class="line">                || o != <span class="literal">null</span></span><br><span class="line">                        &amp;&amp; o.getClass() == <span class="built_in">this</span>.getClass()</span><br><span class="line">                        &amp;&amp; ((Watermark) o).timestamp == timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> (<span class="type">int</span>) (timestamp ^ (timestamp &gt;&gt;&gt; <span class="number">32</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Watermark @ &quot;</span> + timestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Watermark-处理过程"><a href="#Watermark-处理过程" class="headerlink" title="Watermark 处理过程"></a>Watermark 处理过程</h3><p>我们先来回顾一下 Watermark 的生成方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Event&gt; withTimestampsAndWatermarks = source</span><br><span class="line">        .assignTimestampsAndWatermarks(</span><br><span class="line">                WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">20</span>))</span><br><span class="line">        );</span><br></pre></td></tr></table></figure><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>在定义 Watermark 的时候，我们调用 assignTimestampsAndWatermarks 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title function_">assignTimestampsAndWatermarks</span><span class="params">(</span></span><br><span class="line"><span class="params">        WatermarkStrategy&lt;T&gt; watermarkStrategy)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> WatermarkStrategy&lt;T&gt; cleanedStrategy = clean(watermarkStrategy);</span><br><span class="line">    <span class="comment">// match parallelism to input, to have a 1:1 source -&gt; timestamps/watermarks relationship</span></span><br><span class="line">    <span class="comment">// and chain</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">inputParallelism</span> <span class="operator">=</span> getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> TimestampsAndWatermarksTransformation&lt;T&gt; transformation =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">TimestampsAndWatermarksTransformation</span>&lt;&gt;(</span><br><span class="line">                    <span class="string">&quot;Timestamps/Watermarks&quot;</span>,</span><br><span class="line">                    inputParallelism,</span><br><span class="line">                    getTransformation(),</span><br><span class="line">                    cleanedStrategy,</span><br><span class="line">                    <span class="literal">false</span>);</span><br><span class="line">    getExecutionEnvironment().addOperator(transformation);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SingleOutputStreamOperator</span>&lt;&gt;(getExecutionEnvironment(), transformation);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个方法接收了一个 WatermarkStrategy 参数，把它封装到 TimestampsAndWatermarksTransformation 中之后，就添加到 transformations 列表中了。在生成 StreamGraph 的过程中，会调用每个 transformation 的 transform 方法。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766047572/Blog/flink/15/watermarkTranslate.png" alt="transform"></p><p>通过这个调用链路，创建出了 TimestampsAndWatermarksOperatorFactory，在初始化 StreamTask 时，会调用 <code>TimestampsAndWatermarksOperatorFactory.createStreamOperator</code> 方法来创建 TimestampsAndWatermarksOperator，并调用它的 open 方法。</p><p>在这个 open 方法中，主要是生成 timestampAssigner 和 watermarkGenerator。timestampAssigner 是用于提取时间戳，watermarkGenerator 是用于生成 Watermark。</p><p>生成完成之后注册了一个定时器，到指定时间后会调用 onProcessingTime 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onProcessingTime</span><span class="params">(<span class="type">long</span> timestamp)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    watermarkGenerator.onPeriodicEmit(wmOutput);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">long</span> <span class="variable">now</span> <span class="operator">=</span> getProcessingTimeService().getCurrentProcessingTime();</span><br><span class="line">    getProcessingTimeService().registerTimer(now + watermarkInterval, <span class="built_in">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个方法的逻辑也很简单，先发送创建并发送 Watermark，然后再注册一个定时器。</p><h4 id="发送-Watermark"><a href="#发送-Watermark" class="headerlink" title="发送 Watermark"></a>发送 Watermark</h4><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766112568/Blog/flink/15/emitWatermark.png" alt="emitWatermark"></p><p>我们以 BoundedOutOfOrdernessWatermarks 为例，它向下游发送了一个 Watermark，时间戳为 maxTimestamp - outOfOrdernessMillis - 1（maxTimestamp 是当前最大的事件时间戳，outOfOrdernessMillis 是我们定义的周期时间毫秒值）。随后在 WatermarkEmitter.emitWatermark 方法中，更新了当前 Watermark 的值。最后 RecordWriterOutput.emitWatermark 则是向下游广播当前的 Watermark。</p><h4 id="下游处理"><a href="#下游处理" class="headerlink" title="下游处理"></a>下游处理</h4><p>下游处理方法我们从 <code>StreamOneInputProcessor.processInput</code> 入手，先来看具体的调用链路。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766133213/Blog/flink/15/processWatermark.png" alt="processWatermark"></p><p>在 inputWatermark 方法中，先是对 alignedSubpartitionStatuses 进行调整，alignedSubpartitionStatuses 这个变量主要是用来获取最小的 Watermark。最后调用了 <code>findAndOutputNewMinWatermarkAcrossAlignedSubpartitions</code> 方法。这个方法中，会获取到所有上游最小的 Watermark，如果它大于最近发送的一个 Watermark，就会向下游发送。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">emitWatermark</span><span class="params">(Watermark watermark)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    watermarkGauge.setCurrentWatermark(watermark.getTimestamp());</span><br><span class="line">    operator.processWatermark(watermark);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个发送方法中，调用了 <code>operator.processWatermark</code>，我们接着看这个处理方法。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1766136153/Blog/flink/15/advanceWatermark.png" alt="advanceWatermark"></p><p>在 tryAdvanceWatermark 方法中如果 Watermark 的时间大于 eventTimeTimersQueue 队列中头节点的时间，那么对 eventTimeTimersQueue 这个队列进行出队操作，这个操作意味着触发了窗口计算。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">tryAdvanceWatermark</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="type">long</span> time, InternalTimeServiceManager.ShouldStopAdvancingFn shouldStopAdvancingFn)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    currentWatermark = time;</span><br><span class="line">    InternalTimer&lt;K, N&gt; timer;</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">interrupted</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">while</span> ((timer = eventTimeTimersQueue.peek()) != <span class="literal">null</span></span><br><span class="line">            &amp;&amp; timer.getTimestamp() &lt;= time</span><br><span class="line">            &amp;&amp; !cancellationContext.isCancelled()</span><br><span class="line">            &amp;&amp; !interrupted) &#123;</span><br><span class="line">        keyContext.setCurrentKey(timer.getKey());</span><br><span class="line">        eventTimeTimersQueue.poll();</span><br><span class="line">        triggerTarget.onEventTime(timer);</span><br><span class="line">        taskIOMetricGroup.getNumFiredTimers().inc();</span><br><span class="line">        <span class="comment">// Check if we should stop advancing after at least one iteration to guarantee progress</span></span><br><span class="line">        <span class="comment">// and prevent a potential starvation.</span></span><br><span class="line">        interrupted = shouldStopAdvancingFn.test();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> !interrupted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之后 Watermark 就随着数据流一直到 sink 节点，在 StreamSink 中，支持用户自己实现方法向 sink 中写入 Watermark，除此之外什么也不做。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们一起梳理了 Watermark 相关的源码，从 Watermark 的定义，到 Watermark 的处理过程。处理过程分成了初始化、上游发送和下游处理三部分。在下游处理部分，关于触发窗口计算的部分我们简单带过了，后面会再详细介绍这部分。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前面我们已经梳理了 Flink 状态和 Checkpoint 相关的源码。从本文开始，我们再来关注另外几个核心概念，即时间、Watermark 和窗口。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：Checkpoint机制（下）</title>
    <link href="https://jackeyzhe.github.io/2025/12/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ACheckpoint%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <id>https://jackeyzhe.github.io/2025/12/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ACheckpoint%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%8B%EF%BC%89/</id>
    <published>2025-12-12T15:15:16.000Z</published>
    <updated>2025-12-16T12:45:58.994Z</updated>
    
    <content type="html"><![CDATA[<p>书接上回，前文我们梳理的 Checkpoint 机制的源码，但是对于如何写入状态数据并没有深入了解。今天就一起来梳理一下这部分代码。<span id="more"></span></p><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>前面我们了解到在 <code>StreamOperatorStateHandler.snapshotState</code> 方法中会创建四个 Future，用来支持不同类型的状态写入。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">snapshotInProgress.setKeyedStateRawFuture(snapshotContext.getKeyedStateStreamFuture());</span><br><span class="line">snapshotInProgress.setOperatorStateRawFuture(</span><br><span class="line">        snapshotContext.getOperatorStateStreamFuture());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (<span class="literal">null</span> != operatorStateBackend) &#123;</span><br><span class="line">    snapshotInProgress.setOperatorStateManagedFuture(</span><br><span class="line">            operatorStateBackend.snapshot(</span><br><span class="line">                    checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (useAsyncState &amp;&amp; <span class="literal">null</span> != asyncKeyedStateBackend) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isCanonicalSavepoint(checkpointOptions.getCheckpointType())) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UnsupportedOperationException</span>(<span class="string">&quot;Not supported yet.&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        snapshotInProgress.setKeyedStateManagedFuture(</span><br><span class="line">                asyncKeyedStateBackend.snapshot(</span><br><span class="line">                        checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们主要关心 ManagedState，ManagedState 都是调用 <code>Snapshotable.snapshot</code> 方法来写入数据的，下面具体看 KeyedState 和 OperatorState 的具体实现。</p><h3 id="KeyedState"><a href="#KeyedState" class="headerlink" title="KeyedState"></a>KeyedState</h3><p>KeyedState 我们以 HeapKeyedStateBackend 为例，这里先是创建了一个 <code>SnapshotStrategyRunner</code> 实例，SnapshotStrategyRunner 是一个快照策略的一个执行类，创建完成后就会调用 snapshot 方法。在这个 snapshot 方法中主要做了做了下面几件事：</p><ol><li><p>同步拷贝状态数据的引用。</p></li><li><p>创建 Checkpoint 输出流 <code>CheckpointStateOutputStream</code></p></li><li><p>完成 Checkpoint 持久化</p></li><li><p>返回元信息结果</p></li></ol><h4 id="状态数据引用拷贝"><a href="#状态数据引用拷贝" class="headerlink" title="状态数据引用拷贝"></a>状态数据引用拷贝</h4><p>在 HeapSnapshotStrategy 的 syncPrepareResources 方法中调用了 <code>HeapSnapshotResources.create</code> 方法。这里有一个比较重要的参数是 registeredKVStates，它代表我们在业务代码中注册的状态数据表。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ValueStateDescriptor</span>&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;average&quot;</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br></pre></td></tr></table></figure><p>例如我们这样注册状态数据表，那么 registeredKVStates 的 key 就是 average，value 就是状态表，它通常是一个 CopyOnWriteStateTable。具体的状态数据引用拷贝的逻辑在 <code>processSnapshotMetaInfoForAllStates</code> 方法中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">processSnapshotMetaInfoForAllStates</span><span class="params">(</span></span><br><span class="line"><span class="params">        List&lt;StateMetaInfoSnapshot&gt; metaInfoSnapshots,</span></span><br><span class="line"><span class="params">        Map&lt;StateUID, StateSnapshot&gt; cowStateStableSnapshots,</span></span><br><span class="line"><span class="params">        Map&lt;StateUID, Integer&gt; stateNamesToId,</span></span><br><span class="line"><span class="params">        Map&lt;String, ? extends StateSnapshotRestore&gt; registeredStates,</span></span><br><span class="line"><span class="params">        StateMetaInfoSnapshot.BackendStateType stateType)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;String, ? <span class="keyword">extends</span> <span class="title class_">StateSnapshotRestore</span>&gt; kvState :</span><br><span class="line">            registeredStates.entrySet()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">StateUID</span> <span class="variable">stateUid</span> <span class="operator">=</span> StateUID.of(kvState.getKey(), stateType);</span><br><span class="line">        stateNamesToId.put(stateUid, stateNamesToId.size());</span><br><span class="line">        <span class="type">StateSnapshotRestore</span> <span class="variable">state</span> <span class="operator">=</span> kvState.getValue();</span><br><span class="line">        <span class="keyword">if</span> (<span class="literal">null</span> != state) &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="type">StateSnapshot</span> <span class="variable">stateSnapshot</span> <span class="operator">=</span> state.stateSnapshot();</span><br><span class="line">            metaInfoSnapshots.add(stateSnapshot.getMetaInfoSnapshot());</span><br><span class="line">            cowStateStableSnapshots.put(stateUid, stateSnapshot);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>针对每个 State，这里都创建一个 CopyOnWriteStateTableSnapshot，然后存在 cowStateStableSnapshots 里。这里 CopyOnWriteStateTableSnapshot 就是拷贝数据的引用，因此可以同步执行。</p><h4 id="创建-CheckpointStateOutputStream"><a href="#创建-CheckpointStateOutputStream" class="headerlink" title="创建 CheckpointStateOutputStream"></a>创建 CheckpointStateOutputStream</h4><p>创建 CheckpointStateOutputStream 的方法是 <code>CheckpointStreamWithResultProvider.createSimpleStream</code>，生产环境通常使用的是 FsCheckpointStateOutputStream。FsCheckpointStateOutputStream 中的参数如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 状态数据写入缓冲数组，数据先写到内存中，然后 flush 到磁盘</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">byte</span>[] writeBuffer;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 缓冲数组当前写入位置</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">int</span> pos;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 文件输出流</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> FSDataOutputStream outStream;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 内存中状态大小阈值，超过阈值会 flush 到磁盘，默认20KB，最大1MB</span></span><br><span class="line"><span class="comment">// 目的是为了减少小文件数量</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> localStateThreshold;</span><br><span class="line"></span><br><span class="line"><span class="comment">// checkpoint 基础路径</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Path basePath;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink 自己封装的文件系统</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> FileSystem fs;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 状态数据完整路径</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> Path statePath;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 相对路径</span></span><br><span class="line"><span class="keyword">private</span> String relativeStatePath;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否已关闭</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">boolean</span> closed;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否允许使用相对路径</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">boolean</span> allowRelativePaths;</span><br></pre></td></tr></table></figure><h4 id="Checkpoint-持久化"><a href="#Checkpoint-持久化" class="headerlink" title="Checkpoint 持久化"></a>Checkpoint 持久化</h4><p>创建完 CheckpointStateOutputStream 之后，会调用 <code>serializationProxy.write(outView)</code> 写入状态的元数据。元数据包括状态的名称、类型、序列化器等一些配置。</p><p>元数据写完之后，就开始分组写入状态数据。在写入时，先写 keyGroupId，然后再写当前分组的状态数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">keyGroupPos</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        keyGroupPos &lt; keyGroupRange.getNumberOfKeyGroups();</span><br><span class="line">        ++keyGroupPos) &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">keyGroupId</span> <span class="operator">=</span> keyGroupRange.getKeyGroupId(keyGroupPos);</span><br><span class="line">    keyGroupRangeOffsets[keyGroupPos] = localStream.getPos();</span><br><span class="line">    <span class="comment">// 写 keyGroupId</span></span><br><span class="line">    outView.writeInt(keyGroupId);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;StateUID, StateSnapshot&gt; stateSnapshot :</span><br><span class="line">            cowStateStableSnapshots.entrySet()) &#123;</span><br><span class="line">        StateSnapshot.<span class="type">StateKeyGroupWriter</span> <span class="variable">partitionedSnapshot</span> <span class="operator">=</span></span><br><span class="line">                stateSnapshot.getValue().getKeyGroupWriter();</span><br><span class="line">        <span class="keyword">try</span> (<span class="type">OutputStream</span> <span class="variable">kgCompressionOut</span> <span class="operator">=</span></span><br><span class="line">                keyGroupCompressionDecorator.decorateWithCompression(localStream)) &#123;</span><br><span class="line">            <span class="type">DataOutputViewStreamWrapper</span> <span class="variable">kgCompressionView</span> <span class="operator">=</span></span><br><span class="line">                    <span class="keyword">new</span> <span class="title class_">DataOutputViewStreamWrapper</span>(kgCompressionOut);</span><br><span class="line">            kgCompressionView.writeShort(stateNamesToId.get(stateSnapshot.getKey()));</span><br><span class="line">            <span class="comment">// 写状态数据</span></span><br><span class="line">            partitionedSnapshot.writeStateInKeyGroup(kgCompressionView, keyGroupId);</span><br><span class="line">        &#125; <span class="comment">// this will just close the outer compression stream</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>状态数据写入的调用链路如下</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765870202/Blog/flink/14/writeState.png" alt="writeState"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">writeState</span><span class="params">(</span></span><br><span class="line"><span class="params">        TypeSerializer&lt;K&gt; keySerializer,</span></span><br><span class="line"><span class="params">        TypeSerializer&lt;N&gt; namespaceSerializer,</span></span><br><span class="line"><span class="params">        TypeSerializer&lt;S&gt; stateSerializer,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nonnull</span> DataOutputView dov,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nullable</span> StateSnapshotTransformer&lt;S&gt; stateSnapshotTransformer)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    SnapshotIterator&lt;K, N, S&gt; snapshotIterator =</span><br><span class="line">            getIterator(</span><br><span class="line">                    keySerializer,</span><br><span class="line">                    namespaceSerializer,</span><br><span class="line">                    stateSerializer,</span><br><span class="line">                    stateSnapshotTransformer);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> snapshotIterator.size();</span><br><span class="line">    dov.writeInt(size);</span><br><span class="line">    <span class="keyword">while</span> (snapshotIterator.hasNext()) &#123;</span><br><span class="line">        StateEntry&lt;K, N, S&gt; stateEntry = snapshotIterator.next();</span><br><span class="line">        namespaceSerializer.serialize(stateEntry.getNamespace(), dov);</span><br><span class="line">        keySerializer.serialize(stateEntry.getKey(), dov);</span><br><span class="line">        stateSerializer.serialize(stateEntry.getState(), dov);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="返回结果"><a href="#返回结果" class="headerlink" title="返回结果"></a>返回结果</h4><p>最后一步就是封装并返回元信息，这里收集的信息包括了每个 keyGroup 的状态数据在状态文件中的存储位置，状态数据存储的文件路径、文件大小等。</p><h3 id="OperatorState"><a href="#OperatorState" class="headerlink" title="OperatorState"></a>OperatorState</h3><p>OperatorState 的处理逻辑比 KeyedState 更简单一些，流程上都是先做状态数据的引用快照，然后写入状态数据和返回结果。在写入数据时，没有了分组写入的逻辑。直接处理 operatorState 和 broadcastState。这里就只贴一下调用流程，不做过多赘述了。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765871416/Blog/flink/14/operatorState.png" alt="operatorState"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们重点梳理了 KeyedState 数据写入的代码。其主要步骤包括：同步拷贝状态数据的引用，创建 Checkpoint 输出流 <code>CheckpointStateOutputStream</code> 并完成 Checkpoint 持久化，最后返回元信息结果。OperatorState 的处理过程和 KeyedState 的过程类似，只是少了分组的逻辑。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;书接上回，前文我们梳理的 Checkpoint 机制的源码，但是对于如何写入状态数据并没有深入了解。今天就一起来梳理一下这部分代码。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：Checkpoint机制（上）</title>
    <link href="https://jackeyzhe.github.io/2025/12/09/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ACheckpoint%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <id>https://jackeyzhe.github.io/2025/12/09/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9ACheckpoint%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%8A%EF%BC%89/</id>
    <published>2025-12-09T07:14:59.000Z</published>
    <updated>2025-12-12T14:23:46.079Z</updated>
    
    <content type="html"><![CDATA[<p>前文我们梳理了 Flink 状态管理相关的源码，我们知道，状态是要与 Checkpoint 配合使用的。因此，本文我们就一起来看一下 Checkpoint 相关的源码。<span id="more"></span></p><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>在<a href="https://jackeyzhe.github.io/2025/08/17/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%A6%82%E4%BD%95%E5%81%9A%E5%AE%B9%E9%94%99/">Flink学习笔记：如何做容错</a>一文中，我们介绍了 Flink 的 Checkpoint 机制。Checkpoint 分为 EXACTLY_ONCE 和 AT_LEAST_ONCE 两种模式。</p><p>我们一起回顾一下一次完整的 Checkpoint 具体流程：Checkpoint 是由 CheckpointCoordinator 触发，Source 节点收到触发请求后，会将 State 进行持久化，同时向下游发送 Barrier 消息，下游节点收到 Barrier 消息后，也同样对 State 进行持久化和发送 Barrier 消息。当所有节点都完成持久化过程后 CheckpointCoordinator 会将一些元数据进行持久化。</p><p>带着这些背景知识，我们再来梳理一下 Checkpoint 相关的代码。</p><h3 id="JobManager-端触发流程"><a href="#JobManager-端触发流程" class="headerlink" title="JobManager 端触发流程"></a>JobManager 端触发流程</h3><p>JobManager 在调用 <code>DefaultExecutionGraphBuilder.buildGraph</code> 生成 ExecutionGraph 之后，会调用 <code>executionGraph.enableCheckpointing</code> 方法来设置 Checkpoint 相关的配置，这个方法中创建了 CheckpointCoordinator 并注册了 CheckpointCoordinatorDeActivator 这个监听，它负责启动和停止 Checkpoint 的调度。</p><p>当作业变成 RUNNING 状态时，CheckpointCoordinator 会部署一个定时任务 ScheduledTrigger，这个定时任务就是用来周期性的触发 Checkpoint。</p><p>触发 Checkpoint 的核心逻辑在  <code>CheckpointCoordinator.startTriggeringCheckpoint</code> 这个方法中。这个方法中使用了多个 CompletableFuture 来完成整个流程的编排。具体流程见下图（图中不同颜色代表着使用不同线程池执行）。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765450882/Blog/flink/13/checkpoint.png" alt="checkpoint"></p><ul><li><p>checkpointPlanFuture：这是生成 Checkpoint 执行计划的 Future，Checkpoint Plan 中维护了三个关键的集合：tasksToTrigger、tasksToWaitFor 和 tasksToCommitTo。tasksToTrigger 是所有的 Source 节点，表示触发 Checkpoint 的节点，另外两个集合都包含了全部节点，分别表示等待进行 Checkpoint 的节点和等待提交的节点。</p></li><li><p>pendingCheckpointCompletableFuture：生成完 Checkpoint Plan 之后，会创建 pendingCheckpointCompletableFuture，这个 Future 中有两个执行任务，分别是生成自增的 CheckpointID 和 创建 PendingCheckpoint。PendingCheckpoint 中维护了等待完成的 task 列表，当所有 task 都确认完成之后，PendingCheckpoint 会变成 CompletedCheckpoint。</p></li><li><p>coordinatorCheckpointsComplete：这个 Future 也有两个任务，第一个是初始化存储路径，第二个是触发所有 OperatorCoordinator Checkpoint，并确认它们的状态。</p></li><li><p>masterStatesComplete：触发快照所有的 Master Hook，这一步主要是 CheckpointCoordinator 用来收集 JobManager 级别状态。</p></li><li><p>masterTriggerCompletionPromise：在 masterStatesComplete 和 coordinatorCheckpointsComplete 都执行完成后，会开始执行 masterTriggerCompletionPromise。masterTriggerCompletionPromise 的任务是调用 triggerCheckpointRequest 来产生 Barrier 消息。具体的触发流程见下图。</p></li></ul><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765506339/Blog/flink/13/triggerTaskCheckpoint.png" alt="triggerTask"></p><p>至此，JobManager 端的触发流程就完成了，接下来就到了 TaskManager 端了。</p><h3 id="TaskManager-端执行流程"><a href="#TaskManager-端执行流程" class="headerlink" title="TaskManager 端执行流程"></a>TaskManager 端执行流程</h3><p>进入 TaskExecutor 后，具体调用过程如下图。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765508196/Blog/flink/13/tmCheckpoint.png" alt="TaskManagerCheckpoint"></p><p>TaskManager 的核心逻辑在 <code>SubtaskCheckpointCoordinatorImpl.checkpointState</code> 方法中。这个方法中的注释也很详细，整体上分为6个步骤：</p><ol start="0"><li><p>判断是否是需要终止的 Checkpoint，如果是，则向下游发送取消 Checkpoint 的广播消息。</p></li><li><p>做一些前置的准备工作，这一步通常情况下是一个空实现。</p></li><li><p>向下游发送 Barrier 消息。</p></li><li><p>注册 Alignment timer，当 aligned 超时时，转换为 unaligned。</p></li><li><p>通知 StateWriter，当前 Subtask 对输出通道的写入已经完成，并提交状态句柄。</p></li><li><p>异步执行状态写入并完成上报。</p></li></ol><p>下面我们来关注几个重点的步骤。</p><h4 id="Barrier-消息"><a href="#Barrier-消息" class="headerlink" title="Barrier 消息"></a>Barrier 消息</h4><p>在步骤2中，首先是创建 Barrier，Barrier 消息包括三个部分</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// checkpointId</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> id;</span><br><span class="line"><span class="comment">// 时间戳</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp;</span><br><span class="line"><span class="comment">// checkpoint 相关参数，包括对齐类型、checkpoint 类型、目前地址</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> CheckpointOptions checkpointOptions;</span><br></pre></td></tr></table></figure><p>生成 Barrier 之后，会调用 <code>operatorChain.broadcastEvent</code> 进行广播消息。这里广播消息就是向下游所有的节点的所有 ResultSubpartition 发送。</p><h4 id="状态写入"><a href="#状态写入" class="headerlink" title="状态写入"></a>状态写入</h4><p><code>SubtaskCheckpointCoordinatorImpl.takeSnapshotSync</code> 方法用来构建 OperatorSnapshotFutures 中的四个 Future，每个 Future 的任务是为不同类型的 State 提供写入逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Nonnull</span> <span class="keyword">private</span> RunnableFuture&lt;SnapshotResult&lt;KeyedStateHandle&gt;&gt; keyedStateManagedFuture;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Nonnull</span> <span class="keyword">private</span> RunnableFuture&lt;SnapshotResult&lt;KeyedStateHandle&gt;&gt; keyedStateRawFuture;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Nonnull</span> <span class="keyword">private</span> RunnableFuture&lt;SnapshotResult&lt;OperatorStateHandle&gt;&gt; operatorStateManagedFuture;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Nonnull</span> <span class="keyword">private</span> RunnableFuture&lt;SnapshotResult&lt;OperatorStateHandle&gt;&gt; operatorStateRawFuture;</span><br></pre></td></tr></table></figure><p>在底层逻辑中，会为每个 Operator 设置对应的 State 的 Future。具体调用流程如下</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765524276/Blog/flink/13/snapshotState.png" alt="snapshotState"></p><p>设置好这些 Future 之后，会在 <code>finishAndReportAsync</code> 方法中创建 AsyncCheckpointRunnable 线程调用 get 来获取执行结果，拿到执行结果后会将 Checkpoint 信息上报给 CheckpointCoordinator。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765526479/Blog/flink/13/tmReport.png" alt="TaskManagerReport"></p><h3 id="JobManager-端确认流程"><a href="#JobManager-端确认流程" class="headerlink" title="JobManager 端确认流程"></a>JobManager 端确认流程</h3><p>TaskManager 通过调用 <code>checkpointCoordinatorGateway.acknowledgeCheckpoint</code> 上报 Checkpoint 信息后，流程就又回到 JobManager 了。</p><p>JobManager 的确认流程主要做了两件事：</p><ol><li><p>将 pendingCheckpoint 转换成 completedCheckpoint，在这个转换过程中，还做了清理过期 Checkpoint 和持久化元数据等操作。</p></li><li><p>向所有 commit 的 Task 发送 Checkpoint 完成的通知。收到这个通知后，大部分 Task 没有什么特殊逻辑，也有一部分 Source 或者 Sink 会做提交事务等操作。</p></li></ol><p>至此，JobManager 和 Source 端算子的一次 Checkpoint 就完成了。接下来我们再看一下非 Source 节点是如何做 Checkpoint 的。</p><h3 id="非-Source-节点处理流程"><a href="#非-Source-节点处理流程" class="headerlink" title="非 Source 节点处理流程"></a>非 Source 节点处理流程</h3><p>非 Source 节点处理 Barrier 的入口和处理业务数据的入口相同，都是 <code>StreamTask.processInput</code> 方法。我们还是先来看具体的调用流程。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765530817/Blog/flink/13/processBarrier.png" alt="processBarrier"></p><p>跟着调用链路，我们一路找到了 processBarrier 方法，这里区分了两个 barrierHandler。SingleCheckpointBarrierHandler 负责处理 EXACTLY_ONCE 语义，CheckpointBarrierTracker 负责处理 AT_LEAST_ONCE 语义。</p><h4 id="EXACTLY-ONCE"><a href="#EXACTLY-ONCE" class="headerlink" title="EXACTLY_ONCE"></a>EXACTLY_ONCE</h4><p>EXACTLY_ONCE 在处理 Barrier 的逻辑如下：</p><ol><li><p>如果只有一个 channel，就立即触发 Checkpoint。</p></li><li><p>如果有多个 channel，分为三种情况</p><p>a) 如果收到的是第一个 channel，标记开始进行 barrier 对齐，并阻塞 channel。</p><p>b) 如果不是第一个 channel，也不是最后一个 channel，只对 channel 进行阻塞。</p><p>c) 如果收到最后一个 channel，就会触发 Checkpoint，并取消所有 channel 阻塞状态。</p></li></ol><p>这里触发的逻辑与 Source 节点相同，通过调用链路可以一直找到 performCheckpoint。</p><h4 id="AT-LEAST-ONCE"><a href="#AT-LEAST-ONCE" class="headerlink" title="AT_LEAST_ONCE"></a>AT_LEAST_ONCE</h4><p>AT_LEAST_ONCE 处理 Barrier 的逻辑如下：</p><ol><li><p>如果只有一个 channel，就立即触发 Checkpoint。</p></li><li><p>如果有多个 channel，同样分为三种情况</p><p>a) 如果收到的是第一个 channel，则更新当前 checkpointID，标记开始 barrier 对齐。</p><p>b) 如果收到的不是第一个 channel，也不是最后一个 channel，就只做计数。</p><p>c) 如果收到的是最后一个 channel，就会开始触发 Checkpoint。</p></li></ol><p>这里触发逻辑也是调用 performCheckpoint，与 Source 节点逻辑相同。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们梳理了 Checkpoint 的源码逻辑。最开始由 JobManager 中的 CheckpointCoordinator 进行调度，并向 TaskManager 发送触发请求。Source 节点收到请求后会向下游发送 Barrier 消息然后写入状态数据和上报 Checkpoint 信息。CheckpointCoordinator 收集完确认消息后，会持久化元数据并通知所有 Task 完成 commit。最后还分别介绍了 EXACTLY_ONCE 和 AT_LEAST_ONCE 模式下非 Source 节点的处理逻辑。</p><p>这里埋一个 Hook，状态数据写入逻辑的细节我们没有深入了解，会在下篇进行深入分析。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前文我们梳理了 Flink 状态管理相关的源码，我们知道，状态是要与 Checkpoint 配合使用的。因此，本文我们就一起来看一下 Checkpoint 相关的源码。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：状态管理</title>
    <link href="https://jackeyzhe.github.io/2025/12/03/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/"/>
    <id>https://jackeyzhe.github.io/2025/12/03/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/</id>
    <published>2025-12-03T03:11:28.000Z</published>
    <updated>2025-12-12T14:34:41.525Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们介绍了 Flink 状态的分类和应用。今天从源码层面再看一下 Flink 是如何管理状态的。<span id="more"></span></p><h3 id="State-概述"><a href="#State-概述" class="headerlink" title="State 概述"></a>State 概述</h3><p>关于 State 的详细介绍可以参考 <a href="https://jackeyzhe.github.io/2025/08/04/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%BA%94%E7%94%A8/">Flink学习笔记：状态类型和应用</a> 和 <a href="https://jackeyzhe.github.io/2025/08/24/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF/">Flink学习笔记：状态后端</a>这两篇文章，为了方面阅读，这里我们再简单介绍一下。</p><h4 id="State-使用"><a href="#State-使用" class="headerlink" title="State 使用"></a>State 使用</h4><p>State 是 Flink 做复杂逻辑所依赖的核心组件。它的分类如下</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1754928263/Blog/flink/4/%E7%8A%B6%E6%80%81%E5%88%86%E7%B1%BB.png" alt="State 分类"></p><p>常见的是 Keyed State 和 Operator State，Keyed State 作用于 KeyedStream 上，Operator State 可以作用于所有的 Operator 上。Keyed State 使用时，需要先创建 StateDescriptor，然后再调用 getState 获取。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ValueStateDescriptor</span>&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;average&quot;</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class="line">ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum = getRuntimeContext().getState(descriptor);</span><br></pre></td></tr></table></figure><p>Opeartor State 的获取方式与 Keyed State 类似，都需要 StateDescriptor。Operator State 在定义时需要实现 CheckpointedFunction。</p><h4 id="State-存储"><a href="#State-存储" class="headerlink" title="State 存储"></a>State 存储</h4><p>State Backend 用来管理 State 存储，根据存储格式和存储类型的组合，可以分为三类：</p><ol><li><p>MemoryStateBackend：HashMapStateBackend 和 JobManagerCheckpointStorage 的组合，即将 State 以 Java 对象的形式存储在 JobManager 内存中。</p></li><li><p>FsStateBackend：HashMapStateBackend 和 FileSystemCheckpointStorage 的组合，将 State 以 Java 对象的形式存储在远端文件系统中。</p></li><li><p>RocksDBStateBackend：EmbeddedRocksDBStateBackend 和 FileSystemCheckpointStorage 的组合，State 序列化后存储在 RocksDB。</p></li></ol><h3 id="创建-State-Backend"><a href="#创建-State-Backend" class="headerlink" title="创建 State Backend"></a>创建 State Backend</h3><p>创建 State Backend 的入口在 StreamTask，StreamTask 是 Flink 部署和运行在 TaskManager 的基本单元。</p><p>在 StreamTask 的 invoke 方法中，会先调用 restoreStateAndGates 方法去创建 State Backend。完整的调用链路如下图所示。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1764852973/Blog/flink/12/statebackend.png" alt="stateBackend"></p><p>在 streamOperatorStateContext 方法中，分别调用了 keyedStatedBackend 和 operatorStateBackend 来创建两种 State Backend。</p><p>我们先来看 keyedStateBackend 的逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> &lt;K, R <span class="keyword">extends</span> <span class="title class_">Disposable</span> &amp; Closeable&gt; R <span class="title function_">keyedStatedBackend</span><span class="params">(</span></span><br><span class="line"><span class="params">        TypeSerializer&lt;K&gt; keySerializer,</span></span><br><span class="line"><span class="params">        String operatorIdentifierText,</span></span><br><span class="line"><span class="params">        PrioritizedOperatorSubtaskState prioritizedOperatorSubtaskStates,</span></span><br><span class="line"><span class="params">        CloseableRegistry backendCloseableRegistry,</span></span><br><span class="line"><span class="params">        MetricGroup metricGroup,</span></span><br><span class="line"><span class="params">        <span class="type">double</span> managedMemoryFraction,</span></span><br><span class="line"><span class="params">        StateObject.StateObjectSizeStatsCollector statsCollector,</span></span><br><span class="line"><span class="params">        KeyedStateBackendCreator&lt;K, R&gt; keyedStateBackendCreator)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (keySerializer == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">KeyGroupRange</span> <span class="variable">keyGroupRange</span> <span class="operator">=</span></span><br><span class="line">            KeyGroupRangeAssignment.computeKeyGroupRangeForOperatorIndex(</span><br><span class="line">                    taskInfo.getMaxNumberOfParallelSubtasks(),</span><br><span class="line">                    taskInfo.getNumberOfParallelSubtasks(),</span><br><span class="line">                    taskInfo.getIndexOfThisSubtask());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Now restore processing is included in backend building/constructing process, so we need</span></span><br><span class="line">    <span class="comment">// to make sure</span></span><br><span class="line">    <span class="comment">// each stream constructed in restore could also be closed in case of task cancel, for</span></span><br><span class="line">    <span class="comment">// example the data</span></span><br><span class="line">    <span class="comment">// input stream opened for serDe during restore.</span></span><br><span class="line">    <span class="type">CloseableRegistry</span> <span class="variable">cancelStreamRegistryForRestore</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CloseableRegistry</span>();</span><br><span class="line">    backendCloseableRegistry.registerCloseable(cancelStreamRegistryForRestore);</span><br><span class="line">    BackendRestorerProcedure&lt;R, KeyedStateHandle&gt; backendRestorer =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">BackendRestorerProcedure</span>&lt;&gt;(</span><br><span class="line">                    (stateHandles) -&gt; &#123;</span><br><span class="line">                        KeyedStateBackendParametersImpl&lt;K&gt; parameters =</span><br><span class="line">                                <span class="keyword">new</span> <span class="title class_">KeyedStateBackendParametersImpl</span>&lt;&gt;(...);</span><br><span class="line">                        <span class="keyword">return</span> keyedStateBackendCreator.create(...),</span><br><span class="line">                                parameters);</span><br><span class="line">                    &#125;,</span><br><span class="line">                    backendCloseableRegistry,</span><br><span class="line">                    logDescription);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> backendRestorer.createAndRestore(</span><br><span class="line">                prioritizedOperatorSubtaskStates.getPrioritizedManagedKeyedState(),</span><br><span class="line">                statsCollector);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (backendCloseableRegistry.unregisterCloseable(cancelStreamRegistryForRestore)) &#123;</span><br><span class="line">            IOUtils.closeQuietly(cancelStreamRegistryForRestore);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的创建过程也比较简单，先是获取 KeyGroupRange，它表示的是当前 Operator 上处理的 key 的范围。然后就是创建 StateBackend 实例，这里通过 BackendRestorerProcedure 封装统一的恢复、异常处理和资源清理逻辑。operatorStateBackend 方法的逻辑相比较来说，只是少了 KeyGroupRange 的处理，直接创建 StateBackend 实例。</p><h3 id="创建和使用-State"><a href="#创建和使用-State" class="headerlink" title="创建和使用 State"></a>创建和使用 State</h3><h4 id="创建-KeyedState"><a href="#创建-KeyedState" class="headerlink" title="创建 KeyedState"></a>创建 KeyedState</h4><p>KeyedState 是通过调用 StreamingRuntimeContext.getState 方法获取的。我们先来看完整的调用流程。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1765164156/Blog/flink/12/getstate.png" alt="getState"></p><p>在调用 getState 这些方法时，都会先调用 keyedStateStore 提供的方法，它是 Flink 提供的一个封装 keyedStateBackend 的接口。调用流程的最后，是调用 keyedStateBackend 中的 createOrUpdateInternalState 方法（这里我们以 HeapStateBackend 为例）。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;N, SV, SEV, S <span class="keyword">extends</span> <span class="title class_">State</span>, IS <span class="keyword">extends</span> <span class="title class_">S</span>&gt; IS <span class="title function_">createOrUpdateInternalState</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="meta">@Nonnull</span> TypeSerializer&lt;N&gt; namespaceSerializer,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nonnull</span> StateDescriptor&lt;S, SV&gt; stateDesc,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nonnull</span> StateSnapshotTransformFactory&lt;SEV&gt; snapshotTransformFactory,</span></span><br><span class="line"><span class="params">        <span class="type">boolean</span> allowFutureMetadataUpdates)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    StateTable&lt;K, N, SV&gt; stateTable =</span><br><span class="line">            tryRegisterStateTable(</span><br><span class="line">                    namespaceSerializer,</span><br><span class="line">                    stateDesc,</span><br><span class="line">                    getStateSnapshotTransformFactory(stateDesc, snapshotTransformFactory),</span><br><span class="line">                    allowFutureMetadataUpdates);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    <span class="type">IS</span> <span class="variable">createdState</span> <span class="operator">=</span> (IS) createdKVStates.get(stateDesc.getName());</span><br><span class="line">    <span class="keyword">if</span> (createdState == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="type">StateCreateFactory</span> <span class="variable">stateCreateFactory</span> <span class="operator">=</span> STATE_CREATE_FACTORIES.get(stateDesc.getType());</span><br><span class="line">        <span class="keyword">if</span> (stateCreateFactory == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">FlinkRuntimeException</span>(stateNotSupportedMessage(stateDesc));</span><br><span class="line">        &#125;</span><br><span class="line">        createdState =</span><br><span class="line">                stateCreateFactory.createState(stateDesc, stateTable, getKeySerializer());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">StateUpdateFactory</span> <span class="variable">stateUpdateFactory</span> <span class="operator">=</span> STATE_UPDATE_FACTORIES.get(stateDesc.getType());</span><br><span class="line">        <span class="keyword">if</span> (stateUpdateFactory == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">FlinkRuntimeException</span>(stateNotSupportedMessage(stateDesc));</span><br><span class="line">        &#125;</span><br><span class="line">        createdState = stateUpdateFactory.updateState(stateDesc, stateTable, createdState);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    createdKVStates.put(stateDesc.getName(), createdState);</span><br><span class="line">    <span class="keyword">return</span> createdState;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;StateDescriptor.Type, StateCreateFactory&gt; STATE_CREATE_FACTORIES =</span><br><span class="line">        Stream.of(</span><br><span class="line">                        Tuple2.of(</span><br><span class="line">                                StateDescriptor.Type.VALUE,</span><br><span class="line">                                (StateCreateFactory) HeapValueState::create),</span><br><span class="line">                        Tuple2.of(</span><br><span class="line">                                StateDescriptor.Type.LIST,</span><br><span class="line">                                (StateCreateFactory) HeapListState::create),</span><br><span class="line">                        Tuple2.of(</span><br><span class="line">                                StateDescriptor.Type.MAP,</span><br><span class="line">                                (StateCreateFactory) HeapMapState::create),</span><br><span class="line">                        Tuple2.of(</span><br><span class="line">                                StateDescriptor.Type.AGGREGATING,</span><br><span class="line">                                (StateCreateFactory) HeapAggregatingState::create),</span><br><span class="line">                        Tuple2.of(</span><br><span class="line">                                StateDescriptor.Type.REDUCING,</span><br><span class="line">                                (StateCreateFactory) HeapReducingState::create))</span><br><span class="line">                .collect(Collectors.toMap(t -&gt; t.f0, t -&gt; t.f1));</span><br></pre></td></tr></table></figure><p>这里首先是注册了一个 StateTable，这个是 State 中一个非常重要的成员变量，它内部是一个类似 Map 的结构，用来保存 key 和 key 的状态。</p><p>STATE_CREATE_FACTORIES 这个变量保存了不同类型的 State 和它对应的创建方法，同理 STATE_UPDATE_FACTORIES 保存的是不同 State 对应的 更新方法。</p><h4 id="创建-OperatorState"><a href="#创建-OperatorState" class="headerlink" title="创建 OperatorState"></a>创建 OperatorState</h4><p>看完了 KeyedState 的创建过程后，我们再来看下 OperatorState 的创建过程。</p><p>OperatorState 的创建方法是通过 FunctionInitializationContext 先获取到 OperatorStateStore，它与 KeyedStateStore 类似，都是对 StateBackend 的方法进行了封装。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(</span><br><span class="line">                    <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">    checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class="line">            bufferedElements.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>OperatorStateStore 的 getListState 方法中，直接创建出了 PartitionableListState，同时也做了一些缓存操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;S&gt; ListState&lt;S&gt; <span class="title function_">getListState</span><span class="params">(</span></span><br><span class="line"><span class="params">        ListStateDescriptor&lt;S&gt; stateDescriptor, OperatorStateHandle.Mode mode)</span></span><br><span class="line">        <span class="keyword">throws</span> StateMigrationException &#123;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    PartitionableListState&lt;S&gt; partitionableListState =</span><br><span class="line">            (PartitionableListState&lt;S&gt;) registeredOperatorStates.get(name);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="literal">null</span> == partitionableListState) &#123;</span><br><span class="line">        <span class="comment">// no restored state for the state name; simply create new state holder</span></span><br><span class="line"></span><br><span class="line">        partitionableListState =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">PartitionableListState</span>&lt;&gt;(</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">RegisteredOperatorStateBackendMetaInfo</span>&lt;&gt;(</span><br><span class="line">                                name, partitionStateSerializer, mode));</span><br><span class="line"></span><br><span class="line">        registeredOperatorStates.put(name, partitionableListState);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ......</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    accessedStatesByName.put(name, partitionableListState);</span><br><span class="line">    <span class="keyword">return</span> partitionableListState;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>PartitionableListState 内部有一个 ArrayList 用于保存数据。</p><h4 id="使用-KeyedState"><a href="#使用-KeyedState" class="headerlink" title="使用 KeyedState"></a>使用 KeyedState</h4><p>了解完 State 的创建之后，接下来就是 State 的使用了。我们以 HeapValueState 为例来看如何获取 State。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HeapValueState 类</span></span><br><span class="line"><span class="keyword">public</span> V <span class="title function_">value</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">V</span> <span class="variable">result</span> <span class="operator">=</span> stateTable.get(currentNamespace);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (result == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> getDefaultValue();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 HeapValueState 类的 value 方法中，直接调用 StateTable 的 get 方法，最终调用的是 CopyOnWriteStateMap 的 get 方法，这个方法与 HashMap 的 get 方法比较类似。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> S <span class="title function_">get</span><span class="params">(K key, N namespace)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">hash</span> <span class="operator">=</span> computeHashForOperationAndDoIncrementalRehash(key, namespace);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">requiredVersion</span> <span class="operator">=</span> highestRequiredSnapshotVersion;</span><br><span class="line">    <span class="keyword">final</span> StateMapEntry&lt;K, N, S&gt;[] tab = selectActiveTable(hash);</span><br><span class="line">    <span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> hash &amp; (tab.length - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (StateMapEntry&lt;K, N, S&gt; e = tab[index]; e != <span class="literal">null</span>; e = e.next) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">K</span> <span class="variable">eKey</span> <span class="operator">=</span> e.key;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">N</span> <span class="variable">eNamespace</span> <span class="operator">=</span> e.namespace;</span><br><span class="line">        <span class="keyword">if</span> ((e.hash == hash &amp;&amp; key.equals(eKey) &amp;&amp; namespace.equals(eNamespace))) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// copy-on-write check for state</span></span><br><span class="line">            <span class="keyword">if</span> (e.stateVersion &lt; requiredVersion) &#123;</span><br><span class="line">                <span class="comment">// copy-on-write check for entry</span></span><br><span class="line">                <span class="keyword">if</span> (e.entryVersion &lt; requiredVersion) &#123;</span><br><span class="line">                    e = handleChainedEntryCopyOnWrite(tab, hash &amp; (tab.length - <span class="number">1</span>), e);</span><br><span class="line">                &#125;</span><br><span class="line">                e.stateVersion = stateMapVersion;</span><br><span class="line">                e.state = getStateSerializer().copy(e.state);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> e.state;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用-OperatorState"><a href="#使用-OperatorState" class="headerlink" title="使用 OperatorState"></a>使用 OperatorState</h4><p>OperatorState 底层使用的是 PartitionableListState，前面也提到了，它的内部用了一个 ArrayList 来保存数据，对于 OperatorState 的各种操作也都是来操作这个 ArrayList。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">clear</span><span class="params">()</span> &#123;</span><br><span class="line">    internalList.clear();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Iterable&lt;S&gt; <span class="title function_">get</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> internalList;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(S value)</span> &#123;</span><br><span class="line">    Preconditions.checkNotNull(value, <span class="string">&quot;You cannot add null to a ListState.&quot;</span>);</span><br><span class="line">    internalList.add(value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">update</span><span class="params">(List&lt;S&gt; values)</span> &#123;</span><br><span class="line">    internalList.clear();</span><br><span class="line"></span><br><span class="line">    addAll(values);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addAll</span><span class="params">(List&lt;S&gt; values)</span> &#123;</span><br><span class="line">    Preconditions.checkNotNull(values, <span class="string">&quot;List of values to add cannot be null.&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (!values.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">for</span> (S value : values) &#123;</span><br><span class="line">            checkNotNull(value, <span class="string">&quot;Any value to add to a list cannot be null.&quot;</span>);</span><br><span class="line">            add(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文对 State 的相关代码进行了梳理。包括 StateBackend 的创建，KeyedState 和 OperatorState 的创建和使用。State 和 Checkpoint 两者需要结合使用，因此后面我们会再梳理 Checkpoint 的相关代码。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前面我们介绍了 Flink 状态的分类和应用。今天从源码层面再看一下 Flink 是如何管理状态的。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：集群启动</title>
    <link href="https://jackeyzhe.github.io/2025/11/27/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8/"/>
    <id>https://jackeyzhe.github.io/2025/11/27/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8/</id>
    <published>2025-11-27T08:35:40.000Z</published>
    <updated>2025-12-02T13:16:13.741Z</updated>
    
    <content type="html"><![CDATA[<p>前文中，我们已经了解了 Flink 的三种执行图是怎么生成的。今天继续看一下 Flink 集群是如何启动的。<span id="more"></span></p><h3 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h3><p>集群启动脚本的位置在：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink-dist/src/main/flink-bin/bin/start-cluster.sh</span><br></pre></td></tr></table></figure><p>脚本会负责启动 JobManager 和 TaskManager，我们主要关注 standalone 启动模式，具体的流程见下图。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1764258366/Blog/flink/11/start-cluster.png" alt="start-cluster"></p><p>从图中可以看出 JobManager 是通过 jobmanager.sh 文件启动的，TaskManager 是通过taskmanager.sh 启动的，两者都调用了 flink-daemon.sh，通过传递不同的参数，最终运行不同的 Java 类。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="variable">$DAEMON</span> <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (sql-gateway)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.table.gateway.SqlGateway</span><br><span class="line">        SQL_GATEWAY_CLASSPATH=<span class="string">&quot;`findSqlGatewayJar`&quot;</span>:<span class="string">&quot;`findFlinkPythonJar`&quot;</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Unknown daemon &#x27;<span class="variable">$&#123;DAEMON&#125;</span>&#x27;. <span class="variable">$USAGE</span>.&quot;</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><h3 id="JobManager-启动流程"><a href="#JobManager-启动流程" class="headerlink" title="JobManager 启动流程"></a>JobManager 启动流程</h3><p>在 StandaloneSessionClusterEntrypoint 的 main 方法中，主要就是加载各种配置和环境变量，然后调用 ClusterEntrypoint.runClusterEntrypoint 来启动集群。跟着调用链一直找到 ClusterEntrypoint.runCluster 方法，这里会启动 ResourceManager、DispatcherRunner 等组件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">runCluster</span><span class="params">(Configuration configuration, PluginManager pluginManager)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">        <span class="comment">// 初始化各种服务</span></span><br><span class="line">        initializeServices(configuration, pluginManager);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 DispatcherResourceManagerComponentFactory，</span></span><br><span class="line">        <span class="comment">// 包含了三个核心组件的 Factory</span></span><br><span class="line">        <span class="comment">// DispatcherRunnerFactory、ResourceManagerFactory、RestEndpointFactory</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">DispatcherResourceManagerComponentFactory</span></span><br><span class="line">                <span class="variable">dispatcherResourceManagerComponentFactory</span> <span class="operator">=</span></span><br><span class="line">                        createDispatcherResourceManagerComponentFactory(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动 ResourceManager、DispatcherRunner、WebMonitorEndpoint</span></span><br><span class="line">        clusterComponent =</span><br><span class="line">                dispatcherResourceManagerComponentFactory.create(</span><br><span class="line">                        configuration,</span><br><span class="line">                        resourceId.unwrap(),</span><br><span class="line">                        ioExecutor,</span><br><span class="line">                        commonRpcService,</span><br><span class="line">                        haServices,</span><br><span class="line">                        blobServer,</span><br><span class="line">                        heartbeatServices,</span><br><span class="line">                        delegationTokenManager,</span><br><span class="line">                        metricRegistry,</span><br><span class="line">                        executionGraphInfoStore,</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">RpcMetricQueryServiceRetriever</span>(</span><br><span class="line">                                metricRegistry.getMetricQueryServiceRpcService()),</span><br><span class="line">                        failureEnrichers,</span><br><span class="line">                        <span class="built_in">this</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭服务</span></span><br><span class="line">        clusterComponent</span><br><span class="line">                .getShutDownFuture()</span><br><span class="line">                .whenComplete(</span><br><span class="line">                        (ApplicationStatus applicationStatus, Throwable throwable) -&gt; &#123;</span><br><span class="line">                            <span class="keyword">if</span> (throwable != <span class="literal">null</span>) &#123;</span><br><span class="line">                                shutDownAsync(</span><br><span class="line">                                        ApplicationStatus.UNKNOWN,</span><br><span class="line">                                        ShutdownBehaviour.GRACEFUL_SHUTDOWN,</span><br><span class="line">                                        ExceptionUtils.stringifyException(throwable),</span><br><span class="line">                                        <span class="literal">false</span>);</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                <span class="comment">// This is the general shutdown path. If a separate more</span></span><br><span class="line">                                <span class="comment">// specific shutdown was</span></span><br><span class="line">                                <span class="comment">// already triggered, this will do nothing</span></span><br><span class="line">                                shutDownAsync(</span><br><span class="line">                                        applicationStatus,</span><br><span class="line">                                        ShutdownBehaviour.GRACEFUL_SHUTDOWN,</span><br><span class="line">                                        <span class="literal">null</span>,</span><br><span class="line">                                        <span class="literal">true</span>);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面来详细看一下这几个方法， initializeServices 就是负责初始化各种服务，有几个比较重要的可以着重关注下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化并启动一个通用的 RPC Service</span></span><br><span class="line">commonRpcService = RpcUtils.createRemoteRpcService(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个 IO 线程池，线程数量位 CPU 核数 * 4</span></span><br><span class="line">ioExecutor = Executors.newFixedThreadPool(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 HA 服务组件，根据配置初始化 Standalone、ZK、K8S 三种</span></span><br><span class="line">haServices = createHaServices(configuration, ioExecutor, rpcSystem);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建并启动 blobServer,blobServer 可以理解为是 Flink 内部的</span></span><br><span class="line">blobServer = BlobUtils.createBlobServer(...);</span><br><span class="line">blobServer.start();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建心跳服务</span></span><br><span class="line">heartbeatServices = createHeartbeatServices(configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个监控服务</span></span><br><span class="line">processMetricGroup = MetricUtils.instantiateProcessMetricGroup(...);</span><br></pre></td></tr></table></figure><p>createDispatcherResourceManagerComponentFactory 这个方法就是创建了三个工厂类，不需要过多介绍。我们重点关注 dispatcherResourceManagerComponentFactory.create 方法，即 ResourceManager、DispatcherRunner、WebMonitorEndpoint 是如何启动的。</p><h4 id="WebMonitorEndpoint"><a href="#WebMonitorEndpoint" class="headerlink" title="WebMonitorEndpoint"></a>WebMonitorEndpoint</h4><p>WebMonitorEndpoint 的启动流程图如下，图中细箭头代表同一个方法中顺序调用，粗箭头代表进入上一个方法内部的调用。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1764428386/Blog/flink/11/webmonitor.png" alt="webMonitorEndpoint"></p><p>WebMonitorEndpoint 创建和启动步骤如下：</p><ol><li><p>通过工厂创建出了 WebMonitorEndpoint，这里就是比较常规的初始化操作。</p></li><li><p>调用 WebMonitorEndpoint 的 start 方法开始启动，start 方法内部先是创建了一个 Router 并调用 initializeHandlers 创建了一大堆 handler（是真的一大堆，这个方法有接近一千行，都是在创建 handler），创建完成之后，对 handler 进行排序和去重，再把它们都注册到 Router 中。这里排序是为了确保路由匹配的正确性，排序规则是先静态路径（/jobs/overview），后动态路径（/jobs/:jobid），假如我们没有排序，先注册了 /jobs/:jobid ，后注册 /jobs/overview ，这时当我们请求 /jobs/overview 时，就会被错误的路由到 /jobs/:jobid 上去。</p></li><li><p>是调用 startInternal 方法，在 startInternal 方法内部只有 leader 选举和启动缓存清理任务两个步骤。</p></li></ol><h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1764517440/Blog/flink/11/resourceManager.png" alt="ResourceManager"></p><p>ResourceManager 创建和启动步骤如下：</p><ol><li><p>调用 ResourceManagerServiceImpl.create 方法创建 ResourceManagerService，这里只是创建 ResourceManager 服务，实际创建 ResourceManager 在后面的步骤中。</p></li><li><p>调用 resourceManagerService.start 方法启动服务，这里就是启动选主服务，standalne 模式直接调用 grantLeadership 成为 leader。</p></li><li><p>成为 leader 后，就会调用 startNewLeaderResourceManager 方法，这个方法中会调用 resourceManagerFactory.createResourceManager 正式创建 resourceManager。创建完成后，就会调用 resourceManager.start 来启动它。</p></li><li><p>启动后会回调 ResourceManager.onStart 方法。这里调用 startHeartbeatServices 启动了两个心跳服务，一个是 ResourceManager 和 TaskManager 之间的心跳，一个是 ResourceManager 和 JobManager 之间的心跳，然后会启动 SlotManager。SlotManager 可以被当作 Flink 集群的资源调度中心。它会负责管理集群中的所有 Slot 资源，也需要响应 JobManager 的资源请求。</p></li></ol><h4 id="DispatcherRunner"><a href="#DispatcherRunner" class="headerlink" title="DispatcherRunner"></a>DispatcherRunner</h4><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1764578180/Blog/flink/11/dispatcherRunner.png" alt="dispatcherRunner"></p><ol><li><p>先创建工厂，创建完成后调用 DefaultDispatcherRunner.create 创建出 DispatcherRunner，接着是调用 start 启动选主流程。</p></li><li><p>选主完成后就调用 startNewDispatcherLeaderProcess 启动新的流程。启动新的流程需要先关闭旧流程，然后创建新的 dispatcherLeaderProcess，并调用 start 启动。</p></li><li><p>启动时，会回调 onStart 方法。</p></li><li><p>回调方法中，先启动 executionPlanStore，它主要是用于持久化 JobGraph。然后恢复执行计划，重建状态（如果是从失败中恢复），实例化 Dispatcher，完成作业启动。</p></li></ol><h3 id="TaskManager-启动流程"><a href="#TaskManager-启动流程" class="headerlink" title="TaskManager 启动流程"></a>TaskManager 启动流程</h3><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1764666573/Blog/flink/11/taskManager.png" alt="taskManager"></p><p>TaskManager 是 Flink 的执行节点，其最小执行单元是 slot。TaskManager 启动流程也主要是和资源管理相关，包括 slot 列表的管理和与 ResourceManager 的通信。</p><p>TaskManager 启动流程大体分为以下几部分：</p><ol><li><p>构建并启动 TaskManagerRunner（蓝色部分）</p></li><li><p>启动 TaskExecutor（红色部分）</p></li><li><p>完成与 ResourceManager 的连接（橙色部分）</p></li></ol><h4 id="启动-TaskManagerRunner"><a href="#启动-TaskManagerRunner" class="headerlink" title="启动 TaskManagerRunner"></a>启动 TaskManagerRunner</h4><p>在 TaskManagerRunner 的 start 方法中，有两个步骤：</p><p>第一步是调用 startTaskManagerRunnerServices 创建和启动了很多服务，这一点和 JobManager 的启动流程比较像。这些服务包括了高可用服务、心跳服务、监控指标服务等，这里也创建了 taskExecutorService，它的启动在第二步。</p><p>第二步是调用 taskExecutorService.start 方法，启动 TaskExecutorService，它内部主要负责启动 TaskExecutor。</p><h4 id="启动-TaskExecutor"><a href="#启动-TaskExecutor" class="headerlink" title="启动 TaskExecutor"></a>启动 TaskExecutor</h4><p>TaskExecutor 是 TaskManager 内部的一个核心组件，负责帮助 TaskManager 完成 task 的部署和执行等核心操作。</p><p>在上一步调用 taskExecutor 的 start 方法后，会回调 onStart 方法，这里主要是三个步骤</p><ol><li><p>连接 ResourceManager 以及注册监听</p></li><li><p>启动 taskSlotTable</p></li><li><p>连接 JobMaster 以及注册监听</p></li></ol><p>第一步我们在下面详细解释。第二步启动的 TaskSlotTable 是 TaskManager 中负责资源的核心组件，它维护了一个 Slot 列表，管理每个 Slot 的状态，负责 Slot 的分配和释放。第三步主要是和 JobMaster 建立连接并保持心跳，同时也会接收 Slot 申请的请求。</p><h4 id="连接-ResourceManager"><a href="#连接-ResourceManager" class="headerlink" title="连接 ResourceManager"></a>连接 ResourceManager</h4><p>TaskExecutor 注册完监听之后，会收到 ResourceManagerLeaderListener.notifyLeaderAddress 方法回调。回调方法中，会创建一个 TaskExecutorToResourceManagerConnection 实例并启动它。这个类是用来将 TaskExecutor 注册到 ResourceManager，注册成功会回调 onRegistrationSuccess 方法。回调成功的方法中，TaskManager 会调用 resourceManagerGateway.sendSlotReport 将 Slot 的状态进行上报。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文介绍了 Flink 集群在 Standalone 模式下的启动过程，其中 JobManager 重点介绍了 WebMonitorEndpoint、ResourceManager 和 DispatcherRunner 这三个组件的启动过程。TaskManager 主要介绍了启动 TaskExecutor 和连接 ResourceManager 的过程。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前文中，我们已经了解了 Flink 的三种执行图是怎么生成的。今天继续看一下 Flink 集群是如何启动的。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：如何生成ExecutionGraph</title>
    <link href="https://jackeyzhe.github.io/2025/11/08/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90ExecutionGraph/"/>
    <id>https://jackeyzhe.github.io/2025/11/08/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90ExecutionGraph/</id>
    <published>2025-11-08T15:23:57.000Z</published>
    <updated>2025-11-25T15:10:23.710Z</updated>
    
    <content type="html"><![CDATA[<p>今天我们一起来了解 Flink 最后一种执行图，ExecutionGraph 的执行过程。<span id="more"></span></p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>在阅读源码之前，我们先来了解一下 ExecutionGraph 中的一些基本概念。</p><ul><li><p><strong>ExecutionJobVertex:</strong>  ExecutionJobVertex 是 ExecutionGraph 中的节点，对应的是 JobGraph 中的 JobVertex。</p></li><li><p><strong>ExecutionVertex:</strong> 每个 ExecutionJobVertex 都包含了一组 ExecutionVertex，ExecutionVertex 的数量就是节点对应的并行度。</p></li><li><p><strong>IntermediateResult:</strong> IntermediateResult 表示节点的输出结果，与之对应的是 JobGraph 中的 IntermediateDataSet。</p></li><li><p><strong>IntermediateResultPartition:</strong> IntermediateResultPartition 是每个 ExecutionVertex 的输出。</p></li><li><p><strong>EdgeManager:</strong> EdgeManager 主要负责存储 ExecutionGraph 中所有之间的连接，包括其并行度。</p></li><li><p><strong>Execution:</strong> Execution 可以认为是一次实际的运行尝试。每次执行时，Flink 都会将ExecutionVertex 封装成一个 Execution，并通过一个 ExecutionAttemptID 来做唯一标识。</p></li></ul><h3 id="ExecutionGraph-生成过程"><a href="#ExecutionGraph-生成过程" class="headerlink" title="ExecutionGraph 生成过程"></a>ExecutionGraph 生成过程</h3><p>了解了这些基本概念之后，我们一起来看一下 ExecutionGraph 的具体生成过程。生成 ExecutionGraph 的代码入口是 DefaultExecutionGraphBuilder.build 方法。</p><p>首先是获取一些基本信息，包括 jobInformation、jobStatusChangedListeners 等。</p><p>接下来就是创建一个 DefaultExecutionGraph 和生成执行计划。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create a new execution graph, if none exists so far</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">DefaultExecutionGraph</span> <span class="variable">executionGraph</span> <span class="operator">=</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">DefaultExecutionGraph</span>(</span><br><span class="line">                jobInformation,</span><br><span class="line">                futureExecutor,</span><br><span class="line">                ioExecutor,</span><br><span class="line">                rpcTimeout,</span><br><span class="line">                executionHistorySizeLimit,</span><br><span class="line">                classLoader,</span><br><span class="line">                blobWriter,</span><br><span class="line">                partitionGroupReleaseStrategyFactory,</span><br><span class="line">                shuffleMaster,</span><br><span class="line">                partitionTracker,</span><br><span class="line">                executionDeploymentListener,</span><br><span class="line">                executionStateUpdateListener,</span><br><span class="line">                initializationTimestamp,</span><br><span class="line">                vertexAttemptNumberStore,</span><br><span class="line">                vertexParallelismStore,</span><br><span class="line">                isDynamicGraph,</span><br><span class="line">                executionJobVertexFactory,</span><br><span class="line">                jobGraph.getJobStatusHooks(),</span><br><span class="line">                markPartitionFinishedStrategy,</span><br><span class="line">                taskDeploymentDescriptorFactory,</span><br><span class="line">                jobStatusChangedListeners,</span><br><span class="line">                executionPlanSchedulingContext);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    executionGraph.setPlan(JsonPlanGenerator.generatePlan(jobGraph));</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    log.warn(<span class="string">&quot;Cannot create plan for job&quot;</span>, t);</span><br><span class="line">    <span class="comment">// give the graph an empty plan</span></span><br><span class="line">    executionGraph.setPlan(<span class="keyword">new</span> <span class="title class_">JobPlanInfo</span>.Plan(<span class="string">&quot;&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面就是两个比较核心的方法 getVerticesSortedTopologicallyFromSources 和 attachJobGraph。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// topologically sort the job vertices and attach the graph to the existing one</span></span><br><span class="line">List&lt;JobVertex&gt; sortedTopology = jobGraph.getVerticesSortedTopologicallyFromSources();</span><br><span class="line"></span><br><span class="line">executionGraph.attachJobGraph(sortedTopology, jobManagerJobMetricGroup);</span><br></pre></td></tr></table></figure><p>这两个方法是先将 JobVertex 进行排序，然后构建 ExecutionGraph 的拓扑图。</p><h4 id="getVerticesSortedTopologicallyFromSources"><a href="#getVerticesSortedTopologicallyFromSources" class="headerlink" title="getVerticesSortedTopologicallyFromSources"></a>getVerticesSortedTopologicallyFromSources</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;JobVertex&gt; <span class="title function_">getVerticesSortedTopologicallyFromSources</span><span class="params">()</span></span><br><span class="line">        <span class="keyword">throws</span> InvalidProgramException &#123;</span><br><span class="line">    <span class="comment">// early out on empty lists</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.taskVertices.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyList();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;JobVertex&gt; sorted = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;JobVertex&gt;(<span class="built_in">this</span>.taskVertices.size());</span><br><span class="line">    Set&lt;JobVertex&gt; remaining = <span class="keyword">new</span> <span class="title class_">LinkedHashSet</span>&lt;JobVertex&gt;(<span class="built_in">this</span>.taskVertices.values());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// start by finding the vertices with no input edges</span></span><br><span class="line">    <span class="comment">// and the ones with disconnected inputs (that refer to some standalone data set)</span></span><br><span class="line">    &#123;</span><br><span class="line">        Iterator&lt;JobVertex&gt; iter = remaining.iterator();</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">            <span class="type">JobVertex</span> <span class="variable">vertex</span> <span class="operator">=</span> iter.next();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (vertex.isInputVertex()) &#123;</span><br><span class="line">                sorted.add(vertex);</span><br><span class="line">                iter.remove();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> <span class="variable">startNodePos</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// traverse from the nodes that were added until we found all elements</span></span><br><span class="line">    <span class="keyword">while</span> (!remaining.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// first check if we have more candidates to start traversing from. if not, then the</span></span><br><span class="line">        <span class="comment">// graph is cyclic, which is not permitted</span></span><br><span class="line">        <span class="keyword">if</span> (startNodePos &gt;= sorted.size()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">InvalidProgramException</span>(<span class="string">&quot;The job graph is cyclic.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">JobVertex</span> <span class="variable">current</span> <span class="operator">=</span> sorted.get(startNodePos++);</span><br><span class="line">        addNodesThatHaveNoNewPredecessors(current, sorted, remaining);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sorted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码是将所有的节点进行排序，先将所有的 Source 节点筛选出来，然后再将剩余节点假如列表。这样就能构建出最终的拓扑图。</p><h4 id="attachJobGraph"><a href="#attachJobGraph" class="headerlink" title="attachJobGraph"></a>attachJobGraph</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">attachJobGraph</span><span class="params">(</span></span><br><span class="line"><span class="params">        List&lt;JobVertex&gt; verticesToAttach, JobManagerJobMetricGroup jobManagerJobMetricGroup)</span></span><br><span class="line">        <span class="keyword">throws</span> JobException &#123;</span><br><span class="line"></span><br><span class="line">    assertRunningInJobMasterMainThread();</span><br><span class="line"></span><br><span class="line">    LOG.debug(</span><br><span class="line">            <span class="string">&quot;Attaching &#123;&#125; topologically sorted vertices to existing job graph with &#123;&#125; &quot;</span></span><br><span class="line">                    + <span class="string">&quot;vertices and &#123;&#125; intermediate results.&quot;</span>,</span><br><span class="line">            verticesToAttach.size(),</span><br><span class="line">            tasks.size(),</span><br><span class="line">            intermediateResults.size());</span><br><span class="line"></span><br><span class="line">    attachJobVertices(verticesToAttach, jobManagerJobMetricGroup);</span><br><span class="line">    <span class="keyword">if</span> (!isDynamic) &#123;</span><br><span class="line">        initializeJobVertices(verticesToAttach);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// the topology assigning should happen before notifying new vertices to failoverStrategy</span></span><br><span class="line">    executionTopology = DefaultExecutionTopology.fromExecutionGraph(<span class="built_in">this</span>);</span><br><span class="line"></span><br><span class="line">    partitionGroupReleaseStrategy =</span><br><span class="line">            partitionGroupReleaseStrategyFactory.createInstance(getSchedulingTopology());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>attachJobGraph 方法主要包含两步逻辑，第一步是调用 attachJobVertices 方法创建 ExecutionJobVertex 实例，第二步是调用 fromExecutionGraph 创建一些其他的核心对象。</p><p><strong>attachJobVertices</strong></p><p>attachJobVertices 方法中就是遍历所有的 JobVertex，然后利用 JobVertex 生成 ExecutionJobVertex。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Attach job vertices without initializing them. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">attachJobVertices</span><span class="params">(</span></span><br><span class="line"><span class="params">        List&lt;JobVertex&gt; topologicallySorted, JobManagerJobMetricGroup jobManagerJobMetricGroup)</span></span><br><span class="line">        <span class="keyword">throws</span> JobException &#123;</span><br><span class="line">    <span class="keyword">for</span> (JobVertex jobVertex : topologicallySorted) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (jobVertex.isInputVertex() &amp;&amp; !jobVertex.isStoppable()) &#123;</span><br><span class="line">            <span class="built_in">this</span>.isStoppable = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">VertexParallelismInformation</span> <span class="variable">parallelismInfo</span> <span class="operator">=</span></span><br><span class="line">                parallelismStore.getParallelismInfo(jobVertex.getID());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create the execution job vertex and attach it to the graph</span></span><br><span class="line">        <span class="type">ExecutionJobVertex</span> <span class="variable">ejv</span> <span class="operator">=</span></span><br><span class="line">                executionJobVertexFactory.createExecutionJobVertex(</span><br><span class="line">                        <span class="built_in">this</span>,</span><br><span class="line">                        jobVertex,</span><br><span class="line">                        parallelismInfo,</span><br><span class="line">                        coordinatorStore,</span><br><span class="line">                        jobManagerJobMetricGroup);</span><br><span class="line"></span><br><span class="line">        <span class="type">ExecutionJobVertex</span> <span class="variable">previousTask</span> <span class="operator">=</span> <span class="built_in">this</span>.tasks.putIfAbsent(jobVertex.getID(), ejv);</span><br><span class="line">        <span class="keyword">if</span> (previousTask != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">JobException</span>(</span><br><span class="line">                    String.format(</span><br><span class="line">                            <span class="string">&quot;Encountered two job vertices with ID %s : previous=[%s] / new=[%s]&quot;</span>,</span><br><span class="line">                            jobVertex.getID(), ejv, previousTask));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">this</span>.verticesInCreationOrder.add(ejv);</span><br><span class="line">        <span class="built_in">this</span>.numJobVerticesTotal++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>initializeJobVertices</strong></p><p>在 DefaultExecutionGraph.initializeJobVertices 中是遍历了刚刚排好序的 JobVertex，获取了 ExecutionJobVertex 之后调用了 ExecutionGraph.initializeJobVertex 方法。</p><p>我们直接来看 ExecutionGraph.initializeJobVertex 的逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title function_">initializeJobVertex</span><span class="params">(ExecutionJobVertex ejv, <span class="type">long</span> createTimestamp)</span></span><br><span class="line">        <span class="keyword">throws</span> JobException &#123;</span><br><span class="line">    initializeJobVertex(</span><br><span class="line">            ejv,</span><br><span class="line">            createTimestamp,</span><br><span class="line">            VertexInputInfoComputationUtils.computeVertexInputInfos(</span><br><span class="line">                    ejv, getAllIntermediateResults()::get));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里先是调用了 VertexInputInfoComputationUtils.computeVertexInputInfos 方法，生成了 Map&lt;IntermediateDataSetID, JobVertexInputInfo&gt; jobVertexInputInfos。它表示的是每个 ExecutionVertex 消费上游 IntermediateResultPartition 的范围。</p><p>这里有两种模式，分别是 POINTWISE （点对点）和 ALL_TO_ALL（全对全）</p><p>在 POINTWISE 模式中，会按照尽量均匀分布的方式处理。</p><ul><li><p>例如上游并发度是4，下游并发度是2时，那么前两个 IntermediateResultPartition 就会被第一个 ExecutionVertex 消费，后两个 IntermediateResultPartition 就会被第二个 ExecutionVertex 消费。</p></li><li><p>如果上游并发度是2，下游是3时，那么下游前两个 IntermediateResultPartition 会被第一个 ExecutionVertex 消费，第三个 IntermediateResultPartition 则会被第二个 ExecutionVertex 消费。</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> JobVertexInputInfo <span class="title function_">computeVertexInputInfoForPointwise</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="type">int</span> sourceCount,</span></span><br><span class="line"><span class="params">        <span class="type">int</span> targetCount,</span></span><br><span class="line"><span class="params">        Function&lt;Integer, Integer&gt; numOfSubpartitionsRetriever,</span></span><br><span class="line"><span class="params">        <span class="type">boolean</span> isDynamicGraph)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> List&lt;ExecutionVertexInputInfo&gt; executionVertexInputInfos = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (sourceCount &gt;= targetCount) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> <span class="number">0</span>; index &lt; targetCount; index++) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> <span class="variable">start</span> <span class="operator">=</span> index * sourceCount / targetCount;</span><br><span class="line">            <span class="type">int</span> <span class="variable">end</span> <span class="operator">=</span> (index + <span class="number">1</span>) * sourceCount / targetCount;</span><br><span class="line"></span><br><span class="line">            <span class="type">IndexRange</span> <span class="variable">partitionRange</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IndexRange</span>(start, end - <span class="number">1</span>);</span><br><span class="line">            <span class="type">IndexRange</span> <span class="variable">subpartitionRange</span> <span class="operator">=</span></span><br><span class="line">                    computeConsumedSubpartitionRange(</span><br><span class="line">                            index,</span><br><span class="line">                            <span class="number">1</span>,</span><br><span class="line">                            () -&gt; numOfSubpartitionsRetriever.apply(start),</span><br><span class="line">                            isDynamicGraph,</span><br><span class="line">                            <span class="literal">false</span>,</span><br><span class="line">                            <span class="literal">false</span>);</span><br><span class="line">            executionVertexInputInfos.add(</span><br><span class="line">                    <span class="keyword">new</span> <span class="title class_">ExecutionVertexInputInfo</span>(index, partitionRange, subpartitionRange));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">partitionNum</span> <span class="operator">=</span> <span class="number">0</span>; partitionNum &lt; sourceCount; partitionNum++) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> <span class="variable">start</span> <span class="operator">=</span> (partitionNum * targetCount + sourceCount - <span class="number">1</span>) / sourceCount;</span><br><span class="line">            <span class="type">int</span> <span class="variable">end</span> <span class="operator">=</span> ((partitionNum + <span class="number">1</span>) * targetCount + sourceCount - <span class="number">1</span>) / sourceCount;</span><br><span class="line">            <span class="type">int</span> <span class="variable">numConsumers</span> <span class="operator">=</span> end - start;</span><br><span class="line"></span><br><span class="line">            <span class="type">IndexRange</span> <span class="variable">partitionRange</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IndexRange</span>(partitionNum, partitionNum);</span><br><span class="line">            <span class="comment">// Variable used in lambda expression should be final or effectively final</span></span><br><span class="line">            <span class="keyword">final</span> <span class="type">int</span> <span class="variable">finalPartitionNum</span> <span class="operator">=</span> partitionNum;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> start; i &lt; end; i++) &#123;</span><br><span class="line">                <span class="type">IndexRange</span> <span class="variable">subpartitionRange</span> <span class="operator">=</span></span><br><span class="line">                        computeConsumedSubpartitionRange(</span><br><span class="line">                                i,</span><br><span class="line">                                numConsumers,</span><br><span class="line">                                () -&gt; numOfSubpartitionsRetriever.apply(finalPartitionNum),</span><br><span class="line">                                isDynamicGraph,</span><br><span class="line">                                <span class="literal">false</span>,</span><br><span class="line">                                <span class="literal">false</span>);</span><br><span class="line">                executionVertexInputInfos.add(</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">ExecutionVertexInputInfo</span>(i, partitionRange, subpartitionRange));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">JobVertexInputInfo</span>(executionVertexInputInfos);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 ALL_TO_ALL 模式中，每个下游都会消费所有上游的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> JobVertexInputInfo <span class="title function_">computeVertexInputInfoForAllToAll</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="type">int</span> sourceCount,</span></span><br><span class="line"><span class="params">        <span class="type">int</span> targetCount,</span></span><br><span class="line"><span class="params">        Function&lt;Integer, Integer&gt; numOfSubpartitionsRetriever,</span></span><br><span class="line"><span class="params">        <span class="type">boolean</span> isDynamicGraph,</span></span><br><span class="line"><span class="params">        <span class="type">boolean</span> isBroadcast,</span></span><br><span class="line"><span class="params">        <span class="type">boolean</span> isSingleSubpartitionContainsAllData)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> List&lt;ExecutionVertexInputInfo&gt; executionVertexInputInfos = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="type">IndexRange</span> <span class="variable">partitionRange</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IndexRange</span>(<span class="number">0</span>, sourceCount - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; targetCount; ++i) &#123;</span><br><span class="line">        <span class="type">IndexRange</span> <span class="variable">subpartitionRange</span> <span class="operator">=</span></span><br><span class="line">                computeConsumedSubpartitionRange(</span><br><span class="line">                        i,</span><br><span class="line">                        targetCount,</span><br><span class="line">                        () -&gt; numOfSubpartitionsRetriever.apply(<span class="number">0</span>),</span><br><span class="line">                        isDynamicGraph,</span><br><span class="line">                        isBroadcast,</span><br><span class="line">                        isSingleSubpartitionContainsAllData);</span><br><span class="line">        executionVertexInputInfos.add(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">ExecutionVertexInputInfo</span>(i, partitionRange, subpartitionRange));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">JobVertexInputInfo</span>(executionVertexInputInfos);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>生成好了 jobVertexInputInfos 之后，我们再回到 DefaultExecutionGraph.initializeJobVertex 方法中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initializeJobVertex</span><span class="params">(</span></span><br><span class="line"><span class="params">        ExecutionJobVertex ejv,</span></span><br><span class="line"><span class="params">        <span class="type">long</span> createTimestamp,</span></span><br><span class="line"><span class="params">        Map&lt;IntermediateDataSetID, JobVertexInputInfo&gt; jobVertexInputInfos)</span></span><br><span class="line">        <span class="keyword">throws</span> JobException &#123;</span><br><span class="line"></span><br><span class="line">    checkNotNull(ejv);</span><br><span class="line">    checkNotNull(jobVertexInputInfos);</span><br><span class="line"></span><br><span class="line">    jobVertexInputInfos.forEach(</span><br><span class="line">            (resultId, info) -&gt;</span><br><span class="line">                    <span class="built_in">this</span>.vertexInputInfoStore.put(ejv.getJobVertexId(), resultId, info));</span><br><span class="line"></span><br><span class="line">    ejv.initialize(</span><br><span class="line">            executionHistorySizeLimit,</span><br><span class="line">            rpcTimeout,</span><br><span class="line">            createTimestamp,</span><br><span class="line">            <span class="built_in">this</span>.initialAttemptCounts.getAttemptCounts(ejv.getJobVertexId()),</span><br><span class="line">            executionPlanSchedulingContext);</span><br><span class="line"></span><br><span class="line">    ejv.connectToPredecessors(<span class="built_in">this</span>.intermediateResults);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (IntermediateResult res : ejv.getProducedDataSets()) &#123;</span><br><span class="line">        <span class="type">IntermediateResult</span> <span class="variable">previousDataSet</span> <span class="operator">=</span></span><br><span class="line">                <span class="built_in">this</span>.intermediateResults.putIfAbsent(res.getId(), res);</span><br><span class="line">        <span class="keyword">if</span> (previousDataSet != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">JobException</span>(</span><br><span class="line">                    String.format(</span><br><span class="line">                            <span class="string">&quot;Encountered two intermediate data set with ID %s : previous=[%s] / new=[%s]&quot;</span>,</span><br><span class="line">                            res.getId(), res, previousDataSet));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    registerExecutionVerticesAndResultPartitionsFor(ejv);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// enrich network memory.</span></span><br><span class="line">    <span class="type">SlotSharingGroup</span> <span class="variable">slotSharingGroup</span> <span class="operator">=</span> ejv.getSlotSharingGroup();</span><br><span class="line">    <span class="keyword">if</span> (areJobVerticesAllInitialized(slotSharingGroup)) &#123;</span><br><span class="line">        SsgNetworkMemoryCalculationUtils.enrichNetworkMemory(</span><br><span class="line">                slotSharingGroup, <span class="built_in">this</span>::getJobVertex, shuffleMaster);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先来看 ExecutionJobVertex.initialize 方法。这个方法主要是生成 IntermediateResult 和 ExecutionVertex。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="type">int</span> executionHistorySizeLimit,</span></span><br><span class="line"><span class="params">        Duration timeout,</span></span><br><span class="line"><span class="params">        <span class="type">long</span> createTimestamp,</span></span><br><span class="line"><span class="params">        SubtaskAttemptNumberStore initialAttemptCounts,</span></span><br><span class="line"><span class="params">        ExecutionPlanSchedulingContext executionPlanSchedulingContext)</span></span><br><span class="line">        <span class="keyword">throws</span> JobException &#123;</span><br><span class="line"></span><br><span class="line">    checkState(parallelismInfo.getParallelism() &gt; <span class="number">0</span>);</span><br><span class="line">    checkState(!isInitialized());</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.taskVertices = <span class="keyword">new</span> <span class="title class_">ExecutionVertex</span>[parallelismInfo.getParallelism()];</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.inputs = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(jobVertex.getInputs().size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the intermediate results</span></span><br><span class="line">    <span class="built_in">this</span>.producedDataSets =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">IntermediateResult</span>[jobVertex.getNumberOfProducedIntermediateDataSets()];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; jobVertex.getProducedDataSets().size(); i++) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">IntermediateDataSet</span> <span class="variable">result</span> <span class="operator">=</span> jobVertex.getProducedDataSets().get(i);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">this</span>.producedDataSets[i] =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">IntermediateResult</span>(</span><br><span class="line">                        result,</span><br><span class="line">                        <span class="built_in">this</span>,</span><br><span class="line">                        <span class="built_in">this</span>.parallelismInfo.getParallelism(),</span><br><span class="line">                        result.getResultType(),</span><br><span class="line">                        executionPlanSchedulingContext);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create all task vertices</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="built_in">this</span>.parallelismInfo.getParallelism(); i++) &#123;</span><br><span class="line">        <span class="type">ExecutionVertex</span> <span class="variable">vertex</span> <span class="operator">=</span></span><br><span class="line">                createExecutionVertex(</span><br><span class="line">                        <span class="built_in">this</span>,</span><br><span class="line">                        i,</span><br><span class="line">                        producedDataSets,</span><br><span class="line">                        timeout,</span><br><span class="line">                        createTimestamp,</span><br><span class="line">                        executionHistorySizeLimit,</span><br><span class="line">                        initialAttemptCounts.getAttemptCount(i));</span><br><span class="line"></span><br><span class="line">        <span class="built_in">this</span>.taskVertices[i] = vertex;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sanity check for the double referencing between intermediate result partitions and</span></span><br><span class="line">    <span class="comment">// execution vertices</span></span><br><span class="line">    <span class="keyword">for</span> (IntermediateResult ir : <span class="built_in">this</span>.producedDataSets) &#123;</span><br><span class="line">        <span class="keyword">if</span> (ir.getNumberOfAssignedPartitions() != <span class="built_in">this</span>.parallelismInfo.getParallelism()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(</span><br><span class="line">                    <span class="string">&quot;The intermediate result&#x27;s partitions were not correctly assigned.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up the input splits, if the vertex has any</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">        InputSplitSource&lt;InputSplit&gt; splitSource =</span><br><span class="line">                (InputSplitSource&lt;InputSplit&gt;) jobVertex.getInputSplitSource();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (splitSource != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span> <span class="variable">currentThread</span> <span class="operator">=</span> Thread.currentThread();</span><br><span class="line">            <span class="type">ClassLoader</span> <span class="variable">oldContextClassLoader</span> <span class="operator">=</span> currentThread.getContextClassLoader();</span><br><span class="line">            currentThread.setContextClassLoader(graph.getUserClassLoader());</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                inputSplits =</span><br><span class="line">                        splitSource.createInputSplits(<span class="built_in">this</span>.parallelismInfo.getParallelism());</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (inputSplits != <span class="literal">null</span>) &#123;</span><br><span class="line">                    splitAssigner = splitSource.getInputSplitAssigner(inputSplits);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                currentThread.setContextClassLoader(oldContextClassLoader);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            inputSplits = <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">JobException</span>(</span><br><span class="line">                <span class="string">&quot;Creating the input splits caused an error: &quot;</span> + t.getMessage(), t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在创建 ExecutionVertex 时，会创建 IntermediateResultPartition 和 Execution，创建 Execution 时，会设置 attemptNumber，这个值默认是0，如果 ExecutionVertex 是重新调度的，那么 attemptNumber 会自增加1。</p><p>ExecutionJobVertex.connectToPredecessors 方法主要是生成 ExecutionVertex 与 IntermediateResultPartition 的关联关系。这里设置关联关系也分成了点对点和全对全两种模式处理，点对点模式需要计算 ExecutionVertex 对应的 IntermediateResultPartition index 的范围。两种模式最终都调用了 connectInternal 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Connect all execution vertices to all partitions. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">connectInternal</span><span class="params">(</span></span><br><span class="line"><span class="params">        List&lt;ExecutionVertex&gt; taskVertices,</span></span><br><span class="line"><span class="params">        List&lt;IntermediateResultPartition&gt; partitions,</span></span><br><span class="line"><span class="params">        ResultPartitionType resultPartitionType,</span></span><br><span class="line"><span class="params">        EdgeManager edgeManager)</span> &#123;</span><br><span class="line">    checkState(!taskVertices.isEmpty());</span><br><span class="line">    checkState(!partitions.isEmpty());</span><br><span class="line"></span><br><span class="line">    <span class="type">ConsumedPartitionGroup</span> <span class="variable">consumedPartitionGroup</span> <span class="operator">=</span></span><br><span class="line">            createAndRegisterConsumedPartitionGroupToEdgeManager(</span><br><span class="line">                    taskVertices.size(), partitions, resultPartitionType, edgeManager);</span><br><span class="line">    <span class="keyword">for</span> (ExecutionVertex ev : taskVertices) &#123;</span><br><span class="line">        ev.addConsumedPartitionGroup(consumedPartitionGroup);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;ExecutionVertexID&gt; consumerVertices =</span><br><span class="line">            taskVertices.stream().map(ExecutionVertex::getID).collect(Collectors.toList());</span><br><span class="line">    <span class="type">ConsumerVertexGroup</span> <span class="variable">consumerVertexGroup</span> <span class="operator">=</span></span><br><span class="line">            ConsumerVertexGroup.fromMultipleVertices(consumerVertices, resultPartitionType);</span><br><span class="line">    <span class="keyword">for</span> (IntermediateResultPartition partition : partitions) &#123;</span><br><span class="line">        partition.addConsumers(consumerVertexGroup);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    consumedPartitionGroup.setConsumerVertexGroup(consumerVertexGroup);</span><br><span class="line">    consumerVertexGroup.setConsumedPartitionGroup(consumedPartitionGroup);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个方法中 ev.addConsumedPartitionGroup(consumedPartitionGroup); 负责将 ExecutionVertex 到 IntermediateResultPartition 的关联关系保存在 EdgeManager.vertexConsumedPartitions 中。</p><p>而 partition.addConsumers(consumerVertexGroup); 则负责将 IntermediateResultPartition 到 ExecutionVertex 的关系保存在 EdgeManager.partitionConsumers 中。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过本文，我们了解了 Flink 是如何将 JobGraph 转换成 ExecutionGraph 的。其中涉及到的一些核心概念名称比较类似，建议认真学习和理解透彻之后再研究其生成方法和对应关系，也可以借助前文中 ExecutionGraph 示意图辅助学习。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天我们一起来了解 Flink 最后一种执行图，ExecutionGraph 的执行过程。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：如何生成JobGraph</title>
    <link href="https://jackeyzhe.github.io/2025/09/18/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90JobGraph/"/>
    <id>https://jackeyzhe.github.io/2025/09/18/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90JobGraph/</id>
    <published>2025-09-18T14:02:40.000Z</published>
    <updated>2025-11-04T15:19:13.037Z</updated>
    
    <content type="html"><![CDATA[<p>前文我们介绍了 Flink 的四种执行图，并且通过源码了解了 Flink 的 StreamGraph 是怎么生成的，本文我们就一起来看下 Flink 的另一种执行图——JobGraph 是如何生成的。<span id="more"></span></p><h3 id="StreamGraph-和-JobGraph-的区别"><a href="#StreamGraph-和-JobGraph-的区别" class="headerlink" title="StreamGraph 和 JobGraph 的区别"></a>StreamGraph 和 JobGraph 的区别</h3><p>在正式开始之前，我们再来回顾一下 StreamGraph 和 JobGraph 的区别。假设我们的任务是建造一座大楼，StreamGraph 就像是设计蓝图，它描述了每个窗户、每根水管的位置和规格，而 JobGraph 像是给到施工队的施工流程图，它描述了每个任务模块，例如先把地基浇筑好，再铺设管线等。总的来说，JobGraph 更偏向执行层面，它是由 StreamGraph 优化而来。</p><p>回到 Flink 本身，我们通过一个表格来了解两个图的区别。</p><table><thead><tr><th></th><th>StreamGraph</th><th>JobGraph</th></tr></thead><tbody><tr><td>生成阶段</td><td>客户端，执行 execute() 时</td><td>客户端，提交前由 StreamGraph 转换生成</td></tr><tr><td>抽象层级</td><td>高层逻辑图，直接对应 API</td><td>优化后的执行图，为调度做准备</td></tr><tr><td>核心优化</td><td>无</td><td>主要是算子链优化</td></tr><tr><td>节点</td><td>StreamNode</td><td>JobVertex</td></tr><tr><td>边</td><td>StreamEdge</td><td>JobEdge</td></tr><tr><td>提交对象</td><td>无</td><td>提交给 JobManager</td></tr><tr><td>包含资源</td><td>无</td><td>包含执行作业所需的 Jar 包、依赖库和资源文件</td></tr></tbody></table><h3 id="JobVertex"><a href="#JobVertex" class="headerlink" title="JobVertex"></a>JobVertex</h3><p>JobGraph 中的节点是 JobVertex，在 StreamGraph 转换成 JobGraph 的过程中，会将多个节点串联起来，最终生成 JobVertex。</p><p>JobVertex包含以下成员变量：</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1758469798/Blog/flink/9/JobVertex.png" alt="JobVertex"></p><p>我们分别看一下这些成员变量及其作用。</p><h4 id="1、标识符相关"><a href="#1、标识符相关" class="headerlink" title="1、标识符相关"></a>1、标识符相关</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JobVertex的id，在作业执行过程中的唯一标识。监控、调度和故障恢复都会使用</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> JobVertexID id;</span><br><span class="line"></span><br><span class="line"><span class="comment">// operator id列表，按照深度优先顺序存储。operator 的管理、状态分配都会用到</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> List&lt;OperatorIDPair&gt; operatorIDs;</span><br></pre></td></tr></table></figure><h4 id="2、输入输出相关"><a href="#2、输入输出相关" class="headerlink" title="2、输入输出相关"></a>2、输入输出相关</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义所有的输入边</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> List&lt;JobEdge&gt; inputs = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义所有的输出数据集</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;IntermediateDataSetID, IntermediateDataSet&gt; results = <span class="keyword">new</span> <span class="title class_">LinkedHashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入分片源，主要用于批处理作业，定义如何将数据分成多个片</span></span><br><span class="line"><span class="keyword">private</span> InputSplitSource&lt;?&gt; inputSplitSource;</span><br></pre></td></tr></table></figure><h4 id="3、执行配置相关"><a href="#3、执行配置相关" class="headerlink" title="3、执行配置相关"></a>3、执行配置相关</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 并行度，即运行时拆分子任务数量，默认使用全局配置</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="variable">parallelism</span> <span class="operator">=</span> ExecutionConfig.PARALLELISM_DEFAULT;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最大并行度</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="variable">maxParallelism</span> <span class="operator">=</span> MAX_PARALLELISM_DEFAULT;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 存储运行时实际执行的类，使 Flink 可以灵活处理不同类型的操作符</span></span><br><span class="line"><span class="comment">// 流任务可以是&quot;org.apache.flink.streaming.runtime.tasks.StreamTask&quot;</span></span><br><span class="line"><span class="comment">// 批任务可以是&quot;org.apache.flink.runtime.operators.BatchTask&quot;</span></span><br><span class="line"><span class="keyword">private</span> String invokableClassName;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义配置</span></span><br><span class="line"><span class="keyword">private</span> Configuration configuration;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否是动态设置并发度</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">dynamicParallelism</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否支持优雅停止</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">isStoppable</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br></pre></td></tr></table></figure><h4 id="4、资源管理相关"><a href="#4、资源管理相关" class="headerlink" title="4、资源管理相关"></a>4、资源管理相关</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JobVertex 最小资源需求</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">ResourceSpec</span> <span class="variable">minResources</span> <span class="operator">=</span> ResourceSpec.DEFAULT;</span><br><span class="line"></span><br><span class="line"><span class="comment">// JobVertex 推荐资源需求</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">ResourceSpec</span> <span class="variable">preferredResources</span> <span class="operator">=</span> ResourceSpec.DEFAULT;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用于资源优化，运行不同的 JobVertex 的子任务运行在同一个 slot</span></span><br><span class="line"><span class="meta">@Nullable</span> <span class="keyword">private</span> SlotSharingGroup slotSharingGroup;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 需要严格共址的 JobVertex 组，每个 JobVertex 的第 n 个子任务运行在同一个 TaskManager</span></span><br><span class="line"><span class="meta">@Nullable</span> <span class="keyword">private</span> CoLocationGroupImpl coLocationGroup;</span><br></pre></td></tr></table></figure><h4 id="5、协调器"><a href="#5、协调器" class="headerlink" title="5、协调器"></a>5、协调器</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 操作符协调器，用于处理全局协调逻辑</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> List&lt;SerializedValue&lt;OperatorCoordinator.Provider&gt;&gt; operatorCoordinators =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br></pre></td></tr></table></figure><h4 id="6、显示和描述信息"><a href="#6、显示和描述信息" class="headerlink" title="6、显示和描述信息"></a>6、显示和描述信息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JobVertex 的名称</span></span><br><span class="line"><span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 操作符名称，比如 &#x27;Flat Map&#x27; 或 &#x27;Join&#x27;</span></span><br><span class="line"><span class="keyword">private</span> String operatorName;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 操作符的描述，比如 &#x27;Hash Join&#x27; 或 &#x27;Sorted Group Reduce&#x27;</span></span><br><span class="line"><span class="keyword">private</span> String operatorDescription;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 提供比 name 更友好的描述信息</span></span><br><span class="line"><span class="keyword">private</span> String operatorPrettyName;</span><br></pre></td></tr></table></figure><h4 id="7、状态和行为标志"><a href="#7、状态和行为标志" class="headerlink" title="7、状态和行为标志"></a>7、状态和行为标志</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 是否支持同一个子任务并发多次执行</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">supportsConcurrentExecutionAttempts</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 标记并发度是否被显式设置</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">parallelismConfigured</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否有阻塞型输出</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">anyOutputBlocking</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br></pre></td></tr></table></figure><h4 id="8、缓存数据集"><a href="#8、缓存数据集" class="headerlink" title="8、缓存数据集"></a>8、缓存数据集</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 存储该 JobVertex 需要消费的缓存中间数据集的 ID，可提高作业执行效率</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> List&lt;IntermediateDataSetID&gt; intermediateDataSetIdsToConsume = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br></pre></td></tr></table></figure><h3 id="JobEdge"><a href="#JobEdge" class="headerlink" title="JobEdge"></a>JobEdge</h3><p>在 StreamGraph 中，StreamEdge 是连接 StreamNode 的桥梁。在 JobGraph 中，与之对应的是 JobEdge，不同点在于  JobEdge 中保存的是输入节点和输出结果。</p><h4 id="1、连接关系成员"><a href="#1、连接关系成员" class="headerlink" title="1、连接关系成员"></a>1、连接关系成员</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义数据流向哪个 JobVertex</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> JobVertex target;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义这条边的源数据</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> IntermediateDataSet source;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入类型的编号</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> typeNumber;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 多个输入间的键是否相关，如果为 true，相同键的数据在一个输入被分割时，在其他数据对应的记录也会发送到相同的下游节点</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">boolean</span> interInputsKeysCorrelated;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同一输入内相同的键是否必须发送到同一下游任务</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">boolean</span> intraInputKeyCorrelated;</span><br></pre></td></tr></table></figure><h4 id="2、数据分发模式"><a href="#2、数据分发模式" class="headerlink" title="2、数据分发模式"></a>2、数据分发模式</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义数据在并行任务期间的分发模式</span></span><br><span class="line"><span class="comment">// 可能值：</span></span><br><span class="line"><span class="comment">// ALL_TO_ALL：全连接，每个上游子任务连接所有下游任务</span></span><br><span class="line"><span class="comment">// POINTWISE：点对点连接，一对一或一对多的本地连接</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> DistributionPattern distributionPattern;</span><br></pre></td></tr></table></figure><h4 id="3、数据传输策略"><a href="#3、数据传输策略" class="headerlink" title="3、数据传输策略"></a>3、数据传输策略</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 是否为广播连接</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">boolean</span> isBroadcast;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否为 forward 连接，forward 连接最高效，直接转发，无需序列化网络传输</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">boolean</span> isForward;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据传输策略名称，用于显示</span></span><br><span class="line"><span class="keyword">private</span> String shipStrategyName;</span><br></pre></td></tr></table></figure><h4 id="4、状态重分布映射器"><a href="#4、状态重分布映射器" class="headerlink" title="4、状态重分布映射器"></a>4、状态重分布映射器</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下游状态重分布映射器，当作业扩容时，决定是否重新分配下游算子的持久化状态</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">SubtaskStateMapper</span> <span class="variable">downstreamSubtaskStateMapper</span> <span class="operator">=</span> SubtaskStateMapper.ROUND_ROBIN;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 上游状态重分布映射器，当作业扩容时，决定是否重新分配上游算子的持久化状态</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">SubtaskStateMapper</span> <span class="variable">upstreamSubtaskStateMapper</span> <span class="operator">=</span> SubtaskStateMapper.ROUND_ROBIN;</span><br></pre></td></tr></table></figure><h4 id="5、描述和缓存信息"><a href="#5、描述和缓存信息" class="headerlink" title="5、描述和缓存信息"></a>5、描述和缓存信息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 预处理操作的名称</span></span><br><span class="line"><span class="keyword">private</span> String preProcessingOperationName;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 操作符级别缓存的描述</span></span><br><span class="line"><span class="keyword">private</span> String operatorLevelCachingDescription;</span><br></pre></td></tr></table></figure><h3 id="StreamGraph-转换成-JobGraph"><a href="#StreamGraph-转换成-JobGraph" class="headerlink" title="StreamGraph 转换成 JobGraph"></a>StreamGraph 转换成 JobGraph</h3><p>现在我们再来看一下 StreamGraph 是如何转换成 JobGraph 的。转换逻辑的入口是 StreamGraph.getJobGraph 方法。它只是调用了 StreamingJobGraphGenerator.createJobGraph，核心逻辑在 createJobGraph 方法中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> JobGraph <span class="title function_">createJobGraph</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 预验证，检查 StreamGraph 配置正确性</span></span><br><span class="line">    preValidate(streamGraph, userClassloader);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 【核心】链化操作符</span></span><br><span class="line">    setChaining();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (jobGraph.isDynamic()) &#123;</span><br><span class="line">        <span class="comment">// 支持动态扩缩容场景，为动态图设置并行度</span></span><br><span class="line">        setVertexParallelismsForDynamicGraphIfNecessary();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Note that we set all the non-chainable outputs configuration here because the</span></span><br><span class="line">    <span class="comment">// &quot;setVertexParallelismsForDynamicGraphIfNecessary&quot; may affect the parallelism of job</span></span><br><span class="line">    <span class="comment">// vertices and partition-reuse</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;Integer, Map&lt;StreamEdge, NonChainedOutput&gt;&gt; opIntermediateOutputs =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="comment">// 设置不能链化的输出边</span></span><br><span class="line">    setAllOperatorNonChainedOutputsConfigs(opIntermediateOutputs, jobVertexBuildContext);</span><br><span class="line">    setAllVertexNonChainedOutputsConfigs(opIntermediateOutputs);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置物理边连接</span></span><br><span class="line">    setPhysicalEdges(jobVertexBuildContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置支持并发执行的 JobVertex</span></span><br><span class="line">    markSupportingConcurrentExecutionAttempts(jobVertexBuildContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 验证混合 shuffle 模式只在批处理模式下使用</span></span><br><span class="line">    validateHybridShuffleExecuteInBatchMode(jobVertexBuildContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 Slot 共享和协同定位</span></span><br><span class="line">    setSlotSharingAndCoLocation(jobVertexBuildContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置托管内存比例</span></span><br><span class="line">    setManagedMemoryFraction(jobVertexBuildContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为 JobVertex 名称添加前缀</span></span><br><span class="line">    addVertexIndexPrefixInVertexName(jobVertexBuildContext, <span class="keyword">new</span> <span class="title class_">AtomicInteger</span>(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置操作符描述信息</span></span><br><span class="line">    setVertexDescription(jobVertexBuildContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wait for the serialization of operator coordinators and stream config.</span></span><br><span class="line">    <span class="comment">// 序列化操作符协调器和流配置</span></span><br><span class="line">    serializeOperatorCoordinatorsAndStreamConfig(serializationExecutor, jobVertexBuildContext);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jobGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，在 createJobGraph 方法中，调用了 setChaining 方法，即进行链化操作。这也是 JobGraph 最核心的优化之一。下面我们来看一下具体怎么做链化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">setChaining</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// we separate out the sources that run as inputs to another operator (chained inputs)</span></span><br><span class="line">    <span class="comment">// from the sources that needs to run as the main (head) operator.</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;Integer, OperatorChainInfo&gt; chainEntryPoints =</span><br><span class="line">            buildChainedInputsAndGetHeadInputs();</span><br><span class="line">    <span class="keyword">final</span> Collection&lt;OperatorChainInfo&gt; initialEntryPoints =</span><br><span class="line">            chainEntryPoints.entrySet().stream()</span><br><span class="line">                    .sorted(Comparator.comparing(Map.Entry::getKey))</span><br><span class="line">                    .map(Map.Entry::getValue)</span><br><span class="line">                    .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// iterate over a copy of the values, because this map gets concurrently modified</span></span><br><span class="line">    <span class="keyword">for</span> (OperatorChainInfo info : initialEntryPoints) &#123;</span><br><span class="line">        createChain(</span><br><span class="line">                info.getStartNodeId(),</span><br><span class="line">                <span class="number">1</span>, <span class="comment">// operators start at position 1 because 0 is for chained source inputs</span></span><br><span class="line">                info,</span><br><span class="line">                chainEntryPoints,</span><br><span class="line">                <span class="literal">true</span>,</span><br><span class="line">                serializationExecutor,</span><br><span class="line">                jobVertexBuildContext,</span><br><span class="line">                <span class="literal">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>setChaining 方法中主要分为两步，第一步是处理 Source 节点，将可以链化的 Source 和不能链化的 Source 节点分开。先来看如何判断一个 Source 是否可被链化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isChainableSource</span><span class="params">(StreamNode streamNode, StreamGraph streamGraph)</span> &#123;</span><br><span class="line">    <span class="comment">// 最基本的一些判空，输出边数量为1</span></span><br><span class="line">    <span class="keyword">if</span> (streamNode.getOperatorFactory() == <span class="literal">null</span></span><br><span class="line">            || !(streamNode.getOperatorFactory() <span class="keyword">instanceof</span> SourceOperatorFactory)</span><br><span class="line">            || streamNode.getOutEdges().size() != <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">StreamEdge</span> <span class="variable">sourceOutEdge</span> <span class="operator">=</span> streamNode.getOutEdges().get(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">StreamNode</span> <span class="variable">target</span> <span class="operator">=</span> streamGraph.getStreamNode(sourceOutEdge.getTargetId());</span><br><span class="line">    <span class="keyword">final</span> <span class="type">ChainingStrategy</span> <span class="variable">targetChainingStrategy</span> <span class="operator">=</span></span><br><span class="line">            Preconditions.checkNotNull(target.getOperatorFactory()).getChainingStrategy();</span><br><span class="line">    <span class="comment">// 链化策略必须 HEAD_WITH_SOURCES，输出边是可链化的</span></span><br><span class="line">    <span class="keyword">return</span> targetChainingStrategy == ChainingStrategy.HEAD_WITH_SOURCES</span><br><span class="line">            &amp;&amp; isChainableInput(sourceOutEdge, streamGraph, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isChainableInput</span><span class="params">(</span></span><br><span class="line"><span class="params">        StreamEdge edge, StreamGraph streamGraph, <span class="type">boolean</span> allowChainWithDefaultParallelism)</span> &#123;</span><br><span class="line">    <span class="type">StreamNode</span> <span class="variable">upStreamVertex</span> <span class="operator">=</span> streamGraph.getSourceVertex(edge);</span><br><span class="line">    <span class="type">StreamNode</span> <span class="variable">downStreamVertex</span> <span class="operator">=</span> streamGraph.getTargetVertex(edge);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!(streamGraph.isChainingEnabled()</span><br><span class="line">            <span class="comment">// 上下游节点是否在同一个 slot 共享组</span></span><br><span class="line">            &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex)</span><br><span class="line">            <span class="comment">// 操作符是否可以链化，主要做并行度检查</span></span><br><span class="line">            &amp;&amp; areOperatorsChainable(</span><br><span class="line">                    upStreamVertex,</span><br><span class="line">                    downStreamVertex,</span><br><span class="line">                    streamGraph,</span><br><span class="line">                    allowChainWithDefaultParallelism)</span><br><span class="line">            <span class="comment">// 分区器和交换模式是否支持链化</span></span><br><span class="line">            &amp;&amp; arePartitionerAndExchangeModeChainable(</span><br><span class="line">                    edge.getPartitioner(), edge.getExchangeMode(), streamGraph.isDynamic()))) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check that we do not have a union operation, because unions currently only work</span></span><br><span class="line">    <span class="comment">// through the network/byte-channel stack.</span></span><br><span class="line">    <span class="comment">// we check that by testing that each &quot;type&quot; (which means input position) is used only once</span></span><br><span class="line">    <span class="comment">// 检查是否为 Union 操作，Union 操作不能链化</span></span><br><span class="line">    <span class="keyword">for</span> (StreamEdge inEdge : downStreamVertex.getInEdges()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (inEdge != edge &amp;&amp; inEdge.getTypeNumber() == edge.getTypeNumber()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Source 的链化条件主要就是这些，我们结合一些例子来看一下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">Source(并行度=4) -&gt; Map(并行度=4) -&gt; Filter(并行度=4)</span><br><span class="line"></span><br><span class="line">Source -&gt; Map 边：</span><br><span class="line">1. isChainingEnabled() = true</span><br><span class="line">2. isSameSlotSharingGroup() = true (都在默认组)</span><br><span class="line">3. areOperatorsChainable() = true (Source可链化，Map是HEAD_WITH_SOURCES)</span><br><span class="line">4. arePartitionerAndExchangeModeChainable() = true (ForwardPartitioner)</span><br><span class="line">5. Union检查通过</span><br><span class="line">结果：可链化</span><br><span class="line"></span><br><span class="line">Map -&gt; Filter 边：</span><br><span class="line">1. isChainingEnabled() = true</span><br><span class="line">2. isSameSlotSharingGroup() = true</span><br><span class="line">3. areOperatorsChainable() = true (Map和Filter都是ALWAYS)</span><br><span class="line">4. arePartitionerAndExchangeModeChainable() = true (ForwardPartitioner)</span><br><span class="line">5. Union检查通过</span><br><span class="line">结果：可链化</span><br><span class="line"></span><br><span class="line">最终：Source -&gt; Map -&gt; Filter 三者链化到一个JobVertex中</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Source(并行度=2) -&gt; Map(并行度=4)  // 并行度不匹配</span><br><span class="line"></span><br><span class="line">Source -&gt; Map 边：</span><br><span class="line">1. isChainingEnabled() = true</span><br><span class="line">2. isSameSlotSharingGroup() = true</span><br><span class="line">3. areOperatorsChainable() = false (并行度不匹配)</span><br><span class="line">结果：不可链化，需要网络传输</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Source1 --\</span><br><span class="line">          Union -&gt; Map</span><br><span class="line">Source2 --/</span><br><span class="line"></span><br><span class="line">Source1 -&gt; Union 边：</span><br><span class="line">虽然满足前4个条件，但Union节点有两个输入边，typeNumber相同</span><br><span class="line">Union检查失败，不可链化</span><br></pre></td></tr></table></figure><p>得到了所有入口之后，就可以进行后续节点的链化操作了，它的逻辑在 createChain 方法中。这里主要是一个递归过程，先将节点的输出边分为可链化和不可链化两个 list，之后对可链化的边进行递归调用链化。对不可链化的边，需要创建出新的链。由于篇幅原因，这里只贴一部分核心的代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;StreamEdge&gt; <span class="title function_">createChain</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> Integer currentNodeId,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> <span class="type">int</span> chainIndex,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> OperatorChainInfo chainInfo,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> Map&lt;Integer, OperatorChainInfo&gt; chainEntryPoints,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> <span class="type">boolean</span> canCreateNewChain,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> Executor serializationExecutor,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> JobVertexBuildContext jobVertexBuildContext,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> <span class="meta">@Nullable</span> Consumer&lt;Integer&gt; visitedStreamNodeConsumer)</span> &#123;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拆分可链化边和不可链化边</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge outEdge : currentNode.getOutEdges()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isChainable(outEdge, streamGraph)) &#123;</span><br><span class="line">                chainableOutputs.add(outEdge);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                nonChainableOutputs.add(outEdge);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理可链化边</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge chainable : chainableOutputs) &#123;</span><br><span class="line">            <span class="type">StreamNode</span> <span class="variable">targetNode</span> <span class="operator">=</span> streamGraph.getStreamNode(chainable.getTargetId());</span><br><span class="line">            <span class="type">Attribute</span> <span class="variable">targetNodeAttribute</span> <span class="operator">=</span> targetNode.getAttribute();</span><br><span class="line">            <span class="keyword">if</span> (isNoOutputUntilEndOfInput) &#123;</span><br><span class="line">                <span class="keyword">if</span> (targetNodeAttribute != <span class="literal">null</span>) &#123;</span><br><span class="line">                    targetNodeAttribute.setNoOutputUntilEndOfInput(<span class="literal">true</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            transitiveOutEdges.addAll(</span><br><span class="line">                    createChain(</span><br><span class="line">                            chainable.getTargetId(),</span><br><span class="line">                            chainIndex + <span class="number">1</span>,</span><br><span class="line">                            chainInfo,</span><br><span class="line">                            chainEntryPoints,</span><br><span class="line">                            canCreateNewChain,</span><br><span class="line">                            serializationExecutor,</span><br><span class="line">                            jobVertexBuildContext,</span><br><span class="line">                            visitedStreamNodeConsumer));</span><br><span class="line">            <span class="comment">// Mark upstream nodes in the same chain as outputBlocking</span></span><br><span class="line">            <span class="keyword">if</span> (targetNodeAttribute != <span class="literal">null</span></span><br><span class="line">                    &amp;&amp; targetNodeAttribute.isNoOutputUntilEndOfInput()) &#123;</span><br><span class="line">                currentNodeAttribute.setNoOutputUntilEndOfInput(<span class="literal">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理不可链化边</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge nonChainable : nonChainableOutputs) &#123;</span><br><span class="line">            transitiveOutEdges.add(nonChainable);</span><br><span class="line">            <span class="comment">// Used to control whether a new chain can be created, this value is true in the</span></span><br><span class="line">            <span class="comment">// full graph generation algorithm and false in the progressive generation</span></span><br><span class="line">            <span class="comment">// algorithm. In the future, this variable can be a boolean type function to adapt</span></span><br><span class="line">            <span class="comment">// to more adaptive scenarios.</span></span><br><span class="line">            <span class="keyword">if</span> (canCreateNewChain) &#123;</span><br><span class="line">                createChain(</span><br><span class="line">                        nonChainable.getTargetId(),</span><br><span class="line">                        <span class="number">1</span>, <span class="comment">// operators start at position 1 because 0 is for chained source</span></span><br><span class="line">                        <span class="comment">// inputs</span></span><br><span class="line">                        chainEntryPoints.computeIfAbsent(</span><br><span class="line">                                nonChainable.getTargetId(),</span><br><span class="line">                                (k) -&gt; chainInfo.newChain(nonChainable.getTargetId())),</span><br><span class="line">                        chainEntryPoints,</span><br><span class="line">                        canCreateNewChain,</span><br><span class="line">                        serializationExecutor,</span><br><span class="line">                        jobVertexBuildContext,</span><br><span class="line">                        visitedStreamNodeConsumer);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 JobVertex</span></span><br><span class="line">        StreamConfig config;</span><br><span class="line">        <span class="keyword">if</span> (currentNodeId.equals(startNodeId)) &#123;</span><br><span class="line">            <span class="type">JobVertex</span> <span class="variable">jobVertex</span> <span class="operator">=</span> jobVertexBuildContext.getJobVertex(startNodeId);</span><br><span class="line">            <span class="keyword">if</span> (jobVertex == <span class="literal">null</span>) &#123;</span><br><span class="line">                jobVertex =</span><br><span class="line">                        createJobVertex(</span><br><span class="line">                                chainInfo, serializationExecutor, jobVertexBuildContext);</span><br><span class="line">            &#125;</span><br><span class="line">            config = <span class="keyword">new</span> <span class="title class_">StreamConfig</span>(jobVertex.getConfiguration());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            config = <span class="keyword">new</span> <span class="title class_">StreamConfig</span>(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否为起始节点，如果不是，将对应的配置信息存到链化起始节点的 key 中</span></span><br><span class="line">        <span class="keyword">if</span> (currentNodeId.equals(startNodeId)) &#123;</span><br><span class="line">            chainInfo.setTransitiveOutEdges(transitiveOutEdges);</span><br><span class="line">            jobVertexBuildContext.addChainInfo(startNodeId, chainInfo);</span><br><span class="line"></span><br><span class="line">            config.setChainStart();</span><br><span class="line">            config.setChainIndex(chainIndex);</span><br><span class="line">            config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName());</span><br><span class="line">            config.setTransitiveChainedTaskConfigs(</span><br><span class="line">                    jobVertexBuildContext.getChainedConfigs().get(startNodeId));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            config.setChainIndex(chainIndex);</span><br><span class="line">            <span class="type">StreamNode</span> <span class="variable">node</span> <span class="operator">=</span> streamGraph.getStreamNode(currentNodeId);</span><br><span class="line">            config.setOperatorName(node.getOperatorName());</span><br><span class="line">            jobVertexBuildContext</span><br><span class="line">                    .getOrCreateChainedConfig(startNodeId)</span><br><span class="line">                    .put(currentNodeId, config);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>是否可链化依赖于 isChainable 方法的结果。它主要判断了下游的输入边数量是否为1，然后调用了 isChainableInput，这个方法我们刚刚已经看过了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isChainable</span><span class="params">(StreamEdge edge, StreamGraph streamGraph)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> isChainable(edge, streamGraph, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isChainable</span><span class="params">(</span></span><br><span class="line"><span class="params">        StreamEdge edge, StreamGraph streamGraph, <span class="type">boolean</span> allowChainWithDefaultParallelism)</span> &#123;</span><br><span class="line">    <span class="type">StreamNode</span> <span class="variable">downStreamVertex</span> <span class="operator">=</span> streamGraph.getTargetVertex(edge);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> downStreamVertex.getInEdges().size() == <span class="number">1</span></span><br><span class="line">            &amp;&amp; isChainableInput(edge, streamGraph, allowChainWithDefaultParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们主要介绍了生成 JobGraph 的相关代码。首先了解了 JobGraph 中的节点和边对应的类，以及它们和 StreamGraph 中的类的映射关系。然后又看了生成 JobGraph 的核心代码，其中重点学习了链化相关的代码。</p><p>最后补充一个生成 JobGraph 的调用链路，感兴趣的同学可以看下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clusterClient.submitJob() → MiniCluster.submitJob() → Dispatcher.submitJob() → JobMasterServiceLeadershipRunnerFactory → DefaultJobMasterServiceFactory → JobMaster → DefaultSchedulerFactory.createInstance()→ StreamGraph.getJobGraph()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;前文我们介绍了 Flink 的四种执行图，并且通过源码了解了 Flink 的 StreamGraph 是怎么生成的，本文我们就一起来看下 Flink 的另一种执行图——JobGraph 是如何生成的。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码阅读：如何生成StreamGraph</title>
    <link href="https://jackeyzhe.github.io/2025/09/10/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90StreamGraph/"/>
    <id>https://jackeyzhe.github.io/2025/09/10/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90StreamGraph/</id>
    <published>2025-09-10T13:55:59.000Z</published>
    <updated>2025-09-16T16:50:56.288Z</updated>
    
    <content type="html"><![CDATA[<p>Flink 中有四种执行图，分别是 StreamGraph、JobGraph、ExecutionGraph 和 Physical Graph。今天我们来看下我们编写的 Flink 程序代码是如何生成 StreamGraph 的。<span id="more"></span></p><p>在开始读代码之前，我们先来简单介绍一下四种图之间的关系和区别。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1757598202/Blog/flink/8/flink_graph.png" alt="flink_graph"></p><p>StreamGraph 是根据用户用 Stream API 编写的代码生成的图，用来表示整个程序的拓扑结构。</p><p>JobGraph 是由 StreamGraph 生成的，它在 StreamGraph 的基础上，对链化了部分算子，将其合并成为一个节点，减少数据在节点之间传输时序列化和反序列化这些消耗。</p><p>ExecutionGraph 是由 JobGraph 生成的，它的主要特点是并行，将多并发的节点拆分。</p><p>PhysicalGraph 是 ExecutionGraph 实际部署后的图，它并不是一种数据结构。</p><h3 id="StreamExecutionEnvironment"><a href="#StreamExecutionEnvironment" class="headerlink" title="StreamExecutionEnvironment"></a>StreamExecutionEnvironment</h3><p>OK，了解了 Flink 四种执行图之后，我们就正式开始源码探索了。首先从 StreamExecutionEnvironment 入手，在编写 Flink 程序时，它是必不可少的一个类。它提供了一系列方法来配置流处理程序的执行环境（如并行度、Checkpoint 配置、时间属性等）。</p><p>本文我们主要关注 StreamGraph 的生成，首先是数据流的入口，即 Source 节点。在 StreamExecutionEnvironment 中有 addSource 和 fromSource 等方法，它们用来定义从哪个数据源读取数据，然后返回一个 DataStreamSource （继承自 DataStream），得到 DataStream 之后，它会在各个算子之间流转，最终到 Sink 端输出。</p><p>我们从 addSource 方法入手，addSource 方法中主要做了三件事：</p><p>1、处理数据类型，优先使用用户执行的数据类型，也可以自动推断</p><p>2、闭包清理，使用户传入的 function 能被序列化并发布到分布式环境执行</p><p>3、创建 DataStreamSource 并返回</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;OUT&gt; DataStreamSource&lt;OUT&gt; <span class="title function_">addSource</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> SourceFunction&lt;OUT&gt; function,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> String sourceName,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nullable</span> <span class="keyword">final</span> TypeInformation&lt;OUT&gt; typeInfo,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> Boundedness boundedness)</span> &#123;</span><br><span class="line">    checkNotNull(function);</span><br><span class="line">    checkNotNull(sourceName);</span><br><span class="line">    checkNotNull(boundedness);</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;OUT&gt; resolvedTypeInfo =</span><br><span class="line">            getTypeInfo(function, sourceName, SourceFunction.class, typeInfo);</span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">isParallel</span> <span class="operator">=</span> function <span class="keyword">instanceof</span> ParallelSourceFunction;</span><br><span class="line"></span><br><span class="line">    clean(function);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> StreamSource&lt;OUT, ?&gt; sourceOperator = <span class="keyword">new</span> <span class="title class_">StreamSource</span>&lt;&gt;(function);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">DataStreamSource</span>&lt;&gt;(</span><br><span class="line">            <span class="built_in">this</span>, resolvedTypeInfo, sourceOperator, isParallel, sourceName, boundedness);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在我们有了 DataStream 了，那如何知道后续要进行哪些转换逻辑呢？答案在 transformations 这个变量中，它保存了后续所有的转换。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> List&lt;Transformation&lt;?&gt;&gt; transformations = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br></pre></td></tr></table></figure><h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>我们来看 Transformation 是如何生成和描述 DataStream 的转换流程的。以最常见的 map 方法为例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title function_">map</span><span class="params">(</span></span><br><span class="line"><span class="params">        MapFunction&lt;T, R&gt; mapper, TypeInformation&lt;R&gt; outputType)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">&quot;Map&quot;</span>, outputType, <span class="keyword">new</span> <span class="title class_">StreamMap</span>&lt;&gt;(clean(mapper)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它调用了 transform 方法，transform 又调用了 doTransform 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title function_">doTransform</span><span class="params">(</span></span><br><span class="line"><span class="params">        String operatorName,</span></span><br><span class="line"><span class="params">        TypeInformation&lt;R&gt; outTypeInfo,</span></span><br><span class="line"><span class="params">        StreamOperatorFactory&lt;R&gt; operatorFactory)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// read the output type of the input Transform to coax out errors about MissingTypeInfo</span></span><br><span class="line">    transformation.getOutputType();</span><br><span class="line"></span><br><span class="line">    OneInputTransformation&lt;T, R&gt; resultTransform =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">OneInputTransformation</span>&lt;&gt;(</span><br><span class="line">                    <span class="built_in">this</span>.transformation,</span><br><span class="line">                    operatorName,</span><br><span class="line">                    operatorFactory,</span><br><span class="line">                    outTypeInfo,</span><br><span class="line">                    environment.getParallelism(),</span><br><span class="line">                    <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings(&#123;&quot;unchecked&quot;, &quot;rawtypes&quot;&#125;)</span></span><br><span class="line">    SingleOutputStreamOperator&lt;R&gt; returnStream =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">SingleOutputStreamOperator</span>(environment, resultTransform);</span><br><span class="line"></span><br><span class="line">    getExecutionEnvironment().addOperator(resultTransform);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> returnStream;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 doTransform 方法中，就是创建 Transformation 和 SingleOutputStreamOperator（DataStream 的一个子类），然后调用 addOperator 方法将 transform 存到 StreamExecutionEnviroment 中的 transformations 变量中。</p><p>每个 Transformation 都有 id、name、parallelism 和 slotSharingGroup 等信息。其子类也记录有输入信息，如 OneInputTransformation 和 TwoInputTransformation。</p><h3 id="StreamOperator"><a href="#StreamOperator" class="headerlink" title="StreamOperator"></a>StreamOperator</h3><p>我们在调用 map 方法时，会传入一个自定义的处理函数，它也会保存在 Transformation 中。在 Flink 中定义了 StreamOperator 方法来抽象这类处理函数。在 map 方法中，它将我们传入的函数转成了 StreamMap，它继承了 AbstractUdfStreamOperator，同时实现了 OneInputStreamOperator 接口。</p><p>StreamOperator 定义了对算子生命周期管理的函数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">open</span><span class="params">()</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">finish</span><span class="params">()</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OperatorSnapshotFutures <span class="title function_">snapshotState</span><span class="params">(</span></span><br><span class="line"><span class="params">            <span class="type">long</span> checkpointId,</span></span><br><span class="line"><span class="params">            <span class="type">long</span> timestamp,</span></span><br><span class="line"><span class="params">            CheckpointOptions checkpointOptions,</span></span><br><span class="line"><span class="params">            CheckpointStreamFactory storageLocation)</span></span><br><span class="line">            <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(StreamTaskStateInitializer streamTaskStateManager)</span> <span class="keyword">throws</span> Exception;</span><br></pre></td></tr></table></figure><p>OneInputStreamOperator 是 StreamOperator 的子接口。在其基础上增加了对具体元素的处理，主要是对 key 的提取。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title function_">setKeyContextElement</span><span class="params">(StreamRecord&lt;IN&gt; record)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    setKeyContextElement1(record);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AbstractUdfStreamOperator 则是提供了对自定义函数生命周期管理的实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="built_in">super</span>.open();</span><br><span class="line">    FunctionUtils.openFunction(userFunction, DefaultOpenContext.INSTANCE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">finish</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="built_in">super</span>.finish();</span><br><span class="line">    <span class="keyword">if</span> (userFunction <span class="keyword">instanceof</span> SinkFunction) &#123;</span><br><span class="line">        ((SinkFunction&lt;?&gt;) userFunction).finish();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="built_in">super</span>.close();</span><br><span class="line">    FunctionUtils.closeFunction(userFunction);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>到这里，我们就知道了 Flink 中 DataStream 是如何转换的。处理逻辑保存在 Transformation 中。下面我们来看一组 Transformation 是如何生成 StreamGraph 的。</p><h3 id="StreamGraph"><a href="#StreamGraph" class="headerlink" title="StreamGraph"></a>StreamGraph</h3><p>生成 StreamGraph 的入口在 <code>org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#generateStreamGraph</code> 。</p><p>在 generate 方法中，会遍历所有 Transformation 并调用 transform 方法。在调用节点的 transform 方法之前，会先确保它的输入节点都已经转换成功。</p><p>目前定义了以下 Transformation：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;rawtypes&quot;)</span></span><br><span class="line">    Map&lt;Class&lt;? <span class="keyword">extends</span> <span class="title class_">Transformation</span>&gt;, TransformationTranslator&lt;?, ? <span class="keyword">extends</span> <span class="title class_">Transformation</span>&gt;&gt;</span><br><span class="line">            tmp = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    tmp.put(OneInputTransformation.class, <span class="keyword">new</span> <span class="title class_">OneInputTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(TwoInputTransformation.class, <span class="keyword">new</span> <span class="title class_">TwoInputTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(MultipleInputTransformation.class, <span class="keyword">new</span> <span class="title class_">MultiInputTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(KeyedMultipleInputTransformation.class, <span class="keyword">new</span> <span class="title class_">MultiInputTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(SourceTransformation.class, <span class="keyword">new</span> <span class="title class_">SourceTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(SinkTransformation.class, <span class="keyword">new</span> <span class="title class_">SinkTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(GlobalCommitterTransform.class, <span class="keyword">new</span> <span class="title class_">GlobalCommitterTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(LegacySinkTransformation.class, <span class="keyword">new</span> <span class="title class_">LegacySinkTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(LegacySourceTransformation.class, <span class="keyword">new</span> <span class="title class_">LegacySourceTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(UnionTransformation.class, <span class="keyword">new</span> <span class="title class_">UnionTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(StubTransformation.class, <span class="keyword">new</span> <span class="title class_">StubTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(PartitionTransformation.class, <span class="keyword">new</span> <span class="title class_">PartitionTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(SideOutputTransformation.class, <span class="keyword">new</span> <span class="title class_">SideOutputTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(ReduceTransformation.class, <span class="keyword">new</span> <span class="title class_">ReduceTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(</span><br><span class="line">            TimestampsAndWatermarksTransformation.class,</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">TimestampsAndWatermarksTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(BroadcastStateTransformation.class, <span class="keyword">new</span> <span class="title class_">BroadcastStateTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(</span><br><span class="line">            KeyedBroadcastStateTransformation.class,</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">KeyedBroadcastStateTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    tmp.put(CacheTransformation.class, <span class="keyword">new</span> <span class="title class_">CacheTransformationTranslator</span>&lt;&gt;());</span><br><span class="line">    translatorMap = Collections.unmodifiableMap(tmp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Flink 会根据不同的 Transformation 类调用其 translateInternal 方法。在 translateInternal 方法中就会去添加节点和边。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">streamGraph.addOperator(</span><br><span class="line">        transformationId,</span><br><span class="line">        slotSharingGroup,</span><br><span class="line">        transformation.getCoLocationGroupKey(),</span><br><span class="line">        operatorFactory,</span><br><span class="line">        inputType,</span><br><span class="line">        transformation.getOutputType(),</span><br><span class="line">        transformation.getName());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Integer inputId : context.getStreamNodeIds(parentTransformations.get(<span class="number">0</span>))) &#123;</span><br><span class="line">    streamGraph.addEdge(inputId, transformationId, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 addOperator 方法中，它通过调用 addNode 来创建 StreamNode。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> StreamNode <span class="title function_">addNode</span><span class="params">(</span></span><br><span class="line"><span class="params">        Integer vertexID,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nullable</span> String slotSharingGroup,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nullable</span> String coLocationGroup,</span></span><br><span class="line"><span class="params">        Class&lt;? extends TaskInvokable&gt; vertexClass,</span></span><br><span class="line"><span class="params">        <span class="meta">@Nullable</span> StreamOperatorFactory&lt;?&gt; operatorFactory,</span></span><br><span class="line"><span class="params">        String operatorName)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (streamNodes.containsKey(vertexID)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;Duplicate vertexID &quot;</span> + vertexID);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">StreamNode</span> <span class="variable">vertex</span> <span class="operator">=</span></span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">StreamNode</span>(</span><br><span class="line">                    vertexID,</span><br><span class="line">                    slotSharingGroup,</span><br><span class="line">                    coLocationGroup,</span><br><span class="line">                    operatorFactory,</span><br><span class="line">                    operatorName,</span><br><span class="line">                    vertexClass);</span><br><span class="line"></span><br><span class="line">    streamNodes.put(vertexID, vertex);</span><br><span class="line">    isEmpty = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vertex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 addEdgeInternal 方法中，对于 sideOutput 和 partition 这类虚拟节点，会先解析出原始节点，再建立实际的边。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">addEdgeInternal</span><span class="params">(</span></span><br><span class="line"><span class="params">        Integer upStreamVertexID,</span></span><br><span class="line"><span class="params">        Integer downStreamVertexID,</span></span><br><span class="line"><span class="params">        <span class="type">int</span> typeNumber,</span></span><br><span class="line"><span class="params">        StreamPartitioner&lt;?&gt; partitioner,</span></span><br><span class="line"><span class="params">        List&lt;String&gt; outputNames,</span></span><br><span class="line"><span class="params">        OutputTag outputTag,</span></span><br><span class="line"><span class="params">        StreamExchangeMode exchangeMode,</span></span><br><span class="line"><span class="params">        IntermediateDataSetID intermediateDataSetId)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (virtualSideOutputNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">virtualId</span> <span class="operator">=</span> upStreamVertexID;</span><br><span class="line">        upStreamVertexID = virtualSideOutputNodes.get(virtualId).f0;</span><br><span class="line">        <span class="keyword">if</span> (outputTag == <span class="literal">null</span>) &#123;</span><br><span class="line">            outputTag = virtualSideOutputNodes.get(virtualId).f1;</span><br><span class="line">        &#125;</span><br><span class="line">        addEdgeInternal(</span><br><span class="line">                upStreamVertexID,</span><br><span class="line">                downStreamVertexID,</span><br><span class="line">                typeNumber,</span><br><span class="line">                partitioner,</span><br><span class="line">                <span class="literal">null</span>,</span><br><span class="line">                outputTag,</span><br><span class="line">                exchangeMode,</span><br><span class="line">                intermediateDataSetId);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (virtualPartitionNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">virtualId</span> <span class="operator">=</span> upStreamVertexID;</span><br><span class="line">        upStreamVertexID = virtualPartitionNodes.get(virtualId).f0;</span><br><span class="line">        <span class="keyword">if</span> (partitioner == <span class="literal">null</span>) &#123;</span><br><span class="line">            partitioner = virtualPartitionNodes.get(virtualId).f1;</span><br><span class="line">        &#125;</span><br><span class="line">        exchangeMode = virtualPartitionNodes.get(virtualId).f2;</span><br><span class="line">        addEdgeInternal(</span><br><span class="line">                upStreamVertexID,</span><br><span class="line">                downStreamVertexID,</span><br><span class="line">                typeNumber,</span><br><span class="line">                partitioner,</span><br><span class="line">                outputNames,</span><br><span class="line">                outputTag,</span><br><span class="line">                exchangeMode,</span><br><span class="line">                intermediateDataSetId);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        createActualEdge(</span><br><span class="line">                upStreamVertexID,</span><br><span class="line">                downStreamVertexID,</span><br><span class="line">                typeNumber,</span><br><span class="line">                partitioner,</span><br><span class="line">                outputTag,</span><br><span class="line">                exchangeMode,</span><br><span class="line">                intermediateDataSetId);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后根据两个物理节点创建 StreamEdge 进行连接。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">createActualEdge</span><span class="params">(</span></span><br><span class="line"><span class="params">        Integer upStreamVertexID,</span></span><br><span class="line"><span class="params">        Integer downStreamVertexID,</span></span><br><span class="line"><span class="params">        <span class="type">int</span> typeNumber,</span></span><br><span class="line"><span class="params">        StreamPartitioner&lt;?&gt; partitioner,</span></span><br><span class="line"><span class="params">        OutputTag outputTag,</span></span><br><span class="line"><span class="params">        StreamExchangeMode exchangeMode,</span></span><br><span class="line"><span class="params">        IntermediateDataSetID intermediateDataSetId)</span> &#123;</span><br><span class="line">    <span class="type">StreamNode</span> <span class="variable">upstreamNode</span> <span class="operator">=</span> getStreamNode(upStreamVertexID);</span><br><span class="line">    <span class="type">StreamNode</span> <span class="variable">downstreamNode</span> <span class="operator">=</span> getStreamNode(downStreamVertexID);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If no partitioner was specified and the parallelism of upstream and downstream</span></span><br><span class="line">    <span class="comment">// operator matches use forward partitioning, use rebalance otherwise.</span></span><br><span class="line">    <span class="keyword">if</span> (partitioner == <span class="literal">null</span></span><br><span class="line">            &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">        partitioner =</span><br><span class="line">                dynamic ? <span class="keyword">new</span> <span class="title class_">ForwardForUnspecifiedPartitioner</span>&lt;&gt;() : <span class="keyword">new</span> <span class="title class_">ForwardPartitioner</span>&lt;&gt;();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner == <span class="literal">null</span>) &#123;</span><br><span class="line">        partitioner = <span class="keyword">new</span> <span class="title class_">RebalancePartitioner</span>&lt;Object&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</span><br><span class="line">        <span class="keyword">if</span> (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardForConsecutiveHashPartitioner) &#123;</span><br><span class="line">                partitioner =</span><br><span class="line">                        ((ForwardForConsecutiveHashPartitioner&lt;?&gt;) partitioner)</span><br><span class="line">                                .getHashPartitioner();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UnsupportedOperationException</span>(</span><br><span class="line">                        <span class="string">&quot;Forward partitioning does not allow &quot;</span></span><br><span class="line">                                + <span class="string">&quot;change of parallelism. Upstream operation: &quot;</span></span><br><span class="line">                                + upstreamNode</span><br><span class="line">                                + <span class="string">&quot; parallelism: &quot;</span></span><br><span class="line">                                + upstreamNode.getParallelism()</span><br><span class="line">                                + <span class="string">&quot;, downstream operation: &quot;</span></span><br><span class="line">                                + downstreamNode</span><br><span class="line">                                + <span class="string">&quot; parallelism: &quot;</span></span><br><span class="line">                                + downstreamNode.getParallelism()</span><br><span class="line">                                + <span class="string">&quot; You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (exchangeMode == <span class="literal">null</span>) &#123;</span><br><span class="line">        exchangeMode = StreamExchangeMode.UNDEFINED;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Just make sure that &#123;<span class="doctag">@link</span> StreamEdge&#125; connecting same nodes (for example as a result of</span></span><br><span class="line"><span class="comment">     * self unioning a &#123;<span class="doctag">@link</span> DataStream&#125;) are distinct and unique. Otherwise it would be</span></span><br><span class="line"><span class="comment">     * difficult on the &#123;<span class="doctag">@link</span> StreamTask&#125; to assign &#123;<span class="doctag">@link</span> RecordWriter&#125;s to correct &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     * StreamEdge&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">uniqueId</span> <span class="operator">=</span> getStreamEdges(upstreamNode.getId(), downstreamNode.getId()).size();</span><br><span class="line"></span><br><span class="line">    <span class="type">StreamEdge</span> <span class="variable">edge</span> <span class="operator">=</span></span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">StreamEdge</span>(</span><br><span class="line">                    upstreamNode,</span><br><span class="line">                    downstreamNode,</span><br><span class="line">                    typeNumber,</span><br><span class="line">                    partitioner,</span><br><span class="line">                    outputTag,</span><br><span class="line">                    exchangeMode,</span><br><span class="line">                    uniqueId,</span><br><span class="line">                    intermediateDataSetId);</span><br><span class="line"></span><br><span class="line">    getStreamNode(edge.getSourceId()).addOutEdge(edge);</span><br><span class="line">    getStreamNode(edge.getTargetId()).addInEdge(edge);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过 StreamNode 和 StreamEdge，就可以得到所有的节点和边，也就是我们的 StreamGraph 就创建完成了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文先介绍了 Flink 的四种执行图以及它们之间的关系。接着又通过源码探索了 StreamGraph 的生成逻辑，Flink 将处理 逻辑保存在 Transformation 中，又由 Transformation 生成了 StreamGraph。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink 中有四种执行图，分别是 StreamGraph、JobGraph、ExecutionGraph 和 Physical Graph。今天我们来看下我们编写的 Flink 程序代码是如何生成 StreamGraph 的。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink学习笔记：反压</title>
    <link href="https://jackeyzhe.github.io/2025/08/26/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%8E%8B/"/>
    <id>https://jackeyzhe.github.io/2025/08/26/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%8E%8B/</id>
    <published>2025-08-26T15:13:46.000Z</published>
    <updated>2025-09-07T14:17:06.951Z</updated>
    
    <content type="html"><![CDATA[<p>今天来聊在 Flink 运维过程中比较常见的一个问题：反压。<span id="more"></span></p><h3 id="什么是反压"><a href="#什么是反压" class="headerlink" title="什么是反压"></a>什么是反压</h3><p>反压是流式系统中关于数据处理能力的动态反馈机制，并且是从下游到上游的反馈，一般发生在上游节点的生产速度大于下游节点的消费速度的情况。</p><h3 id="数据如何传输"><a href="#数据如何传输" class="headerlink" title="数据如何传输"></a>数据如何传输</h3><p>在了解反压的细节之前，首先要知道 Flink 中数据是如何传输的。在 Flink 中，两个算子之间的关系分为三种：</p><ol><li><p>部署在同一个 TaskManager 上，且属于同一算子链。</p></li><li><p>部署在同一个 TaskManager 上，但不是同一个算子链。</p></li><li><p>部署在不同的 TaskManager 上。</p></li></ol><p>三种不同的关系，对应的算子间的数据传输方式也不同。先说第一种。</p><h4 id="同一线程数据传输"><a href="#同一线程数据传输" class="headerlink" title="同一线程数据传输"></a>同一线程数据传输</h4><p>同一线程中的两个算子共享内存，因此数据传输非常简单，上游产出好数据后，直接调用下游的 processElement 方法即可。</p><h4 id="本地线程数据传输"><a href="#本地线程数据传输" class="headerlink" title="本地线程数据传输"></a>本地线程数据传输</h4><p>对于第二种关系，两个算子不在同一线程，但是部署在同一个 TaskManager 上，也就是算子之间的数据传输是跨线程的。我们通过一个图来解释。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1757168841/Blog/flink/7/Flink%E9%80%9A%E4%BF%A1.png" alt="本地线程传输"></p><p>图中，Flat Map Task 是上游算子，sum 是下游的算子。它们共享一块 Buffer 内存。当 Buffer 中没有数据可以消费时，sum 所在的线程会阻塞（步骤1）。随着数据的流入，Flat Map Task 会将处理好的数据写入到 ResultSubpartition（步骤2），然后 flush 到 Buffer 中（步骤3）。此时会唤醒 sum 所在的线程（步骤4），它就可以从 Buffer 中读取数据了（步骤5）。</p><h4 id="远程数据传输"><a href="#远程数据传输" class="headerlink" title="远程数据传输"></a>远程数据传输</h4><p>第三种跨 TaskManager 的数据传输，与第二种类似，不过也有些区别。我们还是通过一张图来解释。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1757170148/Blog/flink/7/Flink%E8%BF%9C%E7%A8%8B%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93.png" alt="远程数据传输"></p><p>从图中可以看到，当 sum 所在线程没有 Buffer 可以消费时，会通过 PartitionRequestClient 向 Flat Map Task 所在的进程发送请求。Flat Map Task 所在进程接收到请求后，会读取 Buffer 中的数据并返回。</p><h3 id="Flink-的反压"><a href="#Flink-的反压" class="headerlink" title="Flink 的反压"></a>Flink 的反压</h3><p>了解了 Flink 的数据传输方式之后，我们再来看下 Flink 是如何感知反压的。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1757170638/Blog/flink/7/Flink%E5%8F%8D%E5%8E%8B%E8%BF%87%E7%A8%8B.png" alt="Flink反压"></p><p>上图是一个数据传输的简图。当 Task1 有 Buffer 空间时，记录 A 被序列化并写入 LocalBufferPool 中，接着发送到 Task2 的 LocalBufferPool 中，Task2 读取并反序列化后交由程序处理。</p><p>这里我们也分两个场景讨论。</p><h4 id="本地传输"><a href="#本地传输" class="headerlink" title="本地传输"></a>本地传输</h4><p>Task1 和 Task2 在同一个 TaskManager 节点，Task1 和 Task2 共用 Buffer，一旦 Task2 消费了 Buffer，该 Buffer 就会被回收。如果 Task2 的处理速度比 Task1 慢，那么 Buffer 的回收速度就赶不上 Task1 取 Buffer 的速度，这样会导致无 Buffer 可用，最终 Task1 就会降速。</p><h4 id="远程传输"><a href="#远程传输" class="headerlink" title="远程传输"></a>远程传输</h4><p>Task1 和 Task2 运行在不同的 TaskManager 上，那 Buffer 会发送到网络后，等接收端消费完再回收。在发送端，会通过 Netty 水位机制来保证不往网络中写太多数据，如果网络中的数据超过了高水位值，就会等其下降到低水位值以下才会继续写数据。如果网络有堆积，发送端就会暂停发送，Buffer 也不会被回收，这就会阻塞 writer 往 ResultSubPartition 中写数据。</p><h3 id="反压监控"><a href="#反压监控" class="headerlink" title="反压监控"></a>反压监控</h3><p>在 Flink Web UI 中，可以找到反压的监控</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1757254334/Blog/flink/7/back_pressure_subtasks.png" alt="反压监控"></p><p>它有三种状态：</p><ul><li><strong>OK</strong>: 0% &lt;= 反压比例 &lt;= 10%，此时一般不用处理。</li><li><strong>LOW</strong>: 10% &lt; 反压比例 &lt;= 50%，这种状态需要关注。</li><li><strong>HIGH</strong>: 50% &lt; 反压比例 &lt;= 100%，已经反压，需要赶快处理。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>今天我们聊了什么是反压，以及 Flink 中数据传输方法和 Flink 任务是如何感知反压的。Flink 的传输方式分为三种，分别是同线程传输、本地跨线程传输以及远程传输。Flink 任务在感知反压时也分别针对本地传输和远程传输做了讨论。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天来聊在 Flink 运维过程中比较常见的一个问题：反压。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink学习笔记：状态后端</title>
    <link href="https://jackeyzhe.github.io/2025/08/24/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF/"/>
    <id>https://jackeyzhe.github.io/2025/08/24/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF/</id>
    <published>2025-08-23T16:16:42.000Z</published>
    <updated>2025-09-16T14:39:27.083Z</updated>
    
    <content type="html"><![CDATA[<p>我们继续来聊 Flink 容错相关的内容。前面在介绍 Checkpoint 和 Savepoint 时提到了 State 的稳定存储，那究竟如何存储以及存储在什么地方呢？相信通过读完本文之后，你会有答案。<span id="more"></span></p><h3 id="State-Backend-分类"><a href="#State-Backend-分类" class="headerlink" title="State Backend 分类"></a>State Backend 分类</h3><p>在 Flink 中状态后端（State Backend）是用来管理状态如何存储的。当前内置了两种 State Backend，分别是 HashMapStateBackend 和 EmbeddedRocksDBStateBackend。Flink 默认使用的是 HashMapStateBackend。</p><h4 id="HashMapStateBackend"><a href="#HashMapStateBackend" class="headerlink" title="HashMapStateBackend"></a>HashMapStateBackend</h4><p>在 HashMapBackend 中，数据是以 Java 对象的形式存储的，它适用于有较大 State，较长 window 和 较大 key/value 状态的场景。同时适用于高可用场景。在使用 HashMapStateBackend 时，建议把 managed memory 设置为 0，以此来增加用户代码可使用的内存。</p><h4 id="EmbeddedRocksDBStateBackend"><a href="#EmbeddedRocksDBStateBackend" class="headerlink" title="EmbeddedRocksDBStateBackend"></a>EmbeddedRocksDBStateBackend</h4><p>对于 EmbeddedRocksDBStateBackend 而言，数据是存储在 RocksDB 中的，在存储之前，要对数据进行序列化。EmbeddedRocksDBStateBackend 也存在一定局限性，那就是最大只能支持每个 key/value 存储 2^31 字节大小的数据，这是当前 RocksDB JNI 的限制。</p><p>EmbeddedRocksDBStateBackend 也有一定的优势，其一是它是目前唯一支持增量 Checkpoint 的 State Backend。其二是因为它是外部存储，因此它可以支持非常大的 State，非常长的窗口。</p><p>增量快照只包含自上一次快照完成后被修改的记录，所以增量快照的一大优点就是可以显著减少快照的耗时。在恢复时间上，要分情况讨论，如果瓶颈在网络带宽，那么增量快照的恢复时间要比全量快照更长，因为增量快照包含的多个 sst 文件之间可能存在重复数据。如果瓶颈在 CPU 或 IO，那增量快照恢复时间更短，因为增量快照不需要恢复不需要解析 Flink 统一的存储格式来重建本地的 RocksDB 表，而是直接基于 sst 文件加载。</p><h3 id="Checkpoint-存储类型"><a href="#Checkpoint-存储类型" class="headerlink" title="Checkpoint 存储类型"></a>Checkpoint 存储类型</h3><p>了解了 State Backend 分类之后，我们再来看 Checkpoint 的存储类型。它也分为两类：JobManagerCheckpointStorage 和 FileSystemCheckpointStorage。</p><h4 id="JobManagerCheckpointStorage"><a href="#JobManagerCheckpointStorage" class="headerlink" title="JobManagerCheckpointStorage"></a>JobManagerCheckpointStorage</h4><p>JobManagerCheckpointStorage 是将快照存储在 JobManager 的堆内存中。JobManagerCheckpointStorage 在使用时有一定限制：</p><ul><li><p>默认每个 State 大小最大为 5MB</p></li><li><p>总的状态大小不能超过 JobManager 内存</p></li></ul><p>基于这些限制，JobManagerCheckpointStorage 只适用于本地的开发和调试。</p><h4 id="FileSystemCheckpointStorage"><a href="#FileSystemCheckpointStorage" class="headerlink" title="FileSystemCheckpointStorage"></a>FileSystemCheckpointStorage</h4><p>FileSystemCheckpointStorage 是将状态数据保存在外部存储中，要适用 FileSystemCheckpointStorage，需要配置文件系统的 URL。例如：“hdfs://namenode:40010/flink/checkpoints”。而元数据则存储在 JobManager 的内存中。</p><h3 id="Checkpoint-存储设置"><a href="#Checkpoint-存储设置" class="headerlink" title="Checkpoint 存储设置"></a>Checkpoint 存储设置</h3><p>有了前面 State Backend 和 存储类型的分类之后，我们就可以将其进行组合，得到最终 Checkpoint 的存储了。</p><p>目前共有三种组合，也对应了旧版本的三种 State Backend。</p><h4 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h4><p>MemoryStateBackend 对应了 HashMapStateBackend 和 JobManagerCheckpointStorage 的组合。</p><p>设置方法为</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">state.backend:</span> <span class="string">hashmap</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optional, Flink will automatically default to JobManagerCheckpointStorage</span></span><br><span class="line"><span class="comment"># when no checkpoint directory is specified.</span></span><br><span class="line"><span class="attr">execution.checkpointing.storage:</span> <span class="string">jobmanager</span></span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">config</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">config.set(StateBackendOptions.STATE_BACKEND, <span class="string">&quot;hashmap&quot;</span>);</span><br><span class="line">config.set(CheckpointingOptions.CHECKPOINT_STORAGE, <span class="string">&quot;jobmanager&quot;</span>);</span><br><span class="line">env.configure(config);</span><br></pre></td></tr></table></figure><h4 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h4><p>FsStateBackend 对应了 HashMapStateBackend 和 FileSystemCheckpointStorage 的组合。</p><p>它的设置方法为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">state.backend:</span> <span class="string">hashmap</span></span><br><span class="line"><span class="attr">execution.checkpointing.dir:</span> <span class="string">file:///checkpoint-dir/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optional, Flink will automatically default to FileSystemCheckpointStorage</span></span><br><span class="line"><span class="comment"># when a checkpoint directory is specified.</span></span><br><span class="line"><span class="attr">execution.checkpointing.storage:</span> <span class="string">filesystem</span></span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">config</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">config.set(StateBackendOptions.STATE_BACKEND, <span class="string">&quot;hashmap&quot;</span>);</span><br><span class="line">config.set(CheckpointingOptions.CHECKPOINT_STORAGE, <span class="string">&quot;filesystem&quot;</span>);</span><br><span class="line">config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, <span class="string">&quot;file:///checkpoint-dir&quot;</span>);</span><br><span class="line">env.configure(config);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Advanced FsStateBackend configurations, such as write buffer size</span></span><br><span class="line"><span class="comment">// can be set manually by using CheckpointingOptions.</span></span><br><span class="line">config.set(CheckpointingOptions.FS_WRITE_BUFFER_SIZE, <span class="number">4</span> * <span class="number">1024</span>);</span><br><span class="line">env.configure(config);</span><br></pre></td></tr></table></figure><h4 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h4><p>RocksDBStateBackend 对应了 EmbeddedRocksDBStateBackend 和 FileSystemCheckpointStorage 的组合。</p><p>它的设置方法为</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">state.backend:</span> <span class="string">rocksdb</span></span><br><span class="line"><span class="attr">execution.checkpointing.dir:</span> <span class="string">file:///checkpoint-dir/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optional, Flink will automatically default to FileSystemCheckpointStorage</span></span><br><span class="line"><span class="comment"># when a checkpoint directory is specified.</span></span><br><span class="line"><span class="attr">execution.checkpointing.storage:</span> <span class="string">filesystem</span></span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">config</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">config.set(StateBackendOptions.STATE_BACKEND, <span class="string">&quot;rocksdb&quot;</span>);</span><br><span class="line">config.set(CheckpointingOptions.CHECKPOINT_STORAGE, <span class="string">&quot;filesystem&quot;</span>);</span><br><span class="line">config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, <span class="string">&quot;file:///checkpoint-dir&quot;</span>);</span><br><span class="line">env.configure(config);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// If you manually passed FsStateBackend into the RocksDBStateBackend constructor</span></span><br><span class="line"><span class="comment">// to specify advanced checkpointing configurations such as write buffer size,</span></span><br><span class="line"><span class="comment">// you can achieve the same results by using CheckpointingOptions.</span></span><br><span class="line">config.set(CheckpointingOptions.FS_WRITE_BUFFER_SIZE, <span class="number">4</span> * <span class="number">1024</span>);</span><br><span class="line">env.configure(config);</span><br></pre></td></tr></table></figure><h3 id="State-序列化与反序列化"><a href="#State-序列化与反序列化" class="headerlink" title="State 序列化与反序列化"></a>State 序列化与反序列化</h3><p>我们前面在创建 State 的描述符时，指定了 State 的类型，这其实就是告诉 Flink 应该如何去序列化我们的 State。当然，也可以自定义 State 序列化器，自定义序列化器需要 TypeSerializer，然后在创建描述符时指定。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomTypeSerializer</span> <span class="keyword">extends</span> <span class="title class_">TypeSerializer</span>&lt;Tuple2&lt;String, Integer&gt;&gt; &#123;...&#125;;</span><br><span class="line"></span><br><span class="line">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(</span><br><span class="line">        <span class="string">&quot;state-name&quot;</span>,</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">CustomTypeSerializer</span>());</span><br><span class="line"></span><br><span class="line">checkpointedState = getRuntimeContext().getListState(descriptor);</span><br></pre></td></tr></table></figure><p>Flink 中状态分为两种类型，一种是基于 Heap，一种是不基于 Heap。</p><h4 id="Heap-state-backends"><a href="#Heap-state-backends" class="headerlink" title="Heap state backends"></a>Heap state backends</h4><p>首先看基于 Heap 的，HashMapStateBackend 是基于 Heap 的。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1756052394/Blog/flink/6/HeapStateBackend.png" alt="HeapStateBackend"></p><p>Heap state backend 存在本地的状态后端中的是非序列化的数据，当触发 Checkpoint / Savepoint 时，会用指定的序列化器将数据序列化，然后存储到指定的稳定存储中。</p><p>如果我们对程序进行了升级，这时要从 State 恢复的话，需要先将稳定存储中的数据进行反序列化，然后将结果加载到 TM 的内存中，供 user code 使用。</p><h4 id="Off-heap-state-backends"><a href="#Off-heap-state-backends" class="headerlink" title="Off-heap state backends"></a>Off-heap state backends</h4><p>EmbeddedRocksDBStateBackend 就是一种不基于 Heap 的状态。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1756053324/Blog/flink/6/OffHeapStateBackend.png" alt="Off-heap State Backend"></p><p>不基于 Heap 的状态在写入本地 State 时就会进行序列化，序列化后的数据会写入到堆外内存。在触发 Checkpoint 时，就只是把数据文件转存到稳定存储中。</p><p>当我们的任务完成升级后，会先将二进制文件恢复到 TM 的内存中，这里是一个文件加载的过程。当我们要使用 State 时，才会进行反序列化，注意这里只会对使用到的 State 进行反序列化读取以及后续的更新，没有使用到的还是保持旧版本的数据。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们重点介绍了状态后端的存储。State Backend 分为 HashMapStateBackend 和 EmbeddedRocksDBStateBackend，其存储类型又分为 JobManagerCheckpointStorage 和 FileSystemCheckpointStorage。最终我们会有三种不同的状态后端：MemoryStateBackend、FsStateBackend 和 RocksDBStateBackend。最后我们还介绍了 State 的两种不同的序列化。</p><p>相信通过本文的介绍，你已经可以回答开篇的问题了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们继续来聊 Flink 容错相关的内容。前面在介绍 Checkpoint 和 Savepoint 时提到了 State 的稳定存储，那究竟如何存储以及存储在什么地方呢？相信通过读完本文之后，你会有答案。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink学习笔记：如何做容错</title>
    <link href="https://jackeyzhe.github.io/2025/08/17/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%A6%82%E4%BD%95%E5%81%9A%E5%AE%B9%E9%94%99/"/>
    <id>https://jackeyzhe.github.io/2025/08/17/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%A6%82%E4%BD%95%E5%81%9A%E5%AE%B9%E9%94%99/</id>
    <published>2025-08-17T03:29:28.000Z</published>
    <updated>2025-08-21T16:38:11.661Z</updated>
    
    <content type="html"><![CDATA[<p>现在我们已经了解了 Flink 的状态如何定义和使用，那 Flink 是如何做容错的呢？今天我们一起来了解一下。<span id="more"></span></p><p>先来回答问题， Flink 是通过状态快照来做容错的，在 Flink 中状态快照分为 Checkpoint 和 Savepoint 两种。</p><h3 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h3><p>Checkpoint 是一种自动执行的快照，其目的是让 Flink 任务可以从故障中恢复。它可以是增量的，并且为快速恢复进行了优化。</p><h4 id="如何开启-Checkpoint"><a href="#如何开启-Checkpoint" class="headerlink" title="如何开启 Checkpoint"></a>如何开启 Checkpoint</h4><p>Checkpoint 默认是关闭的，开启的方法很简单，只需要调用 enableCheckpointing() 方法即可。除了这个方法之外，Checkpoint 还有一些高级特性。我们来看几个比较常用的，更多的选项可以查看<a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/checkpointing/#%e7%9b%b8%e5%85%b3%e7%9a%84%e9%85%8d%e7%bd%ae%e9%80%89%e9%a1%b9">官方文档</a>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 每 1000ms 开始一次 checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 高级选项：</span></span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig().setExternalizedCheckpointRetention(</span><br><span class="line">        ExternalizedCheckpointRetention.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig().enableUnalignedCheckpoints();</span><br></pre></td></tr></table></figure><ul><li><p>CheckpointingMode：支持 EXACTLY_ONCE 和 AT_LEAST_ONCE 两种，精确一次有更好的数据一致性，而至少一次可以提供更低的延迟。</p></li><li><p>MinPauseBetweenCheckpoints：Checkpoint 之间最小间隔时间，单位是毫秒，即前一次 Checkpoint 执行完成之后必须间隔 n 毫秒之后才会开启下一次 Checkpoint。</p></li><li><p>CheckpointTimeout：Checkpoint 超时时间，单位为毫秒，表示 Checkpoint 必须在 n 毫秒内完成，否则就会因超时失败。</p></li><li><p>TolerableCheckpointFailureNumber：可容忍连续失败次数，默认是0。超过这个阈值之后，整个 Flink 作业会触发 fail over。</p></li><li><p>MaxConcurrentCheckpoints：Checkpoint 并发数，默认情况下是1，在同一时间只允许一个 Checkpoint 执行。这个参数不能和最小间隔时间一起使用。</p></li><li><p>ExternalizedCheckpointRetention：周期存储 Checkpoint 到外部存储，这样在任务失败时 Checkpoint 也不会被删除。</p></li><li><p>enableUnalignedCheckpoints：使用非对齐的 Checkpoint，可以减少在产生背压时 Checkpoint 的创建时间。</p></li></ul><h4 id="Checkpoint-存储"><a href="#Checkpoint-存储" class="headerlink" title="Checkpoint 存储"></a>Checkpoint 存储</h4><p>Flink 提供了两种存储类型：JobManagerCheckpointStorage 和 FileSystemCheckpointStorage。默认是 JobManagerCheckpointStorage，即将 Checkpoint 快照存储在 JobManager 的堆内存中，也可以设置 Checkpoint 目录，将快照存储在外部存储系统中。</p><p>Checkpoint 目录通过 execution.checkpointing.dir 设置项设置。其目录结构如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/user-defined-checkpoint-dir</span><br><span class="line">    /&#123;job-id&#125;</span><br><span class="line">        |</span><br><span class="line">        + --shared/</span><br><span class="line">        + --taskowned/</span><br><span class="line">        + --chk-1/</span><br><span class="line">        + --chk-2/</span><br><span class="line">        + --chk-3/</span><br><span class="line">        ...   </span><br></pre></td></tr></table></figure><h4 id="Checkpoint-工作原理"><a href="#Checkpoint-工作原理" class="headerlink" title="Checkpoint 工作原理"></a>Checkpoint 工作原理</h4><p>在前文中，我们曾经提到过 Checkpoint Coordinator，它是 JobManager 的其中一个模块。它在 Checkpoint 过程中担任着重要的角色。</p><p>现在来看下 Checkpoint 的完整流程</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1755702080/Blog/flink/5/Checkpoint.png" alt="Checkpoint"></p><p>1、Checkpoint Coordinator 触发所有 Source 节点开始 Checkpoint，Source 收到触发命令后，会将自己的 State 进行持久化（图中三角形），并且向下游发送 barrier 事件（图中的小矩形）。当 Source 节点的 State 持久化完成之后，会数据存储的地址发送给 Checkpoint Coordinator。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1755706201/Blog/flink/5/CheckpointHandle.png" alt="CheckpointHandle"></p><p>2、barrier 事件随着事件流传输到下游节点，当下游节点收到所有的上游 barrier 事件后，也会将自己的 State 持久化，并继续向下传播 barrier 事件。持久化完成后，也同样将数据存储地址发送给 Checkpoint Coordinator。</p><p>3、当所有的算子都完成持久化过程后，Checkpoint Coordinator 会将一些元数据进行持久化。</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1755706201/Blog/flink/5/CompleteCheckpoint.png" alt="CompleteCheckpoint"></p><p>至此，一次完整的 Checkpoint 流程就结束了。</p><h3 id="Savepoint"><a href="#Savepoint" class="headerlink" title="Savepoint"></a>Savepoint</h3><p>学习完 Checkpoint 之后，我们再来了解下另一种快照——Savepoint。</p><p>Savepoint 是依据 checkpoint 机制创建的一致性镜像。通常用来做 Flink 作业的重启或更新等运维操作。Savepoint 包含稳定存储上的二进制文件（作业状态的镜像）和元数据文件两部分。</p><h4 id="使用-Savepoint"><a href="#使用-Savepoint" class="headerlink" title="使用 Savepoint"></a>使用 Savepoint</h4><p>根据官方文档的提示，在我们的程序中，最好显式调用 uid() 方法来为算子指定一个 ID，这些 ID 被用来恢复每个算子的状态。如果不指定的话，Flink 任务会自动生成算子 ID，但是生成的 ID 与程序结构有关，也就是说，如果程序的结构改变了的话，就没有办法从 Savepoint 恢复对应算子的状态了。</p><p>有了这个前提条件之后，我们就可以使用命令来操作 Savepoint 了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 触发 savepoint</span><br><span class="line">$ bin/flink savepoint :jobId [:targetDirectory]</span><br><span class="line"></span><br><span class="line">// 触发 savepoint, 指定 <span class="built_in">type</span>，默认是 canonical</span><br><span class="line">$ bin/flink savepoint --<span class="built_in">type</span> [native/canonical] :jobId [:targetDirectory]</span><br><span class="line"></span><br><span class="line">// 触发 savepoint，客户端拿到 trigger <span class="built_in">id</span> 后立即返回</span><br><span class="line">$ bin/flink savepoint :jobId [:targetDirectory] -detached</span><br><span class="line"></span><br><span class="line">// 使用 savepoint 停止作业</span><br><span class="line">$ bin/flink stop --<span class="built_in">type</span> [native/canonical] --savepointPath [:targetDirectory] :jobId</span><br><span class="line"></span><br><span class="line">// 从 savepoint 恢复</span><br><span class="line">$ bin/flink run -s :savepointPath [:runArgs]</span><br><span class="line"></span><br><span class="line">// 删除 savepoint</span><br><span class="line">$ bin/flink savepoint -d :savepointPath</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在 触发 savepoint 时，我们可以指定格式，两种格式的区别是：</p><ul><li><p>canonical（标准格式）：在任何存储都保持统一格式，重在保证兼容性。</p></li><li><p>native（原生格式）：标准格式创建和恢复都很慢，原生格式是以特定的状态后端的格式生成，可以更快的创建和恢复。</p></li></ul><h3 id="Checkpoint-与-Savepoint-区别"><a href="#Checkpoint-与-Savepoint-区别" class="headerlink" title="Checkpoint 与 Savepoint 区别"></a>Checkpoint 与 Savepoint 区别</h3><p>这是面试最常见的问题之一，有了 checkpoint，为什么还需要 savepoint？或者说两者之间有什么区别？</p><p>从概念上来讲，Checkpoint 类似数据库的恢复日志，而 Savepoint 类似数据库的备份。Checkpoint 主要用于作业故障的恢复，它的管理和删除也都是 Flink 内部处理，用户不需要过多关注。Savepoint 主要用于有计划的手动运维，例如升级 Flink 版本。它的创建、删除操作都需要用户手动执行。</p><p>下面是官方文档给出的 Checkpoint 和 Savepoint 支持的操作。✓表示完全支持，x表示不支持，!表示目前有效，但没有正式保证支持，使用时存在一定风险。</p><table><thead><tr><th>操作</th><th>标准 Savepoint</th><th>原生 Savepoint</th><th>对齐 Checkpoint</th><th>非对齐 Checkpoint</th></tr></thead><tbody><tr><td>更换状态后端</td><td>✓</td><td>x</td><td>x</td><td>x</td></tr><tr><td>State Processor API (写)</td><td>✓</td><td>x</td><td>x</td><td>x</td></tr><tr><td>State Processor API (读)</td><td>✓</td><td>!</td><td>!</td><td>x</td></tr><tr><td>自包含和可移动</td><td>✓</td><td>✓</td><td>x</td><td>x</td></tr><tr><td>Schema 变更</td><td>✓</td><td>!</td><td>!</td><td>!</td></tr><tr><td>任意 job 升级</td><td>✓</td><td>✓</td><td>✓</td><td>x</td></tr><tr><td>非任意 job 升级</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Flink 小版本升级</td><td>✓</td><td>✓</td><td>✓</td><td>x</td></tr><tr><td>Flink bug/patch 版本升级</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>扩缩容</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们介绍了 Flink 是如何做容错的，分别介绍了 Checkpoint 和 Savepoint，以及它们之间的区别。本文多次提到了 Checkpoint 和 Savepoint 依赖的稳定存储，我会在下一篇文章进行详细的介绍。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;现在我们已经了解了 Flink 的状态如何定义和使用，那 Flink 是如何做容错的呢？今天我们一起来了解一下。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink学习笔记：状态类型和应用</title>
    <link href="https://jackeyzhe.github.io/2025/08/04/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%BA%94%E7%94%A8/"/>
    <id>https://jackeyzhe.github.io/2025/08/04/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%8A%B6%E6%80%81%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%BA%94%E7%94%A8/</id>
    <published>2025-08-04T15:33:44.000Z</published>
    <updated>2025-08-14T14:43:30.348Z</updated>
    
    <content type="html"><![CDATA[<p>Flink 被广泛应用的原因，除了我们前面提到的对时间以及窗口的应用之外，另一点就是它强大的容错机制，以及对 Exactly Once 的支持。<span id="more"></span></p><p>今天就来了解一下 Flink 的状态以及应用，首先第一个问题是：什么是有状态计算？</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>在数据流处理中，大部分操作都是每次只处理一个事件，比如对输入的数据进行结构化解析，这类操作我们称为无状态计算。而有些操作则需要记住多个事件并进行处理，比如前面我们在窗口中对数据做的求和操作，这类操作我们称之为有状态计算。</p><p>在 Flink 中，状态的另一个重要作用是用来做故障恢复，故障恢复主要依赖于 checkpoint 和 savepoint。当我们使用状态时，通常需要从 State Backend 读取。</p><p>通过介绍有状态计算的基本概念，我们又引出了 checkpoint、State Backend 等概念，下面我们再来一一解释。</p><h3 id="状态分类"><a href="#状态分类" class="headerlink" title="状态分类"></a>状态分类</h3><p>Flink 状态分类可以参考下图</p><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1754928263/Blog/flink/4/%E7%8A%B6%E6%80%81%E5%88%86%E7%B1%BB.png" alt="状态分类"></p><p>首先是分为 Raw State 和 Managed State 两大类，我们分别从管理方式、数据类型、适用场景这三个方面来看它们的区别</p><table><thead><tr><th></th><th>Raw State</th><th>Managed State</th></tr></thead><tbody><tr><td>管理方式</td><td>开发者自行管理，需要手动序列化和反序列化</td><td>由 Flink Runtime 管理，自动存储和恢复数据</td></tr><tr><td>数据类型</td><td>仅支持 byte 数组</td><td>支持 value, list, map</td></tr><tr><td>适用场景</td><td>需要自定义 Operator</td><td>支持大部分计算场景</td></tr></tbody></table><p>Managed State 又分为 Keyed State 和 Operator State 两类，下面我们详细介绍这两类状态。</p><h4 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h4><p>Keyed State 只能用在 KeyedStream 上，也就是在使用前，要先对数据流进行 keyBy 操作。Keyed State 支持以下几种状态类型：</p><ul><li><p>ValueState<T>：保存一个值，可以通过 update() 方法更新，通过 value() 方法获取保存的值。</p></li><li><p>ListState<T>：保存一个 list，可以通过 add() 或 addAll() 方法向 list 中添加元素，也可以通过 update() 直接覆盖。使用 get() 方法获取整个列表。</p></li><li><p>ReducingState<T>：保存一个值，表示添加到状态所有值的聚合，使用 add() 方法添加元素，使用 get() 方法获取保存的值。</p></li><li><p>AggregatingState&lt;IN, OUT&gt;：保存一个值，与 ReducingState 不同的是，输入和输出的元素类型可以不同。</p></li><li><p>MapState&lt;UK, UV&gt;：保存一个 map，可以使用 put() 或 putAll() 添加键值对，使用 get() 获取值。</p></li></ul><p>在知道了各个类型的 Keyed State 怎么用之后，我们再来看如何创建一个 Keyed State。以 ValueState 为例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ValueStateDescriptor</span>&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;average&quot;</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class="line">ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum = getRuntimeContext().getState(descriptor);</span><br></pre></td></tr></table></figure><p>要想创建一个 State，必须先创建一个 StateDescriptor，然后通过 RuntimeContext 来获取 State。每个 State 都对应一种 StateDescriptor。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ValueState&lt;T&gt; <span class="title function_">getState</span><span class="params">(ValueStateDescriptor&lt;T&gt;)</span></span><br><span class="line">ReducingState&lt;T&gt; <span class="title function_">getReducingState</span><span class="params">(ReducingStateDescriptor&lt;T&gt;)</span></span><br><span class="line">ListState&lt;T&gt; <span class="title function_">getListState</span><span class="params">(ListStateDescriptor&lt;T&gt;)</span></span><br><span class="line">AggregatingState&lt;IN, OUT&gt; <span class="title function_">getAggregatingState</span><span class="params">(AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;)</span></span><br><span class="line">MapState&lt;UK, UV&gt; <span class="title function_">getMapState</span><span class="params">(MapStateDescriptor&lt;UK, UV&gt;)</span></span><br></pre></td></tr></table></figure><h4 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h4><p>算子状态也称为非 keyed 状态，是绑定到一个并行算子实例的状态。State 需要支持重新分布。 最典型的是 Kafka Connector 中，维护了一个 topic partitions 和 offset 的 map 作为一个算子状态。</p><p>和 Keyed State 类似，想要创建一个 Operator State，同样也需要一个 StateDescriptor，同时，需要实现 CheckpointedFunction，它提供了两个方法，分别是在 checkpoint 时 调用的 snapshotState() 和 自定义函数初始化时调用的 initializeState()。</p><p>Talk is cheap, show me your code!</p><p>我们来看 Flink 官方文档提供的 Demo</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BufferingSink</span></span><br><span class="line">        <span class="keyword">implements</span> <span class="title class_">SinkFunction</span>&lt;Tuple2&lt;String, Integer&gt;&gt;,</span><br><span class="line">                   CheckpointedFunction &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> threshold;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">BufferingSink</span><span class="params">(<span class="type">int</span> threshold)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.threshold = threshold;</span><br><span class="line">        <span class="built_in">this</span>.bufferedElements = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        bufferedElements.add(value);</span><br><span class="line">        <span class="keyword">if</span> (bufferedElements.size() &gt;= threshold) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class="line">                <span class="comment">// send it to the sink</span></span><br><span class="line">            &#125;</span><br><span class="line">            bufferedElements.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        checkpointedState.update(bufferedElements);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class="line">                bufferedElements.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个例子中，我们在 initializeState 方法中使用 getOperatorStateStore().getListState() 创建了一个 ListState，然后将数据缓存到这个 list 中，当缓存数据大小超过一个阈值时，再统一发送到下游。</p><p>这里还有一个方法值得注意，就是 isRestored()，它是用来判断当前任务是否是从故障中恢复的，如果是，我们需要执行故障恢复相关的逻辑。在这个例子中就是把 state 的数据恢复到本地的变量中。</p><h5 id="Broadcast-State"><a href="#Broadcast-State" class="headerlink" title="Broadcast State"></a>Broadcast State</h5><p>了解了如何创建和使用 Operator State 之后，我们再来看一种特殊的 Operator State —— Broadcast State。</p><p>Broadcast State 本身是类似于 Map 类型的格式，使用时需要指定 key 和 value 的类型。它的作用是将一条数据流的数据广播到下游算子的各个节点。</p><p>Broadcast State 的一个比较常见的作用就是大流关联小流。例如，我们有一个订单流，需要关联商品详情，这时可以把商品详情的流作为 broadcast 流进行广播，这样在每个 TaskManager 中会有一份商品详情数据，订单流就可以直接查询 broadcast 的数据，不需要再访问 MySQL 数据库来做查询操作。</p><p>那么具体要怎么实现呢？其实也很简单，可以看下面这段代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MapStateDescriptor&lt;String, Product&gt; productStateDescriptor =</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MapStateDescriptor</span>&lt;&gt;(<span class="string">&quot;productBroadcastState&quot;</span>, String.class, Product.class);</span><br><span class="line"></span><br><span class="line">BroadcastStream&lt;Product&gt; broadcastProductStream = productStream.broadcast(productStateDescriptor);</span><br><span class="line"></span><br><span class="line">BroadcastConnectedStream&lt;Order, Product&gt; connectedStreams = orderStream.connect(broadcastProductStream);</span><br></pre></td></tr></table></figure><p>拿到 BroadcastConnectedStream 之后，我们就可以调用 process 方法进行处理了。完整的代码我放到 <a href="https://github.com/Jackeyzhe/flink-training/blob/feature/wz-demo/common/src/main/java/org/apache/flink/training/examples/state/BroadcastStateDemo.java">GitHub</a> 上了。感兴趣的可以查看。</p><p>在使用 Broadcast State 的时需要注意，目前 RocksDB 不支持保存 Broadcast State，因此，广播流吞吐量必须要小，并且 Flink 任务要预留足够的内存。</p><p>聊完了 Broadcast State，我们再来看看 Operator State 是如何进行重新分布的。正常 Operator State 支持两种重新分布的方式，按照不同的方式，我们可以划分为 ListState 和 UnionListState。</p><ul><li><p>ListState：所有的 element 均匀分布到 task 上</p></li><li><p>UnionListState：每个 element 都要在所有的 task 上</p></li></ul><p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1755107607/Blog/flink/4/OperatorStateResize.png" alt="OperatorStateResize"></p><p>Broadcast State 由于本身就是广播状态，因此重新分布后仍然是需要进行广播的。</p><h4 id="状态有效期"><a href="#状态有效期" class="headerlink" title="状态有效期"></a>状态有效期</h4><p>最后再来扩展一个知识点，就是状态的有效期。在 Flink 中，只有 Keyed State 支持有效期。具体使用方法如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StateTtlConfig</span> <span class="variable">ttlConfig</span> <span class="operator">=</span> StateTtlConfig</span><br><span class="line">    .newBuilder(Duration.ofSeconds(<span class="number">1</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line">    </span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> <span class="title class_">ValueStateDescriptor</span>&lt;&gt;(<span class="string">&quot;text state&quot;</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);</span><br></pre></td></tr></table></figure><p>这里有三个属性，我们分别来解释一下，首先第一个是过期时间，在调用 newBuilder 时就要传入。</p><p>第二个是 UpdateType，也就是更新策略，默认是 OnCreateAndWrite，表示在创建和写入时更新，也可以设置为 OnReadAndWrite，表示在读取和写入时更新。</p><p>第三个是可见性，默认是 NeverReturnExpired，即不返回过期数据，也可以设置为 ReturnExpiredIfNotCleanedUp，表示会返回过期但未被清理的数据。</p><p>状态数据清理策略也分为两种：一种是做全量快照时进行清理，创建 ttl 时调用 cleanupFullSnapshot() 方法即可。</p><p>另一种是增量数据清理，在访问或处理状态时，状态后端保留一个所有状态的惰性迭代器，每次清理时选择已经过期的数据进行清理。设置方法时在创建 ttl 时调用 cleanupIncrementally(10, true) ，可以看到它提供两个参数，第一个参数是设置每次检查的条数，默认是5。第二个参数是是否在处理每条记录时都触发清理，默认是 false。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后我们来总结一下，本文我们主要介绍了 Flink 的状态及应用，首先介绍有状态计算的概念。接着重点学习了 Keyed State 和 Operator State。我们通过一个表格来进行总结。</p><table><thead><tr><th></th><th>Keyed State</th><th>Operator State</th></tr></thead><tbody><tr><td>使用算子类型</td><td>只能被用于 KeyedStream 中的Operator 上</td><td>可以被用于所有 Operator</td></tr><tr><td>状态分配</td><td>每个 Key 对应一个状态，单个 Operator 中可以包含多个 Key</td><td>单个 Operator 对应一个状态</td></tr><tr><td>创建和访问方式</td><td>重写 RichFunction，通过访问 RuntimeContext 对象获取</td><td>实现 CheckpointedFunction 或 ListCheckpointed 接口</td></tr><tr><td>横向拓展</td><td>状态随着 Key 自动在多个算子 Task 上迁移</td><td>有多种重新分配的方式：均匀分布。将所有状态合并再分发到每个实例上</td></tr><tr><td>支持数据类型</td><td>ValueState, ListState, ReducingState, AggregatingState, MapState</td><td>ListState, UnionListState, Broadcast State</td></tr></tbody></table><p>最后，我们又介绍了状态有效期的定义和使用方法。有了状态之后，Flink 就可以为我们提供非常强大的容错能力了，具体怎么做的我们后面再聊。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink 被广泛应用的原因，除了我们前面提到的对时间以及窗口的应用之外，另一点就是它强大的容错机制，以及对 Exactly Once 的支持。</summary>
    
    
    
    
    <category term="Flink" scheme="https://jackeyzhe.github.io/tags/Flink/"/>
    
  </entry>
  
</feed>
