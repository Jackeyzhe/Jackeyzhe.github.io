<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/6.0.5/fancybox/fancybox.css" integrity="sha256-uTcjoMD6rPt4OyV3Rs02Slxl0BJGMNVKAm/1eYPt2go=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jackeyzhe.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.2","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"github-dark","dark":"github-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="本文我们来梳理 Kafka Connector 相关的源码。">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink源码阅读：Kafka Connector">
<meta property="og:url" content="https://jackeyzhe.github.io/2026/01/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AKafka-Connector/index.html">
<meta property="og:site_name" content="Jackeyzhe&#39;s Blog">
<meta property="og:description" content="本文我们来梳理 Kafka Connector 相关的源码。">
<meta property="og:locale">
<meta property="og:image" content="https://res.cloudinary.com/dxydgihag/image/upload/v1768288302/Blog/flink/22/table_connectors.png">
<meta property="og:image" content="https://res.cloudinary.com/dxydgihag/image/upload/v1768294001/Blog/flink/22/KafkaDynamicTableFactory.png">
<meta property="og:image" content="https://res.cloudinary.com/dxydgihag/image/upload/v1768297428/Blog/flink/22/DeserializationFormatFactory.png">
<meta property="og:image" content="https://res.cloudinary.com/dxydgihag/image/upload/v1768318497/Blog/flink/22/KafkaSource.png">
<meta property="og:image" content="https://res.cloudinary.com/dxydgihag/image/upload/v1768363734/Blog/flink/22/KafkaSink.png">
<meta property="article:published_time" content="2026-01-12T06:39:21.000Z">
<meta property="article:modified_time" content="2026-01-14T11:51:36.671Z">
<meta property="article:author" content="Jackey Wang">
<meta property="article:tag" content="Flink">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://res.cloudinary.com/dxydgihag/image/upload/v1768288302/Blog/flink/22/table_connectors.png">


<link rel="canonical" href="https://jackeyzhe.github.io/2026/01/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AKafka-Connector/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"https://jackeyzhe.github.io/2026/01/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AKafka-Connector/","path":"2026/01/12/Flink源码阅读：Kafka-Connector/","title":"Flink源码阅读：Kafka Connector"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Flink源码阅读：Kafka Connector | Jackeyzhe's Blog</title>
  








  
  <script size="300" alpha="0.6" zIndex="1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/6.0.5/fancybox/fancybox.umd.js" integrity="sha256-UiSieVaV/DXce2LW7QH+o77w+AIoAvSCPBkezriZ2DQ=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  



  <script src="/js/third-party/fancybox.js" defer></script>



  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Jackeyzhe's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Jackeyzhe's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">靠脸吃饭</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">Tags<span class="badge">15</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives<span class="badge">120</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section">About</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89-Source-%E5%92%8C-Sink"><span class="nav-number">1.</span> <span class="nav-text">自定义 Source 和 Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Metadata"><span class="nav-number">1.1.</span> <span class="nav-text">Metadata</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Planning"><span class="nav-number">1.2.</span> <span class="nav-text">Planning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Runtime"><span class="nav-number">1.3.</span> <span class="nav-text">Runtime</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">1.4.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Connector-%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.</span> <span class="nav-text">Kafka Connector 的实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Factory"><span class="nav-number">2.1.</span> <span class="nav-text">Factory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Source-%E7%AB%AF"><span class="nav-number">2.2.</span> <span class="nav-text">Source 端</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sink-%E7%AB%AF"><span class="nav-number">2.3.</span> <span class="nav-text">Sink 端</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jackey Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">120</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Jackeyzhe" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Jackeyzhe" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jackeyzhe59@gmail.com" title="E-Mail → mailto:jackeyzhe59@gmail.com" rel="noopener me" target="_blank"><i class="fas fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/jackeyzhe" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;jackeyzhe" rel="noopener me" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://jackeyzhe.github.io/2026/01/12/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AKafka-Connector/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jackey Wang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jackeyzhe's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Flink源码阅读：Kafka Connector | Jackeyzhe's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Flink源码阅读：Kafka Connector
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2026-01-12 14:39:21" itemprop="dateCreated datePublished" datetime="2026-01-12T14:39:21+08:00">2026-01-12</time>
    </span>

  
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>17k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>15 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>本文我们来梳理 Kafka Connector 相关的源码。<span id="more"></span></p>
<h3 id="自定义-Source-和-Sink"><a href="#自定义-Source-和-Sink" class="headerlink" title="自定义 Source 和 Sink"></a>自定义 Source 和 Sink</h3><p>在介绍 Kafka Connector 之前，我们先来看一下在 Flink 中是如何支持自定义 Source 和 Sink 的。我们来看一张 Flink 官方文档提供的图。</p>
<p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768288302/Blog/flink/22/table_connectors.png" alt="table-connector"></p>
<p>这张图展示了 Connector 的基本体系结构，三层架构也非常清晰。</p>
<h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><p>首先是最上层的 MetaData，CREATE TABLE 会更新 Catalog，然后被转换为 TableAPI 的 CatalogTable，CatalogTable 实例用于表示动态表（Source 或 Sink 表）的元信息。</p>
<h4 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h4><p>在解析和优化程序时，会将 CatalogTable 转换为 DynamicTableSource 和 DynamicTableSink，分别用于查询和插入数据，这两个实例的创建都需要对应的工厂类，工厂类的完整路径需要放到这个配置文件中。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">META-INF/services/org.apache.flink.table.factories.Factory</span><br></pre></td></tr></table></figure>
<p>如果有需要的话，我们还可以在解析过程中配置编码和解码方法。</p>
<p>在 Source 端，通过三个接口支持不同的查询能力。</p>
<ul>
<li><p>ScanTableSource：用于消费 changelog 流，扫描的数据支持 insert、updata、delete 三种类型。ScanTableSource 还支持很多其他的功能， 都是通过接口提供的。具体可以看参考这个连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/sourcessinks/#source-abilities</span><br></pre></td></tr></table></figure>
</li>
<li><p>LookupTableSource：LookupTableSource 不会全量读取表的数据，它在需要时会发送请求，懒加载数据。目前只支持 insert-only 变更模式。</p>
</li>
<li><p>VectorSearchTableSource：使用一个输入向量来搜索数据，并返回最相似的 Top-K 行数据。</p>
</li>
</ul>
<p>在 Sink 端，通过 DynamicTableSink 来实现具体的写入逻辑，这里也提供了一些用于扩展能力的接口。具体参考</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/sourcessinks/#sink-abilities</span><br></pre></td></tr></table></figure>
<h4 id="Runtime"><a href="#Runtime" class="headerlink" title="Runtime"></a>Runtime</h4><p>逻辑解析完成后，会到 Runtime 层。这里就是定义几个 Provider，在 Provider 中实现和连接器具体的交互逻辑。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>当我们需要创建一个自定义的 Source 和 Sink 时，就可以通过以下步骤实现。</p>
<ol>
<li><p>定义 Flink SQL 的 DDL，需要定义相应的 Options。</p>
</li>
<li><p>实现 DynamicTableSourceFactory 和 DynamicTableSinkFactory，并把实现类的具体路径写到配置文件中。</p>
</li>
<li><p>实现 DynamicTableSource 和 DynamicTableSink，这里需要处理 SQL 层的元数据。</p>
</li>
<li><p>提供 Provider，将逻辑层与底层 DataStream 关联起来。</p>
</li>
<li><p>编写底层算子，实现 Source 和 Sink 接口。</p>
</li>
</ol>
<h3 id="Kafka-Connector-的实现"><a href="#Kafka-Connector-的实现" class="headerlink" title="Kafka Connector 的实现"></a>Kafka Connector 的实现</h3><p>带着这些知识，我们一起来看一下 Kafka Connector 相关的源码。</p>
<p>Kafka Connector 代码目前已经是一个独立的项目了。项目地址是</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/apache/flink-connector-kafka</span><br></pre></td></tr></table></figure>
<h4 id="Factory"><a href="#Factory" class="headerlink" title="Factory"></a>Factory</h4><p>我们首先找到定义的工厂类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory</span><br><span class="line">org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory</span><br></pre></td></tr></table></figure>
<p>以 KafkaDynamicTableFactory 为例，它同时实现了 DynamicTableSourceFactory 和 DynamicTableSinkFactory 两个接口。</p>
<p>KafkaDynamicTableFactory 包含以下几个方法。</p>
<p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768294001/Blog/flink/22/KafkaDynamicTableFactory.png" alt="KafkaDynamicTableFactory"></p>
<ul>
<li><p>factoryIdentifier：返回一个唯一标识符，对应 Flink SQL 中 connector=’xxx’ 这个配置。</p>
</li>
<li><p>requiredOptions：必填配置集合。</p>
</li>
<li><p>optionalOptions：选填配置集合。</p>
</li>
<li><p>forwardOptions：直接传递到 Runtime 层的配置集合。</p>
</li>
<li><p>createDynamicTableSource：创建 DynamicTableSource。</p>
</li>
<li><p>createDynamicTableSink：创建 DynamicTableSink。</p>
</li>
</ul>
<h4 id="Source-端"><a href="#Source-端" class="headerlink" title="Source 端"></a>Source 端</h4><p>工厂类的 createDynamicTableSource 方法创建了 DynamicTableSource，我们来看一下创建的逻辑。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DynamicTableSource <span class="title function_">createDynamicTableSource</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">TableFactoryHelper</span> <span class="variable">helper</span> <span class="operator">=</span> FactoryUtil.createTableFactoryHelper(<span class="built_in">this</span>, context);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Optional&lt;DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt;&gt; keyDecodingFormat =</span><br><span class="line">            getKeyDecodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; valueDecodingFormat =</span><br><span class="line">            getValueDecodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    helper.validateExcept(PROPERTIES_PREFIX);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">ReadableConfig</span> <span class="variable">tableOptions</span> <span class="operator">=</span> helper.getOptions();</span><br><span class="line"></span><br><span class="line">    validateTableSourceOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    validatePKConstraints(</span><br><span class="line">            context.getObjectIdentifier(),</span><br><span class="line">            context.getPrimaryKeyIndexes(),</span><br><span class="line">            context.getCatalogTable().getOptions(),</span><br><span class="line">            valueDecodingFormat);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">StartupOptions</span> <span class="variable">startupOptions</span> <span class="operator">=</span> getStartupOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">BoundedOptions</span> <span class="variable">boundedOptions</span> <span class="operator">=</span> getBoundedOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> getKafkaProperties(context.getCatalogTable().getOptions());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add topic-partition discovery</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Duration</span> <span class="variable">partitionDiscoveryInterval</span> <span class="operator">=</span></span><br><span class="line">            tableOptions.get(SCAN_TOPIC_PARTITION_DISCOVERY);</span><br><span class="line">    properties.setProperty(</span><br><span class="line">            KafkaSourceOptions.PARTITION_DISCOVERY_INTERVAL_MS.key(),</span><br><span class="line">            Long.toString(partitionDiscoveryInterval.toMillis()));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">DataType</span> <span class="variable">physicalDataType</span> <span class="operator">=</span> context.getPhysicalRowDataType();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] keyProjection = createKeyFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] valueProjection = createValueFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">keyPrefix</span> <span class="operator">=</span> tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Integer</span> <span class="variable">parallelism</span> <span class="operator">=</span> tableOptions.getOptional(SCAN_PARALLELISM).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> createKafkaTableSource(</span><br><span class="line">            physicalDataType,</span><br><span class="line">            keyDecodingFormat.orElse(<span class="literal">null</span>),</span><br><span class="line">            valueDecodingFormat,</span><br><span class="line">            keyProjection,</span><br><span class="line">            valueProjection,</span><br><span class="line">            keyPrefix,</span><br><span class="line">            getTopics(tableOptions),</span><br><span class="line">            getTopicPattern(tableOptions),</span><br><span class="line">            properties,</span><br><span class="line">            startupOptions.startupMode,</span><br><span class="line">            startupOptions.specificOffsets,</span><br><span class="line">            startupOptions.startupTimestampMillis,</span><br><span class="line">            boundedOptions.boundedMode,</span><br><span class="line">            boundedOptions.specificOffsets,</span><br><span class="line">            boundedOptions.boundedTimestampMillis,</span><br><span class="line">            context.getObjectIdentifier().asSummaryString(),</span><br><span class="line">            parallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个方法中，首先要获取到 key 和 value 的解码格式。接着是各种参数校验和获取必要的属性。最后创建 KafkaDynamicSource 实例。</p>
<p>获取解码格式需要用到 DeserializationFormatFactory 工厂，DeserializationFormatFactory 有多个实现类，对应了多种格式的反序列化方法。</p>
<p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768297428/Blog/flink/22/DeserializationFormatFactory.png" alt="DeserializationFormatFactory"></p>
<p>我们来看比较常见的 Json 格式的工厂 JsonFormatFactory。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; <span class="title function_">createDecodingFormat</span><span class="params">(</span></span><br><span class="line"><span class="params">        DynamicTableFactory.Context context, ReadableConfig formatOptions)</span> &#123;</span><br><span class="line">    FactoryUtil.validateFactoryOptions(<span class="built_in">this</span>, formatOptions);</span><br><span class="line">    JsonFormatOptionsUtil.validateDecodingFormatOptions(formatOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">failOnMissingField</span> <span class="operator">=</span> formatOptions.get(FAIL_ON_MISSING_FIELD);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">ignoreParseErrors</span> <span class="operator">=</span> formatOptions.get(IGNORE_PARSE_ERRORS);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">jsonParserEnabled</span> <span class="operator">=</span> formatOptions.get(DECODE_JSON_PARSER_ENABLED);</span><br><span class="line">    <span class="type">TimestampFormat</span> <span class="variable">timestampOption</span> <span class="operator">=</span> JsonFormatOptionsUtil.getTimestampFormat(formatOptions);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ProjectableDecodingFormat</span>&lt;DeserializationSchema&lt;RowData&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> DeserializationSchema&lt;RowData&gt; <span class="title function_">createRuntimeDecoder</span><span class="params">(</span></span><br><span class="line"><span class="params">                DynamicTableSource.Context context,</span></span><br><span class="line"><span class="params">                DataType physicalDataType,</span></span><br><span class="line"><span class="params">                <span class="type">int</span>[][] projections)</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="type">DataType</span> <span class="variable">producedDataType</span> <span class="operator">=</span></span><br><span class="line">                    Projection.of(projections).project(physicalDataType);</span><br><span class="line">            <span class="keyword">final</span> <span class="type">RowType</span> <span class="variable">rowType</span> <span class="operator">=</span> (RowType) producedDataType.getLogicalType();</span><br><span class="line">            <span class="keyword">final</span> TypeInformation&lt;RowData&gt; rowDataTypeInfo =</span><br><span class="line">                    context.createTypeInformation(producedDataType);</span><br><span class="line">            <span class="keyword">if</span> (jsonParserEnabled) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">JsonParserRowDataDeserializationSchema</span>(</span><br><span class="line">                        rowType,</span><br><span class="line">                        rowDataTypeInfo,</span><br><span class="line">                        failOnMissingField,</span><br><span class="line">                        ignoreParseErrors,</span><br><span class="line">                        timestampOption,</span><br><span class="line">                        toProjectedNames(</span><br><span class="line">                                (RowType) physicalDataType.getLogicalType(), projections));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">JsonRowDataDeserializationSchema</span>(</span><br><span class="line">                        rowType,</span><br><span class="line">                        rowDataTypeInfo,</span><br><span class="line">                        failOnMissingField,</span><br><span class="line">                        ignoreParseErrors,</span><br><span class="line">                        timestampOption);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> ChangelogMode <span class="title function_">getChangelogMode</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> ChangelogMode.insertOnly();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">supportsNestedProjection</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> jsonParserEnabled;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在创建解码格式时，最重要的是创建运行时的解码器，也就是 DeserializationSchema，在 JsonFormatFactory 中，有 JsonParserRowDataDeserializationSchema 和 JsonRowDataDeserializationSchema 两种实现，分别是用于将 JsonParser 和 JsonNode 转换成为 RowData，具体的逻辑都在 createNotNullConverter 方法中。</p>
<p>了解完解码格式后，我们把视角拉回到 KafkaDynamicSource，它实现了三个接口 ScanTableSource、SupportsReadingMetadata、SupportsWatermarkPushDown。分别用于消费数据，读取元数据和生成水印。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ScanRuntimeProvider <span class="title function_">getScanRuntimeProvider</span><span class="params">(ScanContext context)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> DeserializationSchema&lt;RowData&gt; keyDeserialization =</span><br><span class="line">            createDeserialization(context, keyDecodingFormat, keyProjection, keyPrefix);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> DeserializationSchema&lt;RowData&gt; valueDeserialization =</span><br><span class="line">            createDeserialization(context, valueDecodingFormat, valueProjection, <span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> TypeInformation&lt;RowData&gt; producedTypeInfo =</span><br><span class="line">            context.createTypeInformation(producedDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> KafkaSource&lt;RowData&gt; kafkaSource =</span><br><span class="line">            createKafkaSource(keyDeserialization, valueDeserialization, producedTypeInfo);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">DataStreamScanProvider</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> DataStream&lt;RowData&gt; <span class="title function_">produceDataStream</span><span class="params">(</span></span><br><span class="line"><span class="params">                ProviderContext providerContext, StreamExecutionEnvironment execEnv)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (watermarkStrategy == <span class="literal">null</span>) &#123;</span><br><span class="line">                watermarkStrategy = WatermarkStrategy.noWatermarks();</span><br><span class="line">            &#125;</span><br><span class="line">            DataStreamSource&lt;RowData&gt; sourceStream =</span><br><span class="line">                    execEnv.fromSource(</span><br><span class="line">                            kafkaSource, watermarkStrategy, <span class="string">&quot;KafkaSource-&quot;</span> + tableIdentifier);</span><br><span class="line">            providerContext.generateUid(KAFKA_TRANSFORMATION).ifPresent(sourceStream::uid);</span><br><span class="line">            <span class="keyword">return</span> sourceStream;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isBounded</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> kafkaSource.getBoundedness() == Boundedness.BOUNDED;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Optional&lt;Integer&gt; <span class="title function_">getParallelism</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> Optional.ofNullable(parallelism);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 ScanRuntimeProvider 的逻辑中，先获取到反序列化器，也就是刚刚我们提到的 DeserializationSchema。</p>
<p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768318497/Blog/flink/22/KafkaSource.png" alt="KafkaSource"></p>
<p>然后开始创建 KafkaSource 实例，它是 Source 的实现类，也就是执行引擎层了，这个过程会依次创建图中这些类。</p>
<p>KafkaSource 中主要是创建 KafkaSourceReader 和 KafkaSourceEnumerator，KafkaSourceEnumerator 是负责和分片相关的逻辑，包括分片分配和分片发现等。</p>
<p>KafkaSourceReader 中主要是和 State 相关的逻辑，包括触发快照和完成 Checkpoint 通知的方法。当做 Snapshot 时，会记录活跃 split 的 offset，同时将 split 作为状态提交。当 Checkpoint 完成时，会调用 <code>KafkaSourceFetcherManager.commitOffsets</code> 提交 offset。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;KafkaPartitionSplit&gt; <span class="title function_">snapshotState</span><span class="params">(<span class="type">long</span> checkpointId)</span> &#123;</span><br><span class="line">    List&lt;KafkaPartitionSplit&gt; splits = <span class="built_in">super</span>.snapshotState(checkpointId);</span><br><span class="line">    <span class="keyword">if</span> (!commitOffsetsOnCheckpoint) &#123;</span><br><span class="line">        <span class="keyword">return</span> splits;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (splits.isEmpty() &amp;&amp; offsetsOfFinishedSplits.isEmpty()) &#123;</span><br><span class="line">        offsetsToCommit.put(checkpointId, Collections.emptyMap());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        Map&lt;TopicPartition, OffsetAndMetadata&gt; offsetsMap =</span><br><span class="line">                offsetsToCommit.computeIfAbsent(checkpointId, id -&gt; <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;());</span><br><span class="line">        <span class="comment">// Put the offsets of the active splits.</span></span><br><span class="line">        <span class="keyword">for</span> (KafkaPartitionSplit split : splits) &#123;</span><br><span class="line">            <span class="comment">// If the checkpoint is triggered before the partition starting offsets</span></span><br><span class="line">            <span class="comment">// is retrieved, do not commit the offsets for those partitions.</span></span><br><span class="line">            <span class="keyword">if</span> (split.getStartingOffset() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                offsetsMap.put(</span><br><span class="line">                        split.getTopicPartition(),</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(split.getStartingOffset()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Put offsets of all the finished splits.</span></span><br><span class="line">        offsetsMap.putAll(offsetsOfFinishedSplits);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">notifyCheckpointComplete</span><span class="params">(<span class="type">long</span> checkpointId)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Committing offsets for checkpoint &#123;&#125;&quot;</span>, checkpointId);</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    ((KafkaSourceFetcherManager) splitFetcherManager)</span><br><span class="line">            .commitOffsets(</span><br><span class="line">                    committedPartitions,</span><br><span class="line">                    (ignored, e) -&gt; &#123;...&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>KafkaSourceFetcherManager 负责管理 fetcher 线程，提交 Offset。</p>
<p>KafkaPartitionSplitReader 的 fetch 方法用来消费 Kafka 的数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> RecordsWithSplitIds&lt;ConsumerRecord&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt;&gt; fetch() <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    ConsumerRecords&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt; consumerRecords;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        consumerRecords = consumer.poll(Duration.ofMillis(POLL_TIMEOUT));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (WakeupException | IllegalStateException e) &#123;</span><br><span class="line">        <span class="comment">// IllegalStateException will be thrown if the consumer is not assigned any partitions.</span></span><br><span class="line">        <span class="comment">// This happens if all assigned partitions are invalid or empty (starting offset &gt;=</span></span><br><span class="line">        <span class="comment">// stopping offset). We just mark empty partitions as finished and return an empty</span></span><br><span class="line">        <span class="comment">// record container, and this consumer will be closed by SplitFetcherManager.</span></span><br><span class="line">        <span class="type">KafkaPartitionSplitRecords</span> <span class="variable">recordsBySplits</span> <span class="operator">=</span></span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">KafkaPartitionSplitRecords</span>(</span><br><span class="line">                        ConsumerRecords.empty(), kafkaSourceReaderMetrics);</span><br><span class="line">        markEmptySplitsAsFinished(recordsBySplits);</span><br><span class="line">        <span class="keyword">return</span> recordsBySplits;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">KafkaPartitionSplitRecords</span> <span class="variable">recordsBySplits</span> <span class="operator">=</span></span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">KafkaPartitionSplitRecords</span>(consumerRecords, kafkaSourceReaderMetrics);</span><br><span class="line">    List&lt;TopicPartition&gt; finishedPartitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : consumer.assignment()) &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">stoppingOffset</span> <span class="operator">=</span> getStoppingOffset(tp);</span><br><span class="line">        <span class="type">long</span> <span class="variable">consumerPosition</span> <span class="operator">=</span> getConsumerPosition(tp, <span class="string">&quot;retrieving consumer position&quot;</span>);</span><br><span class="line">        <span class="comment">// Stop fetching when the consumer&#x27;s position reaches the stoppingOffset.</span></span><br><span class="line">        <span class="comment">// Control messages may follow the last record; therefore, using the last record&#x27;s</span></span><br><span class="line">        <span class="comment">// offset as a stopping condition could result in indefinite blocking.</span></span><br><span class="line">        <span class="keyword">if</span> (consumerPosition &gt;= stoppingOffset) &#123;</span><br><span class="line">            LOG.debug(</span><br><span class="line">                    <span class="string">&quot;Position of &#123;&#125;: &#123;&#125;, has reached stopping offset: &#123;&#125;&quot;</span>,</span><br><span class="line">                    tp,</span><br><span class="line">                    consumerPosition,</span><br><span class="line">                    stoppingOffset);</span><br><span class="line">            recordsBySplits.setPartitionStoppingOffset(tp, stoppingOffset);</span><br><span class="line">            finishSplitAtRecord(</span><br><span class="line">                    tp, stoppingOffset, consumerPosition, finishedPartitions, recordsBySplits);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Only track non-empty partition&#x27;s record lag if it never appears before</span></span><br><span class="line">    consumerRecords</span><br><span class="line">            .partitions()</span><br><span class="line">            .forEach(</span><br><span class="line">                    trackTp -&gt; &#123;</span><br><span class="line">                        kafkaSourceReaderMetrics.maybeAddRecordsLagMetric(consumer, trackTp);</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line">    markEmptySplitsAsFinished(recordsBySplits);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unassign the partitions that has finished.</span></span><br><span class="line">    <span class="keyword">if</span> (!finishedPartitions.isEmpty()) &#123;</span><br><span class="line">        finishedPartitions.forEach(kafkaSourceReaderMetrics::removeRecordsLagMetric);</span><br><span class="line">        unassignPartitions(finishedPartitions);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Update numBytesIn</span></span><br><span class="line">    kafkaSourceReaderMetrics.updateNumBytesInCounter();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> recordsBySplits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至此，Source 端相关的源码我们就梳理完了。接下来我们再看 Sink 端的代码。</p>
<h4 id="Sink-端"><a href="#Sink-端" class="headerlink" title="Sink 端"></a>Sink 端</h4><p>我们从工厂类中的 createDynamicTableSink 方法开始。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DynamicTableSink <span class="title function_">createDynamicTableSink</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">TableFactoryHelper</span> <span class="variable">helper</span> <span class="operator">=</span></span><br><span class="line">            FactoryUtil.createTableFactoryHelper(</span><br><span class="line">                    <span class="built_in">this</span>, autoCompleteSchemaRegistrySubject(context));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Optional&lt;EncodingFormat&lt;SerializationSchema&lt;RowData&gt;&gt;&gt; keyEncodingFormat =</span><br><span class="line">            getKeyEncodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> EncodingFormat&lt;SerializationSchema&lt;RowData&gt;&gt; valueEncodingFormat =</span><br><span class="line">            getValueEncodingFormat(helper);</span><br><span class="line"></span><br><span class="line">    helper.validateExcept(PROPERTIES_PREFIX);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">ReadableConfig</span> <span class="variable">tableOptions</span> <span class="operator">=</span> helper.getOptions();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">DeliveryGuarantee</span> <span class="variable">deliveryGuarantee</span> <span class="operator">=</span> validateDeprecatedSemantic(tableOptions);</span><br><span class="line">    validateTableSinkOptions(tableOptions);</span><br><span class="line"></span><br><span class="line">    KafkaConnectorOptionsUtil.validateDeliveryGuarantee(tableOptions);</span><br><span class="line"></span><br><span class="line">    validatePKConstraints(</span><br><span class="line">            context.getObjectIdentifier(),</span><br><span class="line">            context.getPrimaryKeyIndexes(),</span><br><span class="line">            context.getCatalogTable().getOptions(),</span><br><span class="line">            valueEncodingFormat);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">DataType</span> <span class="variable">physicalDataType</span> <span class="operator">=</span> context.getPhysicalRowDataType();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] keyProjection = createKeyFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span>[] valueProjection = createValueFormatProjection(tableOptions, physicalDataType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">keyPrefix</span> <span class="operator">=</span> tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="type">Integer</span> <span class="variable">parallelism</span> <span class="operator">=</span> tableOptions.getOptional(SINK_PARALLELISM).orElse(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> createKafkaTableSink(</span><br><span class="line">            physicalDataType,</span><br><span class="line">            keyEncodingFormat.orElse(<span class="literal">null</span>),</span><br><span class="line">            valueEncodingFormat,</span><br><span class="line">            keyProjection,</span><br><span class="line">            valueProjection,</span><br><span class="line">            keyPrefix,</span><br><span class="line">            getTopics(tableOptions),</span><br><span class="line">            getTopicPattern(tableOptions),</span><br><span class="line">            getKafkaProperties(context.getCatalogTable().getOptions()),</span><br><span class="line">            getFlinkKafkaPartitioner(tableOptions, context.getClassLoader()).orElse(<span class="literal">null</span>),</span><br><span class="line">            deliveryGuarantee,</span><br><span class="line">            parallelism,</span><br><span class="line">            tableOptions.get(TRANSACTIONAL_ID_PREFIX),</span><br><span class="line">            tableOptions.get(TRANSACTION_NAMING_STRATEGY));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和 Source 的流程很相似，这里首先是获取 key 和 value 的编码格式，然后做了很多校验，最后是创建 KafkaDynamicSink 实例。</p>
<p>获取编码格式用到的工厂类是 SerializationFormatFactory，我们前面介绍的 JsonFormatFactory 也实现了 SerializationFormatFactory，因此它既提供了解码格式，又提供了编码格式。编码格式用到的编码器是 JsonRowDataSerializationSchema，通过 RowDataToJsonConverters 将 RowData 转换成 JsonNode。</p>
<p>在 KafkaDynamicSink 的 getSinkRuntimeProvider 方法中，主要就是创建 KafkaSink 实例。</p>
<p><img src="https://res.cloudinary.com/dxydgihag/image/upload/v1768363734/Blog/flink/22/KafkaSink.png" alt="KafkaSink"></p>
<p>KafkaSink 类实现了 TwoPhaseCommittingStatefulSink 接口，即支持两阶段提交。它创建了 KafkaWrter 和 KafkaCommiter。</p>
<p>创建 KafkaWriter 时，如果配置的是 ExactlyOnce 模式，则会创建出 ExactlyOnceKafkaWriter，否则创建 KafkaWriter。Writer 真正实现两阶段提交的是 ExactlyOnceKafkaWriter。它在启动时，会调用 <code>producer.beginTransaction</code> 开启一个事务。数据写入时会调用 <code>KafkaWriter.write</code> 方法，此操作会被标记为事务内的操作。当 Sink 收到 Barrier 时，会先调用 flush 方法，将缓冲区的数据都发送到 Kafka Broker，然后调用 prepareCommit 方法预提交。预提交方法中记录 epoch 和 transactionalId 返回给框架层。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Collection&lt;KafkaCommittable&gt; <span class="title function_">prepareCommit</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// only return a KafkaCommittable if the current transaction has been written some data</span></span><br><span class="line">    <span class="keyword">if</span> (currentProducer.hasRecordsInTransaction()) &#123;</span><br><span class="line">        <span class="type">KafkaCommittable</span> <span class="variable">committable</span> <span class="operator">=</span> KafkaCommittable.of(currentProducer);</span><br><span class="line">        LOG.debug(<span class="string">&quot;Prepare &#123;&#125;.&quot;</span>, committable);</span><br><span class="line">        currentProducer.precommitTransaction();</span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(committable);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// otherwise, we recycle the producer (the pool will reset the transaction state)</span></span><br><span class="line">    producerPool.recycle(currentProducer);</span><br><span class="line">    <span class="keyword">return</span> Collections.emptyList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>状态保存时，会将预提交的 transactionalId 存到状态中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;KafkaWriterState&gt; <span class="title function_">snapshotState</span><span class="params">(<span class="type">long</span> checkpointId)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">// recycle committed producers</span></span><br><span class="line">    TransactionFinished finishedTransaction;</span><br><span class="line">    <span class="keyword">while</span> ((finishedTransaction = backchannel.poll()) != <span class="literal">null</span>) &#123;</span><br><span class="line">        producerPool.recycleByTransactionId(</span><br><span class="line">                finishedTransaction.getTransactionId(), finishedTransaction.isSuccess());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// persist the ongoing transactions into the state; these will not be aborted on restart</span></span><br><span class="line">    Collection&lt;CheckpointTransaction&gt; ongoingTransactions =</span><br><span class="line">            producerPool.getOngoingTransactions();</span><br><span class="line">    currentProducer = startTransaction(checkpointId + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> createSnapshots(ongoingTransactions);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> List&lt;KafkaWriterState&gt; <span class="title function_">createSnapshots</span><span class="params">(</span></span><br><span class="line"><span class="params">        Collection&lt;CheckpointTransaction&gt; ongoingTransactions)</span> &#123;</span><br><span class="line">    List&lt;KafkaWriterState&gt; states = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="type">int</span>[] subtaskIds = <span class="built_in">this</span>.ownedSubtaskIds;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> <span class="number">0</span>; index &lt; subtaskIds.length; index++) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">ownedSubtask</span> <span class="operator">=</span> subtaskIds[index];</span><br><span class="line">        states.add(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">KafkaWriterState</span>(</span><br><span class="line">                        transactionalIdPrefix,</span><br><span class="line">                        ownedSubtask,</span><br><span class="line">                        totalNumberOfOwnedSubtasks,</span><br><span class="line">                        transactionNamingStrategy.getOwnership(),</span><br><span class="line">                        <span class="comment">// new transactions are only created with the first owned subtask id</span></span><br><span class="line">                        index == <span class="number">0</span> ? ongoingTransactions : List.of()));</span><br><span class="line">    &#125;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Snapshotting state &#123;&#125;&quot;</span>, states);</span><br><span class="line">    <span class="keyword">return</span> states;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当 Checkpoint 完成时，会调用 <code>KafkaCommitter.commit</code> 方法。在 commit 方法中会调用 <code>producer.commitTransaction</code> 正式提交事务。</p>
<p>FlinkKafkaInternalProducer 是 Flink 内部封装的与 Kafka 生产者的交互类，所有与 Kafka 生产者的交互都通过它执行。</p>
<p>关于 Kafka Connector 的 Sink 端的源码我们就梳理到这里。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后还是总结一下。本文我们先了解了 Flink 中自定义 Source 和 Sink 的流程。按照这个流程，我们梳理了 Kafka Connector 的源码。在 Source 端，Flink Kafka 封装了对消费者 Offset 的提交逻辑。在 Sink 端结合了 Kafka 提供的事务支持实现了两阶段提交的逻辑。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button>
    Donate
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/weixinpay.jpg" alt="Jackey Wang WeChat Pay">
        <span>WeChat Pay</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="Jackey Wang Alipay">
        <span>Alipay</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>Welcome to my other publishing channels</span>

  <div class="social-list">

      <div class="social-item">
          <span class="social-link">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </span>

          <img class="social-item-img" src="/images/wechat-qcode.jpg">
      </div>

      <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Flink/" rel="tag"><i class="fa fa-tag"></i> Flink</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2026/01/09/Flink%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%EF%BC%9AMailbox%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/" rel="prev" title="Flink源码阅读：Mailbox线程模型">
                  <i class="fa fa-angle-left"></i> Flink源码阅读：Mailbox线程模型
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2026/01/16/%E4%BB%8E%E9%9B%B6%E5%AD%A6%E4%B9%A0Kafka%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" rel="next" title="从零学习Kafka：集群架构和基本概念">
                  从零学习Kafka：集群架构和基本概念 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Jackey Wang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">574k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">8:42</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Jackeyzhe" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
